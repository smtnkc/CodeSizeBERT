code,docstring,entry,read,write,exit,cfp,entry_pred,read_pred,write_pred,exit_pred,cfp_pred
"def make_authorization_endpoint(missing_redirect_uri,
  authorization_endpoint_uri,
  authorization_template_name):
  @login_required
  @require_http_methods(['GET', 'POST'])
  def authorization_endpoint(request):
  auth_code_generator = AuthorizationCodeGenerator(missing_redirect_uri)
  try:
  auth_code_generator.validate(request)
  except AuthorizationError as authorization_error:
  return auth_code_generator.make_error_redirect(authorization_error)
  if request.method == 'GET':
  return render(request, authorization_template_name, {
  'form': Form(),
  'client': auth_code_generator.client,
  'scopes': auth_code_generator.valid_scope_objects,
  'form_action': update_parameters(
  authorization_endpoint_uri,
  auth_code_generator.get_request_uri_parameters(as_dict=True)),
  })
  if request.method == 'POST':
  form = Form(request)
  if form.is_valid() and request.POST.get('user_action') == 'Accept':
  return auth_code_generator.make_success_redirect()
  else:
  return auth_code_generator.make_error_redirect()
  return authorization_endpoint","Returns a endpoint that handles OAuth authorization requests. The template described by ``authorization_template_name`` is rendered with a Django ``RequestContext`` with the following variables: * ``form``: a Django ``Form`` that may hold data internal to the ``djoauth2`` application. * ``client``: The :py:class:`djoauth2.models.Client` requesting access to the user's scopes. * ``scopes``: A list of :py:class:`djoauth2.models.Scope`, one for each of the scopes requested by the client. * ``form_action``: The URI to which the form should be submitted -- use this value in the ``action=""""`` attribute on a ``<form>`` element. :param missing_redirect_uri: a string, the URI to which to redirect the user when the request is made by a client without a valid redirect URI. :param authorization_endpoint_uri: a string, the URI of this endpoint. Used by the authorization form so that the form is submitted to this same endpoint. :param authorization_template_name: a string, the name of the template to render when handling authorization requests. :rtype: A view function endpoint.",1,0,1,1,3,1,0,0,1,2
"def getNextService(self, discover):
  manager = self.getManager()
  if manager is not None and not manager:
  self.destroyManager()
  if not manager:
  yadis_url, services = discover(self.url)
  manager = self.createManager(services, yadis_url)
  if manager:
  service = manager.next()
  manager.store(self.session)
  else:
  service = None
  return service",Return the next authentication service for the pair of user_input and session. This function handles fallback. @param discover: a callable that takes a URL and returns a list of services @type discover: str -> [service] @return: the next available service,1,0,1,1,3,1,0,0,1,2
"def users_for_perms(cls, perm_names, db_session=None):
  db_session = get_db_session(db_session)
  query = db_session.query(cls.model)
  query = query.filter(
  cls.models_proxy.User.id == cls.models_proxy.UserGroup.user_id
  )
  query = query.filter(
  cls.models_proxy.UserGroup.group_id
  == cls.models_proxy.GroupPermission.group_id
  )
  query = query.filter(cls.models_proxy.GroupPermission.perm_name.in_(perm_names))
  query2 = db_session.query(cls.model)
  query2 = query2.filter(
  cls.models_proxy.User.id == cls.models_proxy.UserPermission.user_id
  )
  query2 = query2.filter(
  cls.models_proxy.UserPermission.perm_name.in_(perm_names)
  )
  users = query.union(query2).order_by(cls.model.id)
  return users",return users hat have one of given permissions :param perm_names: :param db_session: :return:,1,0,1,1,3,1,0,1,0,2
"def read(self, directory, filename, session, spatial=False,
  spatialReferenceID=4236, replaceParamFile=None, **kwargs):
  path = os.path.join(directory, filename)
  filename_split = filename.split('.')
  name = filename_split[0]
  extension = ''
  if len(filename_split) >= 2:
  extension = filename_split[-1]
  if os.path.isfile(path):
  session.add(self)
  self._read(directory, filename, session, path, name, extension,
  spatial, spatialReferenceID, replaceParamFile, **kwargs)
  self._commit(session, self.COMMIT_ERROR_MESSAGE)
  else:
  session.rollback()
  log.warning('Could not find file named {0}. File not read.'.format(filename))","Generic read file into database method. Args: directory (str): Directory containing the file to be read. filename (str): Name of the file which will be read (e.g.: 'example.prj'). session (:mod:`sqlalchemy.orm.session.Session`): SQLAlchemy session object bound to PostGIS enabled database. spatial (bool, optional): If True, spatially enabled objects will be read in as PostGIS spatial objects. Defaults to False. spatialReferenceID (int, optional): Integer id of spatial reference system for the model. Required if spatial is True. Defaults to srid 4236. replaceParamFile (:class:`gsshapy.orm.ReplaceParamFile`, optional): ReplaceParamFile instance. Use this if the file you are reading contains replacement parameters.",0,1,0,0,1,1,1,0,1,3
"def check_database_connected(db):
  from sqlalchemy.exc import DBAPIError, SQLAlchemyError
  errors = []
  try:
  with db.engine.connect() as connection:
  connection.execute('SELECT 1;')
  except DBAPIError as e:
  msg = 'DB-API error: {!s}'.format(e)
  errors.append(Error(msg, id=health.ERROR_DB_API_EXCEPTION))
  except SQLAlchemyError as e:
  msg = 'Database misconfigured: ""{!s}""'.format(e)
  errors.append(Error(msg, id=health.ERROR_SQLALCHEMY_EXCEPTION))
  return errors","A built-in check to see if connecting to the configured default database backend succeeds. It's automatically added to the list of Dockerflow checks if a :class:`~flask_sqlalchemy.SQLAlchemy` object is passed to the :class:`~dockerflow.flask.app.Dockerflow` class during instantiation, e.g.:: from flask import Flask from flask_sqlalchemy import SQLAlchemy from dockerflow.flask import Dockerflow app = Flask(__name__) app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:////tmp/test.db' db = SQLAlchemy(app) dockerflow = Dockerflow(app, db=db)",0,0,1,1,2,0,0,1,0,1
"def authenticated_session(username, password):
  session = requests.Session()
  session.headers.update(headers())
  response = session.get(url())
  login_path = path(response.text)
  login_url = urljoin(response.url, login_path)
  login_post_data = post_data(response.text, username, password)
  response = session.post(login_url, data=login_post_data)
  if response.headers['connection'] == 'close':
  raise Exception('Authencation failed')
  return session","Given username and password, return an authenticated Yahoo `requests` session that can be used for further scraping requests. Throw an AuthencationError if authentication fails.",2,0,0,1,3,1,0,0,1,2
"def getFilterNames(header, filternames=None):
  _keydict = {
  'ACS': ['FILTER1', 'FILTER2'],
  'WFPC2': ['FILTNAM1', 'FILTNAM2'],
  'STIS': ['OPT_ELEM', 'FILTER'],
  'NICMOS': ['FILTER', 'FILTER2'],
  'WFC3': ['FILTER', 'FILTER2']
  }
  if 'INSTRUME' in header:
  instrument = header['INSTRUME']
  else:
  raise ValueError('Header does not contain INSTRUME keyword.')
  if instrument in _keydict:
  _filtlist = _keydict[instrument]
  else:
  _filtlist = filternames
  _filter_values = []
  for _key in _filtlist:
  if _key in header:
  _val = header[_key]
  else:
  _val = ''
  if _val.strip() != '':
  _filter_values.append(header[_key])
  return ','.join(_filter_values)","Returns a comma-separated string of filter names extracted from the input header (PyFITS header object). This function has been hard-coded to support the following instruments: ACS, WFPC2, STIS This function relies on the 'INSTRUME' keyword to define what instrument has been used to generate the observation/header. The 'filternames' parameter allows the user to provide a list of keyword names for their instrument, in the case their instrument is not supported.",0,0,0,1,1,1,0,0,1,2
"def edit_by_id(
  self,
  id_equip_acesso,
  id_tipo_acesso,
  fqdn,
  user,
  password,
  enable_pass):
  if not is_valid_int_param(id_tipo_acesso):
  raise InvalidParameterError(
  u'Access type id is invalid or not informed.')
  equipamento_acesso_map = dict()
  equipamento_acesso_map['fqdn'] = fqdn
  equipamento_acesso_map['user'] = user
  equipamento_acesso_map['pass'] = password
  equipamento_acesso_map['enable_pass'] = enable_pass
  equipamento_acesso_map['id_tipo_acesso'] = id_tipo_acesso
  equipamento_acesso_map['id_equip_acesso'] = id_equip_acesso
  url = 'equipamentoacesso/edit/'
  code, xml = self.submit(
  {'equipamento_acesso': equipamento_acesso_map}, 'POST', url)
  return self.response(code, xml)","Edit access type, fqdn, user, password and enable_pass of the relationship of equipment and access type. :param id_tipo_acesso: Access type identifier. :param id_equip_acesso: Equipment identifier. :param fqdn: Equipment FQDN. :param user: User. :param password: Password. :param enable_pass: Enable access. :return: None :raise InvalidParameterError: The parameters fqdn, user, password or access type id are invalid or none. :raise EquipamentoAcessoNaoExisteError: Equipment access type relationship doesn't exist. :raise DataBaseError: Networkapi failed to access the database. :raise XMLError: Networkapi failed to generate the XML response.",1,0,0,2,3,1,0,0,1,2
"def getUsersWithinRole(self, rolename, filter=None, maxCount=20):
  uURL = self._url + ""/roles/getUsersWithinRole""
  params = {
  ""f"" : ""json"",
  ""rolename"" : rolename,
  ""maxCount"" : maxCount
  }
  if filter is not None and \
  isinstance(filter, str):
  params['filter'] = filter
  return self._post(url=uURL, param_dict=params,
  securityHandler=self._securityHandler,
  proxy_url=self._proxy_url,
  proxy_port=self._proxy_port)",You can use this operation to conveniently see all the user accounts to whom this role has been assigned. Inputs: rolename - name of the role filter - filter to be applied to the resultant user set maxCount - maximum number of results to return Output: JSON Message as dictionary,2,0,0,1,3,2,0,0,1,3
"def db_dict(c):
  db_d = {}
  c.execute('SELECT * FROM library_spectra')
  db_d['library_spectra'] = [list(row) for row in c]
  c.execute('SELECT * FROM library_spectra_meta')
  db_d['library_spectra_meta'] = [list(row) for row in c]
  c.execute('SELECT * FROM library_spectra_annotation')
  db_d['library_spectra_annotations'] = [list(row) for row in c]
  c.execute('SELECT * FROM library_spectra_source')
  db_d['library_spectra_source'] = [list(row) for row in c]
  c.execute('SELECT * FROM metab_compound')
  db_d['metab_compound'] = [list(row) for row in c]
  return db_d","Get a dictionary of the library spectra from a database Example: >>> from msp2db.db import get_connection >>> conn = get_connection('sqlite', 'library.db') >>> test_db_d = db_dict(conn.cursor()) If using a large database the resulting dictionary will be very large! Args: c (cursor): SQL database connection cursor Returns: A dictionary with the following keys 'library_spectra', 'library_spectra_meta', 'library_spectra_annotations', 'library_spectra_source' and 'metab_compound'. Where corresponding values for each key are list of list containing all the rows in the database.",1,0,1,1,3,0,0,1,0,1
"def get_nodes(self, node_type=""""):
  tmp_nodes = self.diagram_graph.nodes(True)
  if node_type == """":
  return tmp_nodes
  else:
  nodes = []
  for node in tmp_nodes:
  if node[1][consts.Consts.type] == node_type:
  nodes.append(node)
  return nodes","Gets all nodes of requested type. If no type is provided by user, all nodes in BPMN diagram graph are returned. Returns a dictionary, where key is an ID of node, value is a dictionary of all node attributes. :param node_type: string with valid BPMN XML tag name (e.g. 'task', 'sequenceFlow').",0,0,0,1,1,1,0,0,1,2
"def get_notification_channel_id(notify_channel, profile=""telemetry""):
  auth = _auth(profile=profile)
  notification_channel_id = _retrieve_channel_id(notify_channel)
  if not notification_channel_id:
  log.info(""%s channel does not exist, creating."", notify_channel)
  post_url = _get_telemetry_base(profile) + ""/notification-channels""
  data = {
  ""_type"": ""EmailNotificationChannel"",
  ""name"": notify_channel[:notify_channel.find('@')] + 'EscalationPolicy',
  ""email"": notify_channel
  }
  response = requests.post(post_url, data=salt.utils.json.dumps(data), headers=auth)
  if response.status_code == 200:
  log.info(""Successfully created EscalationPolicy %s with EmailNotificationChannel %s"",
  data.get('name'), notify_channel)
  notification_channel_id = response.json().get('_id')
  __context__[""telemetry.channels""][notify_channel] = notification_channel_id
  else:
  raise Exception(""Failed to created notification channel {0}"".format(notify_channel))
  return notification_channel_id","Given an email address, creates a notification-channels if one is not found and also returns the corresponding notification channel id. notify_channel Email escalation policy profile A dict of telemetry config information. CLI Example: salt myminion telemetry.get_notification_channel_id userx@company.com profile=telemetry",2,0,0,2,4,2,0,0,1,3
"def list(self, max=None, **request_parameters):
  check_type(max, int)
  params = dict_from_items_with_values(
  request_parameters,
  max=max,
  )
  items = self._session.get_items(API_ENDPOINT, params=params)
  for item in items:
  yield self._object_factory(OBJECT_TYPE, item)","List teams to which the authenticated user belongs. This method supports Webex Teams's implementation of RFC5988 Web Linking to provide pagination support. It returns a generator container that incrementally yields all teams returned by the query. The generator will automatically request additional 'pages' of responses from Webex as needed until all responses have been returned. The container makes the generator safe for reuse. A new API call will be made, using the same parameters that were specified when the generator was created, every time a new iterator is requested from the container. Args: max(int): Limit the maximum number of items returned from the Webex Teams service per request. **request_parameters: Additional request parameters (provides support for parameters that may be added in the future). Returns: GeneratorContainer: A GeneratorContainer which, when iterated, yields the teams returned by the Webex Teams query. Raises: TypeError: If the parameter types are incorrect. ApiError: If the Webex Teams cloud returns an error.",2,0,0,1,3,2,0,0,1,3
"def detect_log_config(arguments):
  config = arguments['--config']
  if config is None:
  config = detect_config_path()
  if not os.path.exists(config):
  error_exit('Nginx config file not found: %s' % config)
  with open(config) as f:
  config_str = f.read()
  access_logs = dict(get_access_logs(config_str))
  if not access_logs:
  error_exit('Access log file is not provided and ngxtop cannot detect it from your config file (%s).' % config)
  log_formats = dict(get_log_formats(config_str))
  if len(access_logs) == 1:
  log_path, format_name = list(access_logs.items())[0]
  if format_name == 'combined':
  return log_path, LOG_FORMAT_COMBINED
  if format_name not in log_formats:
  error_exit('Incorrect format name set in config for access log file ""%s""' % log_path)
  return log_path, log_formats[format_name]
  print('Multiple access logs detected in configuration:')
  log_path = choose_one(list(access_logs.keys()), 'Select access log file to process: ')
  format_name = access_logs[log_path]
  if format_name not in log_formats:
  error_exit('Incorrect format name set in config for access log file ""%s""' % log_path)
  return log_path, log_formats[format_name]",Detect access log config (path and format) of nginx. Offer user to select if multiple access logs are detected. :return: path and format of detected / selected access log,1,0,0,1,2,1,0,0,1,2
"def resolve(self, authorization: http.Header):
  if authorization is None:
  return None
  scheme, token = authorization.split()
  if scheme.lower() != 'basic':
  return None
  username, password = base64.b64decode(token).decode('utf-8').split(':')
  user = authenticate(username=username, password=password)
  return user","Determine the user associated with a request, using HTTP Basic Authentication.",1,0,1,1,3,1,0,0,1,2
"def _execute_select_commands(self, source, commands):
  rows = {}
  for tbl, command in tqdm(commands, total=len(commands), desc='Executing {0} select queries'.format(source)):
  if tbl not in rows:
  rows[tbl] = []
  rows[tbl].extend(self.fetch(command, commit=True))
  self._commit()
  return rows",Execute select queries for all of the tables from a source database.,0,0,1,0,1,0,0,0,1,1
"def reindex(self, mode, includes=""""):
  url = self._url + ""/indexer/reindex""
  params = {
  ""f"" : ""json"",
  ""mode"" : mode,
  ""includes"" : includes
  }
  return self._get(url=url,
  param_dict=params,
  securityHandler=self._securityHandler,
  proxy_port=self._proxy_port,
  proxy_url=self._proxy_url)","This operation allows you to generate or update the indexes for content; such as users, groups, and items stored in the database (store). During the process of upgrading an earlier version of Portal for ArcGIS, you are required to update the indexes by running this operation. You can check the status of your indexes using the status resource. Input: mode - The mode in which the indexer should run. Values: USER_MODE | GROUP_MODE | RELATION_MODE | SEARCH_MODEL | FULL includes An optional comma separated list of elements to include in the index. This is useful if you want to only index certain items or user accounts.",2,0,0,2,4,1,0,0,1,2
"def _set_account_policy(name, policy):
  cmd = 'pwpolicy -u {0} -setpolicy ""{1}""'.format(name, policy)
  try:
  return salt.utils.mac_utils.execute_return_success(cmd)
  except CommandExecutionError as exc:
  if 'Error: user <{0}> not found'.format(name) in exc.strerror:
  raise CommandExecutionError('User not found: {0}'.format(name))
  raise CommandExecutionError('Unknown error: {0}'.format(exc.strerror))","Set a value in the user accountPolicy. For use by this module only :param str name: The user name :param str policy: The policy to apply :return: True if success, otherwise False :rtype: bool :raises: CommandExecutionError on user not found or any other unknown error",1,0,0,1,2,1,0,0,1,2
"def get_user_config():
  initialconf = normpath(os.path.join(get_share_dir(), ""linkcheckerrc""))
  userconf = normpath(""~/.linkchecker/linkcheckerrc"")
  if os.path.isfile(initialconf) and not os.path.exists(userconf) and \
  not Portable:
  try:
  make_userdir(userconf)
  shutil.copy(initialconf, userconf)
  except Exception as errmsg:
  msg = _(""could not copy initial configuration file %(src)r to %(dst)r: %(errmsg)r"")
  args = dict(src=initialconf, dst=userconf, errmsg=errmsg)
  log.warn(LOG_CHECK, msg % args)
  return userconf","Get the user configuration filename. If the user configuration file does not exist, copy it from the initial configuration file, but only if this is not a portable installation. Returns path to user config file (which might not exist due to copy failures or on portable systems). @return configuration filename @rtype string",1,0,0,0,1,1,0,0,1,2
"def _update(obj, fields=None, save=False, overwrite=False):
  if not fields:
  meta = obj._meta
  fields = [f.name for f in meta.fields if not f.primary_key and hasattr(meta, '_get_' + f.name) and hasattr(meta, '_' + f.name)]
  fields_updated = []
  for field in fields:
  if not overwrite and not getattr(obj, field, None) == None:
  continue
  if hasattr(obj, field):
  setattr(obj, field, getattr(obj, '_' + field, None))
  if getattr(obj, field, None) != None:
  fields_updated += [field]
  if save:
  obj.save()
  return fields_updated","Update/populate any database fields that have `_get`ters to populate them with, regardless of whether they are data fields or related fields",0,1,1,0,2,0,1,1,0,2
"def save_post(self, title, text, user_id, tags, draft=False,
  post_date=None, last_modified_date=None, meta_data=None,
  post_id=None):
  raise NotImplementedError(""This method needs to be implemented by ""
  ""the inheriting class"")","Persist the blog post data. If ``post_id`` is ``None`` or ``post_id`` is invalid, the post must be inserted into the storage. If ``post_id`` is a valid id, then the data must be updated. :param title: The title of the blog post :type title: str :param text: The text of the blog post :type text: str :param user_id: The user identifier :type user_id: str :param tags: A list of tags :type tags: list :param draft: If the post is a draft of if needs to be published. :type draft: bool :param post_date: (Optional) The date the blog was posted (default datetime.datetime.utcnow()) :type post_date: datetime.datetime :param last_modified_date: (Optional) The date when blog was last modified (default datetime.datetime.utcnow()) :type last_modified_date: datetime.datetime :param meta_data: The meta data for the blog post :type meta_data: dict :param post_id: The post identifier. This should be ``None`` for an insert call, and a valid value for update. :type post_id: int :return: The post_id value, in case of a successful insert or update. Return ``None`` if there were errors.",0,1,0,1,2,1,0,0,1,2
"def delete_blob(self, blob_name, client=None, generation=None):
  client = self._require_client(client)
  blob = Blob(blob_name, bucket=self, generation=generation)
  client._connection.api_request(
  method=""DELETE"",
  path=blob.path,
  query_params=blob._query_params,
  _target_object=None,
  )","Deletes a blob from the current bucket. If the blob isn't found (backend 404), raises a :class:`google.cloud.exceptions.NotFound`. For example: .. literalinclude:: snippets.py :start-after: [START delete_blob] :end-before: [END delete_blob] If :attr:`user_project` is set, bills the API request to that project. :type blob_name: str :param blob_name: A blob name to delete. :type client: :class:`~google.cloud.storage.client.Client` or ``NoneType`` :param client: Optional. The client to use. If not passed, falls back to the ``client`` stored on the current bucket. :type generation: long :param generation: Optional. If present, permanently deletes a specific revision of this object. :raises: :class:`google.cloud.exceptions.NotFound` (to suppress the exception, call ``delete_blobs``, passing a no-op ``on_error`` callback, e.g.: .. literalinclude:: snippets.py :start-after: [START delete_blobs] :end-before: [END delete_blobs]",1,0,0,1,2,1,0,0,1,2
"async def login(self, user=DEFAULT_USER, password=DEFAULT_PASSWORD,
  account=DEFAULT_ACCOUNT):
  code, info = await self.command(""USER "" + user, (""230"", ""33x""))
  while code.matches(""33x""):
  if code == ""331"":
  cmd = ""PASS "" + password
  elif code == ""332"":
  cmd = ""ACCT "" + account
  else:
  raise errors.StatusCodeError(""33x"", code, info)
  code, info = await self.command(cmd, (""230"", ""33x""))",:py:func:`asyncio.coroutine` Server authentication. :param user: username :type user: :py:class:`str` :param password: password :type password: :py:class:`str` :param account: account (almost always blank) :type account: :py:class:`str` :raises aioftp.StatusCodeError: if unknown code received,2,0,0,0,2,1,0,0,1,2
"def install_environment(args: argparse.Namespace, backend: StorageBackend, log: logging.Logger):
  model = _load_generic_model(args.input, backend, log)
  if model is None:
  return 1
  packages = [""%s==%s"" % (pkg, ver) for pkg, ver in model.environment[""packages""]]
  cmdline = [sys.executable, ""-m"", ""pip"", ""install""] + args.pip + packages
  log.info("" "".join(cmdline))
  subprocess.check_call(cmdline)
  if args.reproduce:
  for dataset in model.datasets:
  download_http(dataset[0], dataset[1], log)","Install the packages mentioned in the model's metadata. :param args: :param args: :class:`argparse.Namespace` with ""input"", ""reproduce"", ""backend"", \ ""args"", ""username"", ""password"", ""remote_repo"" and ""log_level"". :param backend: Backend which is responsible for working with model files. :param log: Logger supplied by supply_backend :return: None",1,0,0,1,2,1,0,0,1,2
"def save_users(users, path=settings.LOGIN_FILE):
  with open(path, ""w"") as fh:
  for username, data in users.items():
  pass_line = username + "":"" + "":"".join([
  data[""pass_hash""],
  data[""uid""],
  data[""gid""],
  data[""full_name""],
  data[""home""],
  data[""shell""]
  ])
  fh.write(pass_line + ""\n"")","Save dictionary with user data to passwd file (default :attr:`ftp.settings.LOGIN_FILE`). Args: users (dict): dictionary with user data. For details look at dict returned from :func:`load_users`. path (str, default settings.LOGIN_FILE): path of the file, where the data will be stored (default :attr:`ftp.settings.LOGIN_FILE`).",1,0,0,0,1,1,0,0,1,2
"def fetch_liked_datasets(self, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('callback'):
  return self.fetch_liked_datasets_with_http_info(**kwargs)
  else:
  (data) = self.fetch_liked_datasets_with_http_info(**kwargs)
  return data","List liked datasets Fetch datasets that the currently authenticated user likes. This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please define a `callback` function to be invoked when receiving the response. >>> def callback_function(response): >>> pprint(response) >>> >>> thread = api.fetch_liked_datasets(callback=callback_function) :param callback function: The callback function for asynchronous request. (optional) :param str limit: Maximum number of items to include in a page of results. :param str next: Token from previous result page to be used when requesting a subsequent page. :return: PaginatedDatasetResults If the method is called asynchronously, returns the request thread.",0,0,1,1,2,2,0,0,1,3
"def _update_database_helper_table(
  self):
  self.log.debug('starting the ``_update_database_helper_table`` method')
  tableName = self.dbTableName
  sqlQuery = u % locals()
  writequery(
  log=self.log,
  sqlQuery=sqlQuery,
  dbConn=self.cataloguesDbConn,
  )
  self.log.debug(
  'completed the ``_update_database_helper_table`` method')
  return None",*Update the sherlock catalogues database helper table with the time-stamp of when this catlogue was last updated* **Usage:** .. code-block:: python self._update_database_helper_table(),0,1,0,0,1,0,1,1,0,2
"def erasure_profile_exists(service, name):
  validator(value=name, valid_type=six.string_types)
  try:
  check_call(['ceph', '--id', service,
  'osd', 'erasure-code-profile', 'get',
  name])
  return True
  except CalledProcessError:
  return False",Check to see if an Erasure code profile already exists. :param service: six.string_types. The Ceph user name to run the command under :param name: six.string_types :return: int or None,1,0,0,0,1,1,0,0,1,2
"def mask_args_value(data, mask):
  if not mask:
  return data
  out = []
  for line in data.split(os.linesep):
  if fnmatch.fnmatch(line.strip(), mask) and ':' in line:
  key, value = line.split(':', 1)
  out.append('{}: {}'.format(salt.utils.stringutils.to_unicode(key.strip()), '** hidden **'))
  else:
  out.append(line)
  return '\n'.join(out)","Mask a line in the data, which matches ""mask"". This can be used for cases where values in your roster file may contain sensitive data such as IP addresses, passwords, user names, etc. Note that this works only when ``data`` is a single string (i.e. when the data in the roster is formatted as ``key: value`` pairs in YAML syntax). :param data: String data, already rendered. :param mask: Mask that matches a single line :return:",1,0,0,1,2,1,0,0,1,2
"def fetch(cls, channel, start, end, host=None, port=None, verbose=False,
  connection=None, verify=False, pad=None, allow_tape=None,
  scaled=None, type=None, dtype=None):
  return cls.DictClass.fetch(
  [channel], start, end, host=host, port=port, verbose=verbose,
  connection=connection, verify=verify, pad=pad, scaled=scaled,
  allow_tape=allow_tape, type=type, dtype=dtype)[str(channel)]","Fetch data from NDS Parameters ---------- channel : `str`, `~gwpy.detector.Channel` the data channel for which to query start : `~gwpy.time.LIGOTimeGPS`, `float`, `str` GPS start time of required data, any input parseable by `~gwpy.time.to_gps` is fine end : `~gwpy.time.LIGOTimeGPS`, `float`, `str` GPS end time of required data, any input parseable by `~gwpy.time.to_gps` is fine host : `str`, optional URL of NDS server to use, if blank will try any server (in a relatively sensible order) to get the data port : `int`, optional port number for NDS server query, must be given with `host` verify : `bool`, optional, default: `False` check channels exist in database before asking for data scaled : `bool`, optional apply slope and bias calibration to ADC data, for non-ADC data this option has no effect connection : `nds2.connection`, optional open NDS connection to use verbose : `bool`, optional print verbose output about NDS progress, useful for debugging; if ``verbose`` is specified as a string, this defines the prefix for the progress meter type : `int`, optional NDS2 channel type integer dtype : `type`, `numpy.dtype`, `str`, optional identifier for desired output data type",1,0,0,1,2,1,0,1,1,3
"def capture(rect=None, filepath='', prompt=True, hideWindow=None):
  widget = XSnapshotWidget(QApplication.desktop())
  widget.setRegion(rect)
  widget.setHideWindow(hideWindow)
  widget.setFilepath(filepath)
  widget.move(1, 1)
  widget.resize(QApplication.desktop().size())
  if prompt or not filepath:
  widget.show()
  else:
  widget.save()","Prompts the user to capture the screen. :param rect | <QRect> filepath | <str> prompt | <bool> :return (<str> filepath, <bool> accepted)",1,0,0,1,2,1,0,0,1,2
"def config(name,
  config,
  write=True):
  _build_config_tree(name, config)
  configs = _render_configuration()
  if __opts__.get('test', False):
  comment = 'State syslog_ng will write \'{0}\' into {1}'.format(
  configs,
  __SYSLOG_NG_CONFIG_FILE
  )
  return _format_state_result(name, result=None, comment=comment)
  succ = write
  if write:
  succ = _write_config(config=configs)
  return _format_state_result(name, result=succ,
  changes={'new': configs, 'old': ''})","Builds syslog-ng configuration. This function is intended to be used from the state module, users should not use it directly! name : the id of the Salt document or it is the format of <statement name>.id config : the parsed YAML code write : if True, it writes the config into the configuration file, otherwise just returns it CLI Example: .. code-block:: bash salt '*' syslog_ng.config name='s_local' config=""[{'tcp':[{'ip':'127.0.0.1'},{'port':1233}]}]""",0,0,0,1,1,1,0,0,1,2
"def user_read_message(self, id, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.user_read_message_with_http_info(id, **kwargs)
  else:
  (data) = self.user_read_message_with_http_info(id, **kwargs)
  return data","Mark a specific message as read # noqa: E501 # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.user_read_message(id, async_req=True) >>> result = thread.get() :param async_req bool :param str id: (required) :return: ResponseContainerMessage If the method is called asynchronously, returns the request thread.",1,0,1,1,3,1,0,0,1,2
"def check_firmware_present(self, firmware_type, version):
  if not isinstance(firmware_type, FirmwareType):
  raise TypeError(""firmware_type can only be an instance of type FirmwareType"")
  if not isinstance(version, basestring):
  raise TypeError(""version can only be an instance of type basestring"")
  (result, url, file_p) = self._call(""checkFirmwarePresent"",
  in_p=[firmware_type, version])
  return (result, url, file_p)","Check if this VirtualBox installation has a firmware of the given type available, either system-wide or per-user. Optionally, this may return a hint where this firmware can be downloaded from. in firmware_type of type :class:`FirmwareType` Type of firmware to check. in version of type str Expected version number, usually empty string (presently ignored). out url of type str Suggested URL to download this firmware from. out file_p of type str Filename of firmware, only valid if result == TRUE. return result of type bool If firmware of this type and version is available.",2,0,0,1,3,1,0,0,1,2
"def user(self, username):
  for user in self.users():
  if username.lower() == user.title.lower():
  return user
  elif (user.username and user.email and user.id and username.lower() in
  (user.username.lower(), user.email.lower(), str(user.id))):
  return user
  raise NotFound('Unable to find user %s' % username)","Returns the :class:`~plexapi.myplex.MyPlexUser` that matches the email or username specified. Parameters: username (str): Username, email or id of the user to return.",2,0,1,1,4,1,0,1,1,3
"def choose(self, prompt, items, default=None):
  if default is not None and (default >= len(items) or default < 0):
  raise IndexError
  prompt = prompt if prompt is not None else ""Choose from following:""
  self.show(prompt + '\n')
  self.show(""\n"".join(number(items)))
  prompt = ""Enter number of chosen item""
  prompt += "" [{0}]: "".format(default) if default is not None else ': '
  return items[self.input(
  curry(filter_int, default=default, start=0, stop=len(items)),
  prompt
  )]","Prompts the user to choose one item from a list. The default, if provided, is an index; the item of that index will be returned.",1,0,0,1,2,1,0,0,1,2
"def _store(self, messages, response, *args, **kwargs):
  contrib_messages = []
  if self.user.is_authenticated():
  if not messages:
  self.backend.inbox_purge(self.user)
  else:
  for m in messages:
  try:
  self.backend.inbox_store([self.user], m)
  except MessageTypeNotSupported:
  contrib_messages.append(m)
  super(StorageMixin, self)._store(contrib_messages, response, *args, **kwargs)","persistent messages are already in the database inside the 'archive', so we can say they're already ""stored"". Here we put them in the inbox, or remove from the inbox in case the messages were iterated. messages contains only new msgs if self.used==True else contains both new and unread messages",0,1,0,0,1,1,0,0,1,2
"def update_link_rewrite(self, old_rel, old_text, new_text, single_link=False):
  links = self.metadata.xpath('./links/link[@rel=""{}"" and text()=""{}""]'.format(old_rel, old_text))
  if len(links) < 1:
  log.warning('No links with link/[@rel=""{}""and text()=""{}""]'.format(str(old_rel), str(old_text)))
  return False
  for link in links:
  link.text = new_text
  if single_link:
  break
  return True","Rewrite the text() value of a link based on the link/@rel and link/text() value. This is similar to update_link_rel_based but users link/@rel AND link/text() values to determine which links have their link/@text() values updated. :param old_rel: The link/@rel value used to select link nodes to update. :param old_text: The link/text() value used to select link nodes to update. :param new_text: The new link/text() value to set on link nodes. :param single_link: Determine if only the first, or multiple, linkes are modified. :return: True, unless there are no links with link/[@rel='old_rel' and text()='old_text']",1,0,0,1,2,1,0,0,1,2
"def flash(message, category='message'):
  flashes = session.get('_flashes', [])
  flashes.append((category, message))
  session['_flashes'] = flashes
  message_flashed.send(current_app._get_current_object(),
  message=message, category=category)","Flashes a message to the next request. In order to remove the flashed message from the session and to display it to the user, the template has to call :func:`get_flashed_messages`. .. versionchanged:: 0.3 `category` parameter added. :param message: the message to be flashed. :param category: the category for the message. The following values are recommended: ``'message'`` for any kind of message, ``'error'`` for errors, ``'info'`` for information messages and ``'warning'`` for warnings. However any kind of string can be used as category.",1,0,0,1,2,1,0,0,1,2
"def get_token(self, user_id):
  if not self.jwt_implementation:
  return self.default_token_implementation(user_id)
  try:
  implementation = import_string(self.jwt_implementation)
  except ImportError:
  msg = 'Failed to import custom JWT implementation. '
  msg += 'Check that configured module exists [{}]'
  raise x.ConfigurationException(msg.format(self.jwt_implementation))
  return implementation(user_id)","Get user token Checks if a custom token implementation is registered and uses that. Otherwise falls back to default token implementation. Returns a string token on success. :param user_id: int, user id :return: str",1,0,0,1,2,1,0,0,1,2
"def _notify_create_process(self, event):
  dwProcessId = event.get_pid()
  dwThreadId = event.get_tid()
  hProcess = event.get_process_handle()
  if dwProcessId not in self.__processDict:
  aProcess = Process(dwProcessId, hProcess)
  self._add_process(aProcess)
  aProcess.fileName = event.get_filename()
  else:
  aProcess = self.get_process(dwProcessId)
  if not aProcess.fileName:
  fileName = event.get_filename()
  if fileName:
  aProcess.fileName = fileName
  return aProcess._notify_create_process(event)","Notify the creation of a new process. This is done automatically by the L{Debug} class, you shouldn't need to call it yourself. @type event: L{CreateProcessEvent} @param event: Create process event. @rtype: bool @return: C{True} to call the user-defined handle, C{False} otherwise.",0,0,0,1,1,0,1,0,0,1
"async def password_dialog(key, title, message, options):
  with PasswordDialog.create(key, title, message, options) as dialog:
  response = await dialog
  if response == Gtk.ResponseType.OK:
  return PasswordResult(dialog.get_text(),
  dialog.use_cache.get_active())
  return None",Show a Gtk password dialog. :returns: the password or ``None`` if the user aborted the operation :raises RuntimeError: if Gtk can not be properly initialized,1,0,0,1,2,1,0,0,1,2
"def create_pipeline(self, name, description, **kwargs):
 if not (name and description):
 return requests.codes.bad_request, None
 kwargs.update({'name':name, 'description':description})
 new_pl = StreakPipeline(**kwargs)
 uri = '/'.join([
 self.api_uri,
 self.pipelines_suffix
 ])
 code, r_data = self._req('put', uri, new_pl.to_dict())
 return code, r_data","Creates a pipeline with the provided attributes. Args: name required name string kwargs {name, description, orgWide, aclEntries} user specifiable ones only return (status code, pipeline_dict) (as created)",1,0,0,2,3,1,0,0,1,2
"def pause(info='Press any key to continue ...', err=False):
  if not isatty(sys.stdin) or not isatty(sys.stdout):
  return
  try:
  if info:
  echo(info, nl=False, err=err)
  try:
  getchar()
  except (KeyboardInterrupt, EOFError):
  pass
  finally:
  if info:
  echo(err=err)","This command stops execution and waits for the user to press any key to continue. This is similar to the Windows batch ""pause"" command. If the program is not run through a terminal, this command will instead do nothing. .. versionadded:: 2.0 .. versionadded:: 4.0 Added the `err` parameter. :param info: the info string to print before pausing. :param err: if set to message goes to ``stderr`` instead of ``stdout``, the same as with echo.",1,0,0,1,2,1,0,0,1,2
"def list_loader(*decorator_args, model):
  def wrapped(fn):
  @wraps(fn)
  def decorated(*args, **kwargs):
  return fn(model.query.all())
  return decorated
  if decorator_args and callable(decorator_args[0]):
  return wrapped(decorator_args[0])
  return wrapped",Decorator to automatically query the database for all records of a model. :param model: The model class to query,1,0,1,1,3,1,0,1,1,3
"def shell_safe_json_parse(json_or_dict_string, preserve_order=False):
  try:
  if not preserve_order:
  return json.loads(json_or_dict_string)
  from collections import OrderedDict
  return json.loads(json_or_dict_string, object_pairs_hook=OrderedDict)
  except ValueError:
  import ast
  return ast.literal_eval(json_or_dict_string)","Allows the passing of JSON or Python dictionary strings. This is needed because certain JSON strings in CMD shell are not received in main's argv. This allows the user to specify the alternative notation, which does not have this problem (but is technically not JSON).",0,0,0,1,1,0,0,0,1,1
"def serve_forever(self, banner=None):
  if hasattr(readline, ""read_history_file""):
  try:
  readline.read_history_file(self.histfile)
  except IOError:
  pass
  atexit.register(self._save_history)
  super(Shell, self).serve_forever(banner)",Interact with the user. :param banner: (optional) the banner to print before the first interaction. Defaults to ``None``.,1,0,0,1,2,1,0,0,1,2
"def set_password(name, password):
  cmd = ""dscl . -passwd /Users/{0} '{1}'"".format(name, password)
  try:
  salt.utils.mac_utils.execute_return_success(cmd)
  except CommandExecutionError as exc:
  if 'eDSUnknownNodeName' in exc.strerror:
  raise CommandExecutionError('User not found: {0}'.format(name))
  raise CommandExecutionError('Unknown error: {0}'.format(exc.strerror))
  return True","Set the password for a named user (insecure, the password will be in the process list while the command is running) :param str name: The name of the local user, which is assumed to be in the local directory service :param str password: The plaintext password to set :return: True if successful, otherwise False :rtype: bool :raises: CommandExecutionError on user not found or any other unknown error CLI Example: .. code-block:: bash salt '*' mac_shadow.set_password macuser macpassword",1,0,0,1,2,1,0,0,1,2
"def keypair_field_data(request, include_empty_option=False):
  keypair_list = []
  try:
  keypairs = api.nova.keypair_list(request)
  keypair_list = [(kp.name, kp.name) for kp in keypairs]
  except Exception:
  exceptions.handle(request, _('Unable to retrieve key pairs.'))
  if not keypair_list:
  if include_empty_option:
  return [("""", _(""No key pairs available"")), ]
  return []
  if include_empty_option:
  return [("""", _(""Select a key pair"")), ] + keypair_list
  return keypair_list","Returns a list of tuples of all keypairs. Generates a list of keypairs available to the user (request). And returns a list of (id, name) tuples. :param request: django http request object :param include_empty_option: flag to include a empty tuple in the front of the list :return: list of (id, name) tuples",2,0,0,1,3,1,0,0,1,2
"def request(self, msg, timeout=30):
  rpc = RawRPC(session = self.session,
  device_handler = self._device_handler,
  timeout = timeout,
  raise_mode = operations.rpc.RaiseMode.NONE)
  m = re.search(r'message-id=""([A-Za-z0-9_\-:
  if m:
  rpc._id = m.group(1)
  rpc._listener.register(rpc._id, rpc)
  logger.debug('Found message-id=""%s"" in your rpc, which is good.', rpc._id)
  else:
  logger.warning('Cannot find message-id in your rpc. You may '
  'expect an exception when receiving rpc-reply '
  'due to missing message-id.')
  return rpc._request(msg).xml","request High-level api: sends message through NetConf session and returns with a reply. Exception is thrown out either the reply is in wrong format or timout. Users can modify timeout value (in seconds) by passing parameter timeout. Users may want to set a larger timeout when making a large query. Parameters ---------- msg : `str` Any message need to be sent out in XML format. The message can be in wrong format if it is a negative test case. Because ncclient tracks same message-id in both rpc and rpc-reply, missing message-id in your rpc may cause exception when receiving rpc-reply. Most other wrong format rpc's can be sent without exception. timeout : `int`, optional An optional keyed argument to set timeout value in seconds. Its default value is 30 seconds. Returns ------- str The reply from the device in string. If something goes wrong, an exception will be raised. Raises ------ Exception If NetConf is not connected, or there is a timeout when receiving reply. Code Example:: >>> from pyats.topology import loader >>> testbed = loader.load('/users/xxx/xxx/asr_20_22.yaml') >>> device = testbed.devices['asr22'] >>> device.connect(alias='nc', via='netconf') >>> netconf_request = """""" ... <rpc message-id=""101"" ... xmlns=""urn:ietf:params:xml:ns:netconf:base:1.0""> ... <get> ... <filter> ... <native xmlns=""http://cisco.com/ns/yang/ned/ios""> ... <version> ... </version> ... </native> ... </filter> ... </get> ... </rpc> ... """""" >>> reply = device.nc.request(netconf_request) >>> Expected Results:: >>> print(reply) <?xml version=""1.0"" encoding=""UTF-8""?> <rpc-reply xmlns=""urn:ietf:params:xml:ns:netconf:base:1.0"" message-id=""101""><data> <native xmlns=""http://cisco.com/ns/yang/ned/ios""> <version>16.3</version></native></data></rpc-reply> >>>",1,0,0,2,3,1,0,0,1,2
"def parse_model(self, model):
  if model and ':' in model:
  controller_name, model_name = model.split(':')
  else:
  controller_name = self.current_controller()
  model_name = model
  if not controller_name:
  controller_name = self.current_controller()
  if not model_name:
  model_name = self.current_model(controller_name, model_only=True)
  if not model_name:
  raise NoModelException('no current model')
  if '/' not in model_name:
  accounts = self.accounts().get(controller_name)
  if accounts is None:
  raise JujuError('No account found for controller {} '.format(controller_name))
  username = accounts.get('user')
  if username is None:
  raise JujuError('No username found for controller {}'.format(controller_name))
  model_name = username + ""/"" + model_name
  return controller_name, model_name","Split the given model_name into controller and model parts. If the controller part is empty, the current controller will be used. If the model part is empty, the current model will be used for the controller. The returned model name will always be qualified with a username. :param model str: The model name to parse. :return (str, str): The controller and model names.",1,0,0,0,1,1,0,0,1,2
"def gene_family(self,
  family_identifier=None,
  family_name=None,
  hgnc_symbol=None,
  hgnc_identifier=None,
  limit=None,
  as_df=False):
  q = self.session.query(models.GeneFamily)
  model_queries_config = (
  (family_identifier, models.GeneFamily.family_identifier),
  (family_name, models.GeneFamily.family_name),
  )
  q = self.get_model_queries(q, model_queries_config)
  many_to_many_queries_config = (
  (hgnc_symbol, models.GeneFamily.hgncs, models.HGNC.symbol),
  (hgnc_identifier, models.GeneFamily.hgncs, models.HGNC.identifier),
  )
  q = self.get_many_to_many_queries(q, many_to_many_queries_config)
  return self._limit_and_df(q, limit, as_df)","Method to query :class:`.models.GeneFamily` objects in database :param family_identifier: gene family identifier(s) :type family_identifier: int or tuple(int) or None :param family_name: gene family name(s) :type family_name: str or tuple(str) or None :param hgnc_symbol: HGNC symbol(s) :type hgnc_symbol: str or tuple(str) or None :param hgnc_identifier: identifiers(s) in :class:`.models.HGNC` :type hgnc_identifier: int or tuple(int) or None :param limit: - if `isinstance(limit,int)==True` -> limit - if `isinstance(limit,tuple)==True` -> format:= tuple(page_number, results_per_page) - if limit == None -> all results :type limit: int or tuple(int) or None :param bool as_df: if `True` results are returned as :class:`pandas.DataFrame` :return: - if `as_df == False` -> list(:class:`.models.AliasSymbol`) - if `as_df == True` -> :class:`pandas.DataFrame` :rtype: list(:class:`.models.AliasSymbol`) or :class:`pandas.DataFrame`",0,0,1,1,2,1,0,1,1,3
"def git_clone(prettyname: str, url: str, directory: str,
  branch: str = None,
  commit: str = None,
  clone_options: List[str] = None,
  run_func: Callable[[List[str]], Any] = None) -> bool:
  run_func = run_func or subprocess.check_call
  clone_options = clone_options or []
  if os.path.isdir(directory):
  log.info(""Not re-cloning {} Git repository: using existing source ""
  ""in {}"".format(prettyname, directory))
  return False
  log.info(""Fetching {} source from {} into {}"",
  prettyname, url, directory)
  require_executable(GIT)
  gitargs = [GIT, ""clone""] + clone_options
  if branch:
  gitargs += [""--branch"", branch]
  gitargs += [url, directory]
  run_func(gitargs)
  if commit:
  log.info(""Resetting {} local Git repository to commit {}"",
  prettyname, commit)
  run_func([GIT,
  ""-C"", directory,
  ""reset"", ""--hard"", commit])
  return True","Fetches a Git repository, unless we have it already. Args: prettyname: name to display to user url: URL directory: destination directory branch: repository branch commit: repository commit tag clone_options: additional options to pass to ``git clone`` run_func: function to use to call an external command Returns: did we need to do anything?",1,0,0,1,2,1,0,0,1,2
"def _is_loggedin(self, auth_secret):
  userid = self._rc.hget(pytwis_constants.AUTHS_KEY, auth_secret)
  if userid is None:
  return (False, None)
  userid_profile_key = pytwis_constants.USER_PROFILE_KEY_FORMAT.format(userid)
  stored_auth_secret = self._rc.hget(userid_profile_key, pytwis_constants.AUTH_KEY)
  if auth_secret == stored_auth_secret:
  return (True, userid)
  return (False, None)","Check if a user is logged-in by verifying the input authentication secret. Parameters ---------- auth_secret: str The authentication secret of a logged-in user. Returns ------- bool True if the authentication secret is valid, False otherwise. userid: str The user ID associated with the authentication secret if the authentication secret valid, None otherwise.",1,0,0,1,2,1,0,0,1,2
"def get_user(name, profile='github', user_details=False):
  if not user_details and name in list_users(profile):
  return True
  response = {}
  client = _get_client(profile)
  organization = client.get_organization(
  _get_config_value(profile, 'org_name')
  )
  try:
  user = client.get_user(name)
  except UnknownObjectException:
  log.exception(""Resource not found"")
  return False
  response['company'] = user.company
  response['created_at'] = user.created_at
  response['email'] = user.email
  response['html_url'] = user.html_url
  response['id'] = user.id
  response['login'] = user.login
  response['name'] = user.name
  response['type'] = user.type
  response['url'] = user.url
  try:
  headers, data = organization._requester.requestJsonAndCheck(
  ""GET"",
  organization.url + ""/memberships/"" + user._identity
  )
  except UnknownObjectException:
  response['membership_state'] = 'nonexistent'
  response['in_org'] = False
  return response
  response['in_org'] = organization.has_in_members(user)
  response['membership_state'] = data.get('state')
  return response","Get a GitHub user by name. name The user for which to obtain information. profile The name of the profile configuration to use. Defaults to ``github``. user_details Prints user information details. Defaults to ``False``. If the user is already in the organization and user_details is set to False, the get_user function returns ``True``. If the user is not already present in the organization, user details will be printed by default. CLI Example: .. code-block:: bash salt myminion github.get_user github-handle salt myminion github.get_user github-handle user_details=true",2,0,0,1,3,2,0,0,1,3
"def update(self, friendly_name=values.unset, log_queries=values.unset,
  unique_name=values.unset, callback_url=values.unset,
  callback_events=values.unset, fallback_actions=values.unset,
  initiation_actions=values.unset, style_sheet=values.unset):
  return self._proxy.update(
  friendly_name=friendly_name,
  log_queries=log_queries,
  unique_name=unique_name,
  callback_url=callback_url,
  callback_events=callback_events,
  fallback_actions=fallback_actions,
  initiation_actions=initiation_actions,
  style_sheet=style_sheet,
  )","Update the AssistantInstance :param unicode friendly_name: A text description for the Assistant. It is non-unique and can up to 255 characters long. :param bool log_queries: A boolean that specifies whether queries should be logged for 30 days further training. If false, no queries will be stored, if true, queries will be stored for 30 days and deleted thereafter. Defaults to true if no value is provided. :param unicode unique_name: A user-provided string that uniquely identifies this resource as an alternative to the sid. Unique up to 64 characters long. :param unicode callback_url: A user-provided URL to send event callbacks to. :param unicode callback_events: Space-separated list of callback events that will trigger callbacks. :param dict fallback_actions: The JSON actions to be executed when the user's input is not recognized as matching any Task. :param dict initiation_actions: The JSON actions to be executed on inbound phone calls when the Assistant has to say something first. :param dict style_sheet: The JSON object that holds the style sheet for the assistant :returns: Updated AssistantInstance :rtype: twilio.rest.preview.understand.assistant.AssistantInstance",1,1,0,1,3,1,0,0,1,2
"async def log_source(self, **params):
 if params.get(""message""):
 params = json.loads(params.get(""message"", ""{}""))
 if not params:
 return {""error"":400, ""reason"":""Missed required fields""}
 database = client[settings.DBNAME]
 source_collection = database[settings.SOURCE]
 await source_collection.update({""public_key"":params.get(""public_key"")},
 {""$addToSet"":{""source"":params.get(""source"")}},
 upsert=True)
 return {""result"": ""ok""}",Logging users request sources,1,1,0,1,3,1,1,1,1,4
"def register(self):
  form = self._get_form('SECURITY_REGISTER_FORM')
  if form.validate_on_submit():
  user = self.security_service.user_manager.create(**form.to_dict())
  self.security_service.register_user(user)
  return self.redirect('SECURITY_POST_REGISTER_REDIRECT_ENDPOINT')
  return self.render('register',
  register_user_form=form,
  **self.security.run_ctx_processor('register'))",View function to register user. Supports html and json requests.,1,0,0,1,2,1,1,0,1,3
"def prompt_and_delete(path, no_input=False):
  if no_input:
  ok_to_delete = True
  else:
  question = (
  ""You've downloaded {} before. ""
  ""Is it okay to delete and re-download it?""
  ).format(path)
  ok_to_delete = read_user_yes_no(question, 'yes')
  if ok_to_delete:
  if os.path.isdir(path):
  rmtree(path)
  else:
  os.remove(path)
  return True
  else:
  ok_to_reuse = read_user_yes_no(
  ""Do you want to re-use the existing version?"", 'yes'
  )
  if ok_to_reuse:
  return False
  sys.exit()","Ask user if it's okay to delete the previously-downloaded file/directory. If yes, delete it. If no, checks to see if the old version should be reused. If yes, it's reused; otherwise, Cookiecutter exits. :param path: Previously downloaded zipfile. :param no_input: Suppress prompt to delete repo and just delete it. :return: True if the content was deleted",1,0,0,1,2,1,0,0,1,2
"def extract_job_build_url(url_string):
  global g_failed_test_info_dict
  global g_jenkins_url
  global g_view_name
  tempString = url_string.strip('/').split('/')
  if len(tempString) < 6:
  print ""Illegal URL resource address.\n""
  sys.exit(1)
  g_failed_test_info_dict[""1.jobName""] = tempString[6]
  g_jenkins_url = tempString[2]
  g_view_name = tempString[4]","From user input, grab the jenkins job name and saved it in g_failed_test_info_dict. In addition, it will grab the jenkins url and the view name into g_jenkins_url, and g_view_name. Parameters ---------- url_string : str contains information on the jenkins job whose console output we are interested in. :return: none",1,0,0,0,1,1,0,0,1,2
"def create_account_user(self, account_id, body, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('asynchronous'):
  return self.create_account_user_with_http_info(account_id, body, **kwargs)
  else:
  (data) = self.create_account_user_with_http_info(account_id, body, **kwargs)
  return data","Create a new user. # noqa: E501 An endpoint for creating or inviting a new user to the account. In case of invitation email address is used only, other attributes are set in the 2nd step. **Example usage:** `curl -X POST https://api.us-east-1.mbedcloud.com/v3/accounts/{accountID}/users -d {\""email\"": \""myemail@company.com\""} -H 'content-type: application/json' -H 'Authorization: Bearer API_KEY'` # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass asynchronous=True >>> thread = api.create_account_user(account_id, body, asynchronous=True) >>> result = thread.get() :param asynchronous bool :param str account_id: Account ID. (required) :param UserInfoReq body: A user object with attributes. (required) :param str action: Create or invite user. :return: UserInfoResp If the method is called asynchronously, returns the request thread.",1,0,0,2,3,1,0,0,1,2
"def _GetConfigValue(self, configKey, strDescriptor, isDir = True):
  goodlogging.Log.Info(""CLEAR"", ""Loading {0} from database:"".format(strDescriptor))
  goodlogging.Log.IncreaseIndent()
  configValue = self._db.GetConfigValue(configKey)
  if configValue is None:
  goodlogging.Log.Info(""CLEAR"", ""No {0} exists in database"".format(strDescriptor))
  configValue = self._UserUpdateConfigValue(configKey, strDescriptor, isDir)
  else:
  goodlogging.Log.Info(""CLEAR"", ""Got {0} {1} from database"".format(strDescriptor, configValue))
  if not isDir or os.path.isdir(configValue):
  goodlogging.Log.Info(""CLEAR"", ""Using {0} {1}"".format(strDescriptor, configValue))
  goodlogging.Log.DecreaseIndent()
  return configValue
  else:
  goodlogging.Log.Info(""CLEAR"", ""Exiting... {0} is not recognised as a directory"".format(configValue))
  sys.exit(0)",Get configuration value from database table. If no value found user will be prompted to enter one. Parameters ---------- configKey : string Name of config field. strDescriptor : string Description of config field. isDir : boolean [optional : default = True] Set to True if config value is expected to be a directory path. Returns ---------- string Value for given config field in database.,1,0,0,0,1,1,0,1,1,3
"def get_course_and_check_rights(self, courseid, taskid=None, allow_all_staff=True):
  try:
  course = self.course_factory.get_course(courseid)
  if allow_all_staff:
  if not self.user_manager.has_staff_rights_on_course(course):
  raise web.notfound()
  else:
  if not self.user_manager.has_admin_rights_on_course(course):
  raise web.notfound()
  if taskid is None:
  return course, None
  else:
  return course, course.get_task(taskid)
  except:
  raise web.notfound()","Returns the course with id ``courseid`` and the task with id ``taskid``, and verify the rights of the user. Raise web.notfound() when there is no such course of if the users has not enough rights. :param courseid: the course on which to check rights :param taskid: If not None, returns also the task with id ``taskid`` :param allow_all_staff: allow admins AND tutors to see the page. If false, all only admins. :returns (Course, Task)",1,0,1,1,3,1,0,1,1,3
"def _recv_return(self, method_frame):
  msg = self._read_returned_msg(method_frame)
  if callable(self._return_listener):
  self._return_listener(msg)
  else:
  self.logger.error(
  ""Published message returned by broker: info=%s, properties=%s"",
  msg.return_info, msg.properties)","Handle basic.return method. If we have a complete message, will call the user's return listener callabck (if any). If there are not enough frames, will re-queue current frames and raise a FrameUnderflow NOTE: if the channel was in confirmation mode when the message was published, then this will still be followed by basic.ack later",1,0,0,0,1,1,0,0,1,2
"def alterar(self, id_perm, id_permission, read, write, id_group):
  if not is_valid_int_param(id_perm):
  raise InvalidParameterError(
  u'The identifier of Administrative Permission is invalid or was not informed.')
  url = 'aperms/' + str(id_perm) + '/'
  perms_map = dict()
  perms_map['id_perm'] = id_perm
  perms_map['id_permission'] = id_permission
  perms_map['read'] = read
  perms_map['write'] = write
  perms_map['id_group'] = id_group
  code, xml = self.submit(
  {'administrative_permission': perms_map}, 'PUT', url)
  return self.response(code, xml)","Change Administrative Permission from by the identifier. :param id_perm: Identifier of the Administrative Permission. Integer value and greater than zero. :param id_permission: Identifier of the Permission. Integer value and greater than zero. :param read: Read. 0 or 1 :param write: Write. 0 or 1 :param id_group: Identifier of the Group of User. Integer value and greater than zero. :return: None :raise InvalidParameterError: The identifier of Administrative Permission, identifier of Permission, identifier of Group of User, read or write is null and invalid. :raise ValorIndicacaoPermissaoInvalidoError: The value of read or write is null and invalid. :raise PermissaoAdministrativaNaoExisteError: Administrative Permission not registered. :raise GrupoUsuarioNaoExisteError: Group of User not registered. :raise DataBaseError: Networkapi failed to access the database. :raise XMLError: Networkapi failed to generate the XML response.",1,0,0,1,2,1,0,0,1,2
"def find_missing(message, all_params, given_opts, default_opts, header=None, prompt_missing=True):
  missing_params = list(set(all_params) - set(given_opts))
  num_prompted = 0
  if not missing_params:
  return given_opts, missing_params, num_prompted
  if not prompt_missing:
  missing_values = set(default_opts) - set(given_opts)
  num_prompted = len(missing_values)
  given_opts.update(default_opts)
  else:
  if header is not None:
  print('-' * len(header))
  print(header)
  missing_values = interactive_prompt(message, missing_params, default_opts)
  num_prompted = len(missing_values)
  given_opts.update(missing_values)
  return given_opts, missing_params, num_prompted","Find and interactively prompt the user for missing parameters, given the list of all valid parameters and a dict of known options. Return the (updated dict of known options, missing, num_prompted), with the user's input.",1,0,0,1,2,1,0,0,1,2
"def domain_parse(url):
  url = url.lower()
  if not url.startswith('http://') and not url.startswith('https://'):
  url = '{schema}{host}'.format(schema='http://', host=url)
  url = urlparse(url)
  if not url.hostname:
  raise ValueError('Invalid domain provided')
  url = urlparse('{scheme}://{host}'.format(scheme=url.scheme, host=url.hostname.lstrip('www.')))
  return url",urlparse wrapper for user input @type url: str @rtype: urlparse.ParseResult,1,0,0,1,2,1,0,0,1,2
"def config_reader(self, config_level=None):
  files = None
  if config_level is None:
  files = [self._get_config_path(f) for f in self.config_level]
  else:
  files = [self._get_config_path(config_level)]
  return GitConfigParser(files, read_only=True)",":return: GitConfigParser allowing to read the full git configuration, but not to write it The configuration will include values from the system, user and repository configuration files. :param config_level: For possible values, see config_writer method If None, all applicable levels will be used. Specify a level in case you know which file you wish to read to prevent reading multiple files. :note: On windows, system configuration cannot currently be read as the path is unknown, instead the global path will be used.",1,0,0,1,2,1,0,0,1,2
"def query(self, properties=None, criteria=None, distinct_key=None,
  **kwargs):
  if properties is not None:
  props, prop_dict = self._parse_properties(properties)
  else:
  props, prop_dict = None, None
  crit = self._parse_criteria(criteria)
  if self.query_post:
  for func in self.query_post:
  func(crit, props)
  cur = self.collection.find(filter=crit, projection=props, **kwargs)
  if distinct_key is not None:
  cur = cur.distinct(distinct_key)
  return QueryListResults(prop_dict, cur, postprocess=self.result_post)
  else:
  return QueryResults(prop_dict, cur, postprocess=self.result_post)","Convenience method for database access. All properties and criteria can be specified using simplified names defined in Aliases. You can use the supported_properties property to get the list of supported properties. Results are returned as an iterator of dicts to ensure memory and cpu efficiency. Note that the dict returned have keys also in the simplified names form, not in the mongo format. For example, if you query for ""analysis.e_above_hull"", the returned result must be accessed as r['analysis.e_above_hull'] instead of mongo's r['analysis']['e_above_hull']. This is a *feature* of the query engine to allow simple access to deeply nested docs without having to resort to some recursion to go deep into the result. However, if you query for 'analysis', the entire 'analysis' key is returned as r['analysis'] and then the subkeys can be accessed in the usual form, i.e., r['analysis']['e_above_hull'] :param properties: Properties to query for. Defaults to None which means all supported properties. :param criteria: Criteria to query for as a dict. :param distinct_key: If not None, the key for which to get distinct results :param \*\*kwargs: Other kwargs supported by pymongo.collection.find. Useful examples are limit, skip, sort, etc. :return: A QueryResults Iterable, which is somewhat like pymongo's cursor except that it performs mapping. In general, the dev does not need to concern himself with the form. It is sufficient to know that the results are in the form of an iterable of dicts.",2,0,1,1,4,1,0,1,1,3
"def init(cloud_url):
  old_cloud_url = config[""cloud_server""][""url""]
  if old_cloud_url and old_cloud_url != cloud_url:
  raise click.ClickException(
  'Server ""{}"" already selected. Call `openag cloud deinit` to '
  'detach from that server before selecting a new one'.format(
  old_cloud_url
  )
  )
  parsed_url = urlparse(cloud_url)
  if not parsed_url.scheme or not parsed_url.netloc or not parsed_url.port:
  raise click.BadParameter(""Invalid url"")
  if config[""local_server""][""url""]:
  utils.replicate_global_dbs(cloud_url=cloud_url)
  config[""cloud_server""][""url""] = cloud_url",Choose a cloud server to use. Sets CLOUD_URL as the cloud server to use and sets up replication of global databases from that cloud server if a local database is already initialized (via `openag db init`).,1,0,0,1,2,1,1,0,1,3
"def search_users(self, user_name):
  action_path = 'users'
  if user_name:
  action_path += '?search={}'.format(user_name)
  res = self._make_ocs_request(
  'GET',
  self.OCS_SERVICE_CLOUD,
  action_path
  )
  if res.status_code == 200:
  tree = ET.fromstring(res.content)
  users = [x.text for x in tree.findall('data/users/element')]
  return users
  raise HTTPResponseError(res)","Searches for users via provisioning API. If you get back an error 999, then the provisioning API is not enabled. :param user_name: name of user to be searched for :returns: list of usernames that contain user_name as substring :raises: HTTPResponseError in case an HTTP error status was returned",2,0,0,1,3,2,0,0,1,3
"def toggle_comment_visibility(uid, comid, collapse, recid):
  if collapse:
  query =
  params = (comid,)
  res = run_sql(query, params)
  if res:
  query =
  params = (res[0][0], comid, uid)
  run_sql(query, params)
  return True
  else:
  query =
  params = (comid, uid, recid)
  run_sql(query, params)
  return False",Toggle the visibility of the given comment (collapse) for the given user. Return the new visibility :param uid: the user id for which the change applies :param comid: the comment id to close/open :param collapse: if the comment is to be closed (1) or opened (0) :param recid: the record id to which the comment belongs :return: if the comment is visible or not after the update,1,1,1,1,4,1,0,1,0,2
"def execute_file(self, filename):
  logger.info(""Executing file: %s"", format_path(filename))
  contents = self.context.execute(filename, capture=True).stdout
  num_lines = len(contents.splitlines())
  logger.debug(""Execution of %s yielded % of output."",
  format_path(filename),
  pluralize(num_lines, 'line'))
  return contents.rstrip()",Execute a file and provide feedback to the user. :param filename: The pathname of the file to execute (a string). :returns: Whatever the executed file returns on stdout (a string).,1,0,0,1,2,1,0,0,1,2
"async def answer_pre_checkout_query(self, pre_checkout_query_id: base.String, ok: base.Boolean,
  error_message: typing.Union[base.String, None] = None) -> base.Boolean:
  payload = generate_payload(**locals())
  result = await self.request(api.Methods.ANSWER_PRE_CHECKOUT_QUERY, payload)
  return result","Once the user has confirmed their payment and shipping details, the Bot API sends the final confirmation in the form of an Update with the field pre_checkout_query. Use this method to respond to such pre-checkout queries. Source: https://core.telegram.org/bots/api#answerprecheckoutquery :param pre_checkout_query_id: Unique identifier for the query to be answered :type pre_checkout_query_id: :obj:`base.String` :param ok: Specify True if everything is alright (goods are available, etc.) and the bot is ready to proceed with the order. Use False if there are any problems. :type ok: :obj:`base.Boolean` :param error_message: Required if ok is False Error message in human readable form that explains the reason for failure to proceed with the checkout (e.g. ""Sorry, somebody just bought the last of our amazing black T-shirts while you were busy filling out your payment details. Please choose a different color or garment!""). Telegram will display this message to the user. :type error_message: :obj:`typing.Union[base.String, None]` :return: On success, True is returned :rtype: :obj:`base.Boolean`",2,0,0,1,3,1,0,0,1,2
"def add_task(self, content, date=None, priority=None):
  response = API.add_item(self.owner.token, content, project_id=self.id,
  date_string=date, priority=priority)
  _fail_if_contains_errors(response)
  task_json = response.json()
  return Task(task_json, self)","Add a task to the project :param content: The task description. :type content: str :param date: The task deadline. :type date: str :param priority: The priority of the task. :type priority: int :return: The added task. :rtype: :class:`pytodoist.todoist.Task` .. note:: See `here <https://todoist.com/Help/timeInsert>`_ for possible date strings. >>> from pytodoist import todoist >>> user = todoist.login('john.doe@gmail.com', 'password') >>> project = user.get_project('PyTodoist') >>> task = project.add_task('Install PyTodoist') >>> print(task.content) Install PyTodoist",1,0,0,1,2,1,0,0,1,2
"def get_kvlayer_stream_item(client, stream_id):
  if client is None:
  client = kvlayer.client()
  client.setup_namespace(STREAM_ITEM_TABLE_DEFS,
  STREAM_ITEM_VALUE_DEFS)
  key = stream_id_to_kvlayer_key(stream_id)
  for k, v in client.get(STREAM_ITEMS_TABLE, key):
  if v is not None:
  errors, bytestr = streamcorpus.decrypt_and_uncompress(v)
  return streamcorpus.deserialize(bytestr)
  raise KeyError(stream_id)","Retrieve a :class:`streamcorpus.StreamItem` from :mod:`kvlayer`. This function requires that `client` already be set up properly:: client = kvlayer.client() client.setup_namespace(STREAM_ITEM_TABLE_DEFS, STREAM_ITEM_VALUE_DEFS) si = get_kvlayer_stream_item(client, stream_id) `stream_id` is in the form of :data:`streamcorpus.StreamItem.stream_id` and contains the ``epoch_ticks``, a hyphen, and the ``doc_id``. :param client: kvlayer client object :type client: :class:`kvlayer.AbstractStorage` :param str stream_id: stream Id to retrieve :return: corresponding :class:`streamcorpus.StreamItem` :raise exceptions.KeyError: if `stream_id` is malformed or does not correspond to anything in the database",1,0,1,1,3,1,0,0,0,1
"def upsert_document(self, document, database_name=None, collection_name=None, document_id=None):
  if document_id is None:
  document_id = str(uuid.uuid4())
  if document is None:
  raise AirflowBadRequest(""You cannot insert a None document"")
  if 'id' in document:
  if document['id'] is None:
  document['id'] = document_id
  else:
  document['id'] = document_id
  created_document = self.get_conn().CreateItem(
  get_collection_link(
  self.__get_database_name(database_name),
  self.__get_collection_name(collection_name)),
  document)
  return created_document",Inserts a new document (or updates an existing one) into an existing collection in the CosmosDB database.,1,1,1,1,4,1,1,1,1,4
"def user_saw_task(self, username, courseid, taskid):
  self._database.user_tasks.update({""username"": username, ""courseid"": courseid, ""taskid"": taskid},
  {""$setOnInsert"": {""username"": username, ""courseid"": courseid, ""taskid"": taskid,
  ""tried"": 0, ""succeeded"": False, ""grade"": 0.0, ""submissionid"": None, ""state"": """"}},
  upsert=True)",Set in the database that the user has viewed this task,0,1,1,0,2,1,1,1,1,4
"def create_single_payment(self, order_number, order_description, order_items, amount, return_url, contact=None, currency=None, lang=None, additional_params=None):
  return self.create_payment(contact, {
  'amount': amount,
  'currency': currency if currency is not None else settings.GOPAY_CURRENCY,
  'lang': lang if lang is not None else settings.GOPAY_LANG,
  'additional_params': [] if additional_params is None else [{'name': key, 'value': str(value)} for key, value in additional_params.items()],
  'order_number': str(order_number),
  'order_description': order_description,
  'items': [{'name': key, 'amount': value} for key, value in order_items.items()],
  'callback': {
  'return_url': return_url,
  'notification_url': '{}{}'.format(settings.GOPAY_DOMAIN, reverse('gopay_notify')),
  },
  })",Create a single payment. Args: contact: JSON describing a payer (see PaymentManager#create_contact) order_number: your identifier to the order which the payment is for order_description: desription of the order which is show to the user order_items: items in order which are shown to the other (item name -> amount) amount: total amount of money which will be paid returl_url: url for rediraction after payment is processed currency: default is set in settings (GOPAY_CURRENCY) lang: default is set in settings (GOPAY_LANG) Returns: dict: payment status,1,0,0,1,2,1,0,0,1,2
"def tissue_specificity(self, comment=None, entry_name=None, limit=None, as_df=False):
  q = self.session.query(models.TissueSpecificity)
  q = self.get_model_queries(q, ((comment, models.TissueSpecificity.comment),))
  q = self.get_one_to_many_queries(q, ((entry_name, models.Entry.name),))
  return self._limit_and_df(q, limit, as_df)","Method to query :class:`.models.TissueSpecificity` objects in database Provides information on the expression of a gene at the mRNA or protein level in cells or in tissues of multicellular organisms. By default, the information is derived from experiments at the mRNA level, unless specified at protein level :param comment: Comment(s) describing tissue specificity :type comment: str or tuple(str) or None :param entry_name: name(s) in :class:`.models.Entry` :type entry_name: str or tuple(str) or None :param limit: - if `isinstance(limit,int)==True` -> limit - if `isinstance(limit,tuple)==True` -> format:= tuple(page_number, results_per_page) - if limit == None -> all results :type limit: int or tuple(int) or None :param bool as_df: if `True` results are returned as :class:`pandas.DataFrame` :return: - if `as_df == False` -> list(:class:`.models.TissueSpecificity`) - if `as_df == True` -> :class:`pandas.DataFrame` :rtype: list(:class:`.models.TissueSpecificity`) or :class:`pandas.DataFrame`",1,0,1,1,3,1,0,1,1,3
"def setup_user_signals(self, ):
  log.debug(""Setting up user page signals."")
  self.user_task_view_pb.clicked.connect(self.user_view_task)
  self.user_prj_view_pb.clicked.connect(self.user_view_prj)
  self.user_prj_add_pb.clicked.connect(self.user_add_prj)
  self.user_prj_remove_pb.clicked.connect(self.user_remove_prj)
  self.user_username_le.editingFinished.connect(self.user_save)
  self.user_first_le.editingFinished.connect(self.user_save)
  self.user_last_le.editingFinished.connect(self.user_save)
  self.user_email_le.editingFinished.connect(self.user_save)",Setup the signals for the user page :returns: None :rtype: None :raises: None,1,0,0,1,2,1,1,0,1,3
"def get_objects(self, oid=None,
  since=None, until=None, last=None, first=None):
  resource = self.kvpath('registry/objects', ('int', oid),
  since=('isobasic', absdatetime(since)),
  until=('isobasic', absdatetime(until)),
  first=('int', first), last=('int', last))
  return self.request('get', resource)","FETCHES a filtered collection of objects created by the authenticated user. :type oid: ``bigint`` :param oid: Object ID :type since: ``datetime``/``dict`` :param since: Object has to be newer than this timestamp (absolute ``datetime``, or relative ``dict``). Valid keys for relative `since` timestamp dictionary are same as keyword arguments for `datetime.timedelta` (``days``, ``seconds``, ``minutes``, ``hours``, ``weeks``). :type until: ``datetime``/``dict`` :param until: Object has to be older than this timestamp (for format, see the `since` parameter above). :type last: ``bigint`` :param last: The number of newest objects (that satisfy all other criteria) to return. :type first: ``bigint`` :param first: The number of oldest objects (that satisfy all other criteria) to return. :rtype: ``list``/``dict`` :returns: A list of object description dictionaries. If ``oid`` is specified, a single dictionary is returned instead of a list. :raises GeneralException: :resource: ``registry/objects[/<oid>]`` ``[/since=<since>][/until=<until>][/last=<last>][/first=<first>]`` :access: authorized users (only objects owned by the authenticated user are returned)",2,0,1,1,4,2,0,0,1,3
"def onlasso(self, verts):
  p = path.Path(verts)
  ind = p.contains_points(self.pix, radius=1)
  self.history.append(self.selection_array.copy())
  self.selection_array = self.updateArray(self.selection_array,
  ind,
  self.solar_class_var.get())
  self.mask.set_data(self.selection_array)
  self.fig.canvas.draw_idle()","Main function to control the action of the lasso, allows user to draw on data image and adjust thematic map :param verts: the vertices selected by the lasso :return: nothin, but update the selection array so lassoed region now has the selected theme, redraws canvas",1,0,0,1,2,1,0,0,1,2
"def ordered_covering(routing_table, target_length, aliases=dict(),
  no_raise=False):
  aliases = dict(aliases)
  routing_table = sorted(
  routing_table,
  key=lambda entry: _get_generality(entry.key, entry.mask)
  )
  while target_length is None or len(routing_table) > target_length:
  merge = _get_best_merge(routing_table, aliases)
  if merge.goodness <= 0:
  break
  routing_table, aliases = merge.apply(aliases)
  if (not no_raise and
  target_length is not None and
  len(routing_table) > target_length):
  raise MinimisationFailedError(target_length, len(routing_table))
  return routing_table, aliases","Reduce the size of a routing table by merging together entries where possible. .. warning:: The input routing table *must* also include entries which could be removed and replaced by default routing. .. warning:: It is assumed that the input routing table is not in any particular order and may be reordered into ascending order of generality (number of don't cares/Xs in the key-mask) without affecting routing correctness. It is also assumed that if this table is unordered it is at least orthogonal (i.e., there are no two entries which would match the same key) and reorderable. .. note:: If *all* the keys in the table are derived from a single instance of :py:class:`~rig.bitfield.BitField` then the table is guaranteed to be orthogonal and reorderable. .. note:: Use :py:meth:`~rig.routing_table.expand_entries` to generate an orthogonal table and receive warnings if the input table is not orthogonal. Parameters ---------- routing_table : [:py:class:`~rig.routing_table.RoutingTableEntry`, ...] Routing entries to be merged. target_length : int or None Target length of the routing table; the minimisation procedure will halt once either this target is reached or no further minimisation is possible. If None then the table will be made as small as possible. Other Parameters ---------------- aliases : {(key, mask): {(key, mask), ...}, ...} Dictionary of which keys and masks in the routing table are combinations of other (now removed) keys and masks; this allows us to consider only the keys and masks the user actually cares about when determining if inserting a new entry will break the correctness of the table. This should be supplied when using this method to update an already minimised table. no_raise : bool If False (the default) then an error will be raised if the table cannot be minimised to be smaller than `target_length` and `target_length` is not None. If True then a table will be returned regardless of the size of the final table. Raises ------ MinimisationFailedError If the smallest table that can be produced is larger than `target_length` and `no_raise` is False. Returns ------- [:py:class:`~rig.routing_table.RoutingTableEntry`, ...] Reduced routing table entries. {(key, mask): {(key, mask), ...}, ...} A new aliases dictionary.",1,0,0,1,2,1,0,0,1,2
"def decrease_user_property(self, user_id, property_name, value=0, headers=None, endpoint_url=None):
  endpoint_url = endpoint_url or self._endpoint_url
  url = endpoint_url + ""/users/"" + user_id + ""/properties/"" + property_name + ""/decrease/"" + value.__str__()
  headers = headers or self._default_headers(content_type="""")
  response = requests.post(url, headers=headers)
  return response",Decrease a user's property by a value. :param str user_id: identified user's ID :param str property_name: user property name to increase :param number value: amount by which to decrease the property :param dict headers: custom request headers (if isn't set default values are used) :param str endpoint_url: where to send the request (if isn't set default value is used) :return: Response,1,0,0,1,2,1,0,0,1,2
"def get_next_step(self):
  if self.rbAggLayerFromCanvas.isChecked():
  new_step = self.parent.step_fc_agglayer_from_canvas
  elif self.rbAggLayerFromBrowser.isChecked():
  new_step = self.parent.step_fc_agglayer_from_browser
  else:
  new_step = self.parent.step_fc_summary
  return new_step",Find the proper step when user clicks the Next button. :returns: The step to be switched to :rtype: WizardStep instance or None,1,0,0,1,2,1,0,0,1,2
"def _deploy_helper(filename, module_name, get_module, get_today_fn, hash_check=True, auth=None):
  path = ArtifactoryPath(
  get_module(module_name),
  auth=get_arty_auth() if auth is None else auth
  )
  path.mkdir(exist_ok=True)
  if hash_check:
  deployed_semantic_hashes = {
  get_bel_resource_hash(subpath.as_posix())
  for subpath in path
  }
  semantic_hash = get_bel_resource_hash(filename)
  if semantic_hash in deployed_semantic_hashes:
  return
  target = path / get_today_fn(module_name)
  target.deploy_file(filename)
  log.info('deployed %s', module_name)
  return target.as_posix()","Deploys a file to the Artifactory BEL namespace cache :param str filename: The physical path :param str module_name: The name of the module to deploy to :param tuple[str] auth: A pair of (str username, str password) to give to the auth keyword of the constructor of :class:`artifactory.ArtifactoryPath`. Defaults to the result of :func:`get_arty_auth`. :return: The resource path, if it was deployed successfully, else none. :rtype: Optional[str]",1,0,0,1,2,1,0,0,1,2
"def _add_model(self, model):
  name = model._name
  existing = self._models.get(name, None)
  if not existing:
  self._models[name] = model
  elif model.__name__ != existing.__name__ or model._creation_source != existing._creation_source:
  raise ImplementationError(
  'A model with namespace ""%s"" and name ""%s"" is already defined '
  'on this database' % (model.namespace, model.__name__))
  return self._models[name]","Save this model as one existing on this database, to deny many models with same namespace and name. If the model already exists, check if it is the same. It can happen if the module is imported twice in different ways. If it's a new model or an existing and valid one, return the model in database: the one added or the existing one",0,0,0,0,0,0,1,0,1,2
"def republish_module_trigger(plpy, td):
  is_legacy_publication = td['new']['version'] is not None
  if not is_legacy_publication:
  return ""OK""
  plpy.log('Trigger fired on %s' % (td['new']['moduleid'],))
  modified = republish_module(td, plpy)
  plpy.log('modified: {}'.format(modified))
  plpy.log('insert values:\n{}\n'.format('\n'.join([
  '{}: {}'.format(key, value)
  for key, value in td['new'].items()])))
  return modified","Trigger called from postgres database when republishing a module. When a module is republished, the versions of the collections that it is part of will need to be updated (a minor update). e.g. there is a collection c1 v2.1, which contains module m1 v3 m1 is updated, we have a new row in the modules table with m1 v4 this trigger will create increment the minor version of c1, so we'll have c1 v2.2 we need to create a collection tree for c1 v2.2 which is exactly the same as c1 v2.1, but with m1 v4 instead of m1 v3, and c1 v2.2 instead of c1 v2.2",0,1,1,0,2,1,1,0,0,2
"def host_update(hostid, **kwargs):
  conn_args = _login(**kwargs)
  ret = {}
  try:
  if conn_args:
  method = 'host.update'
  params = {""hostid"": hostid}
  params = _params_extend(params, _ignore_name=True, **kwargs)
  ret = _query(method, params, conn_args['url'], conn_args['auth'])
  return ret['result']['hostids']
  else:
  raise KeyError
  except KeyError:
  return ret",".. versionadded:: 2016.3.0 Update existing hosts .. note:: This function accepts all standard host and host.update properties: keyword argument names differ depending on your zabbix version, see the documentation for `host objects`_ and the documentation for `updating hosts`_. .. _`host objects`: https://www.zabbix.com/documentation/2.4/manual/api/reference/host/object#host .. _`updating hosts`: https://www.zabbix.com/documentation/2.4/manual/api/reference/host/update :param hostid: ID of the host to update :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring) :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring) :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring) :param visible_name: string with visible name of the host, use 'visible_name' instead of 'name' parameter to not mess with value supplied from Salt sls file. :return: ID of the updated host. CLI Example: .. code-block:: bash salt '*' zabbix.host_update 10084 name='Zabbix server2'",1,0,0,1,2,1,1,0,1,3
"def get_task_caches(self, usernames, courseid, taskid):
  match = {""courseid"": courseid, ""taskid"": taskid}
  if usernames is not None:
  match[""username""] = {""$in"": usernames}
  data = self._database.user_tasks.find(match)
  retval = {username: None for username in usernames}
  for result in data:
  username = result[""username""]
  del result[""username""]
  del result[""_id""]
  retval[username] = result
  return retval",":param usernames: List of username for which we want info. If usernames is None, data from all users will be returned. :param courseid: the course id :param taskid: the task id :return: A dict in the form: :: { ""username"": { ""courseid"": courseid, ""taskid"": taskid, ""tried"": 0, ""succeeded"": False, ""grade"": 0.0 } }",0,0,1,1,2,1,0,1,1,3
"def get(self, user_id=None):
  if user_id is None:
  res = self._get(
  'groups/get',
  result_processor=lambda x: x['groups']
  )
  else:
  res = self._post(
  'groups/getid',
  data={'openid': user_id},
  result_processor=lambda x: x['groupid']
  )
  return res"," ID  http://mp.weixin.qq.com/wiki/0/56d992c605a97245eb7e617854b169fc.html :param user_id:  ID :return:  ID :: from wechatpy import WeChatClient client = WeChatClient('appid', 'secret') group = client.group.get('openid')",2,0,0,1,3,2,0,0,1,3
"def metadata(self, map_id, secure=False):
  path_values = dict(
  map_id=map_id
  )
  path_part = ""/{map_id}.json""
  uri = URITemplate(self.base_uri + path_part).expand(**path_values)
  query_parameters = dict()
  if secure:
  query_parameters[""secure""] = """"
  response = self.session.get(uri, params=query_parameters)
  self.handle_http_error(response)
  return response","Returns TileJSON metadata for a tileset. Parameters ---------- map_id : str The map's unique identifier in the format username.id. secure : bool, optional The representation of the requested resources, where True indicates representation as HTTPS endpoints. The default value is False. Returns ------- request.Response The response object with TileJSON metadata for the specified tileset.",2,0,0,1,3,2,0,0,1,3
"def get_user(self):
  for name in ('LOGNAME', 'USER', 'LNAME', 'USERNAME'):
  user = os.environ.get(name)
  if user:
  break
  for u in users:
  if u['name'] == user:
  return u['type'], u['name']",returns the username on this computer,1,0,0,1,2,1,0,0,1,2
"def authorized_purchase_object(self, oid, price, huid):
  return self.request(
  'post',
  safeformat('objects/{:int}/purchases', oid),
  json.dumps({
  'price': price,
  'huid': huid,
  'autocommit': True
  }))","Does delegated (pre-authorized) purchase of `oid` in the name of `huid`, at price `price` (vingd transferred from `huid` to consumer's acc). :raises GeneralException: :resource: ``objects/<oid>/purchases`` :access: authorized users with ACL flag ``purchase.object.authorize`` + delegate permission required for the requester to charge the user: ``purchase.object``",1,0,0,2,3,1,0,0,1,2
"def user_choice(prompt, choices=(""yes"", ""no""), default=None):
  assert default is None or default in choices
  choice_list = ', '.join((choice.title() if choice == default else choice for choice in choices))
  response = None
  while response not in choices:
  response = input(prompt + ' [' + choice_list + ']: ')
  response = response.lower() if response else default
  return response","Prompts the user for confirmation. The default value, if any, is capitalized. :param prompt: Information to display to the user. :param choices: an iterable of possible choices. :param default: default choice :return: the user's choice",1,0,0,1,2,1,0,0,1,2
"def acquire_lock(self, force=False):
  if self.readonly:
  raise exc.ReadOnlyDatabase()
  if not self._locked:
  self.log.debug(""Acquiring lock file: {0}"".format(self.lockfile))
  if os.path.exists(self.lockfile) and not force:
  raise exc.DatabaseAlreadyLocked('Lock file already exists: {0}'.format(self.lockfile))
  open(self.lockfile, 'w').close()
  self._locked = True","Takes out a lock (creates a <dbname>.lock file) for the database. :param force: Whether to force taking ""ownership"" of the lock file. :type force: bool :raises: :class:`keepassdb.exc.DatabaseAlreadyLocked` - If the database is already locked (and force not set to True).",0,0,0,0,0,1,0,0,0,1
"def issueViaEmail(self, issuer, email, product, templateData,
  domainName, httpPort=80):
  ticket = self.createTicket(issuer,
  unicode(email, 'ascii'),
  product)
  nonce = ticket.nonce
  signupInfo = {'from': 'signup@'+domainName,
  'to': email,
  'date': rfc822.formatdate(),
  'message-id': smtp.messageid(),
  'link': self.ticketLink(domainName, httpPort, nonce)}
  msg = templateData % signupInfo
  return ticket, _sendEmail(signupInfo['from'], email, msg)","Send a ticket via email to the supplied address, which, when claimed, will create an avatar and allow the given product to endow it with things. @param issuer: An object, preferably a user, to track who issued this ticket. @param email: a str, formatted as an rfc2821 email address (user@domain) -- source routes not allowed. @param product: an instance of L{Product} @param domainName: a domain name, used as the domain part of the sender's address, and as the web server to generate a link to within the email. @param httpPort: a port number for the web server running on domainName @param templateData: A string containing an rfc2822-format email message, which will have several python values interpolated into it dictwise: %(from)s: To be used for the From: header; will contain an rfc2822-format address. %(to)s: the address that we are going to send to. %(date)s: an rfc2822-format date. %(message-id)s: an rfc2822 message-id %(link)s: an HTTP URL that we are generating a link to.",0,1,0,1,2,1,0,0,1,2
"def download_file(self, filename, timeout=5):
  path_file = os.path.join(self.output_directory, DOWNLOADS_PATH, self.session_id[-8:], filename)
  file_url = '{}/download/{}/{}'.format(self.server_url, self.session_id, filename)
  if self.browser_remote:
  self.__download_file(file_url, path_file, timeout)
  return path_file
  return None",download a file from remote selenoid and removing the file in the server. request: http://<username>:<password>@<ggr_host>:<ggr_port>/download/<ggr_session_id>/<filename> :param filename: file name with extension to download :param timeout: threshold until the video file is downloaded :return: downloaded file path or None,1,0,0,1,2,1,0,0,1,2
"def list_objects(self, query=None, limit=-1, offset=-1):
  result = super(DefaultExperimentManager, self).list_objects(
  query=query,
  limit=limit,
  offset=offset
  )
  if not self.coll_predictions is None:
  counts = {}
  pipeline = [
  { '$match': {'active': True}},
  { '$group': { '_id': ""$experiment"", 'count': { '$sum': 1 } } }
  ]
  for doc in self.coll_predictions.aggregate(pipeline):
  counts[doc['_id']] = doc['count']
  for item in result.items:
  if item.identifier in counts:
  item.properties[PROPERTY_RUN_COUNT] = counts[item.identifier]
  else:
  item.properties[PROPERTY_RUN_COUNT] = 0
  return result",List of all experiments in the database. Overrides the super class method to allow the returned object's property lists to be extended with the run count. Parameters ---------- query : Dictionary Filter objects by property-value pairs defined by dictionary. limit : int Limit number of items in the result set offset : int Set offset in list (order as defined by object store) Returns ------- ObjectListing,1,0,1,1,3,1,0,1,1,3
"def writeCommit(self, varBind, **context):
  name, val = varBind
  (debug.logger & debug.FLAG_INS and
  debug.logger('%s: writeCommit(%s, %r)' % (self, name, val)))
  cbFun = context['cbFun']
  instances = context['instances'].setdefault(self.name, {self.ST_CREATE: {}, self.ST_DESTROY: {}})
  idx = context['idx']
  if idx in instances[self.ST_CREATE]:
  self.createCommit(varBind, **context)
  return
  if idx in instances[self.ST_DESTROY]:
  self.destroyCommit(varBind, **context)
  return
  try:
  node = self.getBranch(name, **context)
  except (error.NoSuchInstanceError, error.NoSuchObjectError) as exc:
  cbFun(varBind, **dict(context, error=exc))
  else:
  node.writeCommit(varBind, **context)","Commit new value of the Managed Object Instance. Implements the second of the multi-step workflow of the SNMP SET command processing (:RFC:`1905#section-4.2.5`). The goal of the second phase is to actually modify the requested Managed Object Instance. When multiple Managed Objects Instances are modified at once (likely coming all in one SNMP PDU), each of them has to run through the second (*commit*) phase successfully for the system to transition to the third (*cleanup*) phase. If any single *commit* step fails, the system transitions into the *undo* state for each of Managed Objects Instances being processed at once. The role of this object in the MIB tree is non-terminal. It does not access the actual Managed Object Instance, but just traverses one level down the MIB tree and hands off the query to the underlying objects. Parameters ---------- varBind: :py:class:`~pysnmp.smi.rfc1902.ObjectType` object representing new Managed Object Instance value to set Other Parameters ---------------- \*\*context: Query parameters: * `cbFun` (callable) - user-supplied callable that is invoked to pass the new value of the Managed Object Instance or an error. Notes ----- The callback functions (e.g. `cbFun`) have the same signature as this method where `varBind` contains the new Managed Object Instance value. In case of an error, the `error` key in the `context` dict will contain an exception object.",1,1,1,0,3,1,1,0,0,2
"def fetch_contributing_datasets(self, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('callback'):
  return self.fetch_contributing_datasets_with_http_info(**kwargs)
  else:
  (data) = self.fetch_contributing_datasets_with_http_info(**kwargs)
  return data","List datasets as contributor Fetch datasets that the currently authenticated user has access to because he or she is a contributor. This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please define a `callback` function to be invoked when receiving the response. >>> def callback_function(response): >>> pprint(response) >>> >>> thread = api.fetch_contributing_datasets(callback=callback_function) :param callback function: The callback function for asynchronous request. (optional) :param str limit: Maximum number of items to include in a page of results. :param str next: Token from previous result page to be used when requesting a subsequent page. :return: PaginatedDatasetResults If the method is called asynchronously, returns the request thread.",2,0,0,1,3,2,0,0,1,3
"def shutdown(message=None, timeout=5, force_close=True, reboot=False,
  in_seconds=False, only_on_pending_reboot=False):
  if six.PY2:
  message = _to_unicode(message)
  timeout = _convert_minutes_seconds(timeout, in_seconds)
  if only_on_pending_reboot and not get_pending_reboot():
  return False
  if message and not isinstance(message, six.string_types):
  message = message.decode('utf-8')
  try:
  win32api.InitiateSystemShutdown('127.0.0.1', message, timeout,
  force_close, reboot)
  return True
  except pywintypes.error as exc:
  (number, context, message) = exc.args
  log.error('Failed to shutdown the system')
  log.error('nbr: %s', number)
  log.error('ctx: %s', context)
  log.error('msg: %s', message)
  return False","Shutdown a running system. Args: message (str): The message to display to the user before shutting down. timeout (int): The length of time (in seconds) that the shutdown dialog box should be displayed. While this dialog box is displayed, the shutdown can be aborted using the ``system.shutdown_abort`` function. If timeout is not zero, InitiateSystemShutdown displays a dialog box on the specified computer. The dialog box displays the name of the user who called the function, the message specified by the lpMessage parameter, and prompts the user to log off. The dialog box beeps when it is created and remains on top of other windows (system modal). The dialog box can be moved but not closed. A timer counts down the remaining time before the shutdown occurs. If timeout is zero, the computer shuts down immediately without displaying the dialog box and cannot be stopped by ``system.shutdown_abort``. Default is 5 minutes in_seconds (bool): ``True`` will cause the ``timeout`` parameter to be in seconds. ``False`` will be in minutes. Default is ``False``. .. versionadded:: 2015.8.0 force_close (bool): ``True`` will force close all open applications. ``False`` will display a dialog box instructing the user to close open applications. Default is ``True``. reboot (bool): ``True`` restarts the computer immediately after shutdown. ``False`` powers down the system. Default is ``False``. only_on_pending_reboot (bool): If this is set to True, then the shutdown will only proceed if the system reports a pending reboot. To optionally shutdown in a highstate, consider using the shutdown state instead of this module. only_on_pending_reboot (bool): If ``True`` the shutdown will only proceed if there is a reboot pending. ``False`` will shutdown the system. Default is ``False``. Returns: bool: ``True`` if successful (a shutdown or reboot will occur), otherwise ``False`` CLI Example: .. code-block:: bash salt '*' system.shutdown ""System will shutdown in 5 minutes""",1,0,0,0,1,1,0,0,1,2
"def custom_model(self, func):
  y_pred = func(self.baseline_in, self.baseline_out)
  self.custom_metrics = {}
  self.custom_metrics['r2'] = r2_score(self.baseline_out, y_pred)
  self.custom_metrics['mse'] = mean_squared_error(self.baseline_out, y_pred)
  self.custom_metrics['rmse'] = math.sqrt(self.custom_metrics['mse'])
  self.custom_metrics['adj_r2'] = self.adj_r2(self.custom_metrics['r2'], self.baseline_in.shape[0], self.baseline_in.shape[1])
  return self.custom_metrics","Run custom model provided by user. To Do, 1. Define custom function's parameters, its data types, and return types Parameters ---------- func : function Custom function Returns ------- dict Custom function's metrics",1,0,0,1,2,0,0,0,1,1
"def friendships_create(self, user_id=None, screen_name=None,
  follow=None):
  params = {}
  set_str_param(params, 'user_id', user_id)
  set_str_param(params, 'screen_name', screen_name)
  set_bool_param(params, 'follow', follow)
  return self._post_api('friendships/create.json', params)",Allows the authenticating users to follow the specified user. https://dev.twitter.com/docs/api/1.1/post/friendships/create :param str user_id: The screen name of the user for whom to befriend. Required if ``screen_name`` isn't given. :param str screen_name: The ID of the user for whom to befriend. Required if ``user_id`` isn't given. :param bool follow: Enable notifications for the target user. :returns: A dict containing the newly followed user.,1,0,0,2,3,1,0,0,2,3
"def restore(self, password, db, dump, copy=False):
  if dump.closed:
  raise error.InternalError(""Dump file closed"")
  b64_data = base64.standard_b64encode(dump.read()).decode()
  self._odoo.json(
  '/jsonrpc',
  {'service': 'db',
  'method': 'restore',
  'args': [password, db, b64_data, copy]})","Restore the `dump` database into the new `db` database. The `dump` file object can be obtained with the :func:`dump <DB.dump>` method. If `copy` is set to `True`, the restored database will have a new UUID. >>> odoo.db.restore('super_admin_passwd', 'test', dump_file) # doctest: +SKIP If you get a timeout error, increase this one before performing the request: >>> timeout_backup = odoo.config['timeout'] >>> odoo.config['timeout'] = 7200 # Timeout set to 2 hours >>> odoo.db.restore('super_admin_passwd', 'test', dump_file) # doctest: +SKIP >>> odoo.config['timeout'] = timeout_backup The super administrator password is required to perform this method. *Python 2:* :raise: :class:`odoorpc.error.RPCError` (access denied / database already exists) :raise: :class:`odoorpc.error.InternalError` (dump file closed) :raise: `urllib2.URLError` (connection error) *Python 3:* :raise: :class:`odoorpc.error.RPCError` (access denied / database already exists) :raise: :class:`odoorpc.error.InternalError` (dump file closed) :raise: `urllib.error.URLError` (connection error)",1,1,0,1,3,1,0,0,1,2
"def create(model, count, *args, **kwargs):
  from .compat import get_model
  if isinstance(model, string_types):
  model = get_model(*model.split('.', 1))
  if model in REGISTRY:
  autofixture_class = REGISTRY[model]
  else:
  autofixture_class = AutoFixture
  argnames = set(getargnames(autofixture_class.create_one))
  argnames -= set(['self'])
  create_kwargs = {}
  for argname in argnames:
  if argname in kwargs:
  create_kwargs[argname] = kwargs.pop(argname)
  autofixture = autofixture_class(model, *args, **kwargs)
  return autofixture.create(count, **create_kwargs)","Create *count* instances of *model* using the either an appropiate autofixture that was :ref:`registry <registry>` or fall back to the default:class:`AutoFixture` class. *model* can be a model class or its string representation (e.g. ``""app.ModelClass""``). All positional and keyword arguments are passed to the autofixture constructor. It is demonstrated in the example below which will create ten superusers:: import autofixture admins = autofixture.create('auth.User', 10, field_values={'is_superuser': True}) .. note:: See :ref:`AutoFixture` for more information. :func:`create` will return a list of the created objects.",0,0,0,1,1,1,1,0,1,3
"def __get_user_ratings(self):
  return self.parse_raw_response(requests_util.run_request('get', self.API_BASE_URL + '/user/ratings',
  headers=self.__get_header_with_auth()))",Returns a list of the ratings provided by the current user. :return: a python dictionary with either the result of the search or an error from TheTVDB.,2,0,0,1,3,2,0,0,1,3
"def add_instance(self,
  role,
  instance,
  username='root',
  key_filename=None,
  output_shell=False):
  if not role in self.Instances.keys():
  self.Instances[role] = []
  self.logger.debug('Adding ' + role + ' with private_hostname ' +
  instance['private_hostname'] +
  ', public_hostname ' + instance['public_hostname'])
  self.Instances[role].append(Connection(instance,
  username,
  key_filename,
  output_shell=output_shell))",Add instance to the setup @param role: instance's role @type role: str @param instance: host parameters we would like to establish connection to @type instance: dict @param username: user name for creating ssh connection @type username: str @param key_filename: file name with ssh private key @type key_filename: str @param output_shell: write output from this connection to standard output @type output_shell: bool,1,0,0,0,1,1,1,0,1,3
"def allow_staff_or_superuser(func):
  is_object_permission = ""has_object"" in func.__name__
  @wraps(func)
  def func_wrapper(*args, **kwargs):
  request = args[0]
  if is_object_permission:
  request = args[1]
  if request.user.is_staff or request.user.is_superuser:
  return True
  return func(*args, **kwargs)
  return func_wrapper",This decorator is used to abstract common is_staff and is_superuser functionality out of permission checks. It determines which parameter is the request based on name.,1,0,0,0,1,1,0,0,1,2
"def get_rejection_reasons(self, keyword=None):
  keys = ['selected', 'other']
  if keyword is None:
  return sum(map(self.get_rejection_reasons, keys), [])
  if keyword not in keys:
  return []
  rejection_reasons = self.context.getRejectionReasons()
  rejection_reasons = rejection_reasons and rejection_reasons[0] or {}
  if keyword == 'other':
  return rejection_reasons.get(keyword, '') and [rejection_reasons.get(keyword, '')] or []
  return rejection_reasons.get(keyword, [])","Returns a list with the rejection reasons as strings :param keyword: set of rejection reasons to be retrieved. Possible values are: - 'selected': Get, amongst the set of predefined reasons, the ones selected - 'other': Get the user free-typed reason for rejection - None: Get all rejection reasons :return: list of rejection reasons as strings or an empty list",1,0,0,1,2,1,0,0,1,2
"def on_select_high_level(self,event,called_by_parent=False):
  UPPER_LEVEL=self.level_box.GetValue()
  if UPPER_LEVEL=='sample':
  self.level_names.SetItems(self.parent.samples)
  self.level_names.SetStringSelection(self.parent.Data_hierarchy['sample_of_specimen'][self.parent.s])
  if UPPER_LEVEL=='site':
  self.level_names.SetItems(self.parent.sites)
  self.level_names.SetStringSelection(self.parent.Data_hierarchy['site_of_specimen'][self.parent.s])
  if UPPER_LEVEL=='location':
  self.level_names.SetItems(self.parent.locations)
  self.level_names.SetStringSelection(self.parent.Data_hierarchy['location_of_specimen'][self.parent.s])
  if UPPER_LEVEL=='study':
  self.level_names.SetItems(['this study'])
  self.level_names.SetStringSelection('this study')
  if not called_by_parent:
  self.parent.level_box.SetStringSelection(UPPER_LEVEL)
  self.parent.onSelect_high_level(event,True)
  self.on_select_level_name(event)",alters the possible entries in level_names combobox to give the user selections for which specimen interpretations to display in the logger @param: event -> the wx.COMBOBOXEVENT that triggered this function,1,0,0,1,2,1,0,0,1,2
"def update(self, client=None):
  client = self._require_client(client)
  query_params = self._query_params
  query_params[""projection""] = ""full""
  api_response = client._connection.api_request(
  method=""PUT"",
  path=self.path,
  data=self._properties,
  query_params=query_params,
  _target_object=self,
  )
  self._set_properties(api_response)","Sends all properties in a PUT request. Updates the ``_properties`` with the response from the backend. If :attr:`user_project` is set, bills the API request to that project. :type client: :class:`~google.cloud.storage.client.Client` or ``NoneType`` :param client: the client to use. If not passed, falls back to the ``client`` stored on the current object.",1,0,0,1,2,1,0,0,1,2
"def add_path(self, path, pattern=""*.json""):
  if os.path.isdir(path):
  configs = glob.glob(_opj(path, pattern))
  else:
  configs = [path]
  for config in configs:
  cfg = dbconfig.DBConfig(config_file=config)
  cs = cfg.settings
  if dbconfig.DB_KEY not in cs:
  raise ValueError(""No database in '{}'"".format(config))
  if dbconfig.COLL_KEY in cs:
  name = ""{}.{}"".format(cs[dbconfig.DB_KEY],
  cs[dbconfig.COLL_KEY])
  else:
  name = cs[dbconfig.DB_KEY]
  self.add(name, cfg)
  return self","Add configuration file(s) in `path`. The path can be a single file or a directory. If path is a directory, then `pattern` (Unix glob-style) will be used to get a list of all config files in the directory. The name given to each file is the database name and collection name (if any) combined with a '.'. :param path: File or directory name :return: self, for chaining",1,0,0,0,1,1,0,0,1,2
"def create_authorization_code(self, authorization_request, subject_identifier, scope=None):
  if not self._is_valid_subject_identifier(subject_identifier):
  raise InvalidSubjectIdentifier('{} unknown'.format(subject_identifier))
  scope = ' '.join(scope or authorization_request['scope'])
  logger.debug('creating authz code for scope=%s', scope)
  authorization_code = rand_str()
  authz_info = {
  'used': False,
  'exp': int(time.time()) + self.authorization_code_lifetime,
  'sub': subject_identifier,
  'granted_scope': scope,
  self.KEY_AUTHORIZATION_REQUEST: authorization_request.to_dict()
  }
  self.authorization_codes[authorization_code] = authz_info
  logger.debug('new authz_code=%s to client_id=%s for sub=%s valid_until=%s', authorization_code,
  authorization_request['client_id'], subject_identifier, authz_info['exp'])
  return authorization_code",Creates an authorization code bound to the authorization request and the authenticated user identified by the subject identifier.,1,0,0,1,2,1,0,0,1,2
"def remove_user_from_user_groups(self, id, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.remove_user_from_user_groups_with_http_info(id, **kwargs)
  else:
  (data) = self.remove_user_from_user_groups_with_http_info(id, **kwargs)
  return data","Removes specific user groups from the user # noqa: E501 # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.remove_user_from_user_groups(id, async_req=True) >>> result = thread.get() :param async_req bool :param str id: (required) :param list[str] body: The list of user groups that should be removed from the user :return: UserModel If the method is called asynchronously, returns the request thread.",2,0,0,1,3,1,1,0,1,3
"def json_parse(self, response):
  try:
  data = response.json()
  except ValueError:
  data = {'meta': { 'status': 500, 'msg': 'Server Error'}, 'response': {""error"": ""Malformed JSON or HTML was returned.""}}
  if 200 <= data['meta']['status'] <= 399:
  return data['response']
  else:
  return data",Wraps and abstracts response validation and JSON parsing to make sure the user gets the correct response. :param response: The response returned to us from the request :returns: a dict of the json response,1,0,0,1,2,0,0,0,1,1
"def find_user(cls, session, mailbox, user):
  return cls(
  '/mailboxes/%d/users/%s/conversations.json' % (
  mailbox.id, user.id,
  ),
  session=session,
  )",Return conversations for a specific user in a mailbox. Args: session (requests.sessions.Session): Authenticated session. mailbox (helpscout.models.Mailbox): Mailbox to search. user (helpscout.models.User): User to search for. Returns: RequestPaginator(output_type=helpscout.models.Conversation): Conversations iterator.,2,0,0,1,3,1,0,0,1,2
"def sample_ising(self, h, J, **kwargs):
  if isinstance(h, list):
  h = dict(enumerate(h))
  variables = set(h).union(*J)
  try:
  active_variables = sorted(variables)
  except TypeError:
  active_variables = list(variables)
  num_variables = len(active_variables)
  future = self.solver.sample_ising(h, J, **kwargs)
  return dimod.SampleSet.from_future(future, _result_to_response_hook(active_variables, dimod.SPIN))","Sample from the specified Ising model. Args: h (list/dict): Linear biases of the Ising model. If a list, the list's indices are used as variable labels. J (dict[(int, int): float]): Quadratic biases of the Ising model. **kwargs: Optional keyword arguments for the sampling method, specified per solver in :attr:`.DWaveSampler.parameters` Returns: :class:`dimod.SampleSet`: A `dimod` :obj:`~dimod.SampleSet` object. Examples: This example submits a two-variable Ising problem mapped directly to qubits 0 and 1 on a D-Wave system selected by the user's default :std:doc:`D-Wave Cloud Client configuration file <cloud-client:intro>`. >>> from dwave.system.samplers import DWaveSampler >>> sampler = DWaveSampler() >>> response = sampler.sample_ising({0: -1, 1: 1}, {}) >>> for sample in response.samples(): # doctest: +SKIP ... print(sample) ... {0: 1, 1: -1} See `Ocean Glossary <https://docs.ocean.dwavesys.com/en/latest/glossary.html>`_ for explanations of technical terms in descriptions of Ocean tools.",2,0,0,2,4,1,0,0,1,2
"def export(self, template_file_name, output_file_name,
  sort=""public"", data=None, limit=0):
  exportedData = {}
  exportedUsers = self.getSortedUsers()
  template = self.__getTemplate(template_file_name)
  position = 1
  if not limit:
  exportedData[""users""] = exportedUsers
  else:
  exportedData[""users""] = exportedUsers[:limit]
  for u in exportedData[""users""]:
  u[""position""] = position
  u[""comma""] = position < len(exportedData[""users""])
  position += 1
  exportedData[""extraData""] = data
  renderer = Renderer()
  output = renderer.render(template, exportedData)
  with open(output_file_name, ""w"") as text_file:
  text_file.write(output)",Export ranking to a file. Args: template_file_name (str): where is the template (moustache template) output_file_name (str): where create the file with the ranking sort (str): field to sort the users,1,1,0,0,2,1,0,0,1,2
"def read_wv_master_from_array(master_table, lines='brightest', debugplot=0):
  if lines not in ['brightest', 'all']:
  raise ValueError('Unexpected lines=' + str(lines))
  if master_table.ndim == 1:
  wv_master = master_table
  else:
  wv_master_all = master_table[:, 0]
  if master_table.shape[1] == 2:
  wv_master = np.copy(wv_master_all)
  elif master_table.shape[1] == 3:
  if lines == 'brightest':
  wv_flag = master_table[:, 1]
  wv_master = wv_master_all[np.where(wv_flag == 1)]
  else:
  wv_master = np.copy(wv_master_all)
  else:
  raise ValueError('Lines_catalog file does not have the '
  'expected number of columns')
  if abs(debugplot) >= 10:
  print(""Reading master table from numpy array"")
  print(""wv_master:\n"", wv_master)
  return wv_master","read arc line wavelengths from numpy array Parameters ---------- master_table : Numpy array Numpy array containing the wavelength database. lines : string Indicates which lines to read. For files with a single column or two columns this parameter is irrelevant. For files with three columns, lines='brightest' indicates that only the brightest lines are read, whereas lines='all' means that all the lines are considered. debugplot : int Determines whether intermediate computations and/or plots are displayed. The valid codes are defined in numina.array.display.pause_debugplot. Returns ------- wv_master : 1d numpy array Array with arc line wavelengths.",0,0,0,1,1,1,0,0,1,2
"def format_message(self, msg, botreply=False):
  if sys.version_info[0] < 3 and isinstance(msg, str):
  msg = msg.decode()
  msg = msg.lower()
  msg = self.substitute(msg, ""sub"")
  if self.utf8:
  msg = re.sub(RE.utf8_meta, '', msg)
  msg = re.sub(self.master.unicode_punctuation, '', msg)
  if botreply:
  msg = re.sub(RE.utf8_punct, '', msg)
  else:
  msg = utils.strip_nasties(msg)
  msg = msg.strip()
  msg = RE.ws.sub("" "",msg)
  return msg",Format a user's message for safe processing. This runs substitutions on the message and strips out any remaining symbols (depending on UTF-8 mode). :param str msg: The user's message. :param bool botreply: Whether this formatting is being done for the bot's last reply (e.g. in a ``%Previous`` command). :return str: The formatted message.,1,0,0,1,2,1,0,0,1,2
"def select_token(request, scopes='', new=False):
  @tokens_required(scopes=scopes, new=new)
  def _token_list(r, tokens):
  context = {
  'tokens': tokens,
  'base_template': app_settings.ESI_BASE_TEMPLATE,
  }
  return render(r, 'esi/select_token.html', context=context)
  return _token_list(request)",Presents the user with a selection of applicable tokens for the requested view.,1,0,0,1,2,1,0,0,1,2
"def reload(self, client=None):
  path = self.reload_path
  client = self._require_client(client)
  query_params = {}
  if self.user_project is not None:
  query_params[""userProject""] = self.user_project
  self.entities.clear()
  found = client._connection.api_request(
  method=""GET"", path=path, query_params=query_params
  )
  self.loaded = True
  for entry in found.get(""items"", ()):
  self.add_entity(self.entity_from_dict(entry))","Reload the ACL data from Cloud Storage. If :attr:`user_project` is set, bills the API request to that project. :type client: :class:`~google.cloud.storage.client.Client` or ``NoneType`` :param client: Optional. The client to use. If not passed, falls back to the ``client`` stored on the ACL's parent.",0,0,1,0,1,1,0,0,1,2
"def recursive_input(input_label, type_class):
  import sys
  type_str = str(type_class).split(""'"")[1]
  msg = 'Enter {} (type `{}`): '.format(input_label, type_str)
  try:
  output = input(msg)
  print('')
  try:
  output = type_class(output)
  return output
  except:
  print('Input must be of type `{}`\n'.format(type_str))
  return recursive_input(input_label, type_class)
  except KeyboardInterrupt:
  return sys.exit()","Recursive user input prompter with type checker Args ---- type_class: type name of python type (e.g. float, no parentheses) Returns ------- output: str value entered by user converted to type `type_class` Note ---- Use `ctrl-c` to exit input cycling",1,0,0,1,2,1,0,0,1,2
"def auth(self, request):
  try:
  auth = self.gh.authorize(self.username,
  self.password,
  self.scope,
  '',
  '',
  self.consumer_key,
  self.consumer_secret)
  request.session['oauth_token'] = auth.token
  request.session['oauth_id'] = auth.id
  except AuthenticationFailed as e:
  messages.add_message(request, messages.ERROR, message=""GITHUB RENEW FAILED : Reason {}"".format(e))
  return reverse('user_services')
  return self.callback_url(request)",let's auth the user to the Service :param request: request object :return: callback url :rtype: string that contains the url to redirect after auth,2,0,0,2,4,2,0,0,1,3
"def verify(cls, user_id, verification_hash):
  user = yield cls.get(user_id)
  if 'verification_hash' not in user._resource:
  raise Return(user)
  if user.verification_hash != verification_hash:
  raise exceptions.ValidationError('Invalid verification hash')
  del user.verification_hash
  yield user._save()
  raise Return(user)",Verify a user using the verification hash The verification hash is removed from the user once verified :param user_id: the user ID :param verification_hash: the verification hash :returns: a User instance,1,1,1,1,4,1,0,1,1,3
"def with_argument_list(*args: List[Callable], preserve_quotes: bool = False) -> Callable[[List], Optional[bool]]:
  import functools
  def arg_decorator(func: Callable):
  @functools.wraps(func)
  def cmd_wrapper(cmd2_instance, statement: Union[Statement, str]):
  _, parsed_arglist = cmd2_instance.statement_parser.get_command_arg_list(command_name,
  statement,
  preserve_quotes)
  return func(cmd2_instance, parsed_arglist)
  command_name = func.__name__[len(COMMAND_FUNC_PREFIX):]
  cmd_wrapper.__doc__ = func.__doc__
  return cmd_wrapper
  if len(args) == 1 and callable(args[0]):
  return arg_decorator(args[0])
  else:
  return arg_decorator","A decorator to alter the arguments passed to a do_* cmd2 method. Default passes a string of whatever the user typed. With this decorator, the decorated method will receive a list of arguments parsed from user input. :param args: Single-element positional argument list containing do_* method this decorator is wrapping :param preserve_quotes: if True, then argument quotes will not be stripped :return: function that gets passed a list of argument strings",1,0,0,1,2,1,0,0,1,2
"def get_chat_administrators(self, chat_id):
  assert_type_or_raise(chat_id, (int, unicode_type), parameter_name=""chat_id"")
  result = self.do(""getChatAdministrators"", chat_id=chat_id)
  if self.return_python_objects:
  logger.debug(""Trying to parse {data}"".format(data=repr(result)))
  from pytgbot.api_types.receivable.peer import ChatMember
  try:
  return ChatMember.from_array_list(result, list_level=1)
  except TgApiParseException:
  logger.debug(""Failed parsing as api_type ChatMember"", exc_info=True)
  raise TgApiParseException(""Could not parse result."")
  return result","Use this method to get a list of administrators in a chat. On success, returns an Array of ChatMember objects that contains information about all chat administrators except other bots. If the chat is a group or a supergroup and no administrators were appointed, only the creator will be returned. https://core.telegram.org/bots/api#getchatadministrators Parameters: :param chat_id: Unique identifier for the target chat or username of the target supergroup or channel (in the format @channelusername) :type chat_id: int | str|unicode Returns: :return: On success, returns an Array of ChatMember objects that contains information about all chat administrators except other bots :rtype: list of pytgbot.api_types.receivable.peer.ChatMember",2,0,0,1,3,2,0,0,1,3
"def ref_seq(self, accession=None, hgnc_symbol=None, hgnc_identifier=None, limit=None, as_df=False):
  q = self.session.query(models.RefSeq)
  model_queries_config = (
  (accession, models.RefSeq.accession),
  )
  q = self.get_model_queries(q, model_queries_config)
  many_to_many_queries_config = (
  (hgnc_symbol, models.RefSeq.hgncs, models.HGNC.symbol),
  (hgnc_identifier, models.RefSeq.hgncs, models.HGNC.identifier),
  )
  q = self.get_many_to_many_queries(q, many_to_many_queries_config)
  return self._limit_and_df(q, limit, as_df)","Method to query :class:`.models.RefSeq` objects in database :param accession: RefSeq accessionl(s) :type accession: str or tuple(str) or None :param hgnc_symbol: HGNC symbol(s) :type hgnc_symbol: str or tuple(str) or None :param hgnc_identifier: identifiers(s) in :class:`.models.HGNC` :type hgnc_identifier: int or tuple(int) or None :param limit: - if `isinstance(limit,int)==True` -> limit - if `isinstance(limit,tuple)==True` -> format:= tuple(page_number, results_per_page) - if limit == None -> all results :type limit: int or tuple(int) or None :param bool as_df: if `True` results are returned as :class:`pandas.DataFrame` :return: - if `as_df == False` -> list(:class:`.models.RefSeq`) - if `as_df == True` -> :class:`pandas.DataFrame` :rtype: list(:class:`.models.RefSeq`) or :class:`pandas.DataFrame`",1,0,1,1,3,1,0,1,1,3
"def db_create(database, containment='NONE', new_database_options=None, **kwargs):
  if containment not in ['NONE', 'PARTIAL']:
  return 'CONTAINMENT can be one of NONE and PARTIAL'
  sql = ""CREATE DATABASE [{0}] CONTAINMENT = {1} "".format(database, containment)
  if new_database_options:
  sql += ' WITH ' + ', '.join(new_database_options)
  conn = None
  try:
  conn = _get_connection(**kwargs)
  conn.autocommit(True)
  conn.cursor().execute(sql)
  except Exception as e:
  return 'Could not create the login: {0}'.format(e)
  finally:
  if conn:
  conn.autocommit(False)
  conn.close()
  return True",Creates a new database. Does not update options of existing databases. new_database_options can only be a list of strings CLI Example: .. code-block:: bash salt minion mssql.db_create DB_NAME,1,1,0,0,2,1,1,1,0,3
"def add_candidate_adapter_ports(self, ports):
  body = {
  'adapter-port-uris': [p.uri for p in ports],
  }
  self.manager.session.post(
  self.uri + '/operations/add-candidate-adapter-ports',
  body=body)","Add a list of storage adapter ports to this storage group's candidate adapter ports list. This operation only applies to storage groups of type ""fcp"". These adapter ports become candidates for use as backing adapters when creating virtual storage resources when the storage group is attached to a partition. The adapter ports should have connectivity to the storage area network (SAN). Candidate adapter ports may only be added before the CPC discovers a working communications path, indicated by a ""verified"" status on at least one of this storage group's WWPNs. After that point, all adapter ports in the storage group are automatically detected and manually adding them is no longer possible. Because the CPC discovers working communications paths automatically, candidate adapter ports do not need to be added by the user. Any ports that are added, are validated by the CPC during discovery, and may or may not actually be used. Authorization requirements: * Object-access permission to this storage group. * Object-access permission to the adapter of each specified port. * Task permission to the ""Configure Storage - System Programmer"" task. Parameters: ports (:class:`py:list`): List of :class:`~zhmcclient.Port` objects representing the ports to be added. All specified ports must not already be members of this storage group's candidate adapter ports list. Raises: :exc:`~zhmcclient.HTTPError` :exc:`~zhmcclient.ParseError` :exc:`~zhmcclient.AuthError` :exc:`~zhmcclient.ConnectionError`",0,0,0,1,1,1,0,0,1,2
"def _identity(self, *args, **kwargs):
  LOCAL = 'local accounts'
  EXT = 'external accounts'
  data = dict()
  data[LOCAL] = self._get_local_users(disabled=kwargs.get('disabled'))
  data[EXT] = self._get_external_accounts(data[LOCAL].keys()) or 'N/A'
  data['local groups'] = self._get_local_groups()
  return data","Local users and groups. accounts Can be either 'local', 'remote' or 'all' (equal to ""local,remote""). Remote accounts cannot be resolved on all systems, but only those, which supports 'passwd -S -a'. disabled True (or False, default) to return only disabled accounts.",0,0,0,1,1,1,0,0,1,2
"def execute_on_keys(self, keys, entry_processor):
  key_list = []
  for key in keys:
  check_not_none(key, ""key can't be None"")
  key_list.append(self._to_data(key))
  if len(keys) == 0:
  return ImmediateFuture([])
  return self._encode_invoke(map_execute_on_keys_codec, entry_processor=self._to_data(entry_processor),
  keys=key_list)","Applies the user defined EntryProcessor to the entries mapped by the collection of keys. Returns the results mapped by each key in the collection. :param keys: (Collection), collection of the keys for the entries to be processed. :param entry_processor: (object), A stateful serializable object which represents the EntryProcessor defined on server side. This object must have a serializable EntryProcessor counter part registered on server side with the actual ``org.hazelcast.map.EntryProcessor`` implementation. :return: (Sequence), list of map entries which includes the keys and the results of the entry process.",1,0,0,1,2,1,0,0,1,2
"def init(db_url, alembic_ini=None, debug=False, create=False):
  engine = create_engine(db_url, echo=debug)
  if create:
  BASE.metadata.create_all(engine)
  if alembic_ini is not None:
  from alembic.config import Config
  from alembic import command
  alembic_cfg = Config(alembic_ini)
  command.stamp(alembic_cfg, ""head"")
  scopedsession = scoped_session(sessionmaker(bind=engine))
  return scopedsession","Create the tables in the database using the information from the url obtained. :arg db_url, URL used to connect to the database. The URL contains information with regards to the database engine, the host to connect to, the user and password and the database name. ie: <engine>://<user>:<password>@<host>/<dbname> :kwarg alembic_ini, path to the alembic ini file. This is necessary to be able to use alembic correctly, but not for the unit-tests. :kwarg debug, a boolean specifying wether we should have the verbose output of sqlalchemy or not. :return a session that can be used to query the database.",1,1,1,1,4,1,1,0,0,2
"def execute_concurrent(session, statements_and_parameters, concurrency=100, raise_on_first_error=True, results_generator=False):
  if concurrency <= 0:
  raise ValueError(""concurrency must be greater than 0"")
  if not statements_and_parameters:
  return []
  executor = ConcurrentExecutorGenResults(session, statements_and_parameters) if results_generator else ConcurrentExecutorListResults(session, statements_and_parameters)
  return executor.execute(concurrency, raise_on_first_error)","Executes a sequence of (statement, parameters) tuples concurrently. Each ``parameters`` item must be a sequence or :const:`None`. The `concurrency` parameter controls how many statements will be executed concurrently. When :attr:`.Cluster.protocol_version` is set to 1 or 2, it is recommended that this be kept below 100 times the number of core connections per host times the number of connected hosts (see :meth:`.Cluster.set_core_connections_per_host`). If that amount is exceeded, the event loop thread may attempt to block on new connection creation, substantially impacting throughput. If :attr:`~.Cluster.protocol_version` is 3 or higher, you can safely experiment with higher levels of concurrency. If `raise_on_first_error` is left as :const:`True`, execution will stop after the first failed statement and the corresponding exception will be raised. `results_generator` controls how the results are returned. * If :const:`False`, the results are returned only after all requests have completed. * If :const:`True`, a generator expression is returned. Using a generator results in a constrained memory footprint when the results set will be large -- results are yielded as they return instead of materializing the entire list at once. The trade for lower memory footprint is marginal CPU overhead (more thread coordination and sorting out-of-order results on-the-fly). A sequence of ``ExecutionResult(success, result_or_exc)`` namedtuples is returned in the same order that the statements were passed in. If ``success`` is :const:`False`, there was an error executing the statement, and ``result_or_exc`` will be an :class:`Exception`. If ``success`` is :const:`True`, ``result_or_exc`` will be the query result. Example usage:: select_statement = session.prepare(""SELECT * FROM users WHERE id=?"") statements_and_params = [] for user_id in user_ids: params = (user_id, ) statements_and_params.append((select_statement, params)) results = execute_concurrent( session, statements_and_params, raise_on_first_error=False) for (success, result) in results: if not success: handle_error(result) # result will be an Exception else: process_user(result[0]) # result will be a list of rows Note: in the case that `generators` are used, it is important to ensure the consumers do not block or attempt further synchronous requests, because no further IO will be processed until the consumer returns. This may also produce a deadlock in the IO event thread.",0,1,1,1,3,1,0,0,1,2
"def from_raw_profile_info(cls, raw_profile, profile_name, cli_vars,
  user_cfg=None, target_override=None,
  threads_override=None):
  target_name, profile_data = cls.render_profile(
  raw_profile, profile_name, target_override, cli_vars
  )
  threads = profile_data.pop('threads', DEFAULT_THREADS)
  if threads_override is not None:
  threads = threads_override
  credentials = cls._credentials_from_profile(
  profile_data, profile_name, target_name
  )
  return cls.from_credentials(
  credentials=credentials,
  profile_name=profile_name,
  target_name=target_name,
  threads=threads,
  user_cfg=user_cfg
  )","Create a profile from its raw profile information. (this is an intermediate step, mostly useful for unit testing) :param raw_profile dict: The profile data for a single profile, from disk as yaml and its values rendered with jinja. :param profile_name str: The profile name used. :param cli_vars dict: The command-line variables passed as arguments, as a dict. :param user_cfg Optional[dict]: The global config for the user, if it was present. :param target_override Optional[str]: The target to use, if provided on the command line. :param threads_override Optional[str]: The thread count to use, if provided on the command line. :raises DbtProfileError: If the profile is invalid or missing, or the target could not be found :returns Profile: The new Profile object.",0,0,0,1,1,1,0,0,1,2
"def add(self, name, path=None, **kwargs):
  path = path or kwargs.pop('default_path', None)
  if not self._path_is_valid(path):
  return
  if not self._is_unique(name, path):
  p = Project.select().where(
  (Project.name == name) |
  (Project.path == path)
  )[0]
  self._print(self._ERROR_PROJECT_EXISTS.format(name, p.path), 'red')
  return
  Project.create(name=name, path=path)
  self._print(self._SUCCESS_PROJECT_ADDED.format(name), 'green')","add new project with given name and path to database if the path is not given, current working directory will be taken ...as default",1,0,1,1,3,1,1,1,1,4
"def fetcharrowbatches(self, strings_as_dictionary=False, adaptive_integers=False):
  self._assert_valid_result_set()
  if _has_arrow_support():
  from turbodbc_arrow_support import make_arrow_result_set
  rs = make_arrow_result_set(
  self.impl.get_result_set(),
  strings_as_dictionary,
  adaptive_integers)
  first_run = True
  while True:
  table = rs.fetch_next_batch()
  is_empty_batch = (len(table) == 0)
  if is_empty_batch and not first_run:
  return
  first_run = False
  yield table
  else:
  raise Error(_NO_ARROW_SUPPORT_MSG)","Fetches rows in the active result set generated with ``execute()`` or ``executemany()`` as an iterable of arrow tables. :param strings_as_dictionary: If true, fetch string columns as dictionary[string] instead of a plain string column. :param adaptive_integers: If true, instead of the integer type returned by the database (driver), this produce integer columns with the smallest possible integer type in which all values can be stored. Be aware that here the type depends on the resulting data. :return: generator of ``pyarrow.Table``",1,0,1,1,3,1,0,1,1,3
"def setUser(self, user_or_username):
  user = None
  userid = None
  if isinstance(user_or_username, types.StringTypes):
  userid = user_or_username
  user = api.user.get(userid)
  if hasattr(user_or_username, ""getId""):
  userid = user_or_username.getId()
  user = user_or_username
  if user is None:
  return False
  return self._linkUser(user)","Link the user to the Contact :returns: True if OK, False if the User could not be linked :rtype: bool",2,1,1,0,4,1,0,1,1,3
"def login(self, username, password):
  data = self._authenticate(
  grant_type='password',
  username=username,
  password=password,
  client_id=self._client_id,
  client_secret=self._client_secret
  )
  self._username = username
  self._password = password
  return data",Authenticate with the server. :param username: The username of an existing user :param password: The password for the user :return: The authentication response from the REST endpoint,1,0,0,1,2,1,0,0,1,2
"def status(self, migration_rqst_id="""", block_name="""", dataset="""", user=""""):
  try:
  return self.dbsMigrate.listMigrationRequests(migration_rqst_id,
  block_name, dataset, user)
  except dbsException as de:
  dbsExceptionHandler(de.eCode, de.message, self.logger.exception, de.serverError)
  except Exception as ex:
  sError = ""DBSMigrateModle/status. %s\n Exception trace: \n %s."" \
  % (ex, traceback.format_exc() )
  if hasattr(ex, 'status') and ex.status == 400:
 dbsExceptionHandler('dbsException-invalid-input2', str(ex), self.logger.exception, sError)
  else:
 dbsExceptionHandler('dbsException-server-error', str(ex), self.logger.exception, sError)","Interface to query status of a migration request In this preference order of input parameters : migration_rqst_id, block, dataset, user (if multi parameters are provided, only the precedence order is followed)",1,0,1,1,3,1,0,0,1,2
"def open(port=None, serial_number=None):
  if port is None and serial_number is None:
  dev = Aardvark()
  elif serial_number is not None:
  for d in find_devices():
  if d['serial_number'] == serial_number:
  break
  else:
  _raise_error_if_negative(ERR_UNABLE_TO_OPEN)
  dev = Aardvark(d['port'])
  if dev.unique_id_str() != serial_number:
  dev.close()
  _raise_error_if_negative(ERR_UNABLE_TO_OPEN)
  else:
  dev = Aardvark(port)
  return dev","Open an aardvark device and return an :class:`Aardvark` object. If the device cannot be opened an :class:`IOError` is raised. The `port` can be retrieved by :func:`find_devices`. Usually, the first device is 0, the second 1, etc. If you are using only one device, you can therefore omit the parameter in which case 0 is used. Another method to open a device is to use the serial number. You can either find the number on the device itself or in the in the corresponding USB property. The serial number is a string which looks like `NNNN-MMMMMMM`. Raises an :class:`IOError` if the port (or serial number) does not exist, is already connected or an incompatible device is found. .. note:: There is a small chance that this function raises an :class:`IOError` although the correct device is available and not opened. The open-by-serial-number method works by scanning the devices. But as explained in :func:`find_devices`, the returned information may be outdated. Therefore, :func:`open` checks the serial number once the device is opened and if it is not the expected one, raises :class:`IOError`. No retry mechanism is implemented. As long as nobody comes along with a better idea, this failure case is up to the user.",2,0,0,0,2,1,0,0,1,2
"def get_file_for_id(_id, language=DEFAULT_LANG):
  file_start = '%s-' % _id
  json_path = DBVuln.get_json_path(language=language)
  for _file in os.listdir(json_path):
  if _file.startswith(file_start):
  return os.path.join(json_path, _file)
  raise NotFoundException('No data for ID %s' % _id)","Given _id, search the DB for the file which contains the data :param _id: The id to search (int) :param language: The user's language (en, es, etc.) :return: The filename",0,0,0,1,1,1,0,1,1,3
"def grant_permission_to_users(self, permission, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.grant_permission_to_users_with_http_info(permission, **kwargs)
  else:
  (data) = self.grant_permission_to_users_with_http_info(permission, **kwargs)
  return data","Grants a specific user permission to multiple users # noqa: E501 # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.grant_permission_to_users(permission, async_req=True) >>> result = thread.get() :param async_req bool :param str permission: Permission to grant to the users. Please note that 'host_tag_management' is the equivalent of the 'Source Tag Management' permission (required) :param list[str] body: list of users which should be revoked by specified permission :return: UserModel If the method is called asynchronously, returns the request thread.",1,0,0,1,2,1,0,0,1,2
"def update_user_display_name(user,**kwargs):
  try:
  user_i = db.DBSession.query(User).filter(User.id==user.id).one()
  user_i.display_name = user.display_name
  return user_i
  except NoResultFound:
  raise ResourceNotFoundError(""User (id=%s) not found""%(user.id))",Update a user's display name,1,1,1,1,4,1,1,1,1,4
"def get_tgt_for(user):
  if not settings.CAS_PROXY_CALLBACK:
  raise CasConfigException(""No proxy callback set in settings"")
  try:
  return Tgt.objects.get(username=user.username)
  except ObjectDoesNotExist:
  logger.warning('No ticket found for user {user}'.format(
  user=user.username
  ))
  raise CasTicketException(""no ticket found for user "" + user.username)",Fetch a ticket granting ticket for a given user. :param user: UserObj :return: TGT or Exepction,1,0,1,1,3,1,0,1,1,3
"def calculate_rate(country_code, exception_name):
  if not country_code or not isinstance(country_code, str_cls) or len(country_code) != 2:
  raise ValueError('Invalidly formatted country code')
  if exception_name and not isinstance(exception_name, str_cls):
  raise ValueError('Exception name is not None or a string')
  country_code = country_code.upper()
  if country_code not in rates.BY_COUNTRY:
  return (Decimal('0.0'), country_code, None)
  country_info = rates.BY_COUNTRY[country_code]
  if not exception_name:
  return (country_info['rate'], country_code, None)
  if exception_name not in country_info['exceptions']:
  raise ValueError('""%s"" is not a valid exception for %s' % (exception_name, country_code))
  rate_info = country_info['exceptions'][exception_name]
  if isinstance(rate_info, Decimal):
  rate = rate_info
  else:
  rate, country_code, exception_name = rate_info
  return (rate, country_code, exception_name)","Calculates the VAT rate for a customer based on their declared country and any declared exception information. :param country_code: The two-character country code where the user resides :param exception_name: The name of an exception for the country, as returned from vat_moss.declared_residence.options() :raises: ValueError - if country_code is not two characers, or exception_name is not None or a valid exception from options() :return: A tuple of (Decimal VAT rate, country_code, exception name [or None])",1,0,0,1,2,1,0,0,1,2
"def reload(self, client=None):
  client = self._require_client(client)
  query_params = self._query_params
  query_params[""projection""] = ""noAcl""
  api_response = client._connection.api_request(
  method=""GET"",
  path=self.path,
  query_params=query_params,
  headers=self._encryption_headers(),
  _target_object=self,
  )
  self._set_properties(api_response)","Reload properties from Cloud Storage. If :attr:`user_project` is set, bills the API request to that project. :type client: :class:`~google.cloud.storage.client.Client` or ``NoneType`` :param client: the client to use. If not passed, falls back to the ``client`` stored on the current object.",1,0,0,1,2,1,0,0,1,2
"def login(
  cls, username=None, password=None, requests_session=None,
  rate_limit=None
  ):
  requests_session = requests_session or requests.Session()
  session = cls(requests_session, rate_limit)
  username = username or settings.USERNAME
  password = password or settings.PASSWORD
  session.do_login(username, password)
  return session","Get a session that has authenticated with okcupid.com. If no username and password is supplied, the ones stored in :class:`okcupyd.settings` will be used. :param username: The username to log in with. :type username: str :param password: The password to log in with. :type password: str :param rate_limit: Average time in seconds to wait between requests to OKC. :type rate_limit: float",2,0,0,1,3,1,0,0,1,2
"def get_or_create_user(self, username, ldap_user):
  model = self.get_user_model()
  username_field = getattr(model, 'USERNAME_FIELD', 'username')
  kwargs = {
  username_field + '__iexact': username,
  'defaults': {username_field: username.lower()}
  }
  return model.objects.get_or_create(**kwargs)","This must return a (User, created) 2-tuple for the given LDAP user. username is the Django-friendly username of the user. ldap_user.dn is the user's DN and ldap_user.attrs contains all of their LDAP attributes.",1,1,1,0,3,1,1,1,1,4
"def generate(self,
  key=""india"",
  host=""india.futuresystems.org"",
  username=None,
  force=False,
  verbose=False):
  data = {
  ""host"": host,
  ""key"": key,
  ""username"": username
  }
  if verbose and key in self.names():
  Console.error(""{key} already in ~/.ssh/config"".format(**data), traceflag=False)
  return """"
  else:
  entry = dedent(.format(**data))
  try:
  with open(self.filename, ""a"") as config_ssh:
  config_ssh.write(entry)
  config_ssh.close()
  self.load()
  if verbose:
  Console.ok(""Added india to ~/.ssh/config"")
  except Exception as e:
  if verbose:
  Console.error(e.message)",adds a host to the config file with given parameters. #TODO: make sure this is better documented :param key: the key :param host: the host :param username: the username :param force: not used :param verbose: prints debug messages :return:,1,0,0,1,2,1,0,0,1,2
"def parse_date_input(value):
  try:
  limit = parse_date(value)
  except ValueError:
  limit = None
  if limit is None:
  raise ValueError(""'{}' is not a valid date."".format(value))
  limit = datetime(limit.year, limit.month, limit.day)
  if settings.USE_TZ:
  limit = make_aware(limit)
  return limit",Return datetime based on the user's input. @param value: User's input @type value: str @raise ValueError: If the input is not valid. @return: Datetime of the beginning of the user's date.,1,0,0,1,2,1,0,0,1,2
"def find_config_file(file_name, extra_path=None, load_user=True):
  paths = []
  if extra_path is not None:
  paths.append(extra_path)
  if os.getenv(ENVAR.CONFIG):
  paths.append(os.getenv(ENVAR.CONFIG))
  if os.getenv(ENVAR.VIRT):
  paths.append(os.path.join(os.getenv(ENVAR.VIRT), USER_DIR))
  if load_user:
  paths.append(os.path.expanduser('~/' + USER_DIR))
  paths.append(ROOT_DIR)
  for path in paths:
  if os.path.isdir(path) and os.path.exists(os.path.join(path, file_name)):
  f = os.path.join(path, file_name)
  return f
  raise ConfigurationError(
  ""Failed to find configuration file '{}'. Looked for : {} "".format(file_name, paths))","Find a configuration file in one of these directories, tried in this order: - A path provided as an argument - A path specified by the AMBRY_CONFIG environmenal variable - ambry in a path specified by the VIRTUAL_ENV environmental variable - ~/ambry - /etc/ambry :param file_name: :param extra_path: :param load_user: :param path: :return:",0,0,0,1,1,1,0,0,1,2
"def permission_required(perm):
  def _(f):
  def __(request, *args, **kwargs):
  if request.user.has_perm(perm):
  return f(request, *args, **kwargs)
  else:
  return response({}, 401, 'Unauthorized')
  return __
  return _","A json pendant to permission_required. Will return a 401 response if the user is not allowed. The body of the response will be the following json data:: { ""status"": 401, ""message"": ""Unauthorized"", ""data"": {} } Example:: import jason @jason.permission_required(""my_perm"") def my_view(request): ...",1,0,0,1,2,1,0,0,1,2
"def get_partitioned_view_result(self, partition_key, ddoc_id, view_name,
  raw_result=False, **kwargs):
  ddoc = DesignDocument(self, ddoc_id)
  view = View(ddoc, view_name, partition_key=partition_key)
  return self._get_view_result(view, raw_result, **kwargs)","Retrieves the partitioned view result based on the design document and view name. See :func:`~cloudant.database.CouchDatabase.get_view_result` method for further details. :param str partition_key: Partition key. :param str ddoc_id: Design document id used to get result. :param str view_name: Name of the view used to get result. :param bool raw_result: Dictates whether the view result is returned as a default Result object or a raw JSON response. Defaults to False. :param kwargs: See :func:`~cloudant.database.CouchDatabase.get_view_result` method for available keyword arguments. :returns: The result content either wrapped in a QueryResult or as the raw response JSON content. :rtype: QueryResult, dict",1,0,1,0,2,1,0,0,1,2
"def remove_account_user_from_groups(self, account_id, user_id, body, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('asynchronous'):
  return self.remove_account_user_from_groups_with_http_info(account_id, user_id, body, **kwargs)
  else:
  (data) = self.remove_account_user_from_groups_with_http_info(account_id, user_id, body, **kwargs)
  return data","Remove user from groups. # noqa: E501 An endpoint for removing user from groups. **Example usage:** `curl -X DELETE https://api.us-east-1.mbedcloud.com/v3/accounts/{accountID}/users/{user-id}/groups -d '[0162056a9a1586f30242590700000000,0117056a9a1586f30242590700000000]' -H 'content-type: application/json' -H 'Authorization: Bearer API_KEY'` # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass asynchronous=True >>> thread = api.remove_account_user_from_groups(account_id, user_id, body, asynchronous=True) >>> result = thread.get() :param asynchronous bool :param str account_id: Account ID. (required) :param str user_id: The ID of the user to be removed from the group. (required) :param list[str] body: A list of IDs of the groups to be updated. (required) :return: UpdatedResponse If the method is called asynchronously, returns the request thread.",1,0,0,1,2,1,0,0,1,2
"def CleanClientVersions(clients=None, dry_run=True, token=None):
  if not clients:
  index = client_index.CreateClientIndex(token=token)
  clients = index.LookupClients(["".""])
  clients.sort()
  with data_store.DB.GetMutationPool() as pool:
  logging.info(""checking %d clients"", len(clients))
  client_infos = data_store.DB.MultiResolvePrefix(
  clients, ""aff4:type"", data_store.DB.ALL_TIMESTAMPS)
  for client, type_list in client_infos:
  logging.info(""%s: has %d versions"", client, len(type_list))
  cleared = 0
  kept = 1
  last_kept = type_list[0][2]
  for _, _, ts in type_list[1:]:
  if last_kept - ts > 60 * 60 * 1000000:
  last_kept = ts
  kept += 1
  else:
  if not dry_run:
  pool.DeleteAttributes(client, [""aff4:type""], start=ts, end=ts)
  cleared += 1
  if pool.Size() > 10000:
  pool.Flush()
  logging.info(""%s: kept %d and cleared %d"", client, kept, cleared)","A script to remove excessive client versions. Especially when a client is heavily cloned, we sometimes write an excessive number of versions of it. Since these version all go into the same database row and are displayed as a dropdown list in the adminui, it is sometimes necessary to clear them out. This deletes version from clients so that we have at most one version per hour. Args: clients: A list of ClientURN, if empty cleans all clients. dry_run: whether this is a dry run token: datastore token.",1,1,1,0,3,1,1,1,1,4
"def delete(self, config_file=None):
  path_to_remove = config_file or _DEFAULT_PATH
  try:
  os.remove(path_to_remove)
  print('Credentials at {} successfully removed.'.format(
  path_to_remove))
  except OSError as err:
  warnings.warn('No credential file found at {}.'.format(
  path_to_remove))","Deletes the credentials file specified in `config_file`. If no file is specified, it deletes the default user credential file. Args: config_file (str): Path to configuration file. Defaults to delete the user default location if `None`. .. Tip:: To see if there is a default user credential file stored, do the following:: >>> creds = Credentials() >>> print(creds) Credentials(username=eschbacher, key=abcdefg, base_url=https://eschbacher.carto.com/)",1,0,0,1,2,1,0,0,1,2
"def index(self, record):
  index, doc_type = self.record_to_index(record)
  return self.client.index(
  id=str(record.id),
  version=record.revision_id,
  version_type=self._version_type,
  index=index,
  doc_type=doc_type,
  body=self._prepare_record(record, index, doc_type),
  )",Index a record. The caller is responsible for ensuring that the record has already been committed to the database. If a newer version of a record has already been indexed then the provided record will not be indexed. This behavior can be controlled by providing a different ``version_type`` when initializing ``RecordIndexer``. :param record: Record instance.,0,0,0,1,1,0,0,0,1,1
"def generate_login_url(self, scopes=None, redirect_uri=None):
  url = self.base_url + ""/oauth/authorize""
  split = list(urlparse(url))
  params = {
  ""client_id"": self.client_id,
  ""response_type"": ""code"",
  }
  if scopes:
  params[""scopes""] = OAuthScopes.serialize(scopes)
  if redirect_uri:
  params[""redirect_uri""] = redirect_uri
  split[4] = urlencode(params)
  return urlunparse(split)","Generates a url to send users so that they may authenticate to this application. This url is suitable for redirecting a user to. For example, in `Flask`_, a login route might be implemented like this:: @app.route(""/login"") def begin_oauth_login(): login_client = LinodeLoginClient(client_id, client_secret) return redirect(login_client.generate_login_url()) .. _Flask:: http://flask.pocoo.org :param scopes: The OAuth scopes to request for this login. :type scopes: list :param redirect_uri: The requested redirect uri. The login service enforces that this is under the registered redirect path. :type redirect_uri: str :returns: The uri to send users to for this login attempt. :rtype: str",1,0,0,1,2,2,0,0,1,3
"def key_values(cls, *key_fields, **kwargs):
  flat = kwargs.pop('flat', False)
  assert not kwargs, ""'flat' is the only kwarg accepted""
  key_fields = key_fields or cls.KEY_FIELDS
  cache_key = cls.key_values_cache_key_name(*key_fields)
  cached = cache.get(cache_key)
  if cached is not None:
  return cached
  values = list(cls.objects.values_list(*key_fields, flat=flat).order_by().distinct())
  cache.set(cache_key, values, cls.cache_timeout)
  return values","Get the set of unique values in the configuration table for the given key[s]. Calling cls.current(*value) for each value in the resulting list should always produce an entry, though any such entry may have enabled=False. Arguments: key_fields: The positional arguments are the KEY_FIELDS to return. For example if you had a course embargo configuration where each entry was keyed on (country, course), then you might want to know ""What countries have embargoes configured?"" with cls.key_values('country'), or ""Which courses have country restrictions?"" with cls.key_values('course'). You can also leave this unspecified for the default, which returns the distinct combinations of all keys. flat: If you pass flat=True as a kwarg, it has the same effect as in Django's 'values_list' method: Instead of returning a list of lists, you'll get one list of values. This makes sense to use whenever there is only one key being queried. Return value: List of lists of each combination of keys found in the database. e.g. [(""Italy"", ""course-v1:SomeX+some+2015""), ...] for the course embargo example",0,0,1,0,1,1,0,1,0,2
"def createuser(self, name, username, password, email, **kwargs):
  data = {'name': name, 'username': username, 'password': password, 'email': email}
  if kwargs:
  data.update(kwargs)
  request = requests.post(
  self.users_url, headers=self.headers, data=data,
  verify=self.verify_ssl, auth=self.auth, timeout=self.timeout)
  if request.status_code == 201:
  return request.json()
  elif request.status_code == 404:
  return False","Create a user :param name: Obligatory :param username: Obligatory :param password: Obligatory :param email: Obligatory :param kwargs: Any param the the Gitlab API supports :return: True if the user was created,false if it wasn't(already exists)",1,0,0,1,2,1,0,0,2,3
"def calculate_affinity(self, username):
  scores = self.comparison(username)
  if len(scores) <= 10:
  raise NoAffinityError(""Shared rated anime count between ""
  ""`{}` and `{}` is less than eleven""
  .format(self._base_user, username))
  values = scores.values()
  scores1, scores2 = list(zip(*values))
  pearson = calcs.pearson(scores1, scores2)
  pearson *= 100
  if self._round is not False:
  pearson = round(pearson, self._round)
  return models.Affinity(affinity=pearson, shared=len(scores))",,1,0,0,1,2,1,0,0,1,2
"def search(self):
  self.__interrupt_search()
  search_pattern = self.Search_comboBox.currentText()
  replacement_pattern = self.Replace_With_comboBox.currentText()
  if not search_pattern:
  return False
  SearchAndReplace.insert_pattern(search_pattern, self.__search_patterns_model)
  SearchAndReplace.insert_pattern(replacement_pattern, self.__replace_with_patterns_model)
  location = umbra.ui.common.parse_location(
  foundations.strings.to_string(self.Where_lineEdit.text()) or
  self.__targets_format.format(self.__default_target))
  self.__ignore_hidden_files and location.filters_out.append(""\\\.|/\."")
  settings = self.__get_settings()
  self.__search_worker_thread = Search_worker(self, search_pattern, location, settings)
  self.__search_worker_thread.searchFinished.connect(self.__search_worker_thread__searchFinished)
  self.__container.engine.worker_threads.append(self.__search_worker_thread)
  self.__container.engine.start_processing(""Searching In Files ..."")
  self.__search_worker_thread.start()
  return True",Searchs user defined locations for search pattern. :return: Method success. :rtype: bool,1,0,0,1,2,1,0,0,1,2
"def by_id(cls, user_id, db_session=None):
  db_session = get_db_session(db_session)
  query = db_session.query(cls.model)
  query = query.filter(cls.model.id == user_id)
  query = query.options(sa.orm.eagerload(""groups""))
  return query.first()",fetch user by user id :param user_id: :param db_session: :return:,0,0,1,1,2,1,0,1,0,2
"def solve(self, x0, params=(), internal_x0=None, solver=None, attached_solver=None, **kwargs):
  if not isinstance(solver, (tuple, list)):
  solver = [solver]
  if not isinstance(attached_solver, (tuple, list)):
  attached_solver = [attached_solver] + [None]*(len(solver) - 1)
  _x0, self.internal_params = self.pre_process(x0, params)
  for solv, attached_solv in zip(solver, attached_solver):
  if internal_x0 is not None:
  _x0 = internal_x0
  elif self.internal_x0_cb is not None:
  _x0 = self.internal_x0_cb(x0, params)
  nfo = self._get_solver_cb(solv, attached_solv)(_x0, **kwargs)
  _x0 = nfo['x'].copy()
  self.internal_x = _x0
  x0 = self.post_process(self.internal_x, self.internal_params)[0]
  return x0, nfo","Solve with user specified ``solver`` choice. Parameters ---------- x0: 1D array of floats Guess (subject to ``self.post_processors``) params: 1D array_like of floats Parameters (subject to ``self.post_processors``) internal_x0: 1D array of floats When given it overrides (processed) ``x0``. ``internal_x0`` is not subject to ``self.post_processors``. solver: str or callable or None or iterable of such if str: uses _solve_``solver``(\*args, \*\*kwargs). if ``None``: chooses from PYNEQSYS_SOLVER environment variable. if iterable: chain solving. attached_solver: callable factory Invokes: solver = attached_solver(self). Returns ------- array: solution vector (post-processed by self.post_processors) dict: info dictionary containing 'success', 'nfev', 'njev' etc. Examples -------- >>> neqsys = NeqSys(2, 2, lambda x, p: [ ... (x[0] - x[1])**p[0]/2 + x[0] - 1, ... (x[1] - x[0])**p[0]/2 + x[1] ... ]) >>> x, sol = neqsys.solve([1, 0], [3], solver=(None, 'mpmath')) >>> assert sol['success'] >>> print(x) [0.841163901914009663684741869855] [0.158836098085990336315258130144]",0,0,0,1,1,1,0,0,1,2
"def param_converter(*decorator_args, **decorator_kwargs):
  def wrapped(fn):
  @wraps(fn)
  def decorated(*view_args, **view_kwargs):
  view_kwargs = _convert_models(view_kwargs, decorator_kwargs)
  view_kwargs = _convert_query_params(view_kwargs, decorator_kwargs)
  return fn(*view_args, **view_kwargs)
  return decorated
  if decorator_args and callable(decorator_args[0]):
  return wrapped(decorator_args[0])
  return wrapped","Call with the url parameter names as keyword argument keys, their values being the model to convert to. Models will be looked up by the url param names. If a url param name is prefixed with the snake-cased model name, the prefix will be stripped. If a model isn't found, abort with a 404. The action's argument names must match the snake-cased model names. For example:: @bp.route('/users/<int:user_id>/posts/<int:id>') @param_converter(user_id=User, id=Post) def show_post(user, post): # the param converter does the database lookups: # user = User.query.filter_by(id=user_id).first() # post = Post.query.filter_by(id=id).first() # and calls the decorated action: show_post(user, post) # or to customize the argument names passed to the action: @bp.route('/users/<int:user_id>/posts/<int:post_id>') @param_converter(user_id={'user_arg_name': User}, post_id={'post_arg_name': Post}) def show_post(user_arg_name, post_arg_name): # do stuff ... Also supports parsing arguments from the query string. For query string keyword arguments, use a lookup (dict, Enum) or callable:: @bp.route('/users/<int:id>') @param_converter(id=User, foo=str, optional=int) def show_user(user, foo, optional=10): # GET /users/1?foo=bar # calls show_user(user=User.get(1), foo='bar')",1,0,1,1,3,1,0,0,1,2
"def add_expect_string_healthcheck(self, expect_string):
  healthcheck_map = dict()
  healthcheck_map['expect_string'] = expect_string
  url = 'healthcheckexpect/add/expect_string/'
  code, xml = self.submit({'healthcheck': healthcheck_map}, 'POST', url)
  return self.response(code, xml)",Inserts a new healthckeck_expect with only expect_string. :param expect_string: expect_string. :return: Dictionary with the following structure: :: {'healthcheck_expect': {'id': < id >}} :raise InvalidParameterError: The value of expect_string is invalid. :raise HealthCheckExpectJaCadastradoError: There is already a healthcheck_expect registered with the same data. :raise HealthCheckExpectNaoExisteError: Healthcheck_expect not registered. :raise DataBaseError: Networkapi failed to access the database. :raise XMLError: Networkapi failed to generate the XML response.,1,1,0,2,4,1,0,0,1,2
"def remove(self):
  title = '%s.remove' % self.__class__.__name__
  for id, name, mimetype in self._list_directory():
  try:
  self.drive.delete(fileId=id).execute()
  except Exception as err:
  if str(err).find('File not found') > -1:
  pass
  else:
  raise DriveConnectionError(title)
  insert = 'collection'
  if self.collection_name:
  insert = self.collection_name
  exit_msg = 'Contents of %s will be removed from Google Drive.' % insert
  return exit_msg","a method to remove all records in the collection NOTE: this method removes all the files in the collection, but the collection folder itself created by oauth2 cannot be removed. only the user can remove access to the app folder :return: string with confirmation of deletion",1,0,0,1,2,1,1,0,1,3
"def taql(command, style='Python', tables=[], globals={}, locals={}):
  cmd = command
  tabs = []
  for tab in tables:
  tabs += [tab]
  try:
  import casacore.util
  if len(locals) == 0:
  locals = casacore.util.getlocals(3)
  cmd = casacore.util.substitute(cmd, [(table, '', tabs)],
  globals, locals)
  except Exception:
  pass
  if style:
  cmd = 'using style ' + style + ' ' + cmd
  tab = table(cmd, tabs, _oper=2)
  result = tab._getcalcresult()
  if len(result) == 0:
  return tab
  return result['values']","Execute a TaQL command and return a table object. A `TaQL <../../doc/199.html>`_ command is an SQL-like command to do a selection of rows and/or columns in a table. The default style used in a TaQL command is python, which means 0-based indexing, C-ordered arrays, and non-inclusive end in ranges. It is possible to use python variables directly in the command using `$var` where `var` is the name of the variable to use. For example:: t = table('3c343.MS') value = 5.1 t1 = taql('select from $t where COL > $value') In this example the table `$t` is replaced by a sequence number (such as `$1`) and `$value` by its value 5.1. The table object of `t` will be appended to a copy of the `tables` argument such that the sequence number inserted matches the table object in the list. The more advanced user can already use `$n` in the query string and supply the associated table object in the `tables` argument (where `n` represents the (n-1)th `tables` element). The :func:`query` command makes use of this feature. The arguments `globals` and `locals` can be used to pass in a dict containing the possible variables used in the TaQL command. They can be obtained with the python functions locals() and globals(). If `locals` is empty, the local variables in the calling function will be used, so normally one does not need to use these arguments.",1,0,0,1,2,1,0,0,1,2
"def select_uoa(i):
  se=i.get('skip_enter','')
  lst=i.get('choices',[])
  if i.get('skip_sort','')!='yes':
  klst=sorted(lst, key=lambda v: v['data_uoa'])
  else:
  klst=lst
  zz={}
  iz=0
  for z1 in klst:
  z=z1['data_uid']
  zu=z1['data_uoa']
  zs=str(iz)
  zz[zs]=z
  out(zs+') '+zu+' ('+z+')')
  iz+=1
  out('')
  y='Select UOA'
  if se!='yes': y+=' (or press Enter for 0)'
  y+=': '
  rx=inp({'text':y})
  x=rx['string'].strip()
  if x=='' and se!='yes': x='0'
  if x not in zz:
  return {'return':1, 'error':'number is not recognized'}
  dduoa=zz[x]
  return {'return':0, 'choice':dduoa}","Input: { choices - list from search function (skip_enter) - if 'yes', do not select 0 when user presses Enter (skip_sort) - if 'yes', do not sort array } Output: { return - return code = 0, if successful > 0, if error (error) - error text if return > 0 choice - data UOA }",1,0,0,1,2,1,0,0,1,2
"def _try_close_dirty_tabs(self, exept=None):
  widgets, filenames = self._collect_dirty_tabs(exept=exept)
  if not len(filenames):
  return True
  dlg = DlgUnsavedFiles(self, files=filenames)
  if dlg.exec_() == dlg.Accepted:
  if not dlg.discarded:
  for item in dlg.listWidget.selectedItems():
  filename = item.text()
  widget = None
  for widget in widgets:
  if widget.file.path == filename:
  break
  if widget != exept:
  self._save_editor(widget)
  self.removeTab(self.indexOf(widget))
  return True
  return False",Tries to close dirty tabs. Uses DlgUnsavedFiles to ask the user what he wants to do.,0,0,0,1,1,1,0,0,1,2
"def create_or_update_user(self, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.create_or_update_user_with_http_info(**kwargs)
  else:
  (data) = self.create_or_update_user_with_http_info(**kwargs)
  return data","Creates or updates a user # noqa: E501 # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.create_or_update_user(async_req=True) >>> result = thread.get() :param async_req bool :param bool send_email: Whether to send email notification to the user, if created. Default: false :param UserToCreate body: Example Body: <pre>{ \""emailAddress\"": \""user@example.com\"", \""groups\"": [ \""user_management\"" ], \""userGroups\"": [ \""8b23136b-ecd2-4cb5-8c92-62477dcc4090\"" ] }</pre> :return: UserModel If the method is called asynchronously, returns the request thread.",1,1,0,2,4,1,1,0,1,3
"def add_auth_attempt(self, auth_type, successful, **kwargs):
  entry = {'timestamp': datetime.utcnow(),
  'auth': auth_type,
  'id': uuid.uuid4(),
  'successful': successful}
  log_string = ''
  for key, value in kwargs.iteritems():
  if key == 'challenge' or key == 'response':
  entry[key] = repr(value)
  else:
  entry[key] = value
  log_string += '{0}:{1}, '.format(key, value)
  self.login_attempts.append(entry)",:param username: :param password: :param auth_type: possible values: plain: plaintext username/password :return:,1,0,0,0,1,1,0,0,1,2
"def authenticate(self, username, password):
  r = self._query_('/users/authenticate', 'POST',
  params={'username': username,
  'password': password})
  if r.status_code == 201:
  return r.text.strip('""')
  else:
  raise ValueError('Authentication invalid.')",Authenticates your user and returns an auth token. :param str username: Hummingbird username. :param str password: Hummingbird password. :returns: str -- The Auth Token :raises: ValueError -- If the Authentication is wrong,2,0,0,1,3,2,0,0,1,3
"def prompt(message, default=None, strip=True, suffix=' '):
  if default is not None:
  prompt_text = ""{0} [{1}]{2}"".format(message, default, suffix)
  else:
  prompt_text = ""{0}{1}"".format(message, suffix)
  input_value = get_input(prompt_text)
  if input_value and strip:
  input_value = input_value.strip()
  if not input_value:
  input_value = default
  return input_value",Print a message and prompt user for input. Return user input.,1,0,0,1,2,1,0,0,1,2
"def get_community_badge_progress(self, steamID, badgeID, format=None):
  parameters = {'steamid' : steamID, 'badgeid' : badgeID}
  if format is not None:
  parameters['format'] = format
  url = self.create_request_url(self.interface, 'GetCommunityBadgeProgress', 1,
  parameters)
  data = self.retrieve_request(url)
  return self.return_data(data, format=format)","Gets all the quests needed to get the specified badge, and which are completed. steamID: The users ID badgeID: The badge we're asking about format: Return format. None defaults to json. (json, xml, vdf)",2,0,0,1,3,2,0,0,1,3
"def set(self, name, value, feature_group=None, user=None, **kwargs):
  path = '%s/%s' % (self.path, name.replace('/', '%2F'))
  data = {'value': value, 'feature_group': feature_group, 'user': user}
  server_data = self.gitlab.http_post(path, post_data=data, **kwargs)
  return self._obj_cls(self, server_data)",Create or update the object. Args: name (str): The value to set for the object value (bool/int): The value to set for the object feature_group (str): A feature group name user (str): A GitLab username **kwargs: Extra options to send to the server (e.g. sudo) Raises: GitlabAuthenticationError: If authentication is not correct GitlabSetError: If an error occured Returns: obj: The created/updated attribute,1,0,0,2,3,1,0,0,2,3
"def login(self, username, password, login_token=None):
  if login_token is None:
  token_doc = self.post(action='query', meta='tokens', type='login')
  login_token = token_doc['query']['tokens']['logintoken']
  login_doc = self.post(
  action=""clientlogin"", username=username, password=password,
  logintoken=login_token, loginreturnurl=""http://example.org/"")
  if login_doc['clientlogin']['status'] == ""UI"":
  raise ClientInteractionRequest.from_doc(
  login_token, login_doc['clientlogin'])
  elif login_doc['clientlogin']['status'] != 'PASS':
  raise LoginError.from_doc(login_doc['clientlogin'])
  return login_doc['clientlogin']","Authenticate with the given credentials. If authentication is successful, all further requests sent will be signed the authenticated user. Note that passwords are sent as plaintext. This is a limitation of the Mediawiki API. Use a https host if you want your password to be secure :Parameters: username : str The username of the user to be authenticated password : str The password of the user to be authenticated :Raises: :class:`mwapi.errors.LoginError` : if authentication fails :class:`mwapi.errors.ClientInteractionRequest` : if authentication requires a continue_login() call :class:`mwapi.errors.APIError` : if the API responds with an error",2,0,0,1,3,2,0,0,1,3
"def compute_log_likelihood(ll_func, parameters, data=None, cl_runtime_info=None):
  def get_cl_function():
  nmr_params = parameters.shape[1]
  if len(parameters.shape) > 2:
  return SimpleCLFunction.from_string( + str(nmr_params) + + str(parameters.shape[2]) + + str(nmr_params) + + str(parameters.shape[2]) + + ll_func.get_cl_function_name() + , dependencies=[ll_func])
  return SimpleCLFunction.from_string( + ll_func.get_cl_function_name() + , dependencies=[ll_func])
  kernel_data = {'data': data,
  'parameters': Array(parameters, 'mot_float_type', mode='r')}
  shape = parameters.shape
  if len(shape) > 2:
  kernel_data.update({
  'log_likelihoods': Zeros((shape[0], shape[2]), 'mot_float_type'),
  })
  else:
  kernel_data.update({
  'log_likelihoods': Zeros((shape[0],), 'mot_float_type'),
  })
  get_cl_function().evaluate(kernel_data, parameters.shape[0], use_local_reduction=True,
  cl_runtime_info=cl_runtime_info)
  return kernel_data['log_likelihoods'].get_data()","Calculate and return the log likelihood of the given model for the given parameters. This calculates the log likelihoods for every problem in the model (typically after optimization), or a log likelihood for every sample of every model (typically after sample). In the case of the first (after optimization), the parameters must be an (d, p) array for d problems and p parameters. In the case of the second (after sample), you must provide this function with a matrix of shape (d, p, n) with d problems, p parameters and n samples. Args: ll_func (mot.lib.cl_function.CLFunction): The log-likelihood function. A CL function with the signature: .. code-block:: c double <func_name>(local const mot_float_type* const x, void* data); parameters (ndarray): The parameters to use in the evaluation of the model. This is either an (d, p) matrix or (d, p, n) matrix with d problems, p parameters and n samples. data (mot.lib.kernel_data.KernelData): the user provided data for the ``void* data`` pointer. cl_runtime_info (mot.configuration.CLRuntimeInfo): the runtime information Returns: ndarray: per problem the log likelihood, or, per problem and per sample the log likelihood.",0,0,0,1,1,1,0,0,1,2
"def authenticate_http_request(token=None):
  if token:
  auth = token
  else:
  auth = request.headers.get('Authorization', None)
  if not auth:
  auth = request.cookies.get('token', None)
  if auth:
  auth = unquote_plus(auth)
  log.debug(""Validating Auth header [%s]"" % auth)
  if not auth:
  raise AuthMissingHeaderError('There is no Authorization header in the HTTP request')
  parts = auth.split()
  if parts[0].lower() != 'bearer':
  raise AuthInvalidTokenError('Authorization header must start with Bearer')
  elif len(parts) == 1:
  raise AuthInvalidTokenError('Token not found in Authorization header')
  elif len(parts) > 2:
  raise AuthInvalidTokenError('Authorization header must be Bearer + \s + token')
  token = parts[1]
  return load_auth_token(token)","Validate auth0 tokens passed in the request's header, hence ensuring that the user is authenticated. Code copied from: https://github.com/auth0/auth0-python/tree/master/examples/flask-api Return a PntCommonException if failed to validate authentication. Otherwise, return the token's payload (Also stored in stack.top.current_user)",2,0,0,1,3,1,0,0,1,2
"def query(question, values, default=None, list_values = False, ignorecase = True ):
  values = list(values)
  for i in range(len(values)):
  if not isinstance(values[i], dict):
  values[i] = {'values': [values[i]]}
  try:
  import readline, rlcomplete
  wordlist = [ str(v) for value in values
  for v in value['values']]
  completer = rlcomplete.ListCompleter(wordlist, ignorecase)
  readline.parse_and_bind(""tab: complete"")
  readline.set_completer(completer.complete)
  except ImportError:
  pass
  valuelist = []
  for item in values:
  entry = ( display('bright', item.get('fg'), item.get('bg')) +
  str(item['values'][0]) + display(['default']) )
  if str(item['values'][0]) == str(default): entry = '['+entry+']'
  if list_values: entry += ' : ' + item['desc']
  valuelist.append(entry)
  if list_values: question += os.linesep + os.linesep.join(valuelist) + os.linesep
  else: question += ' (' + '/'.join(valuelist) + ')'
  return input_object(question, cast = query_cast, default=default,
  castarg=[values,ignorecase])","Preset a few options The question argument is a string, nothing magical. The values argument accepts input in two different forms. The simpler form (a tuple with strings) looks like: .. code-block:: python ('Male','Female') And it will pop up a question asking the user for a gender and requiring the user to enter either 'male' or 'female' (case doesn't matter unless you set the third arguement to false). The other form is something like: .. code-block:: python ({'values':('Male','M'),'fg':'cyan'}, {'values':('Female','F'),'fg':'magenta'}) This will pop up a question with Male/Female (each with appropriate colouring). Additionally, if the user types in just 'M', it will be treated as if 'Male' was typed in. The first item in the 'values' tuple is treated as default and is the one that is returned by the function if the user chooses one in that group. In addition the function can handle non-string objects quite fine. It simple displays the output object.__str__() and compares the user's input against that. So the the code .. code-block:: python query(""Python rocks? "",(True, False)) will return a bool (True) when the user types in the string 'True' (Of course there isn't any other reasonable answer than True anyways :P) ``default`` is the value function returns if the user types nothing in. This is can be used to cancel the input so-to-speek Using list_values = False will display a list, with descriptions printed out from the 'desc' keyword",1,0,0,1,2,1,0,0,1,2
"def requestFields(self, field_names, required=False, strict=False):
  if isinstance(field_names, basestring):
  raise TypeError('Fields should be passed as a list of '
  'strings (not %r)' % (type(field_names),))
  for field_name in field_names:
  self.requestField(field_name, required, strict=strict)",Add the given list of fields to the request @param field_names: The simple registration data fields to request @type field_names: [str] @param required: Whether these values should be presented to the user as required @param strict: whether to raise an exception when a field is added to a request more than once @raise ValueError: when a field requested is not a simple registration field or strict is set and a field was requested more than once,1,0,0,0,1,1,0,0,1,2
"def read_custom_resource_definition(self, name, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.read_custom_resource_definition_with_http_info(name, **kwargs)
  else:
  (data) = self.read_custom_resource_definition_with_http_info(name, **kwargs)
  return data","read the specified CustomResourceDefinition This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.read_custom_resource_definition(name, async_req=True) >>> result = thread.get() :param async_req bool :param str name: name of the CustomResourceDefinition (required) :param str pretty: If 'true', then the output is pretty printed. :param bool exact: Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. Deprecated. Planned for removal in 1.18. :param bool export: Should this value be exported. Export strips fields that a user can not specify. Deprecated. Planned for removal in 1.18. :return: V1beta1CustomResourceDefinition If the method is called asynchronously, returns the request thread.",1,0,0,1,2,1,0,0,1,2
"def from_json(cls, json_file, json_path=None, key_mapping=None):
  cls._validate_key_mapping(key_mapping)
  with open(json_file, ""rb"") as f:
  data = json.loads(f.read().decode(""utf-8""))
  return cls._from_json_data(data, json_path, key_mapping)","Load connection credential from json file. :param json_file: str, path to json file :param json_path: str, dot notation of the path to the credential dict. :param key_mapping: dict, map 'host', 'port', 'database', 'username', 'password' to custom alias, for example ``{'host': 'h', 'port': 'p', 'database': 'db', 'username': 'user', 'password': 'pwd'}``. This params are used to adapt any json data. :rtype: :return: Example: Your json file:: { ""credentials"": { ""db1"": { ""h"": ""example.com"", ""p"": 1234, ""db"": ""test"", ""user"": ""admin"", ""pwd"": ""admin"", }, ""db2"": { ... } } } Usage:: cred = Credential.from_json( ""path-to-json-file"", ""credentials.db1"", dict(host=""h"", port=""p"", database=""db"", username=""user"", password=""pwd"") )",1,0,0,1,2,1,0,0,1,2
"def set_context(self, cell_type):
  if self.model is None:
  return
  monomer_names = [m.name for m in self.model.monomers]
  res = context_client.get_protein_expression(monomer_names, [cell_type])
  amounts = res.get(cell_type)
  if not amounts:
  logger.warning('Could not get context for %s cell type.' %
  cell_type)
  self.add_default_initial_conditions()
  return
  self.set_expression(amounts)","Set protein expression amounts from CCLE as initial conditions. This method uses :py:mod:`indra.databases.context_client` to get protein expression levels for a given cell type and set initial conditions for Monomers in the model accordingly. Parameters ---------- cell_type : str Cell type name for which expression levels are queried. The cell type name follows the CCLE database conventions. Example: LOXIMVI_SKIN, BT20_BREAST",1,0,0,0,1,1,0,0,1,2
"def _get_pgtiou(pgt):
  pgtIou = None
  retries_left = 5
  if not settings.CAS_PGT_FETCH_WAIT:
  retries_left = 1
  while not pgtIou and retries_left:
  try:
  return PgtIOU.objects.get(tgt=pgt)
  except PgtIOU.DoesNotExist:
  if settings.CAS_PGT_FETCH_WAIT:
  time.sleep(1)
  retries_left -= 1
  logger.info('Did not fetch ticket, trying again. {tries} tries left.'.format(
  tries=retries_left
  ))
  raise CasTicketException(""Could not find pgtIou for pgt %s"" % pgt)","Returns a PgtIOU object given a pgt. The PgtIOU (tgt) is set by the CAS server in a different request that has completed before this call, however, it may not be found in the database by this calling thread, hence the attempt to get the ticket is retried for up to 5 seconds. This should be handled some better way. Users can opt out of this waiting period by setting CAS_PGT_FETCH_WAIT = False :param: pgt",0,0,1,0,1,1,0,1,1,3
"def complete_login(self, request, app, token, **kwargs):
  headers = {'Authorization': 'Bearer ' + token.token}
  userinfo_response = requests.get(
  self.profile_url,
  headers=headers,
  )
  userinfo_response.raise_for_status()
  extra_data = userinfo_response.json()['user']
  if userinfo_response.json()['audience'] != app.client_id:
  raise ProviderException(
  'Dataporten returned a user with an audience field \
  which does not correspond to the client id of the \
  application.'
  )
  return self.get_provider().sociallogin_from_response(
  request,
  extra_data,
  )","Arguments: request - The get request to the callback URL /accounts/dataporten/login/callback. app - The corresponding SocialApp model instance token - A token object with access token given in token.token Returns: Should return a dict with user information intended for parsing by the methods of the DataportenProvider view, i.e. extract_uid(), extract_extra_data(), and extract_common_fields()",2,0,0,1,3,2,0,0,1,3
"def auth(self, request):
  callback_url = self.callback_url(request)
  request_token = Pocket.get_request_token(consumer_key=self.consumer_key, redirect_uri=callback_url)
  request.session['request_token'] = request_token
  auth_url = Pocket.get_auth_url(code=request_token, redirect_uri=callback_url)
  return auth_url",let's auth the user to the Service :param request: request object :return: callback url :rtype: string that contains the url to redirect after auth,2,0,0,2,4,2,0,0,1,3
"def QueryPermissions(self, user_link, query, options=None):
  if options is None:
  options = {}
  path = base.GetPathFromLink(user_link, 'permissions')
  user_id = base.GetResourceIdOrFullNameFromLink(user_link)
  def fetch_fn(options):
  return self.__QueryFeed(path,
  'permissions',
  user_id,
  lambda r: r['Permissions'],
  lambda _, b: b,
  query,
  options), self.last_response_headers
  return query_iterable.QueryIterable(self, query, options, fetch_fn)",Queries permissions for a user. :param str user_link: The link to the user entity. :param (str or dict) query: :param dict options: The request options for the request. :return: Query Iterable of Permissions. :rtype: query_iterable.QueryIterable,2,0,0,1,3,2,0,1,1,4
"def _query(self, request, filter=None):
  limit = None
  offset = None
  if 'maxfeatures' in request.params:
  limit = int(request.params['maxfeatures'])
  if 'limit' in request.params:
  limit = int(request.params['limit'])
  if 'offset' in request.params:
  offset = int(request.params['offset'])
  if filter is None:
  filter = create_filter(request, self.mapped_class, self.geom_attr)
  query = self.Session().query(self.mapped_class)
  if filter is not None:
  query = query.filter(filter)
  order_by = self._get_order_by(request)
  if order_by is not None:
  query = query.order_by(order_by)
  query = query.limit(limit).offset(offset)
  return query.all()","Build a query based on the filter and the request params, and send the query to the database.",0,0,2,1,3,1,0,1,1,3
"def match(self, path, method):
  if path != '/':
  path = path.rstrip('/')
  method = method.upper()
  status = 404
  for p, n, m in self.endpoints:
  matched, url_vars = match_path(p, path)
  if not matched:
  continue
  if method not in m:
  status = 405
  raise HTTPError(status=status, body=f'Method not found: {path} {method}')
  callback, type_hints = m[method]
  type_matched, typed_url_vars = match_url_vars_type(url_vars, type_hints)
  if not type_matched:
  continue
  return callback, typed_url_vars
  raise HTTPError(status=status, body=f'Not found: {path}')","Get callback and url_vars. >>> from kobin import Response >>> r = Router() >>> def view(user_id: int) -> Response: ... return Response(f'You are {user_id}') ... >>> r.add('/users/{user_id}', 'GET', 'user-detail', view) >>> callback, url_vars = r.match('/users/1', 'GET') >>> url_vars {'user_id': 1} >>> response = callback(**url_vars) >>> response.body [b'You are 1'] >>> callback, url_vars = r.match('/notfound', 'GET') Traceback (most recent call last): ... kobin.responses.HTTPError",2,0,0,1,3,1,0,0,1,2
"def get_x(self, rows=None, cols=None):
  if rows is None:
  rows = list(range(0, self.get_sample_size()))
  if cols is None:
  cols = list(range(0, self.get_dimensionality()))
  if not hasattr(rows, ""__iter__""):
  rows = [rows]
  rows = sorted(list(set(rows)))
  retValue = self.X[rows, :]
  if len(rows) == 0:
  return []
  return retValue[:, cols]","Returns the input data requested by the user @ In, rows, a list of non-negative integers specifying the row indices to return @ In, cols, a list of non-negative integers specifying the column indices to return @ Out, a matrix of floating point values specifying the input data values filtered by the two input parameters.",0,0,0,1,1,1,0,0,1,2
"def __add_bootstrap_tour_step(self, message, selector=None, name=None,
  title=None, alignment=None, duration=None):
  if selector != ""html"":
  selector = self.__make_css_match_first_element_only(selector)
  element_row = ""element: '%s',"" % selector
  else:
  element_row = """"
  if not duration:
  duration = ""0""
  else:
  duration = str(float(duration) * 1000.0)
  step = ( % (element_row, title, message, alignment, duration))
  self._tour_steps[name].append(step)","Allows the user to add tour steps for a website. @Params message - The message to display. selector - The CSS Selector of the Element to attach to. name - If creating multiple tours at the same time, use this to select the tour you wish to add steps to. title - Additional header text that appears above the message. alignment - Choose from ""top"", ""bottom"", ""left"", and ""right"". (""top"" is the default alignment). duration - (Bootstrap Tours ONLY) The amount of time, in seconds, before automatically advancing to the next tour step.",1,0,0,0,1,1,0,0,1,2
"def getAsWkt(self, session):
  statement = .format(self.geometryColumnName,
  self.tableName,
  self.id)
  result = session.execute(statement)
  for row in result:
  return row.wkt",Retrieve the geometry in Well Known Text format. This method is a veneer for an SQL query that calls the ``ST_AsText()`` function on the geometry column. Args: session (:mod:`sqlalchemy.orm.session.Session`): SQLAlchemy session object bound to PostGIS enabled database. Returns: str: Well Known Text string representation of geometry.,0,0,1,1,2,1,0,1,0,2
"def show_terms_if_not_agreed(context, field=TERMS_HTTP_PATH_FIELD):
  request = context['request']
  url = urlparse(request.META[field])
  not_agreed_terms = TermsAndConditions.get_active_terms_not_agreed_to(request.user)
  if not_agreed_terms and is_path_protected(url.path):
  return {'not_agreed_terms': not_agreed_terms, 'returnTo': url.path}
  else:
  return {}","Displays a modal on a current page if a user has not yet agreed to the given terms. If terms are not specified, the default slug is used. A small snippet is included into your template if a user who requested the view has not yet agreed the terms. The snippet takes care of displaying a respective modal.",1,0,0,1,2,1,0,0,1,2
"def _update_plugin(package_name):
  try:
  update_json_log = plugin_install(package_name)
  except RuntimeError, exception:
  if 'CondaHTTPError' in str(exception):
  raise IOError('Error accessing update server.')
  else:
  raise
  if update_json_log.get('success'):
  if 'actions' in update_json_log:
  actions = update_json_log['actions']
  update_json_log['old_versions'] = actions.get('UNLINK', [])
  update_json_log['new_versions'] = actions.get('LINK', [])
  else:
  version_dict = ch.package_version(package_name)
  update_json_log.update(version_dict)
  return update_json_log","Update plugin (no user interface). .. versionadded:: 0.19 Parameters ---------- package_name : str, optional Conda MicroDrop plugin package name, e.g., `microdrop.mr-box-plugin`. Returns ------- dict If plugin was updated successfully, output will *at least* contain items with the keys ``old_versions`` and ``new_versions``. If no update was available, output will *at least* contain an item with the key ``version`` and will **not** contain the keys ``old_versions`` and ``new_versions``. Raises ------ IOError If Conda update server cannot be reached, e.g., if there is no network connection available. See also -------- _update_plugin_ui",2,0,0,1,3,1,0,0,1,2
"def oauth_authentication_flow(client_id, client_secret, scopes=None,
  protocol=None, **kwargs):
  credentials = (client_id, client_secret)
  protocol = protocol or MSGraphProtocol()
  con = Connection(credentials, scopes=protocol.get_scopes_for(scopes),
  **kwargs)
  consent_url = con.get_authorization_url(**kwargs)
  print('Visit the following url to give consent:')
  print(consent_url)
  token_url = input('Paste the authenticated url here: ')
  if token_url:
  result = con.request_token(token_url, **kwargs)
  if result:
  print('Authentication Flow Completed. Oauth Access Token Stored. '
  'You can now use the API.')
  else:
  print('Something go wrong. Please try again.')
  return bool(result)
  else:
  print('Authentication Flow aborted.')
  return False","A helper method to perform the OAuth2 authentication flow. Authenticate and get the oauth token :param str client_id: the client_id :param str client_secret: the client_secret :param list[str] scopes: a list of protocol user scopes to be converted by the protocol or raw scopes :param Protocol protocol: the protocol to be used. Defaults to MSGraphProtocol :param kwargs: other configuration to be passed to the Connection instance, connection.get_authorization_url or connection.request_token :return: Success or Failure :rtype: bool",2,0,0,1,3,2,0,0,1,3
"def browse_dialog_dir():
  _go_to_package()
  logger_directory.info(""enter browse_dialog"")
  _path_bytes = subprocess.check_output(['python', 'gui_dir_browse.py'], shell=False)
  _path = _fix_path_bytes(_path_bytes, file=False)
  if len(_path) >= 1:
  _path = _path[0]
  else:
  _path = """"
  logger_directory.info(""chosen path: {}"".format(_path))
  logger_directory.info(""exit browse_dialog"")
  return _path",Open up a GUI browse dialog window and let to user pick a target directory. :return str: Target directory path,1,0,0,1,2,1,0,0,1,2
"def delete_invitation(self, invitation_id, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('asynchronous'):
  return self.delete_invitation_with_http_info(invitation_id, **kwargs)
  else:
  (data) = self.delete_invitation_with_http_info(invitation_id, **kwargs)
  return data","Delete a user invitation. # noqa: E501 An endpoint for deleting an active user invitation which has been sent for a new or an existing user to join the account. **Example usage:** `curl -X DELETE https://api.us-east-1.mbedcloud.com/v3/user-invitations/{invitation-id} -H 'Authorization: Bearer API_KEY'` # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass asynchronous=True >>> thread = api.delete_invitation(invitation_id, asynchronous=True) >>> result = thread.get() :param asynchronous bool :param str invitation_id: The ID of the invitation to be deleted. (required) :return: None If the method is called asynchronously, returns the request thread.",1,0,0,1,2,1,0,0,1,2
"def query_file_location(question, default_address):
  while True:
  if default_address == None:
  prompt = '{}:'.format(question, default_address)
  else:
  prompt = '{} [{}]'.format(question, default_address)
  sys.stdout.write(prompt)
  input_address = raw_input()
  if default_address is not None and input_address == '':
  input_address = default_address
  if os.path.isfile(input_address):
  return input_address
  else:
  print 'sorry no file was found at that location\n'","This function asks for a location file address from the command terminal it checks if the file exists before proceeding with the code ""question"" is a string that is presented to the user. ""default_address"" is the presumed file location.",1,0,0,1,2,1,0,0,1,2
"def setup_dashboard_panels_visibility_registry(section_name):
  registry_info = get_dashboard_registry_record()
  role_permissions_list = []
  roles = []
  acl_users = get_tool(""acl_users"")
  roles_tree = acl_users.portal_role_manager.listRoleIds()
  for role in roles_tree:
  roles.append(role)
  for role in roles:
  role_permissions_list.append(role)
  visible = 'no'
  if role in ['LabManager', 'Manager']:
  visible = 'yes'
  role_permissions_list.append(visible)
  role_permissions = ','.join(role_permissions_list)
  registry_info[get_unicode(section_name)] = get_unicode(role_permissions)
  set_dashboard_registry_record(registry_info)
  return registry_info","Initializes the values for panels visibility in registry_records. By default, only users with LabManager or Manager roles can see the panels. :param section_name: :return: An string like: ""role1,yes,role2,no,rol3,no""",0,1,1,0,2,1,0,1,1,3
"def pre_save(self, instance, add: bool):
  slugs = LocalizedValue()
  for lang_code, value in self._get_populate_values(instance):
  if not value:
  continue
  if self.include_time:
  value += '-%s' % datetime.now().microsecond
  def is_unique(slug: str, language: str) -> bool:
  unique_filter = {
  '%s__%s' % (self.name, language): slug
  }
  return not type(instance).objects.filter(**unique_filter).exists()
  slug = self._make_unique_slug(
  slugify(value, allow_unicode=True),
  lang_code,
  is_unique
  )
  slugs.set(lang_code, slug)
  setattr(instance, self.name, slugs)
  return slugs","Ran just before the model is saved, allows us to built the slug. Arguments: instance: The model that is being saved. add: Indicates whether this is a new entry to the database or an update.",0,1,0,0,1,1,1,1,1,4
"def cubehelix_palette(n_colors=6, start=0, rot=.4, gamma=1.0, hue=0.8,
  light=.85, dark=.15, reverse=False, as_cmap=False):
  cdict = mpl._cm.cubehelix(gamma, start, rot, hue)
  cmap = mpl.colors.LinearSegmentedColormap(""cubehelix"", cdict)
  x = np.linspace(light, dark, n_colors)
  pal = cmap(x)[:, :3].tolist()
  if reverse:
  pal = pal[::-1]
  if as_cmap:
  x_256 = np.linspace(light, dark, 256)
  if reverse:
  x_256 = x_256[::-1]
  pal_256 = cmap(x_256)
  cmap = mpl.colors.ListedColormap(pal_256)
  return cmap
  else:
  return pal","Make a sequential palette from the cubehelix system. This produces a colormap with linearly-decreasing (or increasing) brightness. That means that information will be preserved if printed to black and white or viewed by someone who is colorblind. ""cubehelix"" is also availible as a matplotlib-based palette, but this function gives the user more control over the look of the palette and has a different set of defaults. Parameters ---------- n_colors : int Number of colors in the palette. start : float, 0 <= start <= 3 The hue at the start of the helix. rot : float Rotations around the hue wheel over the range of the palette. gamma : float 0 <= gamma Gamma factor to emphasize darker (gamma < 1) or lighter (gamma > 1) colors. hue : float, 0 <= hue <= 1 Saturation of the colors. dark : float 0 <= dark <= 1 Intensity of the darkest color in the palette. light : float 0 <= light <= 1 Intensity of the lightest color in the palette. reverse : bool If True, the palette will go from dark to light. as_cmap : bool If True, return a matplotlib colormap instead of a list of colors. Returns ------- palette : list or colormap References ---------- Green, D. A. (2011). ""A colour scheme for the display of astronomical intensity images"". Bulletin of the Astromical Society of India, Vol. 39, p. 289-295.",0,0,0,1,1,1,0,0,1,2
"def promote_chat_member(self, chat_id, user_id, can_change_info=None, can_post_messages=None,
  can_edit_messages=None, can_delete_messages=None, can_invite_users=None,
  can_restrict_members=None, can_pin_messages=None, can_promote_members=None):
  return apihelper.promote_chat_member(self.token, chat_id, user_id, can_change_info, can_post_messages,
  can_edit_messages, can_delete_messages, can_invite_users,
  can_restrict_members, can_pin_messages, can_promote_members)","Use this method to promote or demote a user in a supergroup or a channel. The bot must be an administrator in the chat for this to work and must have the appropriate admin rights. Pass False for all boolean parameters to demote a user. Returns True on success. :param chat_id: Unique identifier for the target chat or username of the target channel ( in the format @channelusername) :param user_id: Int : Unique identifier of the target user :param can_change_info: Bool: Pass True, if the administrator can change chat title, photo and other settings :param can_post_messages: Bool : Pass True, if the administrator can create channel posts, channels only :param can_edit_messages: Bool : Pass True, if the administrator can edit messages of other users, channels only :param can_delete_messages: Bool : Pass True, if the administrator can delete messages of other users :param can_invite_users: Bool : Pass True, if the administrator can invite new users to the chat :param can_restrict_members: Bool: Pass True, if the administrator can restrict, ban or unban chat members :param can_pin_messages: Bool: Pass True, if the administrator can pin messages, supergroups only :param can_promote_members: Bool: Pass True, if the administrator can add new administrators with a subset of his own privileges or demote administrators that he has promoted, directly or indirectly (promoted by administrators that were appointed by him) :return:",1,0,0,2,3,1,0,0,2,3
"def copy_database_data(self, source, destination, optimized=False):
  self.enable_printing = False
  self.change_db(source)
  tables = self.tables
  if optimized:
  self._copy_database_data_serverside(source, destination, tables)
  else:
  self._copy_database_data_clientside(tables, source, destination)
  self.enable_printing = True",Copy the data from one database to another. Retrieve existing data from the source database and insert that data into the destination database.,0,1,0,0,1,1,1,0,0,2
"def prompt_for_value(self, ctx):
  default = self.get_default(ctx)
  if isinstance(default, AutoDefault):
  return self.type_cast_value(ctx, default.value)
  if self.is_bool_flag:
  return confirm(self.prompt, default)
  return prompt(self.prompt, default=default,
  hide_input=self.hide_input,
  confirmation_prompt=self.confirmation_prompt,
  value_proc=lambda x: self.process_value(ctx, x))",This is an alternative flow that can be activated in the full value processing if a value does not exist. It will prompt the user until a valid value exists and then returns the processed value as result.,1,0,0,1,2,1,0,0,1,2
"def update_data(ctx, models=True, pickles=False, f=False):
  if pickles:
  save_sql_to_files(f)
  if models:
  if f:
  manage(ctx, 'create_models_from_sql --force True', env={})
  else:
  manage(ctx, 'create_models_from_sql', env={})",Updates local django db projects and pickle files using salic database from MinC Pickles are saved in /data/raw/ from sql queries in /data/scripts/ Models are created from /data/scripts/models/,1,1,1,0,3,1,1,0,0,2
"def split_role_content(role_rawsource):
  parts = {
  'last_component': False,
  'display': None,
  'ref': None
  }
  if role_rawsource.startswith('~'):
  parts['last_component'] = True
  role_rawsource = role_rawsource.lstrip('~')
  match = ROLE_DISPLAY_PATTERN.match(role_rawsource)
  if match:
  parts['display'] = match.group('display').strip()
  parts['ref'] = match.group('reference').strip()
  else:
  parts['display'] = None
  parts['ref'] = role_rawsource.strip()
  return parts","Split the ``rawsource`` of a role into standard components. Parameters ---------- role_rawsource : `str` The content of the role: its ``rawsource`` attribute. Returns ------- parts : `dict` Dictionary with keys: ``last_component`` (`bool`) If `True`, the display should show only the last component of a namespace. The user signals this by prefixing the role's content with a ``~`` character. ``display`` (`str`) Custom display content. See Examples. ``ref`` (`str`) The reference content. If the role doesn't have a custom display, the reference will be the role's content. The ``ref`` never includes a ``~`` prefix. Examples -------- >>> split_role_role('Tables <lsst.afw.table.Table>') {'last_component': False, 'display': 'Tables', 'ref': 'lsst.afw.table.Table'} >>> split_role_role('~lsst.afw.table.Table') {'last_component': True, 'display': None, 'ref': 'lsst.afw.table.Table'}",0,0,0,1,1,1,0,0,1,2
"def base_url(self, base_url=None):
  if base_url:
  if urlparse(base_url).scheme != 'https':
  raise ValueError(
  '`base_url`s need to be over `https`. Update your '
  '`base_url`.'
 )
  self._base_url = base_url
  else:
  return self._base_url","Return or set `base_url`. Args: base_url (str, optional): If set, updates the `base_url`. Otherwise returns current `base_url`. Note: This does not update the `username` attribute. Separately update the username with ``Credentials.username`` or update `base_url` and `username` at the same time with ``Credentials.set``. Example: .. code:: >>> from cartoframes import Credentials # load credentials saved in previous session >>> creds = Credentials() # returns current base_url >>> creds.base_url() 'https://eschbacher.carto.com/' # updates base_url with new value >>> creds.base_url('new_base_url')",1,0,0,1,2,1,0,0,1,2
"def estimator_status_send(self, time_usec, flags, vel_ratio, pos_horiz_ratio, pos_vert_ratio, mag_ratio, hagl_ratio, tas_ratio, pos_horiz_accuracy, pos_vert_accuracy, force_mavlink1=False):
  return self.send(self.estimator_status_encode(time_usec, flags, vel_ratio, pos_horiz_ratio, pos_vert_ratio, mag_ratio, hagl_ratio, tas_ratio, pos_horiz_accuracy, pos_vert_accuracy), force_mavlink1=force_mavlink1)","Estimator status message including flags, innovation test ratios and estimated accuracies. The flags message is an integer bitmask containing information on which EKF outputs are valid. See the ESTIMATOR_STATUS_FLAGS enum definition for further information. The innovaton test ratios show the magnitude of the sensor innovation divided by the innovation check threshold. Under normal operation the innovaton test ratios should be below 0.5 with occasional values up to 1.0. Values greater than 1.0 should be rare under normal operation and indicate that a measurement has been rejected by the filter. The user should be notified if an innovation test ratio greater than 1.0 is recorded. Notifications for values in the range between 0.5 and 1.0 should be optional and controllable by the user. time_usec : Timestamp (micros since boot or Unix epoch) (uint64_t) flags : Integer bitmask indicating which EKF outputs are valid. See definition for ESTIMATOR_STATUS_FLAGS. (uint16_t) vel_ratio : Velocity innovation test ratio (float) pos_horiz_ratio : Horizontal position innovation test ratio (float) pos_vert_ratio : Vertical position innovation test ratio (float) mag_ratio : Magnetometer innovation test ratio (float) hagl_ratio : Height above terrain innovation test ratio (float) tas_ratio : True airspeed innovation test ratio (float) pos_horiz_accuracy : Horizontal position 1-STD accuracy relative to the EKF local origin (m) (float) pos_vert_accuracy : Vertical position 1-STD accuracy relative to the EKF local origin (m) (float)",0,0,0,1,1,1,0,0,1,2
"def load(self, config_template, config_file=None):
  if config_file is None:
  config_file = config_template
  config_path = build_config_file_path(config_file)
  template_path = os.path.join(os.path.dirname(__file__),
  config_template)
  self._copy_template_to_config(template_path, config_path)
  return self._load_template_or_config(template_path, config_path)","Read the config file if it exists, else read the default config. Creates the user config file if it doesn't exist using the template. :type config_template: str :param config_template: The config template file name. :type config_file: str :param config_file: (Optional) The config file name. If None, the config_file name will be set to the config_template. :rtype: :class:`configobj.ConfigObj` :return: The config information for reading and writing.",1,0,0,1,2,1,0,0,1,2
"def create_session(user):
  def cb():
  if user:
  if __options__.get(""require_email_verification"") and not user.email_verified:
  raise exceptions.VerifyEmailError()
  if flask_login.login_user(user):
  user.update(last_login_at=utc_now())
  return user
  return None
  return signals.user_login(cb)",Create the login session :param user: UserModel :return:,1,1,1,0,3,1,0,0,1,2
"def by_mail(self, mail, archived=False, limit=None, page=None):
  path = partial(_path, self.adapter)
  if not archived:
  path = _path(self.adapter)
  else:
  path = _path(self.adapter, 'archived')
  return self._get(path, email=mail, limit=limit, page=page)",lookup users by mail address. you may do in two ways: - specify one address: foo@bar.com - specify a domain : @bar.com both will return a list. the role 'Zeiterfasser' may not lookup users.,1,0,0,1,2,1,0,0,1,2
"def get_current_and_head_revision(
  database_url: str,
  alembic_config_filename: str,
  alembic_base_dir: str = None,
  version_table: str = DEFAULT_ALEMBIC_VERSION_TABLE) -> Tuple[str, str]:
  head_revision = get_head_revision_from_alembic(
  alembic_config_filename=alembic_config_filename,
  alembic_base_dir=alembic_base_dir,
  version_table=version_table
  )
  log.info(""Intended database version: {}"", head_revision)
  current_revision = get_current_revision(
  database_url=database_url,
  version_table=version_table
  )
  log.info(""Current database version: {}"", current_revision)
  return current_revision, head_revision","Returns a tuple of ``(current_revision, head_revision)``; see :func:`get_current_revision` and :func:`get_head_revision_from_alembic`. Arguments: database_url: SQLAlchemy URL for the database alembic_config_filename: config filename alembic_base_dir: directory to start in, so relative paths in the config file work. version_table: table name for Alembic versions",1,0,1,1,3,1,0,0,0,1
"def _text_checker(job, interval, _interval_set=False, quiet=False, output=sys.stdout):
  status = job.status()
  msg = status.value
  prev_msg = msg
  msg_len = len(msg)
  if not quiet:
  print('\r%s: %s' % ('Job Status', msg), end='', file=output)
  while status.name not in ['DONE', 'CANCELLED', 'ERROR']:
  time.sleep(interval)
  status = job.status()
  msg = status.value
  if status.name == 'QUEUED':
  msg += ' (%s)' % job.queue_position()
  if not _interval_set:
  interval = max(job.queue_position(), 2)
  else:
  if not _interval_set:
  interval = 2
  if len(msg) < msg_len:
  msg += ' ' * (msg_len - len(msg))
  elif len(msg) > msg_len:
  msg_len = len(msg)
  if msg != prev_msg and not quiet:
  print('\r%s: %s' % ('Job Status', msg), end='', file=output)
  prev_msg = msg
  if not quiet:
  print('', file=output)","A text-based job status checker Args: job (BaseJob): The job to check. interval (int): The interval at which to check. _interval_set (bool): Was interval time set by user? quiet (bool): If True, do not print status messages. output (file): The file like object to write status messages to. By default this is sys.stdout.",2,0,0,1,3,1,0,0,1,2
"def place(client, place_id, session_token=None, fields=None, language=None):
  params = {""placeid"": place_id}
  if fields:
  invalid_fields = set(fields) - PLACES_DETAIL_FIELDS
  if invalid_fields:
  raise ValueError(""Valid values for the `fields` param for ""
  ""`place` are '%s', these given field(s) ""
  ""are invalid: '%s'"" % (
  ""', '"".join(PLACES_DETAIL_FIELDS),
  ""', '"".join(invalid_fields)))
  params[""fields""] = convert.join_list("","", fields)
  if language:
  params[""language""] = language
  if session_token:
  params[""sessiontoken""] = session_token
  return client._request(""/maps/api/place/details/json"", params)","Comprehensive details for an individual place. :param place_id: A textual identifier that uniquely identifies a place, returned from a Places search. :type place_id: string :param session_token: A random string which identifies an autocomplete session for billing purposes. :type session_token: string :param fields: The fields specifying the types of place data to return, separated by a comma. For full details see: https://cloud.google.com/maps-platform/user-guide/product-changes/#places :type input: list :param language: The language in which to return results. :type language: string :rtype: result dict with the following keys: result: dict containing place details html_attributions: set of attributions which must be displayed",2,0,0,1,3,1,0,0,1,2
"def _get_entity_id(field_name, attrs):
  field_name_id = field_name + '_id'
  if field_name in attrs:
  if attrs[field_name] is None:
  return None
  elif 'id' in attrs[field_name]:
  return attrs[field_name]['id']
  if field_name_id in attrs:
  return attrs[field_name_id]
  else:
  raise MissingValueError(
  'Cannot find a value for the ""{0}"" field. Searched for keys named '
  '{1}, but available keys are {2}.'
  .format(field_name, (field_name, field_name_id), attrs.keys())
  )","Find the ID for a one to one relationship. The server may return JSON data in the following forms for a :class:`nailgun.entity_fields.OneToOneField`:: 'user': None 'user': {'name': 'Alice Hayes', 'login': 'ahayes', 'id': 1} 'user_id': 1 'user_id': None Search ``attrs`` for a one to one ``field_name`` and return its ID. :param field_name: A string. The name of a field. :param attrs: A dict. A JSON payload as returned from a server. :returns: Either an entity ID or None.",1,0,0,0,1,0,0,0,1,1
"def get_query(self):
  assert self.model is not None, (
  ""'{}' should include a `model` attribute, or override the `get_query()` method.""
  .format(self.__class__.__name__)
  )
  return self.request.dbsession.query(self.model)",Get the list of items for this view. You may want to override this if you need to provide different query depending on the incoming request. (Eg. return a list of items that is specific to the user) :return: ``sqlalchemy.orm.query.Query``,0,0,1,1,2,1,0,1,1,3
"def get_graph_csv():
  assert request.method == ""POST"", ""POST request expected received {}"".format(request.method)
  if request.method == ""POST"":
  try:
  selected_variable_table = request.form[""selected_variable_table""]
  filename = utils.generate_graph_csv(selected_variable_table)
  return send_file(filename, as_attachment=True, attachment_filename='{}.csv'.format(selected_variable_table))
  except Exception as e:
  logging.error(e)
  return jsonify({""0"": ""__EMPTY""})",Allows the user to download a graph's data as a CSV file. :return: show a dialog box that allows the user to download the CSV file.,1,0,0,1,2,1,0,0,1,2
"def _get_indices(values, selected, tolerance):
  idx_data = []
  idx_output = []
  for idx_of_selected, one_selected in enumerate(selected):
  if tolerance is None or values.dtype.kind == 'U':
  idx_of_data = where(values == one_selected)[0]
  else:
  idx_of_data = where(abs(values - one_selected) <= tolerance)[0]
  if len(idx_of_data) > 0:
  idx_data.append(idx_of_data[0])
  idx_output.append(idx_of_selected)
  return idx_data, idx_output","Get indices based on user-selected values. Parameters ---------- values : ndarray (any dtype) values present in the axis. selected : ndarray (any dtype) or tuple or list values selected by the user tolerance : float avoid rounding errors. Returns ------- idx_data : list of int indices of row/column to select the data idx_output : list of int indices of row/column to copy into output Notes ----- This function is probably not very fast, but it's pretty robust. It keeps the order, which is extremely important. If you use values in the self.axis, you don't need to specify tolerance. However, if you specify arbitrary points, floating point errors might affect the actual values. Of course, using tolerance is much slower. Maybe tolerance should be part of Select instead of here.",1,0,0,1,2,0,0,0,1,1
"def interruptable_popen(*args, **kwargs):
  kwargs[""preexec_fn""] = os.setsid
  p = subprocess.Popen(*args, **kwargs)
  try:
  out, err = p.communicate()
  except KeyboardInterrupt:
  os.killpg(os.getpgid(p.pid), signal.SIGTERM)
  raise
  if six.PY3:
  if out is not None:
  out = out.decode(""utf-8"")
  if err is not None:
  err = err.decode(""utf-8"")
  return p.returncode, out, err","Shorthand to :py:class:`Popen` followed by :py:meth:`Popen.communicate`. All *args* and *kwargs* are forwatded to the :py:class:`Popen` constructor. The return code, standard output and standard error are returned in a tuple. The call :py:meth:`Popen.communicate` is interruptable by the user.",1,0,0,1,2,1,0,0,1,2
"def export(self, template_file_name, output_file_name,
  sort=""public"", data=None, limit=0):
  exportedData = {}
  exportedUsers = self.__exportUsers(sort, limit)
  exportedData[""users""] = exportedUsers
  exportedData[""extraData""] = data
  with open(template_file_name) as template_file:
  template_raw = template_file.read()
  template = parse(template_raw)
  renderer = Renderer()
  output = renderer.render(template, exportedData)
  with open(output_file_name, ""w"") as text_file:
  text_file.write(output)",Export ranking to a file. Args: template_file_name (str): where is the template (moustache template) output_file_name (str): where create the file with the ranking sort (str): field to sort the users,1,0,1,0,2,1,0,0,1,2
"def statuses_show(self, id, trim_user=None, include_my_retweet=None,
  include_entities=None):
  params = {'id': id}
  set_bool_param(params, 'trim_user', trim_user)
  set_bool_param(params, 'include_my_retweet', include_my_retweet)
  set_bool_param(params, 'include_entities', include_entities)
  return self._get_api('statuses/show.json', params)","Returns a single Tweet, specified by the id parameter. https://dev.twitter.com/docs/api/1.1/get/statuses/show/%3Aid :param str id: (*required*) The numerical ID of the desired tweet. :param bool trim_user: When set to ``True``, the tweet's user object includes only the status author's numerical ID. :param bool include_my_retweet: When set to ``True``, any Tweet returned that has been retweeted by the authenticating user will include an additional ``current_user_retweet`` node, containing the ID of the source status for the retweet. :param bool include_entities: When set to ``False``, the ``entities`` node will not be included. :returns: A tweet dict.",2,0,0,1,3,2,0,0,1,3
"def retention_policy_get(database,
  name,
  user=None,
  password=None,
  host=None,
  port=None):
  client = _client(user=user, password=password, host=host, port=port)
  for policy in client.get_list_retention_policies(database):
  if policy['name'] == name:
  return policy
  return None",Get an existing retention policy. database The database to operate on. name Name of the policy to modify. CLI Example: .. code-block:: bash salt '*' influxdb08.retention_policy_get metrics default,1,0,1,1,3,1,0,0,1,2
"def proximal_composition(proximal, operator, mu):
  r
  def proximal_composition_factory(sigma):
  Id = IdentityOperator(operator.domain)
  Ir = IdentityOperator(operator.range)
  prox_muf = proximal(mu * sigma)
  return (Id +
  (1.0 / mu) * operator.adjoint * ((prox_muf - Ir) * operator))
  return proximal_composition_factory","r""""""Proximal operator factory of functional composed with unitary operator. For a functional ``F`` and a linear unitary `Operator` ``L`` this is the factory for the proximal operator of ``F * L``. Parameters ---------- proximal : callable A factory function that, when called with a step size returns the proximal operator of ``F`` operator : `Operator` The operator to compose the functional with mu : ``operator.field`` element Scalar such that ``(operator.adjoint * operator)(x) = mu * x`` Returns ------- prox_factory : function Factory for the proximal operator to be initialized Notes ----- Given a linear operator :math:`L` with the property that for a scalar :math:`\mu` .. math:: L^*(L(x)) = \mu * x and a convex function :math:`F`, the following identity holds .. math:: \mathrm{prox}_{\sigma F \circ L}(x) = x + \frac{1}{\mu} L^* \left( \mathrm{prox}_{\mu \sigma F}(Lx) - Lx \right) This factory function implements this functionality. There is no simple formula for more general operators. The function cannot verify that the operator is unitary, the user needs to verify this. For reference on the identity used, see [CP2011c]. References ---------- [CP2011c] Combettes, P L, and Pesquet, J-C. *Proximal splitting methods in signal processing.* In: Bauschke, H H, Burachik, R S, Combettes, P L, Elser, V, Luke, D R, and Wolkowicz, H. Fixed-point algorithms for inverse problems in science and engineering, Springer, 2011.",0,0,0,0,0,1,0,0,1,2
"def getData4ID(self, tablename, ID):
 fields = self._getFieldsInDB(tablename)
 SQL = % (self.store_time, self.store_time, tablename, ID)
 array_data = self.execQuery(SQL)
 if len(array_data) > 0:
 for x in range( len(fields) ):
 self.data[fields[x]] = array_data[0][x]
 self.data['date_expiration'] = array_data[0][-3]
 self.data['time_expiration'] = array_data[0][-2]
 self.data['time_computation'] = array_data[0][-1]
 return self.data",get the whole row from the database and store it in a dict,1,0,1,1,3,0,0,1,0,1
"def count_posts(self, tag=None, user_id=None, include_draft=False):
  result = 0
  with self._engine.begin() as conn:
  try:
  count_statement = sqla.select([sqla.func.count()]). \
  select_from(self._post_table)
  sql_filter = self._get_filter(tag, user_id, include_draft,
  conn)
  count_statement = count_statement.where(sql_filter)
  result = conn.execute(count_statement).scalar()
  except Exception as e:
  self._logger.exception(str(e))
  result = 0
  return result",Returns the total number of posts for the give filter :param tag: Filter by a specific tag :type tag: str :param user_id: Filter by a specific user :type user_id: str :param include_draft: Whether to include posts marked as draft or not :type include_draft: bool :return: The number of posts for the given filter.,0,0,1,1,2,1,0,1,1,3
"def display(self, data):
  value = self.config.value
  self.scene = QGraphicsScene(value['x_min'], value['y_min'],
  value['x_max'] - value['x_min'],
  value['y_max'] - value['y_min'])
  self.idx_fig.setScene(self.scene)
  self.add_grid()
  self.resizeEvent(None)
  s_freq = self.parent.traces.data.s_freq
  f, Pxx = welch(data, fs=s_freq,
  nperseg=int(min((s_freq, len(data)))))
  freq_limit = (value['x_min'] <= f) & (f <= value['x_max'])
  if self.config.value['log']:
  Pxx_to_plot = log(Pxx[freq_limit])
  else:
  Pxx_to_plot = Pxx[freq_limit]
  self.scene.addPath(Path(f[freq_limit], Pxx_to_plot),
  QPen(QColor(LINE_COLOR), LINE_WIDTH))",Make graphicsitem for spectrum figure. Parameters ---------- data : ndarray 1D vector containing the data only This function can be called by self.display_window (which reads the data for the selected channel) or by the mouse-events functions in traces (which read chunks of data from the user-made selection).,1,0,0,1,2,1,0,0,1,2
"def add_enrollment(db, uuid, organization, from_date=None, to_date=None):
  if uuid is None:
  raise InvalidValueError('uuid cannot be None')
  if uuid == '':
  raise InvalidValueError('uuid cannot be an empty string')
  if organization is None:
  raise InvalidValueError('organization cannot be None')
  if organization == '':
  raise InvalidValueError('organization cannot be an empty string')
  if not from_date:
  from_date = MIN_PERIOD_DATE
  if not to_date:
  to_date = MAX_PERIOD_DATE
  with db.connect() as session:
  uidentity = find_unique_identity(session, uuid)
  if not uidentity:
  raise NotFoundError(entity=uuid)
  org = find_organization(session, organization)
  if not org:
  raise NotFoundError(entity=organization)
  try:
  enroll_db(session, uidentity, org,
  from_date=from_date, to_date=to_date)
  except ValueError as e:
  raise InvalidValueError(e)","Enroll a unique identity to an organization. The function adds a new relationship between the unique identity identified by 'uuid' and the given 'organization'. The given identity and organization must exist prior to add this enrollment in the registry. Otherwise, a 'NotFoundError' exception will be raised. The period of the enrollment can be given with the parameters 'from_date' and 'to_date', where ""from_date <= to_date"". Default values for these dates are '1900-01-01' and '2100-01-01'. If the given enrollment data is already in the registry, the function will raise a 'AlreadyExistsError' exception. :param db: database manager :param uuid: unique identifier :param organization: name of the organization :param from_date: date when the enrollment starts :param to_date: date when the enrollment ends :raises NotFoundError: when either 'uuid' or 'organization' are not found in the registry. :raises InvalidValeError: raised in three cases, when either identity or organization are None or empty strings; when ""from_date"" < 1900-01-01 or ""to_date"" > 2100-01-01; when ""from_date > to_date"" :raises AlreadyExistsError: raised when given enrollment already exists in the registry.",1,1,1,0,3,1,1,1,1,4
"def user_timeline(self, delegate, user=None, params={}, extra_args=None):
  if user:
  params['id'] = user
  return self.__get('/statuses/user_timeline.xml', delegate, params,
  txml.Statuses, extra_args=extra_args)","Get the most recent updates for a user. If no user is specified, the statuses for the authenticating user are returned. See search for example of how results are returned.",1,0,1,1,3,2,0,0,1,3
"def update_user(self, user_id, body, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('asynchronous'):
  return self.update_user_with_http_info(user_id, body, **kwargs)
  else:
  (data) = self.update_user_with_http_info(user_id, body, **kwargs)
  return data","Update user details. # noqa: E501 An endpoint for updating user details. **Example usage:** `curl -X PUT https://api.us-east-1.mbedcloud.com/v3/users/{user-id} -d '{\""username\"": \""myusername\""}' -H 'content-type: application/json' -H 'Authorization: Bearer API_KEY'` # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass asynchronous=True >>> thread = api.update_user(user_id, body, asynchronous=True) >>> result = thread.get() :param asynchronous bool :param str user_id: The ID of the user whose details are updated. (required) :param UserUpdateReq body: A user object with attributes. (required) :return: UserInfoResp If the method is called asynchronously, returns the request thread.",2,1,0,1,4,1,0,0,1,2
"def GetDate(text=None, selected=None, **kwargs):
  args = ['--date-format=%d/%m/%Y']
  if text:
  args.append('--text=%s' % text)
  if selected:
  args.append('--day=%d' % selected.day)
  args.append('--month=%d' % selected.month)
  args.append('--year=%d' % selected.year)
  for generic_args in kwargs_helper(kwargs):
  args.append('--%s=%s' % generic_args)
  p = run_zenity('--calendar', *args)
  if p.wait() == 0:
  retval = p.stdout.read().strip()
  day, month, year = [int(x) for x in retval.split('/')]
  return date(year, month, day)","Prompt the user for a date. This will raise a Zenity Calendar Dialog for the user to pick a date. It will return a datetime.date object with the date or None if the user hit cancel. text - Text to be displayed in the calendar dialog. selected - A datetime.date object that will be the pre-selected date. kwargs - Optional command line parameters for Zenity such as height, width, etc.",1,0,0,1,2,1,0,0,1,2
"def available(software=True,
  drivers=True,
  summary=False,
  skip_installed=True,
  skip_hidden=True,
  skip_mandatory=False,
  skip_reboot=False,
  categories=None,
  severities=None,):
  wua = salt.utils.win_update.WindowsUpdateAgent()
  updates = wua.available(
  skip_hidden=skip_hidden, skip_installed=skip_installed,
  skip_mandatory=skip_mandatory, skip_reboot=skip_reboot,
  software=software, drivers=drivers, categories=categories,
  severities=severities)
  return updates.summary() if summary else updates.list()",".. versionadded:: 2017.7.0 List updates that match the passed criteria. This allows for more filter options than :func:`list`. Good for finding a specific GUID or KB. Args: software (bool): Include software updates in the results (default is True) drivers (bool): Include driver updates in the results (default is True) summary (bool): - True: Return a summary of updates available for each category. - False (default): Return a detailed list of available updates. skip_installed (bool): Skip updates that are already installed. Default is False. skip_hidden (bool): Skip updates that have been hidden. Default is True. skip_mandatory (bool): Skip mandatory updates. Default is False. skip_reboot (bool): Skip updates that require a reboot. Default is False. categories (list): Specify the categories to list. Must be passed as a list. All categories returned by default. Categories include the following: * Critical Updates * Definition Updates * Drivers (make sure you set drivers=True) * Feature Packs * Security Updates * Update Rollups * Updates * Update Rollups * Windows 7 * Windows 8.1 * Windows 8.1 drivers * Windows 8.1 and later drivers * Windows Defender severities (list): Specify the severities to include. Must be passed as a list. All severities returned by default. Severities include the following: * Critical * Important Returns: dict: Returns a dict containing either a summary or a list of updates: .. code-block:: cfg List of Updates: {'<GUID>': {'Title': <title>, 'KB': <KB>, 'GUID': <the globally unique identifier for the update> 'Description': <description>, 'Downloaded': <has the update been downloaded>, 'Installed': <has the update been installed>, 'Mandatory': <is the update mandatory>, 'UserInput': <is user input required>, 'EULAAccepted': <has the EULA been accepted>, 'Severity': <update severity>, 'NeedsReboot': <is the update installed and awaiting reboot>, 'RebootBehavior': <will the update require a reboot>, 'Categories': [ '<category 1>', '<category 2>', ...] } } Summary of Updates: {'Total': <total number of updates returned>, 'Available': <updates that are not downloaded or installed>, 'Downloaded': <updates that are downloaded but not installed>, 'Installed': <updates installed (usually 0 unless installed=True)>, 'Categories': { <category 1>: <total for that category>, <category 2>: <total for category 2>, ... } } CLI Examples: .. code-block:: bash # Normal Usage (list all software updates) salt '*' win_wua.available # List all updates with categories of Critical Updates and Drivers salt '*' win_wua.available categories=[""Critical Updates"",""Drivers""] # List all Critical Security Updates salt '*' win_wua.available categories=[""Security Updates""] severities=[""Critical""] # List all updates with a severity of Critical salt '*' win_wua.available severities=[""Critical""] # A summary of all available updates salt '*' win_wua.available summary=True # A summary of all Feature Packs and Windows 8.1 Updates salt '*' win_wua.available categories=[""Feature Packs"",""Windows 8.1""] summary=True",2,0,0,1,3,1,0,0,1,2
"def mediatype_create(name, mediatype, **kwargs):
  conn_args = _login(**kwargs)
  ret = {}
  try:
  if conn_args:
  method = 'mediatype.create'
  params = {""description"": name}
  params['type'] = mediatype
  params = _params_extend(params, _ignore_name=True, **kwargs)
  ret = _query(method, params, conn_args['url'], conn_args['auth'])
  return ret['result']['mediatypeid']
  else:
  raise KeyError
  except KeyError:
  return ret","Create new mediatype .. note:: This function accepts all standard mediatype properties: keyword argument names differ depending on your zabbix version, see here__. .. __: https://www.zabbix.com/documentation/3.0/manual/api/reference/mediatype/object :param mediatype: media type - 0: email, 1: script, 2: sms, 3: Jabber, 100: Ez Texting :param exec_path: exec path - Required for script and Ez Texting types, see Zabbix API docs :param gsm_modem: exec path - Required for sms type, see Zabbix API docs :param smtp_email: email address from which notifications will be sent, required for email type :param smtp_helo: SMTP HELO, required for email type :param smtp_server: SMTP server, required for email type :param status: whether the media type is enabled - 0: enabled, 1: disabled :param username: authentication user, required for Jabber and Ez Texting types :param passwd: authentication password, required for Jabber and Ez Texting types :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring) :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring) :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring) return: ID of the created mediatype. CLI Example: .. code-block:: bash salt '*' zabbix.mediatype_create 'Email' 0 smtp_email='noreply@example.com' smtp_server='mailserver.example.com' smtp_helo='zabbix.example.com'",2,1,0,2,5,1,0,0,1,2
"def __construct_lda_model(self):
  repos_of_interest = self.__get_interests()
  cleaned_tokens = self.__clean_and_tokenize(repos_of_interest)
  if not cleaned_tokens:
  cleaned_tokens = [[""zkfgzkfgzkfgzkfgzkfgzkfg""]]
  dictionary = corpora.Dictionary(cleaned_tokens)
  corpus = [dictionary.doc2bow(text) for text in cleaned_tokens]
  self.lda_model = models.ldamodel.LdaModel(
  corpus, num_topics=1, id2word=dictionary, passes=10
  )",Method to create LDA model to procure list of topics from. We do that by first fetching the descriptions of repositories user has shown interest in. We tokenize the hence fetched descriptions to procure list of cleaned tokens by dropping all the stop words and language names from it. We use the cleaned and sanitized token list to train LDA model from which we hope to procure topics of interests to the authenticated user.,0,0,0,1,1,1,0,0,1,2
"def can_edit(self, user=None, request=None):
  can = False
  if request and not self.owner:
  if (getattr(settings, ""UMAP_ALLOW_ANONYMOUS"", False)
  and self.is_anonymous_owner(request)):
  can = True
  if self.edit_status == self.ANONYMOUS:
  can = True
  elif not user.is_authenticated:
  pass
  elif user == self.owner:
  can = True
  elif self.edit_status == self.EDITORS and user in self.editors.all():
  can = True
  return can","Define if a user can edit or not the instance, according to his account or the request.",1,0,0,1,2,1,0,0,1,2
"def alternative_full_name(self, name=None, entry_name=None, limit=None, as_df=False):
  q = self.session.query(models.AlternativeFullName)
  model_queries_config = (
  (name, models.AlternativeFullName.name),
  )
  q = self.get_model_queries(q, model_queries_config)
  q = self.get_one_to_many_queries(q, ((entry_name, models.Entry.name),))
  return self._limit_and_df(q, limit, as_df)","Method to query :class:`.models.AlternativeFullName` objects in database :param name: alternative full name(s) :type name: str or tuple(str) or None :param entry_name: name(s) in :class:`.models.Entry` :type entry_name: str or tuple(str) or None :param limit: - if `isinstance(limit,int)==True` -> limit - if `isinstance(limit,tuple)==True` -> format:= tuple(page_number, results_per_page) - if limit == None -> all results :type limit: int or tuple(int) or None :param bool as_df: if `True` results are returned as :class:`pandas.DataFrame` :return: - if `as_df == False` -> list(:class:`.models.AlternativeFullName`) - if `as_df == True` -> :class:`pandas.DataFrame` :rtype: list(:class:`.models.AlternativeFullName`) or :class:`pandas.DataFrame`",1,0,1,1,3,1,0,1,1,3
"def set_item_metadata(self, token, item_id, element, value,
  qualifier=None):
  parameters = dict()
  parameters['token'] = token
  parameters['itemId'] = item_id
  parameters['element'] = element
  parameters['value'] = value
  if qualifier:
  parameters['qualifier'] = qualifier
  response = self.request('midas.item.setmetadata', parameters)
  return response",Set the metadata associated with an item. :param token: A valid token for the user in question. :type token: string :param item_id: The id of the item for which metadata will be set. :type item_id: int | long :param element: The metadata element name. :type element: string :param value: The metadata value for the field. :type value: string :param qualifier: (optional) The metadata qualifier. Defaults to empty string. :type qualifier: None | string :returns: None. :rtype: None,1,1,0,2,4,1,0,0,1,2
"def copy_db(source_env, destination_env):
  env.update(source_env)
  local_file_path = _get_db()
  env.update(destination_env)
  with cd(env.remote_path):
  sudo('mkdir -p backups', user=env.remote_user)
  sudo(env.python + ' manage.py dump_database | gzip > backups/' + _sql_paths('local', datetime.now()))
  put(local_file_path, 'backups', use_sudo=True)
  sudo(env.python + ' manage.py clear_database', user=env.remote_user)
  if local_file_path.endswith('.gz'):
  sudo('gzip -dc %s | %s manage.py dbshell' % (local_file_path, env.python), user=env.remote_user)
  else:
  sudo('%s manage.py dbshell < %s ' % (env.python, local_file_path), user=env.remote_user)","Copies Db betweem servers, ie develop to qa. Should be called by function from function defined in project fabfile. Example usage: def copy_db_between_servers(source_server, destination_server): source_env = {} destination_env = {} def populate_env_dict(server, local_env): app_dir = 'nutrimom' if server == 'nm-dev': user = 'nutrimom-dev' prefix = ""dev"" environment = 'devel' host_string = 'dev.arabel.la' elif server == 'nm-qa': user = 'nutrimom-qa' prefix = ""qa"" environment = 'qa' host_string = 'qa.arabel.la' elif server.startswith('nm-f'): if server in ['nm-f1', 'nm-f2', 'nm-f3', 'nm-f4', 'nm-f5']: user = 'nutrimom-' + server.split('-')[1] prefix = user.split('-')[1] environment = prefix host_string = 'dev.arabel.la' else: print (""supported params: nm-dev, nm-qa, nm-fx"") sys.exit() local_env['app_dir'] = app_dir local_env['remote_user'] = user local_env['remote_path'] = '/home/%s/www/' % (user) local_env['dir'] = '/home/%s/Envs/%s' % (user, app_dir) local_env['python'] = '/home/%s/Envs/%s/bin/python' % (user, app_dir) local_env['pip'] = '/home/%s/Envs/%s/bin/pip' % (user, app_dir) local_env['prefix'] = prefix local_env['environment'] = environment local_env['host_string'] = host_string local_env['is_feature'] = False return local_env source_env = populate_env_dict(source_server, source_env) destination_env = populate_env_dict(destination_server, destination_env) copy_db(source_env, destination_env)",1,1,1,0,3,1,1,0,0,2
"def prefixsearch(self, prefix, results=10):
  self._check_query(prefix, ""Prefix must be specified"")
  query_params = {
  ""list"": ""prefixsearch"",
  ""pssearch"": prefix,
  ""pslimit"": (""max"" if results > 500 else results),
  ""psnamespace"": 0,
  ""psoffset"": 0,
  }
  raw_results = self.wiki_request(query_params)
  self._check_error_response(raw_results, prefix)
  return [rec[""title""] for rec in raw_results[""query""][""prefixsearch""]]","Perform a prefix search using the provided prefix string Args: prefix (str): Prefix string to use for search results (int): Number of pages with the prefix to return Returns: list: List of page titles Note: **Per the documentation:** ""The purpose of this module is \ similar to action=opensearch: to take user input and provide \ the best-matching titles. Depending on the search engine \ backend, this might include typo correction, redirect \ avoidance, or other heuristics.""",2,0,0,1,3,2,0,0,1,3
"def direct_perms_for_user(cls, instance, user, db_session=None):
  db_session = get_db_session(db_session, instance)
  query = db_session.query(
  cls.models_proxy.UserResourcePermission.user_id,
  cls.models_proxy.UserResourcePermission.perm_name,
  )
  query = query.filter(cls.models_proxy.UserResourcePermission.user_id == user.id)
  query = query.filter(
  cls.models_proxy.UserResourcePermission.resource_id == instance.resource_id
  )
  perms = [
  PermissionTuple(user, row.perm_name, ""user"", None, instance, False, True)
  for row in query
  ]
  if instance.owner_user_id == user.id:
  perms.append(
  PermissionTuple(user, ALL_PERMISSIONS, ""user"", None, instance, True)
  )
  return perms",returns permissions that given user has for this resource without ones inherited from groups that user belongs to :param instance: :param user: :param db_session: :return:,1,0,1,1,3,1,0,1,1,3
"def populate_translation_fields(sender, kwargs):
  populate = mt_settings.AUTO_POPULATE
  if not populate:
  return
  if populate is True:
  populate = 'all'
  opts = translator.get_options_for_model(sender)
  for key, val in list(kwargs.items()):
  if key in opts.fields:
  if populate == 'all':
  for translation_field in opts.fields[key]:
  kwargs.setdefault(translation_field.name, val)
  elif populate == 'default':
  default = build_localized_fieldname(key, mt_settings.DEFAULT_LANGUAGE)
  kwargs.setdefault(default, val)
  elif populate == 'required':
  default = build_localized_fieldname(key, mt_settings.DEFAULT_LANGUAGE)
  if not sender._meta.get_field(key).null:
  kwargs.setdefault(default, val)
  else:
  raise AttributeError(""Unknown population mode '%s'."" % populate)","When models are created or loaded from fixtures, replicates values provided for translatable fields to some / all empty translation fields, according to the current population mode. Population is performed only on keys (field names) present in kwargs. Nothing is returned, but passed kwargs dictionary is altered. With ``mode`` set to: -- ``all``: fills all translation fields, skipping just those for which a translated value is also provided; -- ``default``: fills only the default translation (unless it is additionally provided); -- ``required``: like ``default``, but only if the original field is non-nullable; At least the ``required`` mode should be used when loading untranslated fixtures to keep the database consistent (note that Django management commands are normally forced to run with hardcoded ``en-us`` language active). The ``default`` mode is useful if you need to ensure fallback values are available, and ``all`` if you need to have all translations defined (for example to make lookups / filtering without resorting to query fallbacks).",0,0,1,0,1,1,0,0,1,2
"def connect(self, database, timezone=None, cache_size=0, auto_ensure=True, replica_set=None, *args, **kwds):
 safe = kwds.get('safe', False)
 if 'safe' in kwds:
 del kwds['safe']
 if timezone is not None:
 kwds['tz_aware'] = True
 if replica_set is not None:
 if 'MongoReplicaSetClient' in globals():
 conn = MongoReplicaSetClient(*args, replicaSet=replica_set, **kwds)
 else:
 conn = MongoClient(*args, replicaSet=replica_set, **kwds)
 else:
 conn = MongoClient(*args, **kwds)
 db = conn[database]
 return Session(db, timezone=timezone, safe=safe, cache_size=cache_size, auto_ensure=auto_ensure)","`connect` is a thin wrapper around __init__ which creates the database connection that the session will use. :param database: the database name to use. Should be an instance of \ :class:`basestring` :param safe: The value for the ""safe"" parameter of the Session \ init function :param auto_ensure: Whether to implicitly call ensure_indexes on all write \ operations. :param replica_set: The replica-set to use (as a string). If specified, \ :class:`pymongo.mongo_replica_set_client.MongoReplicaSetClient` is used \ instead of :class:`pymongo.mongo_client.MongoClient` :param args: arguments for :class:`pymongo.mongo_client.MongoClient` :param kwds: keyword arguments for :class:`pymongo.mongo_client.MongoClient`",1,0,0,0,1,1,0,0,1,2
"def get_all_snapshots(self, snapshot_ids=None,
  owner=None, restorable_by=None,
  filters=None):
  params = {}
  if snapshot_ids:
  self.build_list_params(params, snapshot_ids, 'SnapshotId')
  if owner:
  params['Owner'] = owner
  if restorable_by:
  params['RestorableBy'] = restorable_by
  if filters:
  self.build_filter_params(params, filters)
  return self.get_list('DescribeSnapshots', params,
  [('item', Snapshot)], verb='POST')","Get all EBS Snapshots associated with the current credentials. :type snapshot_ids: list :param snapshot_ids: Optional list of snapshot ids. If this list is present, only the Snapshots associated with these snapshot ids will be returned. :type owner: str :param owner: If present, only the snapshots owned by the specified user will be returned. Valid values are: * self * amazon * AWS Account ID :type restorable_by: str :param restorable_by: If present, only the snapshots that are restorable by the specified account id will be returned. :type filters: dict :param filters: Optional filters that can be used to limit the results returned. Filters are provided in the form of a dictionary consisting of filter names as the key and filter values as the value. The set of allowable filter names/values is dependent on the request being performed. Check the EC2 API guide for details. :rtype: list of :class:`boto.ec2.snapshot.Snapshot` :return: The requested Snapshot objects",2,0,0,1,3,2,0,0,1,3
"def get_user_by_email(self, email):
  results = self.get_users(filter='email eq ""%s""' % (email))
  if results['totalResults'] == 0:
  logging.warning(""Found no matches for given email."")
  return
  elif results['totalResults'] > 1:
  logging.warning(""Found %s matches for email %s"" %
  (results['totalResults'], email))
  return results['resources'][0]",Returns details for user with the given email address. If there is more than one match will only return the first. Use get_users() for full result set.,2,0,0,1,3,2,0,1,1,4
"def symlink_exists(self, symlink):
  if not isinstance(symlink, basestring):
  raise TypeError(""symlink can only be an instance of type basestring"")
  exists = self._call(""symlinkExists"",
  in_p=[symlink])
  return exists","Checks whether a symbolic link exists in the guest. in symlink of type str Path to the alleged symbolic link. Guest path style. return exists of type bool Returns @c true if the symbolic link exists. Returns @c false if it does not exist, if the file system object identified by the path is not a symbolic link, or if the object type is inaccessible to the user, or if the @a symlink argument is empty. raises :class:`OleErrorNotimpl` The method is not implemented yet.",0,0,0,0,0,1,0,0,1,2
"def find(self, tname, where=None, where_not=None, columns=None, astype=None):
  try:
  tname = self._check_tname(tname, noload=True)
  except ValueError:
  return self._output(DataFrame(), astype=astype)
  where = PandasDatabase._check_conditions(where)
  where_not = PandasDatabase._check_conditions(where_not)
  columns = PandasDatabase._check_type_iter(str, columns)
  dataframe = self._db[tname]
  if len(columns) > 0 and len(dataframe) > 0:
  dataframe = dataframe[columns]
  if len(where) > 0:
  dataframe = dataframe[self._get_condition_mask(dataframe, where)]
  if len(where_not) > 0:
  dataframe = dataframe[~self._get_condition_mask(dataframe, where_not)]
  self._print('Found %d records in table ""%s"" where %r and where not %r'
  % (len(dataframe), tname, where, where_not))
  return self._output(dataframe, astype=astype)","Find records in the provided table from the database. If no records are found, return empty list, str or dataframe depending on the value of `astype`. Parameters ---------- tname : str Table to search records from. where : dict or None (default `None`) Dictionary of <column, value> where value can be of str type for exact match or a compiled regex expression for more advanced matching. where_not : dict or None (default `None`) Identical to `where` but for negative-matching. columns: list of str, str or None (default `None`) Column(s) to return for the found records, if any. astype: str, type or None (default `None`) Type to cast the output to. Possible values are: `nonetype`, `dataframe`, `str`, `dict`, `json`. If this is `None`, falls back to the type provided to the constructor. If a type was provided to the constructor but the user wants to avoid any casting, ""nonetype"" should be passed as the value. Returns ------- records : str, list or dataframe Output type depends on `astype` parameter. Examples -------- >>> db = PandasDatabase(""test"") >>> db.insert(""test"", record={""Name"": ""John""}) Name John __id__ dc876999-1f5b-4262-b6bf-c23b875f3a54 dtype: object >>> db.find(""test"", astype=""dict"") [{'Name': 'John', '__id__': 'dc876999-1f5b-4262-b6bf-c23b875f3a54'}] >>> db.find(""test"", astype=""dataframe"") __id__ Name 0 dc876999-1f5b-4262-b6bf-c23b875f3a54 John >>> db.find(""test"", astype=None) __id__ Name 0 dc876999-1f5b-4262-b6bf-c23b875f3a54 John >>> db.find(""test"", where={""Name"": ""John""}, astype=""dict"") [{'Name': 'John', '__id__': 'dc876999-1f5b-4262-b6bf-c23b875f3a54'}] >>> db.find(""test"", where_not={""Name"": ""John""}, astype=""dict"") []",1,0,1,1,3,1,0,0,1,2
"def get_documents(self, sql_string, database_name=None, collection_name=None, partition_key=None):
  if sql_string is None:
  raise AirflowBadRequest(""SQL query string cannot be None"")
  query = {'query': sql_string}
  try:
  result_iterable = self.get_conn().QueryItems(
  get_collection_link(
  self.__get_database_name(database_name),
  self.__get_collection_name(collection_name)),
  query,
  partition_key)
  return list(result_iterable)
  except HTTPFailure:
  return None",Get a list of documents from an existing collection in the CosmosDB database via SQL query.,2,0,1,1,4,1,0,1,1,3
"def reply_chat_action(
  self,
  action: Union[ChatAction, str],
  progress: int = 0
  ) -> ""Message"":
  return self._client.send_chat_action(
  chat_id=self.chat.id,
  action=action,
  progress=progress
  )","Bound method *reply_chat_action* of :obj:`Message <pyrogram.Message>`. Use as a shortcut for: .. code-block:: python client.send_chat_action( chat_id=message.chat.id, action=""typing"" ) Example: .. code-block:: python message.reply_chat_action(""typing"") Args: action (:obj:`ChatAction <pyrogram.ChatAction>` | ``str``): Type of action to broadcast. Choose one from the :class:`ChatAction <pyrogram.ChatAction>` enumeration, depending on what the user is about to receive. You can also provide a string (e.g. ""typing"", ""upload_photo"", ""record_audio"", ...). progress (``int``, *optional*): Progress of the upload process. Currently useless because official clients don't seem to be handling this. Returns: On success, True is returned. Raises: :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error. ``ValueError`` if the provided string is not a valid ChatAction.",1,0,0,1,2,1,0,0,1,2
"def move(self, key, db):
  if not isinstance(db, int):
  raise TypeError(""db argument must be int, not {!r}"".format(db))
  if db < 0:
  raise ValueError(""db argument must be not less than 0, {!r}""
  .format(db))
  fut = self.execute(b'MOVE', key, db)
  return wait_convert(fut, bool)",Move key from currently selected database to specified destination. :raises TypeError: if db is not int :raises ValueError: if db is less than 0,1,0,0,1,2,1,1,0,1,3
"def connect(
  host='localhost',
  port=9000,
  database='default',
  user='default',
  password='',
  client_name='ibis',
  compression=_default_compression,
 ):
  client = ClickhouseClient(
  host,
  port=port,
  database=database,
  user=user,
  password=password,
  client_name=client_name,
  compression=compression,
  )
  if options.default_backend is None:
  options.default_backend = client
  return client","Create an ClickhouseClient for use with Ibis. Parameters ---------- host : str, optional Host name of the clickhouse server port : int, optional Clickhouse server's port database : str, optional Default database when executing queries user : str, optional User to authenticate with password : str, optional Password to authenticate with client_name: str, optional This will appear in clickhouse server logs compression: str, optional Weather or not to use compression. Default is lz4 if installed else False. Possible choices: lz4, lz4hc, quicklz, zstd, True, False True is equivalent to 'lz4'. Examples -------- >>> import ibis >>> import os >>> clickhouse_host = os.environ.get('IBIS_TEST_CLICKHOUSE_HOST', ... 'localhost') >>> clickhouse_port = int(os.environ.get('IBIS_TEST_CLICKHOUSE_PORT', ... 9000)) >>> client = ibis.clickhouse.connect( ... host=clickhouse_host, ... port=clickhouse_port ... ) >>> client # doctest: +ELLIPSIS <ibis.clickhouse.client.ClickhouseClient object at 0x...> Returns ------- ClickhouseClient",1,0,0,1,2,1,0,0,1,2
"def get_pullrequest(self, project, repository, pull_request_id):
  url = 'rest/api/1.0/projects/{project}/repos/{repository}/pull-requests/{pullRequestId}'.format(project=project,
  repository=repository,
  pullRequestId=pull_request_id)
  return self.get(url)",Retrieve a pull request. The authenticated user must have REPO_READ permission for the repository that this pull request targets to call this resource. :param project: :param repository: :param pull_request_id: the ID of the pull request within the repository :return:,2,0,0,1,3,1,0,0,1,2
"def get_sysdig_captures(self, from_sec=None, to_sec=None, scope_filter=None):
  url = '{url}/api/sysdig?source={source}{frm}{to}{scopeFilter}'.format(
  url=self.url,
  source=self.product,
  frm=""&from=%d"" % (from_sec * 10**6) if from_sec else """",
  to=""&to=%d"" % (to_sec * 10**6) if to_sec else """",
  scopeFilter=""&scopeFilter=%s"" % scope_filter if scope_filter else """")
  res = requests.get(url, headers=self.hdrs, verify=self.ssl_verify)
  return self._request_result(res)","**Description** Returns the list of sysdig captures for the user. **Arguments** - from_sec: the start of the timerange for which to get the captures - end_sec: the end of the timerange for which to get the captures - scope_filter: this is a SysdigMonitor-like filter (e.g 'container.image=ubuntu'). When provided, events are filtered by their scope, so only a subset will be returned (e.g. 'container.image=ubuntu' will provide only events that have happened on an ubuntu container). **Success Return Value** A dictionary containing the list of captures. **Example** `examples/list_sysdig_captures.py <https://github.com/draios/python-sdc-client/blob/master/examples/list_sysdig_captures.py>`_",1,0,0,1,2,2,0,0,1,3
"def _database_from_key(self, key):
  if not self.filename:
  return None
  from pyemma.util.files import mkdir_p
  hash_value_long = int(key, 16)
  db_name = str(hash_value_long)[-1] + '.db'
  directory = os.path.dirname(self.filename) + os.path.sep + 'traj_info_usage'
  mkdir_p(directory)
  return os.path.join(directory, db_name)","gets the database name for the given key. Should ensure a uniform spread of keys over the databases in order to minimize waiting times. Since the database has to be locked for updates and multiple processes want to write, each process has to wait until the lock has been released. By default the LRU databases will be stored in a sub directory ""traj_info_usage"" lying next to the main database. :param key: hash of the TrajInfo instance :return: str, database path",0,0,0,0,0,0,0,0,1,1
"def cloudant_bluemix(vcap_services, instance_name=None, service_name=None, **kwargs):
  cloudant_session = Cloudant.bluemix(
  vcap_services,
  instance_name=instance_name,
  service_name=service_name,
  **kwargs
  )
  cloudant_session.connect()
  yield cloudant_session
  cloudant_session.disconnect()","Provides a context manager to create a Cloudant session and provide access to databases, docs etc. :param vcap_services: VCAP_SERVICES environment variable :type vcap_services: dict or str :param str instance_name: Optional Bluemix instance name. Only required if multiple Cloudant instances are available. :param str service_name: Optional Bluemix service name. :param str encoder: Optional json Encoder object used to encode documents for storage. Defaults to json.JSONEncoder. Loads all configuration from the specified VCAP_SERVICES Cloud Foundry environment variable. The VCAP_SERVICES variable contains connection information to access a service instance. For example: .. code-block:: json { ""VCAP_SERVICES"": { ""cloudantNoSQLDB"": [ { ""credentials"": { ""apikey"": ""some123api456key"" ""username"": ""example"", ""password"": ""xxxxxxx"", ""host"": ""example.cloudant.com"", ""port"": 443, ""url"": ""https://example:xxxxxxx@example.cloudant.com"" }, ""syslog_drain_url"": null, ""label"": ""cloudantNoSQLDB"", ""provider"": null, ""plan"": ""Lite"", ""name"": ""Cloudant NoSQL DB"" } ] } } See `Cloud Foundry Environment Variables <http://docs.cloudfoundry.org/ devguide/deploy-apps/environment-variable.html#VCAP-SERVICES>`_. Example usage: .. code-block:: python import os # cloudant_bluemix context manager from cloudant import cloudant_bluemix with cloudant_bluemix(os.getenv('VCAP_SERVICES'), 'Cloudant NoSQL DB') as client: # Context handles connect() and disconnect() for you. # Perform library operations within this context. Such as: print client.all_dbs() # ...",1,0,0,1,2,1,0,0,1,2
"def send_email_confirmation_instructions(self, user):
  token = self.security_utils_service.generate_confirmation_token(user)
  confirmation_link = url_for('security_controller.confirm_email',
  token=token, _external=True)
  self.send_mail(
  _('flask_unchained.bundles.security:email_subject.email_confirmation_instructions'),
  to=user.email,
  template='security/email/email_confirmation_instructions.html',
  user=user,
  confirmation_link=confirmation_link)
  confirm_instructions_sent.send(app._get_current_object(), user=user,
  token=token)",Sends the confirmation instructions email for the specified user. Sends signal `confirm_instructions_sent`. :param user: The user to send the instructions to.,1,0,0,0,1,1,0,0,1,2
"def authorize(self, client_id, redirect_uri, scope,
  state=None, user_name=None, user_email=None):
  query = [
  ('client_id', client_id),
  ('redirect_uri', redirect_uri),
  ('scope', scope)
  ]
  if user_name is not None:
  query.append(('user_name', user_name))
  if user_email is not None:
  query.append(('user_email', user_email))
  if state is not None:
  query.append(('state', state))
  return '%s/oauth2/authorize?%s' % (
  self._api.browser_endpoint, urllib.parse.urlencode(query))",Documentation: `/oauth2/authorize <https://www.wepay.com/developer/reference/oauth2#authorize>`_. .. note:: This is not an API call but an actual uri that you send the user to.,0,0,0,1,1,2,0,0,1,3
"def match_path(pathname,
  included_patterns=None,
  excluded_patterns=None,
  case_sensitive=True):
  included = [""*""] if included_patterns is None else included_patterns
  excluded = [] if excluded_patterns is None else excluded_patterns
  return _match_path(pathname, included, excluded, case_sensitive)","Matches a pathname against a set of acceptable and ignored patterns. :param pathname: A pathname which will be matched against a pattern. :param included_patterns: Allow filenames matching wildcard patterns specified in this list. If no pattern is specified, the function treats the pathname as a match_path. :param excluded_patterns: Ignores filenames matching wildcard patterns specified in this list. If no pattern is specified, the function treats the pathname as a match_path. :param case_sensitive: ``True`` if matching should be case-sensitive; ``False`` otherwise. :returns: ``True`` if the pathname matches; ``False`` otherwise. :raises: ValueError if included patterns and excluded patterns contain the same pattern. Doctests:: >>> match_path(""/Users/gorakhargosh/foobar.py"") True >>> match_path(""/Users/gorakhargosh/foobar.py"", case_sensitive=False) True >>> match_path(""/users/gorakhargosh/foobar.py"", [""*.py""], [""*.PY""], True) True >>> match_path(""/users/gorakhargosh/FOOBAR.PY"", [""*.py""], [""*.PY""], True) False >>> match_path(""/users/gorakhargosh/foobar/"", [""*.py""], [""*.txt""], False) False >>> match_path(""/users/gorakhargosh/FOOBAR.PY"", [""*.py""], [""*.PY""], False) Traceback (most recent call last): ... ValueError: conflicting patterns `set(['*.py'])` included and excluded",1,0,0,1,2,1,0,0,1,2
"def retrieve_public_key(user_repo):
  url = 'https://api.travis-ci.org/repos/{}/key' .format(user_repo)
  response = requests.get(url)
  try:
  return response.json()['key'].replace(' RSA ', ' ')
  except KeyError:
  username, repository = user_repo.split('/')
  raise InvalidCredentialsError(""Either the username: '{}' or the repository: '{}' does not exist. Please enter a valid username or repository name. The username and repository name are both case sensitive."" .format(username, repository))","Retrieve the public key from the Travis API. The Travis API response is accessed as JSON so that Travis-Encrypt can easily find the public key that is to be passed to cryptography's load_pem_public_key function. Due to issues with some public keys being returned from the Travis API as PKCS8 encoded, the key is returned with RSA removed from the header and footer. Parameters ---------- user_repo: str the repository in the format of 'username/repository' Returns ------- response: str the public RSA key of the username's repository Raises ------ InvalidCredentialsError raised when an invalid 'username/repository' is given",2,0,0,1,3,2,0,0,1,3
"def _trigger_offline_retrieval(self, url):
  with self.session.get(url, auth=self.session.auth, timeout=self.timeout) as r:
  if r.status_code == 202:
  self.logger.info(""Accepted for retrieval"")
  elif r.status_code == 503:
  self.logger.error(""Request not accepted"")
  raise SentinelAPILTAError('Request for retrieval from LTA not accepted', r)
  elif r.status_code == 403:
  self.logger.error(""Requests exceed user quota"")
  raise SentinelAPILTAError('Requests for retrieval from LTA exceed user quota', r)
  elif r.status_code == 500:
  self.logger.error(""Trying to download an offline product"")
  raise SentinelAPILTAError('Trying to download an offline product', r)
  return r.status_code",Triggers retrieval of an offline product Trying to download an offline product triggers its retrieval from the long term archive. The returned HTTP status code conveys whether this was successful. Parameters ---------- url : string URL for downloading the product Notes ----- https://scihub.copernicus.eu/userguide/LongTermArchive,1,0,0,1,2,1,0,0,1,2
"def _copy_dist_from_dir(link_path, location):
  if os.path.isdir(location):
  rmtree(location)
  setup_py = 'setup.py'
  sdist_args = [sys.executable]
  sdist_args.append('-c')
  sdist_args.append(SETUPTOOLS_SHIM % setup_py)
  sdist_args.append('sdist')
  sdist_args += ['--dist-dir', location]
  logger.info('Running setup.py sdist for %s', link_path)
  with indent_log():
  call_subprocess(sdist_args, cwd=link_path, show_stdout=False)
  sdist = os.path.join(location, os.listdir(location)[0])
  logger.info('Unpacking sdist %s into %s', sdist, location)
  unpack_file(sdist, location, content_type=None, link=None)",Copy distribution files in `link_path` to `location`. Invoked when user requests to install a local directory. E.g.: pip install . pip install ~/dev/git-repos/python-prompt-toolkit,1,0,0,0,1,1,0,0,1,2
"def disable_device(self):
  cmd_response = self.__send_command(const.CMD_DISABLEDEVICE)
  if cmd_response.get('status'):
  self.is_enabled = False
  return True
  else:
  raise ZKErrorResponse(""Can't disable device"")","disable (lock) device, to ensure no user activity in device while some process run :return: bool",1,0,0,1,2,1,0,0,1,2
"def _printedby_data(self, ws):
  data = {}
  member = self.context.portal_membership.getAuthenticatedMember()
  if member:
  username = member.getUserName()
  data['username'] = username
  data['fullname'] = to_utf8(self.user_fullname(username))
  data['email'] = to_utf8(self.user_email(username))
  c = [x for x in self.bika_setup_catalog(portal_type='LabContact')
  if x.getObject().getUsername() == username]
  if c:
  sf = c[0].getObject().getSignature()
  if sf:
  data['signature'] = sf.absolute_url() + ""/Signature""
  return data","Returns a dict that represents the user who prints the ws Keys: username, fullname, email",1,0,1,1,3,1,0,0,1,2
"def load_map(map, src_file, output_dir, scale=1, cache_dir=None, datasources_cfg=None, user_styles=[], verbose=False):
  scheme, n, path, p, q, f = urlparse(src_file)
  if scheme in ('file', ''):
  assert exists(src_file), ""We'd prefer an input file that exists to one that doesn't""
  if cache_dir is None:
  cache_dir = expanduser(CACHE_DIR)
  if not isdir(cache_dir):
  mkdir(cache_dir)
  chmod(cache_dir, 0755)
  dirs = Directories(output_dir, realpath(cache_dir), dirname(src_file))
  compile(src_file, dirs, verbose, datasources_cfg=datasources_cfg, user_styles=user_styles, scale=scale).to_mapnik(map, dirs)","Apply a stylesheet source file to a given mapnik Map instance, like mapnik.load_map(). Parameters: map: Instance of mapnik.Map. src_file: Location of stylesheet .mml file. Can be relative path, absolute path, or fully-qualified URL of a remote stylesheet. output_dir: ... Keyword Parameters: scale: Optional scale value for output map, 2 doubles the size for high-res displays. cache_dir: ... datasources_cfg: ... user_styles: A optional list of files or URLs, that override styles defined in the map source. These are evaluated in order, with declarations from later styles overriding those from earlier styles. verbose: ...",1,0,0,0,1,1,0,0,1,2
"def untarbz(source_file_path, output_directory_path, silent=False):
  if not path.exists(source_file_path):
  raise Exception(""the provided tar file %s does not exist."" % (source_file_path))
  if output_directory_path[0:1] == ""./"":
  output_directory_path = path.abspath(output_directory_path)
  if output_directory_path[0] != ""/"":
  raise Exception(""your output directory path must start with '/' or './'; you used: %s""
  % (output_directory_path))
  create_folders(output_directory_path)
  if listdir(output_directory_path):
  raise Exception(""Your output directory isn't empty. Aborting as ""
  + ""exiting files are not overwritten by tar."")
  untar_command = (""tar jxfvkCp %s %s --atime-preserve "" %
  (source_file_path, output_directory_path))
  call(untar_command, silent=silent)","Restores your mongo database backup from a .tbz created using this library. This function will ensure that a directory is created at the file path if one does not exist already. If used in conjunction with this library's mongodump operation, the backup data will be extracted directly into the provided directory path. This command will fail if the output directory is not empty as existing files with identical names are not overwritten by tar.",0,1,0,0,1,1,0,0,1,2
"def __check_equals(self, query):
  self.cur1.execute(query)
  records1 = self.cur1.fetchall()
  self.cur2.execute(query)
  records2 = self.cur2.fetchall()
  result = True
  differences = []
  d = difflib.Differ()
  records1 = [str(x) for x in records1]
  records2 = [str(x) for x in records2]
  for line in d.compare(records1, records2):
  if line[0] in ('-', '+'):
  result = False
  if self.verbose_level == 1:
  differences.append(line[0:79])
  elif self.verbose_level == 2:
  differences.append(line)
  return result, differences",Check if the query results on the two databases are equals. Returns ------- bool True if the results are the same False otherwise list A list with the differences,0,0,1,1,2,1,0,1,1,3
"def set_user_role(new_user_id, role_id, **kwargs):
  try:
  _get_user(new_user_id)
  role_i = _get_role(role_id)
  roleuser_i = RoleUser(user_id=new_user_id, role_id=role_id)
  role_i.roleusers.append(roleuser_i)
  db.DBSession.flush()
  except Exception as e:
  log.exception(e)
  raise ResourceNotFoundError(""User or Role does not exist"")
  return role_i",Apply `role_id` to `new_user_id` Note this function returns the `Role` instance associated with `role_id`,1,1,1,1,4,1,1,1,1,4
"def raw(url):
  try:
  parts = url.split('/')
  if parts[2] == 'gist.github.com' and '.' not in parts[:-1]:
  soup = BeautifulSoup(requests.get(url).text, 'html.parser')
  raw_links = [i for i in soup.find_all('a') if i.text == 'Raw']
  return ('https://gist.githubusercontent.com' +
  raw_links[0].attrs['href'])
  except Exception as e:
  print('Failed open and parse', url, e)
  pass
  try:
  protocol, empty, provider, user, project, _, *rest = url.split('/')
  except:
  return url
  rewriter = URL_REWRITERS.get(provider)
  if protocol and (not empty) and user and project and rest and rewriter:
  parts = [protocol, empty, rewriter['provider'], user, project]
  return '/'.join(parts + rewriter['path'] + rest)
  return url","Return a raw version of the URL if there is one. Otherwise returns the original URL. Many repos have ""raw"" and ""user-friendly"" versions of each URL. Usually when you want to download something programmatically, you want the raw version, but users will enter the user-friendly version as it is the one they usually see. If this function recognizes one of those cases, it converts the user-friendly URL into the raw - otherwise it returns the original URL. The function works by default for two git providers: github and gitlab. You can use others by passing in your own url_rewriters list. There is also a special case for Github gists.",1,0,0,1,2,1,0,0,1,2
"def make_client(zhmc, userid=None, password=None):
  global USERID, PASSWORD
  USERID = userid or USERID or \
  six.input('Enter userid for HMC {}: '.format(zhmc))
  PASSWORD = password or PASSWORD or \
  getpass.getpass('Enter password for {}: '.format(USERID))
  session = zhmcclient.Session(zhmc, USERID, PASSWORD)
  session.logon()
  client = zhmcclient.Client(session)
  print('Established logged-on session with HMC {} using userid {}'.
  format(zhmc, USERID))
  return client","Create a `Session` object for the specified HMC and log that on. Create a `Client` object using that `Session` object, and return it. If no userid and password are specified, and if no previous call to this method was made, userid and password are interactively inquired. Userid and password are saved in module-global variables for future calls to this method.",1,0,0,2,3,2,0,0,1,3
"def parse_pgurl(self, url):
  parsed = urlsplit(url)
  return {
  'user': parsed.username,
  'password': parsed.password,
  'database': parsed.path.lstrip('/'),
  'host': parsed.hostname,
  'port': parsed.port or 5432,
  }","Given a Postgres url, return a dict with keys for user, password, host, port, and database.",0,0,0,1,1,1,0,0,1,2
"def trash_for(self, user):
  return self.filter(
  recipient=user,
  recipient_deleted_at__isnull=False,
  ) | self.filter(
  sender=user,
  sender_deleted_at__isnull=False,
  )",Returns all messages that were either received or sent by the given user and are marked as deleted.,1,0,1,1,3,1,0,1,1,3
"def get_msg(self):
  width = 72
  _msg = self.msg % {'distro': self.distro, 'vendor': self.vendor,
  'vendor_url': self.vendor_url,
  'vendor_text': self.vendor_text,
  'tmpdir': self.commons['tmpdir']}
  _fmt = """"
  for line in _msg.splitlines():
  _fmt = _fmt + fill(line, width, replace_whitespace=False) + '\n'
  return _fmt",This method is used to prepare the preamble text to display to the user in non-batch mode. If your policy sets self.distro that text will be substituted accordingly. You can also override this method to do something more complicated.,1,0,0,1,2,1,0,0,1,2
"def db(cls, dbname=None):
  cls._load_entry_points()
  dbname = dbname or config.get('eop', 'dbname', fallback=cls.DEFAULT_DBNAME)
  if dbname not in cls._dbs.keys():
  raise EopError(""Unknown database '%s'"" % dbname)
  if isclass(cls._dbs[dbname]):
  try:
  cls._dbs[dbname] = cls._dbs[dbname]()
  except Exception as e:
  cls._dbs[dbname] = e
  if isinstance(cls._dbs[dbname], Exception):
  raise EopError(""Problem at database instanciation"") from cls._dbs[dbname]
  return cls._dbs[dbname]","Retrieve the database Args: dbname: Specify the name of the database to retrieve. If set to `None`, take the name from the configuration (see :ref:`configuration <eop-dbname>`) Return: object",0,0,1,0,1,0,0,0,0,0
"def agg(self, *exprs):
  assert exprs, ""exprs should not be empty""
  if len(exprs) == 1 and isinstance(exprs[0], dict):
  jdf = self._jgd.agg(exprs[0])
  else:
  assert all(isinstance(c, Column) for c in exprs), ""all exprs should be Column""
  jdf = self._jgd.agg(exprs[0]._jc,
  _to_seq(self.sql_ctx._sc, [c._jc for c in exprs[1:]]))
  return DataFrame(jdf, self.sql_ctx)","Compute aggregates and returns the result as a :class:`DataFrame`. The available aggregate functions can be: 1. built-in aggregation functions, such as `avg`, `max`, `min`, `sum`, `count` 2. group aggregate pandas UDFs, created with :func:`pyspark.sql.functions.pandas_udf` .. note:: There is no partial aggregation with group aggregate UDFs, i.e., a full shuffle is required. Also, all the data of a group will be loaded into memory, so the user should be aware of the potential OOM risk if data is skewed and certain groups are too large to fit in memory. .. seealso:: :func:`pyspark.sql.functions.pandas_udf` If ``exprs`` is a single :class:`dict` mapping from string to string, then the key is the column to perform aggregation on, and the value is the aggregate function. Alternatively, ``exprs`` can also be a list of aggregate :class:`Column` expressions. .. note:: Built-in aggregation functions and group aggregate pandas UDFs cannot be mixed in a single call to this function. :param exprs: a dict mapping from column name (string) to aggregate functions (string), or a list of :class:`Column`. >>> gdf = df.groupBy(df.name) >>> sorted(gdf.agg({""*"": ""count""}).collect()) [Row(name=u'Alice', count(1)=1), Row(name=u'Bob', count(1)=1)] >>> from pyspark.sql import functions as F >>> sorted(gdf.agg(F.min(df.age)).collect()) [Row(name=u'Alice', min(age)=2), Row(name=u'Bob', min(age)=5)] >>> from pyspark.sql.functions import pandas_udf, PandasUDFType >>> @pandas_udf('int', PandasUDFType.GROUPED_AGG) # doctest: +SKIP ... def min_udf(v): ... return v.min() >>> sorted(gdf.agg(min_udf(df.age)).collect()) # doctest: +SKIP [Row(name=u'Alice', min_udf(age)=2), Row(name=u'Bob', min_udf(age)=5)]",1,0,0,1,2,0,0,0,1,1
"def add_default_parameter_values(self, sam_template):
  parameter_definition = sam_template.get(""Parameters"", None)
  if not parameter_definition or not isinstance(parameter_definition, dict):
  return self.parameter_values
  for param_name, value in parameter_definition.items():
  if param_name not in self.parameter_values and isinstance(value, dict) and ""Default"" in value:
  self.parameter_values[param_name] = value[""Default""]","Method to read default values for template parameters and merge with user supplied values. Example: If the template contains the following parameters defined Parameters: Param1: Type: String Default: default_value Param2: Type: String Default: default_value And, the user explicitly provided the following parameter values: { Param2: ""new value"" } then, this method will grab default value for Param1 and return the following result: { Param1: ""default_value"", Param2: ""new value"" } :param dict sam_template: SAM template :param dict parameter_values: Dictionary of parameter values provided by the user :return dict: Merged parameter values",0,0,0,1,1,0,0,0,1,1
"def _ask_how_many():
  batch = True
  invalid = True
  _option = """"
  try:
  while invalid:
  print(""\nChoose a loading option:\n1. Select specific file(s)\n2. Load entire folder"")
  _option = input(""Option: "")
  if _option in [""1"", ""2""]:
  invalid = False
  if _option == ""1"":
  batch = False
  except Exception:
  logger_directory.info(""_askHowMany: Couldn't get a valid input from the user."")
  return batch","Ask user if they want to load in one file or do a batch process of a whole directory. Default to batch ""m"" mode. :return str: Path or none",1,0,0,1,2,1,0,0,1,2
"def get(self, request,pk):
  requested_profile = Profile.objects.get(user=pk)
  if requested_profile.user == self.request.user:
  return render(request, self.template_name, {'profile' : requested_profile})
  else:
  return HttpResponse(status = 403)","If the user requests his profile return it, else return a 403 (Forbidden)",1,0,1,1,3,1,0,1,1,3
"def get_tac_permissions(calendar_id):
  return _process_get_perm_resp(
  get_permissions_url,
  post_tac_resource(get_permissions_url,
  _create_get_perm_body(calendar_id)),
  TrumbaCalendar.TAC_CAMPUS_CODE,
  calendar_id)","Return a list of sorted Permission objects representing the user permissions of a given Tacoma calendar. :return: a list of trumba.Permission objects corresponding to the given campus calendar. None if error, [] if not exists raise DataFailureException or a corresponding TrumbaException if the request failed or an error code has been returned.",2,0,0,1,3,1,0,0,1,2
"def timout(et, pictur, lenout=_default_len_out):
  pictur = stypes.stringToCharP(pictur)
  output = stypes.stringToCharP(lenout)
  lenout = ctypes.c_int(lenout)
  if hasattr(et, ""__iter__""):
  times = []
  for t in et:
  libspice.timout_c(ctypes.c_double(t), pictur, lenout, output)
  checkForSpiceError(None)
  times.append(stypes.toPythonString(output))
  return times
  else:
  et = ctypes.c_double(et)
  libspice.timout_c(et, pictur, lenout, output)
  return stypes.toPythonString(output)","This vectorized routine converts an input epoch represented in TDB seconds past the TDB epoch of J2000 to a character string formatted to the specifications of a user's format picture. http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/timout_c.html :param et: An epoch in seconds past the ephemeris epoch J2000. :type et: Union[float,Iterable[float]] :param pictur: A format specification for the output string. :type pictur: str :param lenout: The length of the output string plus 1. :type lenout: int :return: A string representation of the input epoch. :rtype: str or array of str",0,0,0,1,1,1,0,0,1,2
"def authenticate(self, username, password):
  url = URLS['token']
  data = {
  ""grant_type"": ""password"",
  ""client_id"": self.client_id,
  ""client_secret"": self.client_secret,
  ""username"": username,
  ""password"": password
  }
  r = requests.post(url, data=data)
  r.raise_for_status()
  j = r.json()
  self.access_token = j['access_token']
  self.refresh_token = j['refresh_token']
  self._set_token_expiration_time(expires_in=j['expires_in'])
  return r","Uses a Smappee username and password to request an access token, refresh token and expiry date. Parameters ---------- username : str password : str Returns ------- requests.Response access token is saved in self.access_token refresh token is saved in self.refresh_token expiration time is set in self.token_expiration_time as datetime.datetime",2,0,0,1,3,2,0,0,1,3
"def displayResponse(request, openid_response):
  s = getServer(request)
  try:
  webresponse = s.encodeResponse(openid_response)
  except EncodingError, why:
  text = why.response.encodeToKVForm()
  return direct_to_template(
  request,
  'server/endpoint.html',
  {'error': cgi.escape(text)})
  r = http.HttpResponse(webresponse.body)
  r.status_code = webresponse.code
  for header, value in webresponse.headers.iteritems():
  r[header] = value
  return r","Display an OpenID response. Errors will be displayed directly to the user; successful responses and other protocol-level messages will be sent using the proper mechanism (i.e., direct response, redirection, etc.).",1,0,0,1,2,1,0,0,1,2
"def notify(self, subsystem, recipient, subject, body_html, body_text):
  if not re.match(self.validation, recipient, re.I):
  raise ValueError('Invalid recipient provided')
  if recipient.startswith('
  target_type = 'channel'
  elif recipient.find('@') != -1:
  target_type = 'user'
  else:
  self.log.error('Unknown contact type for Slack: {}'.format(recipient))
  return
  try:
  self._send_message(
  target_type=target_type,
  target=recipient,
  message=body_text,
  title=subject
  )
  except SlackError as ex:
  self.log.error('Failed sending message to {}: {}'.format(recipient, ex))","You can send messages either to channels and private groups by using the following formats #channel-name @username-direct-message Args: subsystem (`str`): Name of the subsystem originating the notification recipient (`str`): Recipient subject (`str`): Subject / title of the notification, not used for this notifier body_html (`str)`: HTML formatted version of the message, not used for this notifier body_text (`str`): Text formatted version of the message Returns: `None`",0,0,0,1,1,1,0,0,1,2
"def send_text_message(self, user_id, content, kf_account=None):
  data = {
  ""touser"": user_id,
  ""msgtype"": ""text"",
  ""text"": {
  ""content"": content
  }
  }
  if kf_account is not None:
  data['customservice'] = {'kf_account': kf_account}
  return self.post(
  url=""https://api.weixin.qq.com/cgi-bin/message/custom/send"",
  data=data
  )", :param user_id:  ID    `Message`  source :param content:  :param kf_account:  NoneNone  :return:  JSON ,1,0,0,1,2,1,0,0,2,3
"def calculate_transmission(thickness_cm: np.float, atoms_per_cm3: np.float, sigma_b: np.array):
  miu_per_cm = calculate_linear_attenuation_coefficient(atoms_per_cm3=atoms_per_cm3, sigma_b=sigma_b)
  transmission = calculate_trans(thickness_cm=thickness_cm, miu_per_cm=miu_per_cm)
  return miu_per_cm, transmission",calculate the transmission signal using the formula transmission = exp( - thickness_cm * atoms_per_cm3 * 1e-24 * sigma_b) Parameters: =========== thickness: float (in cm) atoms_per_cm3: float (number of atoms per cm3 of element/isotope) sigma_b: np.array of sigma retrieved from database Returns: ======== transmission array,0,0,0,0,0,1,0,0,1,2
"def user_agent_info(sdk_version, custom_user_agent):
  python_version = ""."".join(str(x) for x in sys.version_info[0:3])
  user_agent = ""ask-python/{} Python/{}"".format(
  sdk_version, python_version)
  if custom_user_agent is None:
  return user_agent
  else:
  return user_agent + "" {}"".format(custom_user_agent)",Return the user agent info along with the SDK and Python Version information. :param sdk_version: Version of the SDK being used. :type sdk_version: str :param custom_user_agent: Custom User Agent string provided by the developer. :type custom_user_agent: str :return: User Agent Info string :rtype: str,1,0,0,1,2,1,0,0,1,2
"def actor_url(parser, token):
  bits = token.split_contents()
  if len(bits) != 2:
  raise TemplateSyntaxError(""Accepted format ""
  ""{% actor_url [actor_instance] %}"")
  else:
  return DisplayActivityActorUrl(*bits[1:])","Renders the URL for a particular actor instance :: <a href=""{% actor_url request.user %}"">View your actions</a> <a href=""{% actor_url another_user %}"">{{ another_user }}'s actions</a>",1,0,0,1,2,1,0,0,1,2
"def add_features(self, features, append=True, merge='outer',
  duplicates='ignore', min_studies=0.0, threshold=0.001):
  if (not append) or not hasattr(self, 'feature_table'):
  self.feature_table = FeatureTable(self)
  self.feature_table.add_features(features, merge=merge,
  duplicates=duplicates,
  min_studies=min_studies,
  threshold=threshold)","Construct a new FeatureTable from file. Args: features: Feature data to add. Can be: (a) A text file containing the feature data, where each row is a study in the database, with features in columns. The first column must contain the IDs of the studies to match up with the image data. (b) A pandas DataFrame, where studies are in rows, features are in columns, and the index provides the study IDs. append (bool): If True, adds new features to existing ones incrementally. If False, replaces old features. merge, duplicates, min_studies, threshold: Additional arguments passed to FeatureTable.add_features().",1,0,0,0,1,1,0,0,1,2
"def add_xmlid(cr, module, xmlid, model, res_id, noupdate=False):
  cr.execute(
  ""SELECT id FROM ir_model_data WHERE module=%s AND name=%s ""
  ""AND model=%s"",
  (module, xmlid, model))
  already_exists = cr.fetchone()
  if already_exists:
  return False
  else:
  logged_query(
  cr,
  ""INSERT INTO ir_model_data (create_uid, create_date, ""
  ""write_uid, write_date, date_init, date_update, noupdate, ""
  ""name, module, model, res_id) ""
  ""VALUES (%s, (now() at time zone 'UTC'), %s, ""
  ""(now() at time zone 'UTC'), (now() at time zone 'UTC'), ""
  ""(now() at time zone 'UTC'), %s, %s, %s, %s, %s)"", (
  SUPERUSER_ID, SUPERUSER_ID, noupdate,
  xmlid, module, model, res_id))
  return True","Adds an entry in ir_model_data. Typically called in the pre script. One usage example is when an entry has been add in the XML and there is a high probability that the user has already created the entry manually. For example, a currency was added in the XML data of the base module in OpenERP 6 but the user had already created this missing currency by hand in it's 5.0 database. In order to avoid having 2 identical currencies (which is in fact blocked by an sql_constraint), you have to add the entry in ir_model_data before the upgrade.",0,1,1,0,2,1,1,1,0,3
"def parseDockerAppliance(appliance):
  appliance = appliance.lower()
  if ':' in appliance:
  tag = appliance.split(':')[-1]
  appliance = appliance[:-(len(':' + tag))]
  else:
  tag = 'latest'
  registryName = 'docker.io'
  imageName = appliance
  if '/' in appliance and '.' in appliance.split('/')[0]:
  registryName = appliance.split('/')[0]
  imageName = appliance[len(registryName):]
  registryName = registryName.strip('/')
  imageName = imageName.strip('/')
  return registryName, imageName, tag","Takes string describing a docker image and returns the parsed registry, image reference, and tag for that image. Example: ""quay.io/ucsc_cgl/toil:latest"" Should return: ""quay.io"", ""ucsc_cgl/toil"", ""latest"" If a registry is not defined, the default is: ""docker.io"" If a tag is not defined, the default is: ""latest"" :param appliance: The full url of the docker image originally specified by the user (or the default). e.g. ""quay.io/ucsc_cgl/toil:latest"" :return: registryName, imageName, tag",1,0,0,1,2,1,0,0,1,2
"def read_csi_driver(self, name, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.read_csi_driver_with_http_info(name, **kwargs)
  else:
  (data) = self.read_csi_driver_with_http_info(name, **kwargs)
  return data","read the specified CSIDriver This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.read_csi_driver(name, async_req=True) >>> result = thread.get() :param async_req bool :param str name: name of the CSIDriver (required) :param str pretty: If 'true', then the output is pretty printed. :param bool exact: Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. Deprecated. Planned for removal in 1.18. :param bool export: Should this value be exported. Export strips fields that a user can not specify. Deprecated. Planned for removal in 1.18. :return: V1beta1CSIDriver If the method is called asynchronously, returns the request thread.",2,0,0,1,3,1,0,0,1,2
"def wfreq2vocab(wfreq_file, output_file, top=None, gt=None, records=1000000, verbosity=2):
  cmd = ['wfreq2vocab', '-verbosity', verbosity,
  '-records', records]
  cmd = [str(x) for x in cmd]
  if top:
  cmd.extend(['-top',top])
  elif gt:
  cmd.extend(['-gt',gt])
  with open(wfreq_file,'r') as input_f:
  with open(output_file,'w+') as output_f:
  with output_to_debuglogger() as err_f:
  exitcode = subprocess.call(cmd, stdin=input_f, stdout=output_f, stderr=err_f)
  logger = logging.getLogger(__name__)
  logger.debug(""Command '%s' returned with exit code '%d'."" % (' '.join(cmd), exitcode))
  if exitcode != 0:
  raise ConversionError(""'%s' returned with non-zero exit status '%s'"" % (cmd[0], exitcode))","Takes a a word unigram file, as produced by text2wfreq and converts it to a vocabulary file. The top parameter allows the user to specify the size of the vocabulary; if the function is called with the parameter top=20000, then the vocabulary will consist of the most common 20,000 words. The gt parameter allows the user to specify the number of times that a word must occur to be included in the vocabulary; if the function is called with the parameter gt=10, then the vocabulary will consist of all the words which occurred more than 10 times. If neither the gt, nor the top parameters are specified, then the function runs with the default setting of taking the top 20,000 words. The records parameter (default: 1000000) allows the user to specify how many of the word and count records to allocate memory for. If the number of words in the input exceeds this number, then the function will fail and raise a ConversionError, but a high number will obviously result in a higher memory requirement.",1,0,0,1,2,1,0,0,1,2
"def message_cli(subject, message_body, access_token, all_members=False,
  project_member_ids=None, base_url=OH_BASE_URL,
  verbose=False, debug=False):
  if project_member_ids:
  project_member_ids = re.split(r'[ ,\r\n]+', project_member_ids)
  return message(subject, message_body, access_token, all_members,
  project_member_ids, base_url)",Command line function for sending email to a single user or in bulk. For more information visit :func:`message<ohapi.api.message>`.,1,0,0,1,2,1,0,0,1,2
"async def factory(self, request, path = ""/"", as_ = None):
  counter, time = perf_counter(), process_time()
  if not path.startswith(""/""):
  path = ""/{}"".format(path)
  resp = await self.resolve_path(path)
  if resp:
  if ""path"" not in request.json:
  request.json[""path""] = path
  method = getattr(resp[""model""], resp[""model""].factories[as_] if hasattr(resp[""model""], ""factories"") and as_ in resp[""model""].factories else ""create_child"")
  result = await method(request, as_)
  code = result.code if issubclass(result.__class__, ErrorSchema) else 201
  result = result.to_plain_dict()
  result['pref_counter'] = (perf_counter() - counter) * 1000
  result['process_time'] = (process_time() - time) * 1000
  return response.json(result, code)
  else:
  error = self.models.ErrorSchema()
  error.load({""message"": ""{} not found"".format(path), ""code"": 404})
  return response.json(error.to_plain_dict(), 404)",The user will ask for /path/to/the/parent/new/member-list Where member-list is the list where the parent saves the children order So in the test model MinimalMongoTree the only factory will be /new/children,1,1,1,2,5,1,0,0,1,2
"def username(self):
  if len(self._inp_username.value.strip()) == 0:
  if not self.hostname is None:
  config = parse_sshconfig(self.hostname)
  if 'user' in config:
  return config['user']
  else:
  return None
  else:
  return self._inp_username.value",Loking for username in user's input and config file,1,0,0,1,2,1,0,0,1,2
"def search_people_by_bio(query, limit_results=DEFAULT_LIMIT,
  index=['onename_people_index']):
  from pyes import QueryStringQuery, ES
  conn = ES()
  q = QueryStringQuery(query,
  search_fields=['username', 'profile_bio'],
  default_operator='and')
  results = conn.search(query=q, size=20, indices=index)
  count = conn.count(query=q)
  count = count.count
  if(count == 0):
  q = QueryStringQuery(query,
  search_fields=['username', 'profile_bio'],
  default_operator='or')
  results = conn.search(query=q, size=20, indices=index)
  results_list = []
  counter = 0
  for profile in results:
  username = profile['username']
  results_list.append(username)
  counter += 1
  if(counter == limit_results):
  break
  return results_list","queries lucene index to find a nearest match, output is profile username",1,0,1,1,3,1,0,1,1,3
"def create(cls, user_id, client_id, token, secret,
  token_type='', extra_data=None):
  account = RemoteAccount.get(user_id, client_id)
  with db.session.begin_nested():
  if account is None:
  account = RemoteAccount(
  user_id=user_id,
  client_id=client_id,
  extra_data=extra_data or dict(),
  )
  db.session.add(account)
  token = cls(
  token_type=token_type,
  remote_account=account,
  access_token=token,
  secret=secret,
  )
  db.session.add(token)
  return token",Create a new access token. .. note:: Creates RemoteAccount as well if it does not exists. :param user_id: The user id. :param client_id: The client id. :param token: The token. :param secret: The secret key. :param token_type: The token type. (Default: ``''``) :param extra_data: Extra data to set in the remote account if the remote account doesn't exists. (Default: ``None``) :returns: A :class:`invenio_oauthclient.models.RemoteToken` instance.,1,1,1,0,3,1,1,1,1,4
"def memoize(Class, *args, **kwargs):
  def decorator(func):
  if len(args) == 0:
  if hasattr(func, '__name__'):
  _args = (func.__name__,)
  else:
  raise ValueError('You must specify the location to store the vlermv.')
  else:
  _args = args
  v = Class(*_args, **kwargs)
  v.func = func
  return v
  return decorator","Memoize/record a function inside this vlermv. :: @Vlermv.cache('~/.http') def get(url): return requests.get(url, auth = ('username', 'password')) The args and kwargs get passed to the Vlermv with some slight changes. Here are the changes. First, the default ``key_transformer`` is the tuple key_transformer rather than the simple key_transformer. Second, it is valid for cache to be called without arguments. Vlermv would ordinarily fail if no arguments were passed to it. If you pass no arguments to cache, the Vlermv directory argument (the one required argument) will be set to the name of the function. Third, you are more likely to use the ``cache_exceptions`` keyword argument; see :py:class:`~vlermv.Vlermv` for documentation on that.",1,0,0,1,2,1,0,0,1,2
"def enroll_users_in_program(cls, enterprise_customer, program_details, course_mode, emails, cohort=None):
  existing_users, unregistered_emails = cls.get_users_by_email(emails)
  course_ids = get_course_runs_from_program(program_details)
  successes = []
  pending = []
  failures = []
  for user in existing_users:
  succeeded = cls.enroll_user(enterprise_customer, user, course_mode, *course_ids)
  if succeeded:
  successes.append(user)
  else:
  failures.append(user)
  for email in unregistered_emails:
  pending_user = enterprise_customer.enroll_user_pending_registration(
  email,
  course_mode,
  *course_ids,
  cohort=cohort
  )
  pending.append(pending_user)
  return successes, pending, failures","Enroll existing users in all courses in a program, and create pending enrollments for nonexisting users. Args: enterprise_customer: The EnterpriseCustomer which is sponsoring the enrollment program_details: The details of the program in which we're enrolling course_mode (str): The mode with which we're enrolling in the program emails: An iterable of email addresses which need to be enrolled Returns: successes: A list of users who were successfully enrolled in all courses of the program pending: A list of PendingEnterpriseCustomerUsers who were successfully linked and had pending enrollments created for them in the database failures: A list of users who could not be enrolled in the program",1,2,2,1,6,1,1,1,1,4
"async def get_me(self, input_peer=False):
  if input_peer and self._self_input_peer:
  return self._self_input_peer
  try:
  me = (await self(
  functions.users.GetUsersRequest([types.InputUserSelf()])))[0]
  self._bot = me.bot
  if not self._self_input_peer:
  self._self_input_peer = utils.get_input_peer(
  me, allow_self=False
  )
  return self._self_input_peer if input_peer else me
  except errors.UnauthorizedError:
  return None","Gets ""me"" (the self user) which is currently authenticated, or None if the request fails (hence, not authenticated). Args: input_peer (`bool`, optional): Whether to return the :tl:`InputPeerUser` version or the normal :tl:`User`. This can be useful if you just need to know the ID of yourself. Returns: Your own :tl:`User`.",1,0,0,1,2,1,0,0,1,2
"def run(self, value, errors, request):
  thing = super(AccessibleDBThing, self).run(value, errors, request)
  if errors:
  return None
  if not thing.can_access(request.user):
  message = 'Insufficient permissions for {0}'.format(self.param)
  raise HTTPForbidden(message)
  return thing","Return thing, but abort validation if request.user cannot edit.",1,0,1,1,3,1,0,0,1,2
"def create_subject(self,
  authc_token=None,
  account_id=None,
  existing_subject=None,
  subject_context=None):
  if subject_context is None:
  context = self.create_subject_context(existing_subject)
  context.authenticated = True
  context.authentication_token = authc_token
  context.account_id = account_id
  if (existing_subject):
  context.subject = existing_subject
  else:
  context = copy.copy(subject_context)
  context = self.ensure_security_manager(context)
  context = self.resolve_session(context)
  context = self.resolve_identifiers(context)
  subject = self.do_create_subject(context)
  self.save(subject)
  return subject","Creates a ``Subject`` instance for the user represented by the given method arguments. It is an overloaded method, due to porting java to python, and is consequently highly likely to be refactored. It gets called in one of two ways: 1) when creating an anonymous subject, passing create_subject a subject_context argument 2) following a after successful login, passing all but the context argument This implementation functions as follows: - Ensures that the ``SubjectContext`` exists and is as populated as it can be, using heuristics to acquire data that may not have already been available to it (such as a referenced session or remembered identifiers). - Calls subject_context.do_create_subject to perform the Subject instance creation - Calls subject.save to ensure the constructed Subject's state is accessible for future requests/invocations if necessary - Returns the constructed Subject instance :type authc_token: subject_abcs.AuthenticationToken :param account_id: the identifiers of a newly authenticated user :type account: SimpleIdentifierCollection :param existing_subject: the existing Subject instance that initiated the authentication attempt :type subject: subject_abcs.Subject :type subject_context: subject_abcs.SubjectContext :returns: the Subject instance that represents the context and session data for the newly authenticated subject",1,0,0,1,2,1,1,0,1,3
"def check_empty_response(self, orig_request, method_config, start_response):
  response_config = method_config.get('response', {}).get('body')
  if response_config == 'empty':
  cors_handler = self._create_cors_handler(orig_request)
  return util.send_wsgi_no_content_response(start_response, cors_handler)","If the response from the backend is empty, return a HTTP 204 No Content. Args: orig_request: An ApiRequest, the original request from the user. method_config: A dict, the API config of the method to be called. start_response: A function with semantics defined in PEP-333. Returns: If the backend response was empty, this returns a string containing the response body that should be returned to the user. If the backend response wasn't empty, this returns None, indicating that we should not exit early with a 204.",2,0,0,1,3,1,0,0,1,2
"def heartbeat(queue_name, task_id, owner, message, index):
  task = _get_task_with_policy(queue_name, task_id, owner)
  if task.heartbeat_number > index:
  return False
  task.heartbeat = message
  task.heartbeat_number = index
  now = datetime.datetime.utcnow()
  timeout_delta = task.eta - task.last_lease
  task.eta = now + timeout_delta
  task.last_lease = now
  db.session.add(task)
  signals.task_updated.send(app, task=task)
  return True","Sets the heartbeat status of the task and extends its lease. The task's lease is extended by the same amount as its last lease to ensure that any operations following the heartbeat will still hold the lock for the original lock period. Args: queue_name: Name of the queue the work item is on. task_id: ID of the task that is finished. owner: Who or what has the current lease on the task. message: Message to report as the task's current status. index: Number of this message in the sequence of messages from the current task owner, starting at zero. This lets the API receive heartbeats out of order, yet ensure that the most recent message is actually saved to the database. This requires the owner issuing heartbeat messages to issue heartbeat indexes sequentially. Returns: True if the heartbeat message was set, False if it is lower than the current heartbeat index. Raises: TaskDoesNotExistError if the task does not exist. LeaseExpiredError if the lease is no longer active. NotOwnerError if the specified owner no longer owns the task.",0,1,1,0,2,1,1,0,0,2
"def query(self, hash_key, range_key_condition=None,
  attributes_to_get=None, request_limit=None,
  max_results=None, consistent_read=False,
  scan_index_forward=True, exclusive_start_key=None,
  item_class=Item):
  return self.layer2.query(self, hash_key, range_key_condition,
  attributes_to_get, request_limit,
  max_results, consistent_read,
  scan_index_forward, exclusive_start_key,
  item_class=item_class)","Perform a query on the table. :type hash_key: int|long|float|str|unicode :param hash_key: The HashKey of the requested item. The type of the value must match the type defined in the schema for the table. :type range_key_condition: dict :param range_key_condition: A dict where the key is either a scalar value appropriate for the RangeKey in the schema of the database or a tuple of such values. The value associated with this key in the dict will be one of the following conditions: 'EQ'|'LE'|'LT'|'GE'|'GT'|'BEGINS_WITH'|'BETWEEN' The only condition which expects or will accept a tuple of values is 'BETWEEN', otherwise a scalar value should be used as the key in the dict. :type attributes_to_get: list :param attributes_to_get: A list of attribute names. If supplied, only the specified attribute names will be returned. Otherwise, all attributes will be returned. :type request_limit: int :param request_limit: The maximum number of items to retrieve from Amazon DynamoDB on each request. You may want to set a specific request_limit based on the provisioned throughput of your table. The default behavior is to retrieve as many results as possible per request. :type max_results: int :param max_results: The maximum number of results that will be retrieved from Amazon DynamoDB in total. For example, if you only wanted to see the first 100 results from the query, regardless of how many were actually available, you could set max_results to 100 and the generator returned from the query method will only yeild 100 results max. :type consistent_read: bool :param consistent_read: If True, a consistent read request is issued. Otherwise, an eventually consistent request is issued. :type scan_index_forward: bool :param scan_index_forward: Specified forward or backward traversal of the index. Default is forward (True). :type exclusive_start_key: list or tuple :param exclusive_start_key: Primary key of the item from which to continue an earlier query. This would be provided as the LastEvaluatedKey in that query. :type item_class: Class :param item_class: Allows you to override the class used to generate the items. This should be a subclass of :class:`boto.dynamodb.item.Item`",1,0,1,1,3,1,0,1,1,3
"def list_container_processes(container):
  cmd = ['ps', 'ax', '-o', ','.join(PsRow.columns())]
  ps_lines = output_lines(container.exec_run(cmd))
  header = ps_lines.pop(0)
  maxsplit = len(header.strip().split()) - 1
  ps_entries = [line.strip().split(None, maxsplit) for line in ps_lines]
  ps_rows = [PsRow(*entry) for entry in ps_entries]
  cmd_string = ' '.join(cmd)
  ps_rows = [row for row in ps_rows if row.args != cmd_string]
  return ps_rows",List the processes running inside a container. We use an exec rather than `container.top()` because we want to run 'ps' inside the container. This is because we want to get PIDs and usernames in the container's namespaces. `container.top()` uses 'ps' from outside the container in the host's namespaces. Note that this requires the container to have a 'ps' that responds to the arguments we give it-- we use BusyBox's (Alpine's) 'ps' as a baseline for available functionality. :param container: the container to query :return: a list of PsRow objects,1,0,0,1,2,1,0,0,1,2
"def load(self, username):
  config_dir = StorageTools.getStorageForPhone(username)
  logger.debug(""Detecting config for username=%s, dir=%s"" % (username, config_dir))
  exhausted = []
  for ftype in self.MAP_EXT:
  if len(ftype):
  fname = (self.NAME_FILE_CONFIG + ""."" + ftype)
  else:
  fname = self.NAME_FILE_CONFIG
  fpath = os.path.join(config_dir, fname)
  logger.debug(""Trying %s"" % fpath)
  if os.path.isfile(fpath):
  return self.load_path(fpath)
  exhausted.append(fpath)
  logger.error(""Could not find a config for username=%s, paths checked: %s"" % (username, "":"".join(exhausted)))",:param username: :type username: :return: :rtype:,1,0,0,0,1,1,0,0,1,2
"def collection_list(self, resource_id, resource_type=""collection""):
  ret = []
  cursor = self.db.cursor()
  if resource_type == ""collection"":
  cursor.execute(
  ""SELECT resourceComponentId FROM ResourcesComponents WHERE parentResourceComponentId IS NULL AND resourceId=%s"",
  (resource_id),
  )
  else:
  ret.append(resource_id)
  cursor.execute(
  ""SELECT resourceComponentId FROM ResourcesComponents WHERE parentResourceComponentId=%s"",
  (resource_id),
  )
  rows = cursor.fetchall()
  if len(rows):
  for row in rows:
  ret.extend(self.collection_list(row[0], ""description""))
  return ret",Fetches a list of all resource and component IDs within the specified resource. :param long resource_id: The ID of the resource to fetch children from. :param string resource_type: Specifies whether the resource to fetch is a collection or a child element. Defaults to 'collection'. :return: A list of longs representing the database resource IDs for all children of the requested record. :rtype list:,1,0,1,1,3,1,0,1,1,3
"def get_results_priority(args):
  redirect_mode = args.bang or args.search or args.lucky
  if redirect_mode:
  results_priority = ['redirect', 'result', 'abstract']
  else:
  results_priority = ['answer', 'abstract', 'result']
  insert_pos = 0 if args.define else len(results_priority)
  results_priority.insert(insert_pos, 'definition')
  return results_priority",Return a result priority list based on user input,1,0,0,1,2,1,0,0,1,2
"def team(self, name=None, id=None, is_hidden=False, **kwargs):
  _teams = self.teams(name=name, id=id, **kwargs)
  if len(_teams) == 0:
  raise NotFoundError(""No team criteria matches"")
  if len(_teams) != 1:
  raise MultipleFoundError(""Multiple teams fit criteria"")
  return _teams[0]",Team of KE-chain. Provides a team of :class:`Team` of KE-chain. You can filter on team name or provide id. :param name: (optional) team name to filter :type name: basestring or None :param id: (optional) id of the user to filter :type id: basestring or None :param is_hidden: (optional) boolean to show non-hidden or hidden teams or both (None) (default is non-hidden) :type is_hidden: bool or None :param kwargs: Additional filtering keyword=value arguments :type kwargs: dict or None :return: List of :class:`Team` :raises NotFoundError: when a user could not be found :raises MultipleFoundError: when more than a single user can be found,1,0,1,1,3,1,0,0,1,2
"def json_update(self, json_str, exclude=[], ignore_non_defaults=True):
  update_dict = json.loads(json_str, cls=MongoliaJSONDecoder, encoding=""utf-8"")
  if ID_KEY in update_dict:
  del update_dict[ID_KEY]
  for key in frozenset(exclude).intersection(frozenset(update_dict)):
  del update_dict[key]
  if self.DEFAULTS and ignore_non_defaults:
  for key in frozenset(update_dict).difference(frozenset(self.DEFAULTS)):
  del update_dict[key]
  self.update(update_dict)","Updates a database object based on a json object. The intent of this method is to allow passing json to an interface which then subsequently manipulates the object and then sends back an update. Mongolia will also automatically convert any json values that were initially converted from ObjectId and datetime.datetime objects back to their native python object types. Note: if using AngularJS, make sure to pass json back using `angular.toJson(obj)` instead of `JSON.stringify(obj)` since angular sometimes adds `$$hashkey` to javascript objects and this will cause a mongo error due to the ""$"" prefix in keys. @param json_str: the json string containing the new object to use for the update @param exclude: a list of top-level keys to exclude from the update (ID_KEY need not be included in this list; it is automatically deleted since it can't be part of a mongo update operation) @param ignore_non_defaults: if this is True and the database object has non-empty DEFAULTS, then any top-level keys in the update json that do not appear in DEFAULTS will also be excluded from the update",1,1,0,0,2,1,0,0,1,2
"def patch_namespaced_endpoints(self, name, namespace, body, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.patch_namespaced_endpoints_with_http_info(name, namespace, body, **kwargs)
  else:
  (data) = self.patch_namespaced_endpoints_with_http_info(name, namespace, body, **kwargs)
  return data","partially update the specified Endpoints This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.patch_namespaced_endpoints(name, namespace, body, async_req=True) >>> result = thread.get() :param async_req bool :param str name: name of the Endpoints (required) :param str namespace: object name and auth scope, such as for teams and projects (required) :param object body: (required) :param str pretty: If 'true', then the output is pretty printed. :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint. This field is required for apply requests (application/apply-patch) but optional for non-apply patch types (JsonPatch, MergePatch, StrategicMergePatch). :param bool force: Force is going to \""force\"" Apply requests. It means user will re-acquire conflicting fields owned by other people. Force flag must be unset for non-apply patch requests. :return: V1Endpoints If the method is called asynchronously, returns the request thread.",1,0,0,1,2,1,0,0,1,2
"def find_default(self, filename):
  for path in self.DEFAULT_PATH:
  path = os.path.expanduser(os.path.expandvars(path))
  fullname = os.path.realpath(os.path.join(path, filename))
  if os.path.exists(fullname) and os.path.isfile(fullname):
  yield fullname",A generate which looks in common folders for the default configuration file. The paths goes from system defaults to user specific files. :param filename: The name of the file to find :return: The complete path to the found files,0,0,0,1,1,1,0,0,1,2
"def can_create_objective_bank_with_record_types(self, objective_bank_record_types):
  if self._catalog_session is not None:
  return self._catalog_session.can_create_catalog_with_record_types(catalog_record_types=objective_bank_record_types)
  return True","Tests if this user can create a single ``ObjectiveBank`` using the desired record types. While ``LearningManager.getObjectiveBankRecordTypes()`` can be used to examine which records are supported, this method tests which record(s) are required for creating a specific ``ObjectiveBank``. Providing an empty array tests if an ``ObjectiveBank`` can be created with no records. arg: objective_bank_record_types (osid.type.Type[]): array of objective bank record types return: (boolean) - ``true`` if ``ObjectiveBank`` creation using the specified ``Types`` is supported, ``false`` otherwise raise: NullArgument - ``objective_bank_record_types`` is ``null`` *compliance: mandatory -- This method must be implemented.*",2,0,1,1,4,1,0,0,1,2
"def download_to_file(self, file_obj, client=None, start=None, end=None):
  download_url = self._get_download_url()
  headers = _get_encryption_headers(self._encryption_key)
  headers[""accept-encoding""] = ""gzip""
  transport = self._get_transport(client)
  try:
  self._do_download(transport, file_obj, download_url, headers, start, end)
  except resumable_media.InvalidResponse as exc:
  _raise_from_invalid_response(exc)","Download the contents of this blob into a file-like object. .. note:: If the server-set property, :attr:`media_link`, is not yet initialized, makes an additional API request to load it. Downloading a file that has been encrypted with a `customer-supplied`_ encryption key: .. literalinclude:: snippets.py :start-after: [START download_to_file] :end-before: [END download_to_file] :dedent: 4 The ``encryption_key`` should be a str or bytes with a length of at least 32. For more fine-grained control over the download process, check out `google-resumable-media`_. For example, this library allows downloading **parts** of a blob rather than the whole thing. If :attr:`user_project` is set on the bucket, bills the API request to that project. :type file_obj: file :param file_obj: A file handle to which to write the blob's data. :type client: :class:`~google.cloud.storage.client.Client` or ``NoneType`` :param client: Optional. The client to use. If not passed, falls back to the ``client`` stored on the blob's bucket. :type start: int :param start: Optional, the first byte in a range to be downloaded. :type end: int :param end: Optional, The last byte in a range to be downloaded. :raises: :class:`google.cloud.exceptions.NotFound`",1,0,0,0,1,1,0,0,1,2
"def reply(self, incoming, user, message, prefix=None):
  if not isinstance(user, User):
  user = User(user)
  if isinstance(incoming, User):
  if prefix:
  self.msg(user, ""%s: %s"" % (user.nick, message))
  else:
  self.msg(user, message)
  else:
  if prefix is not False:
  self.msg(incoming, ""%s: %s"" % (user.nick, message))
  else:
  self.msg(incoming, message)","Replies to a user in a given channel or PM. If the specified incoming is a user, simply sends a PM to user. If the specified incoming is a channel, prefixes the message with the user's nick and sends it to the channel. This is specifically useful in creating responses to commands that can be used in either a channel or in a PM, and responding to the person who invoked the command.",1,0,0,1,2,1,0,0,1,2
"def _RunInTransaction(self, function, readonly=False):
  start_query = ""START TRANSACTION;""
  if readonly:
  start_query = ""START TRANSACTION WITH CONSISTENT SNAPSHOT, READ ONLY;""
  for retry_count in range(_MAX_RETRY_COUNT):
  with contextlib.closing(self.pool.get()) as connection:
  try:
  with contextlib.closing(connection.cursor()) as cursor:
  cursor.execute(start_query)
  ret = function(connection)
  if not readonly:
  connection.commit()
  return ret
  except MySQLdb.OperationalError as e:
  connection.rollback()
  if retry_count >= _MAX_RETRY_COUNT or not _IsRetryable(e):
  raise
  time.sleep(random.uniform(1.0, 2.0) * math.pow(1.5, retry_count))
  raise Exception(""Looped ended early - last exception swallowed."")","Runs function within a transaction. Allocates a connection, begins a transaction on it and passes the connection to function. If function finishes without raising, the transaction is committed. If function raises, the transaction will be rolled back, if a retryable database error is raised, the operation may be repeated. Args: function: A function to be run, must accept a single MySQLdb.connection parameter. readonly: Indicates that only a readonly (snapshot) transaction is required. Returns: The value returned by the last call to function. Raises: Any exception raised by function.",0,1,1,0,2,1,1,1,0,3
"def get_user_metadata(
  self,
  bucket: str,
  key: str
  ) -> typing.Dict[str, str]:
  try:
  response = self.get_all_metadata(bucket, key)
  metadata = response['Metadata'].copy()
  response = self.s3_client.get_object_tagging(
  Bucket=bucket,
  Key=key,
  )
  for tag in response['TagSet']:
  key, value = tag['Key'], tag['Value']
  metadata[key] = value
  return metadata
  except botocore.exceptions.ClientError as ex:
  if str(ex.response['Error']['Code']) == \
  str(requests.codes.not_found):
  raise BlobNotFoundError(f""Could not find s3://{bucket}/{key}"") from ex
  raise BlobStoreUnknownError(ex)","Retrieves the user metadata for a given object in a given bucket. If the platform has any mandatory prefixes or suffixes for the metadata keys, they should be stripped before being returned. :param bucket: the bucket the object resides in. :param key: the key of the object for which metadata is being retrieved. :return: a dictionary mapping metadata keys to metadata values.",1,0,0,1,2,1,0,0,1,2
"def process_targets_element(cls, scanner_target):
  target_list = []
  for target in scanner_target:
  ports = ''
  credentials = {}
  for child in target:
  if child.tag == 'hosts':
  hosts = child.text
  if child.tag == 'ports':
  ports = child.text
  if child.tag == 'credentials':
  credentials = cls.process_credentials_elements(child)
  if hosts:
  target_list.append([hosts, ports, credentials])
  else:
  raise OSPDError('No target to scan', 'start_scan')
  return target_list","Receive an XML object with the target, ports and credentials to run a scan against. @param: XML element with target subelements. Each target has <hosts> and <ports> subelements. Hosts can be a single host, a host range, a comma-separated host list or a network address. <ports> and <credentials> are optional. Therefore each ospd-scanner should check for a valid ones if needed. Example form: <targets> <target> <hosts>localhosts</hosts> <ports>80,443</ports> </target> <target> <hosts>192.168.0.0/24</hosts> <ports>22</ports> <credentials> <credential type=""up"" service=""ssh"" port=""22""> <username>scanuser</username> <password>mypass</password> </credential> <credential type=""up"" service=""smb""> <username>smbuser</username> <password>mypass</password> </credential> </credentials> </target> </targets> @return: A list of (hosts, port) tuples. Example form: [['localhost', '80,43'], ['192.168.0.0/24', '22', {'smb': {'type': type, 'port': port, 'username': username, 'password': pass, }}]]",1,0,0,1,2,1,0,0,1,2
"def _load(self):
  data = get_data(self.endpoint, self.id_, force_lookup=self.__force_lookup)
  for key, val in data.items():
  if key == 'location_area_encounters' \
  and self.endpoint == 'pokemon':
  params = val.split('/')[-3:]
  ep, id_, subr = params
  encounters = get_data(ep, int(id_), subr)
  data[key] = [_make_obj(enc) for enc in encounters]
  continue
  if isinstance(val, dict):
  data[key] = _make_obj(val)
  elif isinstance(val, list):
  data[key] = [_make_obj(i) for i in val]
  self.__dict__.update(data)
  return None","Function to collect reference data and connect it to the instance as attributes. Internal function, does not usually need to be called by the user, as it is called automatically when an attribute is requested. :return None",1,0,0,0,1,1,0,0,1,2
"def call(self, rs, name, user, fields):
  if name not in self._objects:
  return '[ERR: Object Not Found]'
  func = self._objects[name]
  reply = ''
  try:
  reply = func(rs, fields)
  if reply is None:
  reply = ''
  except Exception as e:
  raise PythonObjectError(""Error executing Python object: "" + str(e))
  return text_type(reply)",Invoke a previously loaded object. :param RiveScript rs: the parent RiveScript instance. :param str name: The name of the object macro to be called. :param str user: The user ID invoking the object macro. :param []str fields: Array of words sent as the object's arguments. :return str: The output of the object macro.,0,0,0,1,1,1,0,0,1,2
"def find_entries(self, users, start, *args, **kwargs):
  forever = kwargs.get('all', False)
  for user in users:
  if forever:
  entries = Entry.objects.filter(user=user).order_by('start_time')
  else:
  entries = Entry.objects.filter(
  user=user, start_time__gte=start).order_by(
  'start_time')
  yield entries","Find all entries for all users, from a given starting point. If no starting point is provided, all entries are returned.",0,0,1,1,2,1,0,1,1,3
"def read_namespaced_persistent_volume_claim(self, name, namespace, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.read_namespaced_persistent_volume_claim_with_http_info(name, namespace, **kwargs)
  else:
  (data) = self.read_namespaced_persistent_volume_claim_with_http_info(name, namespace, **kwargs)
  return data","read_namespaced_persistent_volume_claim # noqa: E501 read the specified PersistentVolumeClaim # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.read_namespaced_persistent_volume_claim(name, namespace, async_req=True) >>> result = thread.get() :param async_req bool :param str name: name of the PersistentVolumeClaim (required) :param str namespace: object name and auth scope, such as for teams and projects (required) :param str pretty: If 'true', then the output is pretty printed. :param bool exact: Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. :param bool export: Should this value be exported. Export strips fields that a user can not specify. :return: V1PersistentVolumeClaim If the method is called asynchronously, returns the request thread.",1,0,0,1,2,1,0,0,1,2
"def interactive(renderer):
  _import_readline()
  _print_heading(renderer)
  contents = []
  more = False
  while True:
  try:
  prompt, more = ('... ', True) if more else ('>>> ', True)
  contents.append(input(prompt) + '\n')
  except EOFError:
  print('\n' + mistletoe.markdown(contents, renderer), end='')
  more = False
  contents = []
  except KeyboardInterrupt:
  print('\nExiting.')
  break","Parse user input, dump to stdout, rinse and repeat. Python REPL style.",1,0,0,1,2,1,0,0,1,2
"def listFields(self, template):
  try:
  temp_source = self.environment.loader.get_source(self.environment,
  template)
  return self.listFieldsFromSource(temp_source)
  except AttributeError:
  err_msg = ""Invalid value for 'template'""
  self.log.error(err_msg)
  raise exception.BadValue(err_msg)","List all the attributes to be rendered from the template file :param template: The template to render. The template is actually a file, which is usually generated by :class:`rtcclient.template.Templater.getTemplate` and can also be modified by user accordingly. :return: a :class:`set` contains all the needed attributes :rtype: set",1,0,0,1,2,1,0,0,1,2
"def can_create_family_with_record_types(self, family_record_types):
  if self._catalog_session is not None:
  return self._catalog_session.can_create_catalog_with_record_types(catalog_record_types=family_record_types)
  return True","Tests if this user can create a single ``Family`` using the desired record types. While ``RelationshipManager.getFamilyRecordTypes()`` can be used to examine which records are supported, this method tests which record(s) are required for creating a specific ``Family``. Providing an empty array tests if a ``Family`` can be created with no records. arg: family_record_types (osid.type.Type[]): array of family record types return: (boolean) - ``true`` if ``Family`` creation using the specified record ``Types`` is supported, ``false`` otherwise raise: NullArgument - ``family_record_types is null`` *compliance: mandatory -- This method must be implemented.*",1,0,0,1,2,1,0,0,1,2
"async def send_activity(self, *activity_or_text: Union[Activity, str]) -> ResourceResponse:
  reference = TurnContext.get_conversation_reference(self.activity)
  output = [TurnContext.apply_conversation_reference(
  Activity(text=a, type='message') if isinstance(a, str) else a, reference)
  for a in activity_or_text]
  for activity in output:
  activity.input_hint = 'acceptingInput'
  async def callback(context: 'TurnContext', output):
  responses = await context.adapter.send_activities(context, output)
  context._responded = True
  return responses
  await self._emit(self._on_send_activities, output, callback(self, output))",Sends a single activity or message to the user. :param activity_or_text: :return:,1,0,0,2,3,1,0,0,1,2
"def restore(mongo_user, mongo_password, backup_tbz_path,
  backup_directory_output_path=""/tmp/mongo_dump"",
  drop_database=False, cleanup=True, silent=False,
  skip_system_and_user_files=False):
  if not path.exists(backup_tbz_path):
  raise Exception(""the provided tar file %s does not exist."" % (backup_tbz_path))
  untarbz(backup_tbz_path, backup_directory_output_path, silent=silent)
  if skip_system_and_user_files:
  system_and_users_path = ""%s/admin"" % backup_directory_output_path
  if path.exists(system_and_users_path):
  rmtree(system_and_users_path)
  mongorestore(mongo_user, mongo_password, backup_directory_output_path,
  drop_database=drop_database, silent=silent)
  if cleanup:
  rmtree(backup_directory_output_path)","Runs mongorestore with source data from the provided .tbz backup, using the provided username and password. The contents of the .tbz will be dumped into the provided backup directory, and that folder will be deleted after a successful mongodb restore unless cleanup is set to False. Note: the skip_system_and_user_files is intended for use with the changes in user architecture introduced in mongodb version 2.6. Warning: Setting drop_database to True will drop the ENTIRE CURRENTLY RUNNING DATABASE before restoring. Mongorestore requires a running mongod process, in addition the provided user must have restore permissions for the database. A mongolia superuser will have more than adequate permissions, but a regular user may not. By default this function will clean up the output of the untar operation.",0,1,0,0,1,1,1,0,0,2
"def add_students(self, student_emails, nid=None):
  r = self.request(
  method=""network.update"",
  data={
  ""from"": ""ClassSettingsPage"",
  ""add_students"": student_emails
  },
  nid=nid,
  nid_key=""id""
  )
  return self._handle_error(r, ""Could not add users."")","Enroll students in a network `nid`. Piazza will email these students with instructions to activate their account. :type student_emails: list of str :param student_emails: A listing of email addresses to enroll in the network (or class). This can be a list of length one. :type nid: str :param nid: This is the ID of the network to add students to. This is optional and only to override the existing `network_id` entered when created the class :returns: Python object containing returned data, a list of dicts of user data of all of the users in the network including the ones that were just added.",2,0,0,2,4,1,0,0,1,2
"def patch_validating_webhook_configuration(self, name, body, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.patch_validating_webhook_configuration_with_http_info(name, body, **kwargs)
  else:
  (data) = self.patch_validating_webhook_configuration_with_http_info(name, body, **kwargs)
  return data","partially update the specified ValidatingWebhookConfiguration This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.patch_validating_webhook_configuration(name, body, async_req=True) >>> result = thread.get() :param async_req bool :param str name: name of the ValidatingWebhookConfiguration (required) :param object body: (required) :param str pretty: If 'true', then the output is pretty printed. :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint. This field is required for apply requests (application/apply-patch) but optional for non-apply patch types (JsonPatch, MergePatch, StrategicMergePatch). :param bool force: Force is going to \""force\"" Apply requests. It means user will re-acquire conflicting fields owned by other people. Force flag must be unset for non-apply patch requests. :return: V1beta1ValidatingWebhookConfiguration If the method is called asynchronously, returns the request thread.",1,0,0,1,2,1,0,0,1,2
"def get_transport(host, username, key):
  if host == shakedown.master_ip():
  transport = paramiko.Transport(host)
  else:
  transport_master = paramiko.Transport(shakedown.master_ip())
  transport_master = start_transport(transport_master, username, key)
  if not transport_master.is_authenticated():
  print(""error: unable to authenticate {}@{} with key {}"".format(username, shakedown.master_ip(), key))
  return False
  try:
  channel = transport_master.open_channel('direct-tcpip', (host, 22), ('127.0.0.1', 0))
  except paramiko.SSHException:
  print(""error: unable to connect to {}"".format(host))
  return False
  transport = paramiko.Transport(channel)
  return transport",Create a transport object :param host: the hostname to connect to :type host: str :param username: SSH username :type username: str :param key: key object used for authentication :type key: paramiko.RSAKey :return: a transport object :rtype: paramiko.Transport,1,0,0,0,1,1,0,0,1,2
"def task_view_user(self, ):
  if not self.cur_task:
  return
  i = self.task_user_tablev.currentIndex()
  item = i.internalPointer()
  if item:
  user = item.internal_data()
  self.view_user(user)",View the user that is currently selected :returns: None :rtype: None :raises: None,1,0,0,1,2,1,0,0,1,2
"def add_session(session=None):
  r
  user_id, sid_s = session['user_id'], session.sid_s
  with db.session.begin_nested():
  session_activity = SessionActivity(
  user_id=user_id,
  sid_s=sid_s,
  ip=request.remote_addr,
  country=_ip2country(request.remote_addr),
  **_extract_info_from_useragent(
  request.headers.get('User-Agent', '')
  )
  )
  db.session.merge(session_activity)","r""""""Add a session to the SessionActivity table. :param session: Flask Session object to add. If None, ``session`` is used. The object is expected to have a dictionary entry named ``""user_id""`` and a field ``sid_s``",1,1,1,0,3,1,1,0,1,3
"def get(name, rc_file='~/.odoorpcrc'):
  conf = ConfigParser()
  conf.read([os.path.expanduser(rc_file)])
  if not conf.has_section(name):
  raise ValueError(
  ""'%s' session does not exist in %s"" % (name, rc_file))
  return {
  'type': conf.get(name, 'type'),
  'host': conf.get(name, 'host'),
  'protocol': conf.get(name, 'protocol'),
  'port': conf.getint(name, 'port'),
  'timeout': conf.getfloat(name, 'timeout'),
  'user': conf.get(name, 'user'),
  'passwd': conf.get(name, 'passwd'),
  'database': conf.get(name, 'database'),
  }","Return the session configuration identified by `name` from the `rc_file` file. >>> import odoorpc >>> from pprint import pprint as pp >>> pp(odoorpc.session.get('foo')) # doctest: +SKIP {'database': 'db_name', 'host': 'localhost', 'passwd': 'password', 'port': 8069, 'protocol': 'jsonrpc', 'timeout': 120, 'type': 'ODOO', 'user': 'admin'} .. doctest:: :hide: >>> import odoorpc >>> session = '%s_session' % DB >>> odoo.save(session) >>> data = odoorpc.session.get(session) >>> data['host'] == HOST True >>> data['protocol'] == PROTOCOL True >>> data['port'] == int(PORT) True >>> data['database'] == DB True >>> data['user'] == USER True >>> data['passwd'] == PWD True >>> data['type'] == 'ODOO' True :raise: `ValueError` (wrong session name)",1,0,0,1,2,1,0,0,1,2
"def user_groups(self, ldap_user, group_search):
  group_info_map = {}
  member_dn_set = {ldap_user.dn}
  handled_dn_set = set()
  while len(member_dn_set) > 0:
  group_infos = self.find_groups_with_any_member(
  member_dn_set, group_search, ldap_user.connection
  )
  new_group_info_map = {info[0]: info for info in group_infos}
  group_info_map.update(new_group_info_map)
  handled_dn_set.update(member_dn_set)
  member_dn_set = set(new_group_info_map.keys()) - handled_dn_set
  return group_info_map.values()","This searches for all of a user's groups from the bottom up. In other words, it returns the groups that the user belongs to, the groups that those groups belong to, etc. Circular references will be detected and pruned.",1,0,0,1,2,1,0,1,1,3
"def scale(text="""", value=0, min=0 ,max=100, step=1, draw_value=True, title="""",
  width=DEFAULT_WIDTH, height=DEFAULT_HEIGHT, timeout=None):
  dialog = ZScale(text, value, min, max, step,
  draw_value, title, width, height, timeout)
  dialog.run()
  return dialog.response",Select a number with a range widget :param text: text inside window :type text: str :param value: current value :type value: int :param min: minimum value :type min: int :param max: maximum value :type max: int :param step: incrementation value :type step: int :param draw_value: hide/show cursor value :type draw_value: bool :param title: title of the window :type title: str :param width: window width :type width: int :param height: window height :type height: int :param timeout: close the window after n seconds :type timeout: int :return: The value selected by the user :rtype: float,1,0,0,1,2,1,0,0,1,2
"def open_as_file(self, filename=None, clean_data=True, raw=False,
  trash=True):
  if filename or not self.filename:
  full_path = filename and get_filename(filename or self.filename,
  trash)
  if full_path != self.filename or not os.path.exists(full_path):
  self.filename = self.save(filename or self.name, clean_data,
  raw, trash)
  os.system('open ' + self.filename)",This will save the file and then run a system command to open it. The default path will be the users trash folder :param clean_data: func call that will clean the data before saving it :param raw: obj of the return object from request :param trash: bool if true puts the file in the Trash folder :param filename: str of the name of the file :return: str of the full path,1,0,0,1,2,1,0,0,1,2
"def gfuds(udfuns, udqdec, relate, refval, adjust, step, nintvls, cnfine, result):
  relate = stypes.stringToCharP(relate)
  refval = ctypes.c_double(refval)
  adjust = ctypes.c_double(adjust)
  step = ctypes.c_double(step)
  nintvls = ctypes.c_int(nintvls)
  libspice.gfuds_c(udfuns, udqdec, relate, refval, adjust, step, nintvls, ctypes.byref(cnfine), ctypes.byref(result))
  return result","Perform a GF search on a user defined scalar quantity. https://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/gfuds_c.html :param udfuns: Name of the routine that computes the scalar quantity of interest at some time. :type udfuns: ctypes.CFunctionType :param udqdec: Name of the routine that computes whether the scalar quantity is decreasing. :type udqdec: ctypes.CFunctionType :param relate: Operator that either looks for an extreme value (max, min, local, absolute) or compares the geometric quantity value and a number. :type relate: str :param refval: Value used as reference for scalar quantity condition. :type refval: float :param adjust: Allowed variation for absolute extremal geometric conditions. :type adjust: float :param step: Step size used for locating extrema and roots. :type step: float :param nintvls: Workspace window interval count. :type nintvls: int :param cnfine: SPICE window to which the search is restricted. :type cnfine: spiceypy.utils.support_types.SpiceCell :param result: SPICE window containing results. :type result: spiceypy.utils.support_types.SpiceCell :return: result :rtype: spiceypy.utils.support_types.SpiceCell",0,0,0,1,1,1,0,0,1,2
"def get_logs(self, login=None, **kwargs):
  _login = kwargs.get(
  'login',
  login
  )
  log_events_url = GSA_LOGS_URL.format(login=_login)
  return self._request_api(url=log_events_url).json()",Get a user's logs. :param str login: User's login (Default: self._login) :return: JSON,2,0,0,1,3,2,0,0,1,3
"def run(self, value, errors, request):
  thing = super(ViewableDBThing, self).run(value, errors, request)
  if errors:
  return None
  if not thing.can_view(request.user):
  message = 'Insufficient permissions for {0}'.format(self.param)
  raise HTTPForbidden(message)
  return thing","Return thing, but abort validation if request.user cannot view.",2,0,1,1,4,1,0,0,1,2
"def get_required_query_params(self, request):
  username = get_request_value(request, self.REQUIRED_PARAM_USERNAME, '')
  course_id = get_request_value(request, self.REQUIRED_PARAM_COURSE_ID, '')
  program_uuid = get_request_value(request, self.REQUIRED_PARAM_PROGRAM_UUID, '')
  enterprise_customer_uuid = get_request_value(request, self.REQUIRED_PARAM_ENTERPRISE_CUSTOMER)
  if not (username and (course_id or program_uuid) and enterprise_customer_uuid):
  raise ConsentAPIRequestError(
  self.get_missing_params_message([
  (""'username'"", bool(username)),
  (""'enterprise_customer_uuid'"", bool(enterprise_customer_uuid)),
  (""one of 'course_id' or 'program_uuid'"", bool(course_id or program_uuid)),
  ])
  )
  return username, course_id, program_uuid, enterprise_customer_uuid","Gets ``username``, ``course_id``, and ``enterprise_customer_uuid``, which are the relevant query parameters for this API endpoint. :param request: The request to this endpoint. :return: The ``username``, ``course_id``, and ``enterprise_customer_uuid`` from the request.",1,0,0,0,1,1,0,0,1,2
"def search_user(self, user_name, quiet=False, limit=9):
  result = self.search(user_name, search_type=1002, limit=limit)
  if result['result']['userprofileCount'] <= 0:
  LOG.warning('User %s not existed!', user_name)
  raise SearchNotFound('user {} not existed'.format(user_name))
  else:
  users = result['result']['userprofiles']
  if quiet:
  user_id, user_name = users[0]['userId'], users[0]['nickname']
  user = User(user_id, user_name)
  return user
  else:
  return self.display.select_one_user(users)",Search user by user name. :params user_name: user name. :params quiet: automatically select the best one. :params limit: user count returned by weapi. :return: a User object.,2,0,0,1,3,2,0,1,1,4
"def get(self, user_id, lang='zh_CN'):
  assert lang in ('zh_CN', 'zh_TW', 'en'), 'lang can only be one of \
  zh_CN, zh_TW, en language codes'
  return self._get(
  'user/info',
  params={
  'openid': user_id,
  'lang': lang
  }
  )","UnionID  https://mp.weixin.qq.com/wiki?t=resource/res_main&id=mp1421140839 :param user_id:   :param lang: zh_CN zh_TW en  :return:  JSON  :: from wechatpy import WeChatClient client = WeChatClient('appid', 'secret') user = client.user.get('openid')",2,0,0,1,3,1,0,0,1,2
"def get_connection(self, url, proxies=None):
  proxy = select_proxy(url, proxies)
  if proxy:
  proxy = prepend_scheme_if_needed(proxy, 'http')
  proxy_url = parse_url(proxy)
  if not proxy_url.host:
  raise InvalidProxyURL(""Please check proxy URL. It is malformed""
  "" and could be missing the host."")
  proxy_manager = self.proxy_manager_for(proxy)
  conn = proxy_manager.connection_from_url(url)
  else:
  parsed = urlparse(url)
  url = parsed.geturl()
  conn = self.poolmanager.connection_from_url(url)
  return conn","Returns a urllib3 connection for the given URL. This should not be called from user code, and is only exposed for use when subclassing the :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`. :param url: The URL to connect to. :param proxies: (optional) A Requests-style dictionary of proxies used on this request. :rtype: urllib3.ConnectionPool",1,0,0,0,1,1,0,0,1,2
"def _get_account_policy(name):
  cmd = 'pwpolicy -u {0} -getpolicy'.format(name)
  try:
  ret = salt.utils.mac_utils.execute_return_result(cmd)
  except CommandExecutionError as exc:
  if 'Error: user <{0}> not found'.format(name) in exc.strerror:
  raise CommandExecutionError('User not found: {0}'.format(name))
  raise CommandExecutionError('Unknown error: {0}'.format(exc.strerror))
  try:
  policy_list = ret.split('\n')[1].split(' ')
  policy_dict = {}
  for policy in policy_list:
  if '=' in policy:
  key, value = policy.split('=')
  policy_dict[key] = value
  return policy_dict
  except IndexError:
  return {}",Get the entire accountPolicy and return it as a dictionary. For use by this module only :param str name: The user name :return: a dictionary containing all values for the accountPolicy :rtype: dict :raises: CommandExecutionError on user not found or any other unknown error,1,0,0,1,2,1,0,0,1,2
"def login(self, email, password):
  if self.options.get(""email"") and self.options.get(""password""):
  res = self.session.http.post(self.login_url, data={""email"": email,
  ""password"": password,
  ""redirect"": None,
  ""submit"": ""Login""})
  if res.cookies.get(""password"") and res.cookies.get(""email""):
  return res.cookies.get(""email"")
  else:
  log.error(""Failed to login to Schoolism, incorrect email/password combination"")
  else:
  log.error(""An email and password are required to access Schoolism streams"")",Login to the schoolism account and return the users account :param email: (str) email for account :param password: (str) password for account :return: (str) users email,2,0,0,2,4,2,0,0,1,3
"def sync(self, api_token, sync_token, resource_types='[""all""]', **kwargs):
  params = {
  'token': api_token,
  'sync_token': sync_token,
  }
  req_func = self._post
  if 'commands' not in kwargs:
  req_func = self._get
  params['resource_types'] = resource_types
  return req_func('sync', params, **kwargs)","Update and retrieve Todoist data. :param api_token: The user's login api_token. :type api_token: str :param seq_no: The request sequence number. On initial request pass ``0``. On all others pass the last seq_no you received. :type seq_no: int :param seq_no_global: The request sequence number. On initial request pass ``0``. On all others pass the last seq_no you received. :type seq_no_global: int :param resource_types: Specifies which subset of data you want to receive e.g. only projects. Defaults to all data. :type resources_types: str :param commands: A list of JSON commands to perform. :type commands: list (str) :return: The HTTP response to the request. :rtype: :class:`requests.Response` >>> from pytodoist.api import TodoistAPI >>> api = TodoistAPI() >>> response = api.register('john.doe@gmail.com', 'John Doe', ... 'password') >>> user_info = response.json() >>> api_token = user_info['api_token'] >>> response = api.sync(api_token, 0, 0, '[""projects""]') >>> print(response.json()) {'seq_no_global': 3848029654, 'seq_no': 3848029654, 'Projects': ...}",2,0,1,2,5,1,0,0,1,2
"def get_object_from_session(entity, key):
  def object_from_session_function(service, message):
  id_ = get_value_from_session(key)(service, message)
  result = service.session.query(entity).get(id_)
  if not result:
  raise SelenolInvalidArgumentException(key, id_)
  return result
  return object_from_session_function",Get an object from the database given an entity and the session key. :param entity: Class type of the object to retrieve. :param key: Array that defines the path of the value inside the message.,0,0,1,0,1,0,0,1,0,1
"def connect_patch_namespaced_service_proxy_with_path(self, name, namespace, path, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.connect_patch_namespaced_service_proxy_with_path_with_http_info(name, namespace, path, **kwargs)
  else:
  (data) = self.connect_patch_namespaced_service_proxy_with_path_with_http_info(name, namespace, path, **kwargs)
  return data","connect_patch_namespaced_service_proxy_with_path # noqa: E501 connect PATCH requests to proxy of Service # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.connect_patch_namespaced_service_proxy_with_path(name, namespace, path, async_req=True) >>> result = thread.get() :param async_req bool :param str name: name of the ServiceProxyOptions (required) :param str namespace: object name and auth scope, such as for teams and projects (required) :param str path: path to the resource (required) :param str path2: Path is the part of URLs that include service endpoints, suffixes, and parameters to use for the current proxy request to service. For example, the whole request URL is http://localhost/api/v1/namespaces/kube-system/services/elasticsearch-logging/_search?q=user:kimchy. Path is _search?q=user:kimchy. :return: str If the method is called asynchronously, returns the request thread.",1,0,0,1,2,1,0,0,1,2
"def promptyn(msg, default=None):
  while True:
  yes = ""Y"" if default else ""y""
  if default or default is None:
  no = ""n""
  else:
  no = ""N""
  confirm = prompt(""%s [%s/%s]"" % (msg, yes, no), """").lower()
  if confirm in (""y"", ""yes""):
  return True
  elif confirm in (""n"", ""no""):
  return False
  elif not confirm and default is not None:
  return default",Display a blocking prompt until the user confirms,1,0,0,1,2,1,0,0,1,2
"def console_to_str(data):
  encoding = locale.getpreferredencoding()
  if (not encoding) or codecs.lookup(encoding).name == ""ascii"":
  encoding = ""utf-8""
  try:
  decoded_data = data.decode(encoding)
  except UnicodeDecodeError:
  logger.warning(
  ""Subprocess output does not appear to be encoded as %s"",
  encoding,
  )
  decoded_data = data.decode(encoding, errors=backslashreplace_decode)
  output_encoding = getattr(getattr(sys, ""__stderr__"", None),
  ""encoding"", None)
  if output_encoding:
  output_encoded = decoded_data.encode(
  output_encoding,
  errors=""backslashreplace""
  )
  decoded_data = output_encoded.decode(output_encoding)
  return decoded_data","Return a string, safe for output, of subprocess output. We assume the data is in the locale preferred encoding. If it won't decode properly, we warn the user but decode as best we can. We also ensure that the output can be safely written to standard output without encoding errors.",1,0,0,1,2,1,0,0,1,2
"def json(self, url, params):
  data = self._connector.proxy_json(url, params)
  if data.get('error'):
  raise error.RPCError(
  data['error']['data']['message'],
  data['error'])
  return data","Low level method to execute JSON queries. It basically performs a request and raises an :class:`odoorpc.error.RPCError` exception if the response contains an error. You have to know the names of each parameter required by the function called, and set them in the `params` dictionary. Here an authentication request: .. doctest:: :options: +SKIP >>> data = odoo.json( ... '/web/session/authenticate', ... {'db': 'db_name', 'login': 'admin', 'password': 'admin'}) >>> from pprint import pprint >>> pprint(data) {'id': 645674382, 'jsonrpc': '2.0', 'result': {'db': 'db_name', 'session_id': 'fa740abcb91784b8f4750c5c5b14da3fcc782d11', 'uid': 1, 'user_context': {'lang': 'en_US', 'tz': 'Europe/Brussels', 'uid': 1}, 'username': 'admin'}} .. doctest:: :hide: >>> data = odoo.json( ... '/web/session/authenticate', ... {'db': DB, 'login': USER, 'password': PWD}) >>> data['result']['db'] == DB True >>> data['result']['uid'] in [1, 2] True >>> data['result']['username'] == USER True And a call to the ``read`` method of the ``res.users`` model: .. doctest:: :options: +SKIP >>> data = odoo.json( ... '/web/dataset/call', ... {'model': 'res.users', 'method': 'read', ... 'args': [[2], ['name']]}) >>> from pprint import pprint >>> pprint(data) {'id': ..., 'jsonrpc': '2.0', 'result': [{'id': 2, 'name': 'Mitchell Admin'}]} *Python 2:* :return: a dictionary (JSON response) :raise: :class:`odoorpc.error.RPCError` :raise: `urllib2.HTTPError` (if `params` is not a dictionary) :raise: `urllib2.URLError` (connection error) *Python 3:* :return: a dictionary (JSON response) :raise: :class:`odoorpc.error.RPCError` :raise: `urllib.error.HTTPError` (if `params` is not a dictionary) :raise: `urllib.error.URLError` (connection error)",1,0,0,1,2,2,0,0,1,3
"def call_user_func(settings_module, func_name, *args, **kwargs):
  if settings_module:
  if hasattr(settings_module, func_name):
  func = getattr(settings_module, func_name)
  try:
  return func(*args, **kwargs)
  finally:
  delattr(settings_module, func_name)","Call a user-supplied settings function and clean it up afterwards. settings_module may be None, or the function may not exist. If the function exists, it is called with the specified *args and **kwargs, and the result is returned.",0,0,0,1,1,1,0,0,1,2
"def update_user(self):
  if self.user_info_url:
  response = self._access_user_info()
  self.user = self._update_or_create_user(response.data,
  content=response.content)
  return authomatic.core.UserInfoResponse(self.user,
  response.httplib_response)",Updates the :attr:`.BaseProvider.user`. .. warning:: Fetches the :attr:`.user_info_url`! :returns: :class:`.UserInfoResponse`,2,0,0,1,3,2,1,0,1,4
"def get_structure_from_id(self, task_id, final_structure=True):
  args = {'task_id': task_id}
  field = 'output.crystal' if final_structure else 'input.crystal'
  results = tuple(self.query([field], args))
  if len(results) > 1:
  raise QueryError(""More than one result found for task_id {}!"".format(task_id))
  elif len(results) == 0:
  raise QueryError(""No structure found for task_id {}!"".format(task_id))
  c = results[0]
  return Structure.from_dict(c[field])",Returns a structure from the database given the task id. Args: task_id: The task_id to query for. final_structure: Whether to obtain the final or initial structure. Defaults to True.,1,0,1,1,3,1,0,1,1,3
"def reset_password(self, user, password):
  user.password = password
  self.user_manager.save(user)
  if app.config.SECURITY_SEND_PASSWORD_RESET_NOTICE_EMAIL:
  self.send_mail(
  _('flask_unchained.bundles.security:email_subject.password_reset_notice'),
  to=user.email,
  template='security/email/password_reset_notice.html',
  user=user)
  password_reset.send(app._get_current_object(), user=user)",Service method to reset a user's password. The same as :meth:`change_password` except we this method sends a different notification email. Sends signal `password_reset`. :param user: :param password: :return:,1,1,0,0,2,1,1,0,1,3
"def get_list_function_result(self, ddoc_id, list_name, view_name, **kwargs):
  ddoc = DesignDocument(self, ddoc_id)
  headers = {'Content-Type': 'application/json'}
  resp = get_docs(self.r_session,
  '/'.join([ddoc.document_url, '_list', list_name, view_name]),
  self.client.encoder,
  headers,
  **kwargs)
  return resp.text","Retrieves a customized MapReduce view result from the specified database based on the list function provided. List functions are used, for example, when you want to access Cloudant directly from a browser, and need data to be returned in a different format, such as HTML. Note: All query parameters for View requests are supported. See :class:`~cloudant.database.get_view_result` for all supported query parameters. For example: .. code-block:: python # Assuming that 'view001' exists as part of the # 'ddoc001' design document in the remote database... # Retrieve documents where the list function is 'list1' resp = db.get_list_function_result('ddoc001', 'list1', 'view001', limit=10) for row in resp['rows']: # Process data (in text format). For more detail on list functions, refer to the `Cloudant list documentation <https://console.bluemix.net/docs/services/Cloudant/api/ design_documents.html#list-functions>`_. :param str ddoc_id: Design document id used to get result. :param str list_name: Name used in part to identify the list function. :param str view_name: Name used in part to identify the view. :return: Formatted view result data in text format",1,0,0,1,2,2,0,0,1,3
"def get_logins(self, user_id, start_date=None):
  if start_date is None:
  date_object = datetime.datetime.today() - datetime.timedelta(days=30)
  start_date = date_object.strftime(""%m/%d/%Y 0:0:0"")
  date_filter = {
  'loginAttempts': {
  'createDate': {
  'operation': 'greaterThanDate',
  'options': [{'name': 'date', 'value': [start_date]}]
  }
  }
  }
  login_log = self.user_service.getLoginAttempts(id=user_id, filter=date_filter)
  return login_log","Gets the login history for a user, default start_date is 30 days ago :param int id: User id to get :param string start_date: ""%m/%d/%Y %H:%M:%s"" formatted string. :returns: list https://softlayer.github.io/reference/datatypes/SoftLayer_User_Customer_Access_Authentication/ Example:: get_logins(123, '04/08/2018 0:0:0')",2,0,0,1,3,1,0,0,1,2
"def design_documents(self):
  url = '/'.join((self.database_url, '_all_docs'))
  query = ""startkey=\""_design\""&endkey=\""_design0\""&include_docs=true""
  resp = self.r_session.get(url, params=query)
  resp.raise_for_status()
  data = response_to_json_dict(resp)
  return data['rows']",Retrieve the JSON content for all design documents in this database. Performs a remote call to retrieve the content. :returns: All design documents found in this database in JSON format,2,0,1,1,4,2,0,0,1,3
"def sendNotification(snmpDispatcher, authData, transportTarget,
  notifyType, *varBinds, **options):
  def cbFun(*args, **kwargs):
  response[:] = args
  options['cbFun'] = cbFun
  errorIndication, errorStatus, errorIndex = None, 0, 0
  response = [None, 0, 0, []]
  while True:
  if varBinds:
  ntforg.sendNotification(snmpDispatcher, authData, transportTarget,
  notifyType, *varBinds, **options)
  snmpDispatcher.transportDispatcher.runDispatcher()
  errorIndication, errorStatus, errorIndex, varBinds = response
  varBinds = (yield errorIndication, errorStatus, errorIndex, varBinds)
  if not varBinds:
  break","Creates a generator to send one or more SNMP notifications. On each iteration, new SNMP TRAP or INFORM notification is send (:RFC:`1905#section-4,2,6`). The iterator blocks waiting for INFORM acknowledgement to arrive or error to occur. Parameters ---------- snmpDispatcher: :py:class:`~pysnmp.hlapi.v1arch.asyncore.SnmpDispatcher` Class instance representing asyncore-based asynchronous event loop and associated state information. authData: :py:class:`~pysnmp.hlapi.CommunityData` or :py:class:`~pysnmp.hlapi.UsmUserData` Class instance representing SNMP credentials. transportTarget: :py:class:`~pysnmp.hlapi.asyncore.UdpTransportTarget` or :py:class:`~pysnmp.hlapi.asyncore.Udp6TransportTarget` Class instance representing transport type along with SNMP peer address. notifyType: str Indicates type of notification to be sent. Recognized literal values are *trap* or *inform*. \*varBinds: :class:`tuple` of OID-value pairs or :py:class:`~pysnmp.smi.rfc1902.ObjectType` or :py:class:`~pysnmp.smi.rfc1902.NotificationType` One or more objects representing MIB variables to place into SNMP notification. It could be tuples of OID-values or :py:class:`~pysnmp.smi.rfc1902.ObjectType` class instances of :py:class:`~pysnmp.smi.rfc1902.NotificationType` objects. Besides user variable-bindings, SNMP Notification PDU requires at least two variable-bindings to be present: 0. SNMPv2-MIB::sysUpTime.0 = <agent uptime> 1. SNMPv2-SMI::snmpTrapOID.0 = <notification ID> When sending SNMPv1 TRAP, more variable-bindings could be present: 2. SNMP-COMMUNITY-MIB::snmpTrapAddress.0 = <agent-IP> 3. SNMP-COMMUNITY-MIB::snmpTrapCommunity.0 = <snmp-community-name> 4. SNMP-COMMUNITY-MIB::snmpTrapEnterprise.0 = <enterprise-OID> If user does not supply some or any of the above variable-bindings or if they are at the wrong positions, the system will add/reorder the missing ones automatically. On top of that, some notification types imply including some additional variable-bindings providing additional details on the event being reported. Therefore it is generally easier to use :py:class:`~pysnmp.smi.rfc1902.NotificationType` object which will help adding relevant variable-bindings. Other Parameters ---------------- \*\*options : Request options: * `lookupMib` - load MIB and resolve response MIB variables at the cost of slightly reduced performance. Default is `False`. Yields ------ errorIndication: str True value indicates local SNMP error. errorStatus: str True value indicates SNMP PDU error reported by remote. errorIndex: int Non-zero value refers to `varBinds[errorIndex-1]` varBinds: tuple A sequence of :py:class:`~pysnmp.smi.rfc1902.ObjectType` class instances representing MIB variables returned in SNMP response. Raises ------ PySnmpError Or its derivative indicating that an error occurred while performing SNMP operation. Notes ----- The `sendNotification` generator will be exhausted immediately unless an instance of :py:class:`~pysnmp.smi.rfc1902.NotificationType` class or a sequence of :py:class:`~pysnmp.smi.rfc1902.ObjectType` `varBinds` are send back into running generator (supported since Python 2.6). Examples -------- >>> from pysnmp.hlapi.v1arch import * >>> >>> g = sendNotification(SnmpDispatcher(), >>> CommunityData('public'), >>> UdpTransportTarget(('demo.snmplabs.com', 162)), >>> 'trap', >>> NotificationType(ObjectIdentity('IF-MIB', 'linkDown'))) >>> next(g) (None, 0, 0, [])",1,0,0,1,2,1,0,0,1,2
"def make_choice_validator(
  choices, default_key=None, normalizer=None):
  def normalize_all(_choices):
  if normalizer:
  _choices = [(normalizer(key), value) for key, value in choices]
  return _choices
  choices = normalize_all(choices)
  def choice_validator(value):
  if normalizer:
  value = normalizer(value)
  if not value and default_key:
  value = choices[default_key][0]
  results = []
  for choice, mapped in choices:
  if value == choice:
  return mapped
  if choice.startswith(value):
  results.append((choice, mapped))
  if len(results) == 1:
  return results[0][1]
  elif not results:
  raise ValueError('Invalid choice.')
  else:
  raise ValueError(
  'Choice ambiguous between (%s)' % ', '.join(
  k for k, v in normalize_all(results))
  )
  return choice_validator","Returns a callable that accepts the choices provided. Choices should be provided as a list of 2-tuples, where the first element is a string that should match user input (the key); the second being the value associated with the key. The callable by default will match, upon complete match the first value associated with the result will be returned. Partial matches are supported. If a default is provided, that value will be returned if the user provided input is empty, i.e. the value that is mapped to the empty string. Finally, a normalizer function can be passed. This normalizes all keys and validation value.",1,0,0,1,2,1,0,0,1,2
"def institute(ctx, internal_id, display_name, sanger_recipients):
  adapter = ctx.obj['adapter']
  if not internal_id:
  logger.warning(""A institute has to have an internal id"")
  ctx.abort()
  if not display_name:
  display_name = internal_id
  if sanger_recipients:
  sanger_recipients = list(sanger_recipients)
  try:
  load_institute(
  adapter=adapter,
  internal_id=internal_id,
  display_name=display_name,
  sanger_recipients=sanger_recipients
  )
  except Exception as e:
  logger.warning(e)
  ctx.abort()",Create a new institute and add it to the database,1,1,0,0,2,1,1,0,1,3
"def save(self):
  headers = {}
  headers.setdefault('Content-Type', 'application/json')
  if not self.exists():
  self.create()
  return
  put_resp = self.r_session.put(
  self.document_url,
  data=self.json(),
  headers=headers
  )
  put_resp.raise_for_status()
  data = response_to_json_dict(put_resp)
  super(Document, self).__setitem__('_rev', data['rev'])
  return",Saves changes made to the locally cached Document object's data structures to the remote database. If the document does not exist remotely then it is created in the remote database. If the object does exist remotely then the document is updated remotely. In either case the locally cached Document object is also updated accordingly based on the successful response of the operation.,2,1,1,1,5,1,1,0,1,3
"def mediatype_delete(mediatypeids, **kwargs):
  conn_args = _login(**kwargs)
  ret = {}
  try:
  if conn_args:
  method = 'mediatype.delete'
  if isinstance(mediatypeids, list):
  params = mediatypeids
  else:
  params = [mediatypeids]
  ret = _query(method, params, conn_args['url'], conn_args['auth'])
  return ret['result']['mediatypeids']
  else:
  raise KeyError
  except KeyError:
  return ret","Delete mediatype :param interfaceids: IDs of the mediatypes to delete :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring) :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring) :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring) :return: ID of deleted mediatype, False on failure. CLI Example: .. code-block:: bash salt '*' zabbix.mediatype_delete 3",1,0,0,1,2,1,0,0,1,2
"def init_farm(farm_name):
  utils.check_for_cloud_server()
  utils.check_for_cloud_user()
  old_farm_name = config[""cloud_server""][""farm_name""]
  if old_farm_name and old_farm_name != farm_name:
  raise click.ClickException(
  ""Farm \""{}\"" already initialized. Run `openag cloud deinit_farm` ""
  ""to deinitialize it"".format(old_farm_name)
  )
  if config[""local_server""][""url""]:
  utils.replicate_per_farm_dbs(farm_name=farm_name)
  config[""cloud_server""][""farm_name""] = farm_name",Select a farm to use. This command sets up the replication between your local database and the selected cloud server if you have already initialized your local database with the `openag db init` command.,1,1,1,0,3,1,1,1,1,4
"def login(confluence_url, user=None, api_entrypoint='confluence2'):
  server = ServerProxy(confluence_url + '/rpc/xmlrpc')
  user = user or getpass.getuser()
  password = getpass.getpass('Please enter confluence password for %s: ' % user)
  if api_entrypoint in (None, 'confluence1'):
  api = server.confluence1
  fmt = 'markdown'
  elif api_entrypoint == 'confluence2':
  api = server.confluence2
  fmt = 'xhtml'
  else:
  raise ConfluenceError(""Don't understand api_entrypoint %s"" % api_entrypoint)
  try:
  return Confluence(api, confluence_url, api.login(user, password), fmt)
  except XMLRPCError as e:
  raise ConfluenceError('Failed to log in to %s: %s' % (confluence_url, e))","Prompts the user to log in to confluence, and returns a Confluence object. :param confluence_url: Base url of wiki, e.g. https://confluence.atlassian.com/ :param user: Username :param api_entrypoint: 'confluence1' or None results in Confluence 3.x. The default value is 'confluence2' which results in Confluence 4.x or 5.x :rtype: returns a connected Confluence instance raises ConfluenceError if login is unsuccessful.",1,0,0,1,2,2,0,0,1,3
"def readable(value,
  allow_empty = False,
  **kwargs):
  if not value and not allow_empty:
  raise errors.EmptyValueError('value (%s) was empty' % value)
  elif not value:
  return None
  value = file_exists(value, force_run = True)
  try:
  with open(value, mode='r'):
  pass
  except (OSError, IOError):
  raise errors.NotReadableError('file at %s could not be opened for '
  'reading' % value)
  return value","Validate that ``value`` is a path to a readable file. .. caution:: **Use of this validator is an anti-pattern and should be used with caution.** Validating the readability of a file *before* attempting to read it exposes your code to a bug called `TOCTOU <https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use>`_. This particular class of bug can expose your code to **security vulnerabilities** and so this validator should only be used if you are an advanced user. A better pattern to use when reading from a file is to apply the principle of EAFP (""easier to ask forgiveness than permission""), and simply attempt to write to the file using a ``try ... except`` block: .. code-block:: python try: with open('path/to/filename.txt', mode = 'r') as file_object: # read from file here except (OSError, IOError) as error: # Handle an error if unable to write. :param value: The path to a file on the local filesystem whose readability is to be validated. :type value: Path-like object :param allow_empty: If ``True``, returns :obj:`None <python:None>` if ``value`` is empty. If ``False``, raises a :class:`EmptyValueError <validator_collection.errors.EmptyValueError>` if ``value`` is empty. Defaults to ``False``. :type allow_empty: :class:`bool <python:bool>` :returns: Validated path-like object or :obj:`None <python:None>` :rtype: Path-like object or :obj:`None <python:None>` :raises EmptyValueError: if ``allow_empty`` is ``False`` and ``value`` is empty :raises NotPathlikeError: if ``value`` is not a path-like object :raises PathExistsError: if ``value`` does not exist on the local filesystem :raises NotAFileError: if ``value`` is not a valid file :raises NotReadableError: if ``value`` cannot be opened for reading",1,0,0,0,1,1,0,0,1,2
"def run_license_checker(config_path):
  whitelist_licenses = _get_whitelist_licenses(config_path)
  table = PrintTable(ROW_HEADERS)
  warnings = []
  for pkg in _get_packages():
  allowed = pkg.license in whitelist_licenses
  table.add_row((pkg.name, pkg.version, pkg.license, str(allowed)))
  if not allowed:
  warnings.append(pkg)
  print(table)
  print('{} RESTRICTED LICENSES DETECTED'.format(len(warnings)))",Generate table of installed packages and check for license warnings based off user defined restricted license values. :param config_path: str :return:,1,0,0,1,2,1,0,0,1,2
"def get_user_attempts(request: AxesHttpRequest, credentials: dict = None) -> QuerySet:
  attempts = filter_user_attempts(request, credentials)
  if settings.AXES_COOLOFF_TIME is None:
  log.debug('AXES: Getting all access attempts from database because no AXES_COOLOFF_TIME is configured')
  return attempts
  threshold = get_cool_off_threshold(request.axes_attempt_time)
  log.debug('AXES: Getting access attempts that are newer than %s', threshold)
  return attempts.filter(attempt_time__gte=threshold)",Get valid user attempts that match the given request and credentials.,0,0,1,0,1,1,0,1,1,3
"def add_database_user(self, new_username, new_password, permissions=None):
  url = ""db/{0}/users"".format(self._database)
  data = {
  'name': new_username,
  'password': new_password
  }
  if permissions:
  try:
  data['readFrom'], data['writeTo'] = permissions
  except (ValueError, TypeError):
  raise TypeError(
  ""'permissions' must be (readFrom, writeTo) tuple""
  )
  self.request(
  url=url,
  method='POST',
  data=data,
  expected_response_code=200
  )
  return True","Add database user. :param permissions: A ``(readFrom, writeTo)`` tuple",1,1,0,1,3,2,0,0,1,3
"def logout(self):
  if not hasattr(self, ""_fb_h""):
  h_r = self._post(self.req_url.MODERN_SETTINGS_MENU, {""pmid"": ""4""})
  self._fb_h = re.search(r'name=\\""h\\"" value=\\""(.*?)\\""', h_r.text).group(1)
  data = {""ref"": ""mb"", ""h"": self._fb_h}
  r = self._get(self.req_url.LOGOUT, data)
  self._resetValues()
  return r.ok",Safely logs out the client :param timeout: See `requests timeout <http://docs.python-requests.org/en/master/user/advanced/#timeouts>`_ :return: True if the action was successful :rtype: bool,2,1,0,1,4,1,0,0,1,2
"def update(self, role_sid=values.unset, attributes=values.unset,
  friendly_name=values.unset):
  data = values.of({'RoleSid': role_sid, 'Attributes': attributes, 'FriendlyName': friendly_name, })
  payload = self._version.update(
  'POST',
  self._uri,
  data=data,
  )
  return UserInstance(
  self._version,
  payload,
  service_sid=self._solution['service_sid'],
  sid=self._solution['sid'],
  )",Update the UserInstance :param unicode role_sid: The SID id of the Role assigned to this user :param unicode attributes: A valid JSON string that contains application-specific data :param unicode friendly_name: A string to describe the resource :returns: Updated UserInstance :rtype: twilio.rest.chat.v2.service.user.UserInstance,2,0,0,2,4,1,0,0,1,2
"def get_url_report(self, this_url, scan='0', allinfo=1):
  params = {'apikey': self.api_key, 'resource': this_url, 'scan': scan, 'allinfo': allinfo}
  try:
  response = requests.get(self.base + 'url/report', params=params, proxies=self.proxies)
  except requests.RequestException as e:
  return dict(error=e.message)
  return _return_response_and_status_code(response)","Get the scan results for a URL. :param this_url: A URL for which you want to retrieve the most recent report. You may also specify a scan_id (sha256-timestamp as returned by the URL submission API) to access a specific report. At the same time, you can specify a CSV list made up of a combination of urls and scan_ids (up to 25 items) so as to perform a batch request with one single call. The CSV list must be separated by new line characters. :param scan: (optional) This is an optional parameter that when set to ""1"" will automatically submit the URL for analysis if no report is found for it in VirusTotal's database. In this case the result will contain a scan_id field that can be used to query the analysis report later on. :param allinfo: (optional) If this parameter is specified and set to ""1"" additional info regarding the URL (other than the URL scanning engine results) will also be returned. This additional info includes VirusTotal related metadata (first seen date, last seen date, files downloaded from the given URL, etc.) and the output of other tools and datasets when fed with the URL. :return: JSON response",2,0,0,1,3,2,0,0,2,4
"def folders(self, ann_id=None):
  ann_id = self._annotator(ann_id)
  if len(self.prefix) > 0:
  prefix = '|'.join([urllib.quote(self.prefix, safe='~'),
  'topic', ann_id, ''])
  else:
  prefix = '|'.join(['topic', ann_id, ''])
  logger.info('Scanning for folders with prefix %r', prefix)
  return imap(lambda id: self.unwrap_folder_content_id(id)['folder_id'],
  self.store.scan_prefix_ids(prefix))","Yields an unordered generator for all available folders. By default (with ``ann_id=None``), folders are shown for all anonymous users. Optionally, ``ann_id`` can be set to a username, which restricts the list to only folders owned by that user. :param str ann_id: Username :rtype: generator of folder_id",1,0,1,1,3,1,0,0,1,2
"def get_course_grade(self, course_id, username):
  results = self.client.courses(course_id).get(username=username)
  for row in results:
  if row.get('username') == username:
  return row
  raise HttpNotFoundError('No grade record found for course={}, username={}'.format(course_id, username))",Retrieve the grade for the given username for the given course_id. Args: * ``course_id`` (str): The string value of the course's unique identifier * ``username`` (str): The username ID identifying the user for which to retrieve the grade. Raises: HttpNotFoundError if no grade found for the given user+course. Returns: a dict containing: * ``username``: A string representation of a user's username passed in the request. * ``course_key``: A string representation of a Course ID. * ``passed``: Boolean representing whether the course has been passed according the course's grading policy. * ``percent``: A float representing the overall grade for the course * ``letter_grade``: A letter grade as defined in grading_policy (e.g. 'A' 'B' 'C' for 6.002x) or None,2,0,0,1,3,2,0,1,1,4
"def get_none_policy_text(none_policy,
  verbose=False
  ):
  if none_policy is NonePolicy.SKIP:
  return ""accept None without performing validation"" if verbose else 'SKIP'
  elif none_policy is NonePolicy.FAIL:
  return ""fail on None without performing validation"" if verbose else 'FAIL'
  elif none_policy is NonePolicy.VALIDATE:
  return ""validate None as any other values"" if verbose else 'VALIDATE'
  elif none_policy is NoneArgPolicy.SKIP_IF_NONABLE_ELSE_FAIL:
  return ""accept None without validation if the argument is optional, otherwise fail on None"" if verbose \
  else 'SKIP_IF_NONABLE_ELSE_FAIL'
  elif none_policy is NoneArgPolicy.SKIP_IF_NONABLE_ELSE_VALIDATE:
  return ""accept None without validation if the argument is optional, otherwise validate None as any other "" \
  ""values"" if verbose else 'SKIP_IF_NONABLE_ELSE_VALIDATE'
  else:
  raise ValueError('Invalid none_policy ' + str(none_policy))",Returns a user-friendly description of a NonePolicy taking into account NoneArgPolicy :param none_policy: :param verbose: :return:,0,0,0,1,1,1,0,0,1,2
"def update(self):
  for key, value in self.__dict__.iteritems():
  key = self._compose(key)
  if self._validate_key(key):
  if not self.obj.hasAttr(key):
  pm.addAttr(self.obj, ln=key, dt=""string"")
  self.obj.attr(key).set(encode(value))","This method should be called to ensure all cached attributes are in sync with the metadata at runtime. This happens because attributes could store mutable objects and be modified outside the scope of this class. The most common idiom that isn't automagically caught is mutating a list or dictionary. Lets say 'user' object have an attribute named 'friends' containing a list, calling 'user.friends.append(new_friend)' only get the attribute, Wrapper isn't aware that the object returned was modified and the cached data is not updated.",0,0,0,0,0,0,1,0,1,2
"def rados_df(self,
  host_list=None,
  remote_user=None,
  remote_pass=None):
  result, failed_hosts = self.runner.ansible_perform_operation(
  host_list=host_list,
  remote_user=remote_user,
  remote_pass=remote_pass,
  module=""command"",
  module_args=""rados df"")
  parsed_result = self.rados_parse_df(result)
  return parsed_result",Invoked the rados df command and return output to user,1,0,0,1,2,1,0,0,1,2
"def _resolve_input(variable, variable_name, config_key, config):
  if variable is None:
  try:
  variable = config.get(PROFILE, config_key)
  except NoOptionError:
  raise ValueError((
  'no {} found - either provide a command line argument or '
  'set up a default by running `apparate configure`'
  ).format(variable_name))
  return variable","Resolve input entered as option values with config values If option values are provided (passed in as `variable`), then they are returned unchanged. If `variable` is None, then we first look for a config value to use. If no config value is found, then raise an error. Parameters ---------- variable: string or numeric value passed in as input by the user variable_name: string name of the variable, for clarity in the error message config_key: string key in the config whose value could be used to fill in the variable config: ConfigParser contains keys/values in .apparatecfg",1,0,0,1,2,1,0,0,1,2
"def has_permission(user, *permissions, **role_kwargs):
  Role = get_model('role')
  Perm = get_model('permission')
  if isinstance(user, (unicode, str)):
  User = get_model('user')
  user = User.get(User.c.username==user)
  for name in permissions:
  perm = Perm.get(Perm.c.name==name)
  if not perm:
  continue
  flag = has_role(user, *list(perm.perm_roles.with_relation().all()), **role_kwargs)
  if flag:
  return flag
  return False","Judge if an user has permission, and if it does return role object, and if it doesn't return False. role_kwargs will be passed to role functions. With role object, you can use role.relation to get Role_Perm_Rel object.",1,0,1,0,2,1,0,1,1,3
"def build_payment_parameters(amount: Money, client_ref: str) -> PaymentParameters:
  merchant_id = web_merchant_id
  amount, currency = money_to_amount_and_currency(amount)
  refno = client_ref
  sign = sign_web(merchant_id, amount, currency, refno)
  parameters = PaymentParameters(
  merchant_id=merchant_id,
  amount=amount,
  currency=currency,
  refno=refno,
  sign=sign,
  use_alias=False,
  )
  logger.info('build-payment-parameters', parameters=parameters)
  return parameters",Builds the parameters needed to present the user with a datatrans payment form. :param amount: The amount and currency we want the user to pay :param client_ref: A unique reference for this payment :return: The parameters needed to display the datatrans form,1,0,0,1,2,1,0,0,1,2
"def handle_create_payload(
  entity: BaseEntity,
  author_user: UserType,
  protocol_name: str,
  to_user_key: RsaKey = None,
  parent_user: UserType = None,
 ) -> str:
  mappers = importlib.import_module(f""federation.entities.{protocol_name}.mappers"")
  protocol = importlib.import_module(f""federation.protocols.{protocol_name}.protocol"")
  protocol = protocol.Protocol()
  outbound_entity = mappers.get_outbound_entity(entity, author_user.private_key)
  if parent_user:
  outbound_entity.sign_with_parent(parent_user.private_key)
  send_as_user = parent_user if parent_user else author_user
  data = protocol.build_send(entity=outbound_entity, from_user=send_as_user, to_user_key=to_user_key)
  return data","Create a payload with the given protocol. Any given user arguments must have ``private_key`` and ``handle`` attributes. :arg entity: Entity object to send. Can be a base entity or a protocol specific one. :arg author_user: User authoring the object. :arg protocol_name: Protocol to create payload for. :arg to_user_key: Public key of user private payload is being sent to, required for private payloads. :arg parent_user: (Optional) User object of the parent object, if there is one. This must be given for the Diaspora protocol if a parent object exists, so that a proper ``parent_author_signature`` can be generated. If given, the payload will be sent as this user. :returns: Built payload message (str)",1,0,0,1,2,1,0,0,1,2
"def retrieve(self, request, *args, **kwargs):
  data = UserSerializer().to_representation(request.user)
  if request.user.is_superuser:
  data['is_superuser'] = True
  secret = getattr(settings, 'FIREBASE_SECRET', None)
  if secret:
  firebase_auth_payload = {
  'id': request.user.pk,
  'username': request.user.username,
  'email': request.user.email,
  'is_staff': request.user.is_staff
  }
  data['firebase_token'] = create_token(secret, firebase_auth_payload)
  return Response(data)",gets basic information about the user :param request: a WSGI request object :param args: inline arguments (optional) :param kwargs: keyword arguments (optional) :return: `rest_framework.response.Response`,1,0,1,1,3,1,0,1,1,3
"def save(self):
  with Repo.db:
  self._do_save()
  our_name = inflector.singularize(Repo.table_name(self.__class__))
  for record in self._related_records:
  if not self._id:
  related_key = associations.foreign_keys_for(
  record.__class__)[our_name]
  setattr(record, related_key, self.__id)
  record._do_save()
  for record in self._delete_related_records:
  record._do_destroy()
  self._finish_save()","Save a record to the database, creating it if needed, updating it otherwise. Also saves related records (children and dependents) as needed.",1,1,1,0,3,0,1,1,1,3
"def import_custom_views(filename):
  with open(filename) as csvfile:
  reader = csv.DictReader(csvfile)
  for view in reader:
  name = view['name']
  upperview = view['upperview']
  if len(upperview) is 0:
  upperview = None
  create_custom_views(name,upperview,auth=auth.creds, url=auth.url)","Function which takes in a csv files as input to the create_custom_views function from the pyhpeimc python module available at https://github.com/HPNetworking/HP-Intelligent-Management-Center :param filename: user-defined filename which contains two column named ""name"" and ""upperview"" as input into the create_custom_views function from the pyhpeimc module. :return: returns output of the create_custom_vies function (str) for each item in the CSV file.",0,0,0,1,1,1,0,0,1,2
"def connect_with_username_and_password(cls, url=None, username=None,
  password=None):
  from .v4_client import LuminosoClient as v4LC
  if username is None:
  username = input('Username: ')
  v4client = v4LC.connect(url=url, username=username, password=password)
  if url is None:
  url = '/'
  if url.startswith('http'):
  root_url = get_root_url(url)
  else:
  url = URL_BASE + '/' + url.lstrip('/')
  root_url = URL_BASE
  return cls(v4client.session, root_url)","Returns an object that makes requests to the API, authenticated with a short-lived token retrieved from username and password. If username or password is not supplied, the method will prompt for a username and/or password to be entered interactively. See the connect method for more details about the `url` argument. PLEASE NOTE: This method is being provided as a temporary measure. We strongly encourage users of the Luminoso API to use a long-lived token instead, as explained in the V5_README file.",2,0,0,0,2,2,0,0,1,3
"def pairs(self, strand, cutoff=0.001, temp=37.0, pseudo=False,
  material=None, dangles='some', sodium=1.0, magnesium=0.0):
  material = self._set_material(strand, material)
  cmd_args = self._prep_cmd_args(temp, dangles, material, pseudo, sodium,
  magnesium, multi=False)
  lines = [str(strand)]
  self._run('pairs', cmd_args, lines)
  ppairs = self._read_tempfile('pairs.ppairs')
  data = re.search('\n\n\d*\n(.*)', ppairs, flags=re.DOTALL).group(1)
  N = len(strand)
  data_lines = [line.split('\t') for line in data.split('\n') if line]
  prob_matrix = self._pairs_to_np(data_lines, N)
  return prob_matrix","Compute the pair probabilities for an ordered complex of strands. Runs the \'pairs\' command. :param strand: Strand on which to run pairs. Strands must be either coral.DNA or coral.RNA). :type strand: list :param cutoff: Only probabilities above this cutoff appear in the output. :type cutoff: float :param temp: Temperature setting for the computation. Negative values are not allowed. :type temp: float :param pseudo: Enable pseudoknots. :type pseudo: bool :param material: The material setting to use in the computation. If set to None (the default), the material type is inferred from the strands. Other settings available: 'dna' for DNA parameters, 'rna' for RNA (1995) parameters, and 'rna1999' for the RNA 1999 parameters. :type material: str :param dangles: How to treat dangles in the computation. From the user guide: For \'none\': Dangle energies are ignored. For \'some\': \'A dangle energy is incorporated for each unpaired base flanking a duplex\'. For 'all': all dangle energy is considered. :type dangles: str :param sodium: Sodium concentration in solution (molar), only applies to DNA. :type sodium: float :param magnesium: Magnesium concentration in solution (molar), only applies to DNA> :type magnesium: float :returns: The probability matrix, where the (i, j)th entry is the probability that base i is bound to base j. The matrix is augmented (it's N+1 by N+1, where N is the number of bases in the sequence) with an (N+1)th column containing the probability that each base is unpaired. :rtype: numpy.array",1,0,0,1,2,1,0,0,1,2
"def organism_host(self, taxid=None, entry_name=None, limit=None, as_df=False):
  q = self.session.query(models.OrganismHost)
  q = self.get_model_queries(q, ((taxid, models.OrganismHost.taxid),))
  q = self.get_one_to_many_queries(q, ((entry_name, models.Entry.name),))
  return self._limit_and_df(q, limit, as_df)","Method to query :class:`.models.OrganismHost` objects in database :param taxid: NCBI taxonomy identifier(s) :type taxid: int or tuple(int) or None :param entry_name: name(s) in :class:`.models.Entry` :type entry_name: str or tuple(str) or None :param limit: - if `isinstance(limit,int)==True` -> limit - if `isinstance(limit,tuple)==True` -> format:= tuple(page_number, results_per_page) - if limit == None -> all results :type limit: int or tuple(int) or None :param bool as_df: if `True` results are returned as :class:`pandas.DataFrame` :return: - if `as_df == False` -> list(:class:`.models.OrganismHost`) - if `as_df == True` -> :class:`pandas.DataFrame` :rtype: list(:class:`.models.OrganismHost`) or :class:`pandas.DataFrame`",1,0,1,1,3,1,0,1,1,3
"def add_computer(self, ip, hostname, domain, os, dc=None):
  domain = domain.split('.')[0].upper()
  cur = self.conn.cursor()
  cur.execute('SELECT * FROM computers WHERE ip LIKE ?', [ip])
  results = cur.fetchall()
  if not len(results):
  cur.execute(""INSERT INTO computers (ip, hostname, domain, os, dc) VALUES (?,?,?,?,?)"", [ip, hostname, domain, os, dc])
  else:
  for host in results:
  if (hostname != host[2]) or (domain != host[3]) or (os != host[4]):
  cur.execute(""UPDATE computers SET hostname=?, domain=?, os=? WHERE id=?"", [hostname, domain, os, host[0]])
  if dc != None and (dc != host[5]):
  cur.execute(""UPDATE computers SET dc=? WHERE id=?"", [dc, host[0]])
  cur.close()
  return cur.lastrowid","Check if this host has already been added to the database, if not add it in.",0,1,1,0,2,1,1,1,1,4
"def invoice(request, invoice_id, access_code=None):
  current_invoice = InvoiceController.for_id_or_404(invoice_id)
  if not current_invoice.can_view(
  user=request.user,
  access_code=access_code,
  ):
  raise Http404()
  data = {
  ""invoice"": current_invoice.invoice,
  }
  return render(request, ""registrasion/invoice.html"", data)","Displays an invoice. This view is not authenticated, but it will only allow access to either: the user the invoice belongs to; staff; or a request made with the correct access code. Arguments: invoice_id (castable to int): The invoice_id for the invoice you want to view. access_code (Optional[str]): The access code for the user who owns this invoice. Returns: render: Renders ``registrasion/invoice.html``, with the following data:: { ""invoice"": models.commerce.Invoice(), } Raises: Http404: if the current user cannot view this invoice and the correct access_code is not provided.",1,0,1,1,3,1,0,1,1,3
"def generate_delete_user_command(username=None, manage_home=None):
  command = None
  remove_home = '-r' if manage_home else ''
  if get_platform() in ('Linux', 'OpenBSD'):
  command = '{0} {1} {2} {3}'.format(sudo_check(), LINUX_CMD_USERDEL, remove_home, username)
  elif get_platform() == 'FreeBSD':
  command = '{0} {1} userdel {2} -n {3}'.format(sudo_check(), FREEBSD_CMD_PW, remove_home, username)
  if command:
  return shlex.split(str(command))",Generate command to delete a user. args: username (str): user name manage_home (bool): manage home directory returns: list: The user delete command string split into shell-like syntax,1,0,0,0,1,1,0,0,1,2
"def insert_bucket_acl(self, bucket_name, entity, role, user_project=None):
  self.log.info('Creating a new ACL entry in bucket: %s', bucket_name)
  client = self.get_conn()
  bucket = client.bucket(bucket_name=bucket_name)
  bucket.acl.reload()
  bucket.acl.entity_from_dict(entity_dict={""entity"": entity, ""role"": role})
  if user_project:
  bucket.acl.user_project = user_project
  bucket.acl.save()
  self.log.info('A new ACL entry created in bucket: %s', bucket_name)","Creates a new ACL entry on the specified bucket_name. See: https://cloud.google.com/storage/docs/json_api/v1/bucketAccessControls/insert :param bucket_name: Name of a bucket_name. :type bucket_name: str :param entity: The entity holding the permission, in one of the following forms: user-userId, user-email, group-groupId, group-email, domain-domain, project-team-projectId, allUsers, allAuthenticatedUsers. See: https://cloud.google.com/storage/docs/access-control/lists#scopes :type entity: str :param role: The access permission for the entity. Acceptable values are: ""OWNER"", ""READER"", ""WRITER"". :type role: str :param user_project: (Optional) The project to be billed for this request. Required for Requester Pays buckets. :type user_project: str",1,0,0,1,2,1,1,1,1,4
"def can_create_gradebook_with_record_types(self, gradebook_record_types):
  if self._catalog_session is not None:
  return self._catalog_session.can_create_catalog_with_record_types(catalog_record_types=gradebook_record_types)
  return True","Tests if this user can create a single ``Gradebook`` using the desired record types. While ``GradingManager.getGradebookRecordTypes()`` can be used to examine which records are supported, this method tests which record(s) are required for creating a specific ``Gradebook``. Providing an empty array tests if a ``Gradebook`` can be created with no records. arg: gradebook_record_types (osid.type.Type[]): array of gradebook record types return: (boolean) - ``true`` if ``Gradebook`` creation using the specified ``Types`` is supported, ``false`` otherwise raise: NullArgument - ``gradebook_record_types`` is ``null`` *compliance: mandatory -- This method must be implemented.*",1,0,0,1,2,2,0,0,1,3
"def auth_creds(cls, username, password):
  store = goldman.sess.store
  login = store.find(cls.RTYPE, 'username', username)
  if not login:
  msg = 'No login found by that username. Spelling error?'
  raise AuthRejected(**{'detail': msg})
  elif login.locked:
  msg = 'The login account is currently locked out.'
  raise AuthRejected(**{'detail': msg})
  elif not cmp_val_salt_hash(password, login.salt, login.password):
  msg = 'The password provided is incorrect. Spelling error?'
  raise AuthRejected(**{'detail': msg})
  else:
  if not login.token:
  login.token = random_str()
  login.post_authenticate()
  return login.token",Validate a username & password A token is returned if auth is successful & can be used to authorize future requests or ignored entirely if the authorization mechanizm does not need it. :return: string token,1,1,1,1,4,1,0,1,1,3
"def sign(
  self,
  end_user_ip,
  user_visible_data,
  personal_number=None,
  requirement=None,
  user_non_visible_data=None,
  **kwargs
  ):
  data = {""endUserIp"": end_user_ip}
  if personal_number:
  data[""personalNumber""] = personal_number
  data[""userVisibleData""] = self._encode_user_data(user_visible_data)
  if user_non_visible_data:
  data[""userNonVisibleData""] = self._encode_user_data(user_non_visible_data)
  if requirement and isinstance(requirement, dict):
  data[""requirement""] = requirement
  data.update(kwargs)
  response = self.client.post(self._sign_endpoint, json=data)
  if response.status_code == 200:
  return response.json()
  else:
  raise get_json_error_class(response)","Request an signing order. The :py:meth:`collect` method is used to query the status of the order. Note that personal number is not needed when signing is to be done on the same device, provided that the returned ``autoStartToken`` is used to open the BankID Client. Example data returned: .. code-block:: json { ""orderRef"":""131daac9-16c6-4618-beb0-365768f37288"", ""autoStartToken"":""7c40b5c9-fa74-49cf-b98c-bfe651f9a7c6"" } :param end_user_ip: IP address of the user requesting the authentication. :type end_user_ip: str :param user_visible_data: The information that the end user is requested to sign. :type user_visible_data: str :param personal_number: The Swedish personal number in format YYYYMMDDXXXX. :type personal_number: str :param requirement: An optional dictionary stating how the signature must be created and verified. See BankID Relying Party Guidelines, section 13.5 for more details. :type requirement: dict :param user_non_visible_data: Optional information sent with request that the user never sees. :type user_non_visible_data: str :return: The order response. :rtype: dict :raises BankIDError: raises a subclass of this error when error has been returned from server.",1,0,0,1,2,1,0,0,2,3
"def get_display_opts(options, argv = sys.argv):
  from Xlib import display, Xatom
  import os
  name = os.path.splitext(os.path.basename(argv[0]))[0]
  optdb = ResourceDB()
  leftargv = optdb.getopt(name, argv[1:], options)
  dname = optdb.get(name + '.display', name + '.Display', None)
  d = display.Display(dname)
  rdbstring = d.screen(0).root.get_full_property(Xatom.RESOURCE_MANAGER,
  Xatom.STRING)
  if rdbstring:
  data = rdbstring.value
  else:
  data = None
  db = ResourceDB(string = data)
  db.update(optdb)
  return d, name, db, leftargv","display, name, db, args = get_display_opts(options, [argv]) Parse X OPTIONS from ARGV (or sys.argv if not provided). Connect to the display specified by a *.display resource if one is set, or to the default X display otherwise. Extract the RESOURCE_MANAGER property and insert all resources from ARGV. The four return values are: DISPLAY -- the display object NAME -- the application name (the filname of ARGV[0]) DB -- the created resource database ARGS -- any remaining arguments",1,0,0,0,1,1,1,0,1,3
"def add_user(
  self, user,
  first_name=None, last_name=None,
  email=None, password=None
  ):
  self.project_service.set_auth(self._token_project)
  self.project_service.add_user(
  user, first_name, last_name, email, password)",Add a new user. Args: user (string): User name. first_name (optional[string]): User's first name. Defaults to None. last_name (optional[string]): User's last name. Defaults to None. email: (optional[string]): User's email address. Defaults to None. password: (optional[string]): User's password. Defaults to None. Raises: requests.HTTPError on failure.,1,0,0,1,2,1,1,0,1,3
"def suspend(self):
  task_invitation = TaskInvitation.objects.get(self.task_invitation_key)
  wfi = task_invitation.instance
  if wfi.current_actor.exist and wfi.current_actor == self.current.role:
  for m in RoleModel.objects.filter(abstract_role=self.current.role.abstract_role,
  unit=self.current.role.unit):
  if m != self.current.role:
  task_invitation.key = ''
  task_invitation.role = m
  task_invitation.save()
  wfi.current_actor = RoleModel()
  wfi.save()
  title = _(u""Successful"")
  msg = _(u""You left the workflow."")
  else:
  title = _(u""Unsuccessful"")
  msg = _(u""Unfortunately, this workflow does not belong to you or is already idle."")
  self.current.msg_box(title=title, msg=msg)","If there is a role assigned to the workflow and it is the same as the user, it can drop the workflow. If it does not exist, it can not do anything. .. code-block:: python # request: { 'task_inv_key': string, }",1,2,2,1,6,1,1,1,1,4
"def create(cls, name=None, description='', privacy_policy=None,
  subscription_policy=None, is_managed=False, admins=None):
  assert name
  assert privacy_policy is None or PrivacyPolicy.validate(privacy_policy)
  assert subscription_policy is None or \
  SubscriptionPolicy.validate(subscription_policy)
  assert admins is None or isinstance(admins, list)
  with db.session.begin_nested():
  obj = cls(
  name=name,
  description=description,
  privacy_policy=privacy_policy,
  subscription_policy=subscription_policy,
  is_managed=is_managed,
  )
  db.session.add(obj)
  for a in admins or []:
  db.session.add(GroupAdmin(
  group=obj, admin_id=a.get_id(),
  admin_type=resolve_admin_type(a)))
  return obj",Create a new group. :param name: Name of group. Required and must be unique. :param description: Description of group. Default: ``''`` :param privacy_policy: PrivacyPolicy :param subscription_policy: SubscriptionPolicy :param admins: list of user and/or group objects. Default: ``[]`` :returns: Newly created group :raises: IntegrityError: if group with given name already exists,1,1,0,1,3,1,1,1,1,4
"def from_credentials(cls, credentials, threads, profile_name, target_name,
  user_cfg=None):
  config = UserConfig.from_dict(user_cfg)
  profile = cls(
  profile_name=profile_name,
  target_name=target_name,
  config=config,
  threads=threads,
  credentials=credentials
  )
  profile.validate()
  return profile","Create a profile from an existing set of Credentials and the remaining information. :param credentials dict: The credentials dict for this profile. :param threads int: The number of threads to use for connections. :param profile_name str: The profile name used for this profile. :param target_name str: The target name used for this profile. :param user_cfg Optional[dict]: The user-level config block from the raw profiles, if specified. :raises DbtProfileError: If the profile is invalid. :returns Profile: The new Profile object.",1,0,0,0,1,1,0,0,1,2
"def new_env(environment):
  if not environment:
  print(""You need to supply an environment name"")
  return
  parser = read_config()
  if environment in parser.sections():
  print(""Environment '%s' already exists"" % environment)
  return
  print(""Please introduce (in order) the commands for '%s'\n"" % environment)
  print(""Press RETURN to end command and RETURN with empty line to finish\n"")
  commands = []
  cmd = """"
  while True:
  try:
  cmd = raw_input(""> "")
  if not cmd:
  break
  commands.append(cmd)
  except KeyboardInterrupt:
  return
  parser.add_section(environment)
  parser.set(environment, ""cmd"", ""\n"".join(commands))
  write_config(parser)
  print(""Added environment '%s'"" % environment)",Create a new environment in the configuration and ask the user for the commands for this specific environment.,1,1,1,1,4,1,0,0,1,2
"def get_user(self, user_id=None, username=None, email=None):
  if user_id:
  uri = ""/users/%s"" % user_id
  elif username:
  uri = ""/users?name=%s"" % username
  elif email:
  uri = ""/users?email=%s"" % email
  else:
  raise ValueError(""You must include one of 'user_id', ""
  ""'username', or 'email' when calling get_user()."")
  resp, resp_body = self.method_get(uri)
  if resp.status_code == 404:
  raise exc.NotFound(""No such user exists."")
  users = resp_body.get(""users"", [])
  if users:
  return [User(self, user) for user in users]
  else:
  user = resp_body.get(""user"", {})
  if user:
  return User(self, user)
  else:
  raise exc.NotFound(""No such user exists."")","Returns the user specified by either ID, username or email. Since more than user can have the same email address, searching by that term will return a list of 1 or more User objects. Searching by username or ID will return a single User. If a user_id that doesn't belong to the current account is searched for, a Forbidden exception is raised. When searching by username or email, a NotFound exception is raised if there is no matching user.",2,0,0,2,4,2,0,0,1,3
"def delete_account_invitation(self, account_id, invitation_id, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('asynchronous'):
  return self.delete_account_invitation_with_http_info(account_id, invitation_id, **kwargs)
  else:
  (data) = self.delete_account_invitation_with_http_info(account_id, invitation_id, **kwargs)
  return data","Delete a user invitation. # noqa: E501 An endpoint for deleting an active user invitation which has been sent for a new or an existing user to join the account. **Example usage:** `curl -X DELETE https://api.us-east-1.mbedcloud.com/v3/accounts/{account-id}/user-invitations/{invitation-id} -H 'Authorization: Bearer API_KEY'` # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass asynchronous=True >>> thread = api.delete_account_invitation(account_id, invitation_id, asynchronous=True) >>> result = thread.get() :param asynchronous bool :param str account_id: Account ID. (required) :param str invitation_id: The ID of the invitation to be deleted. (required) :return: None If the method is called asynchronously, returns the request thread.",1,0,0,1,2,2,0,0,1,3
"def connect_options_namespaced_service_proxy(self, name, namespace, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.connect_options_namespaced_service_proxy_with_http_info(name, namespace, **kwargs)
  else:
  (data) = self.connect_options_namespaced_service_proxy_with_http_info(name, namespace, **kwargs)
  return data","connect OPTIONS requests to proxy of Service This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.connect_options_namespaced_service_proxy(name, namespace, async_req=True) >>> result = thread.get() :param async_req bool :param str name: name of the ServiceProxyOptions (required) :param str namespace: object name and auth scope, such as for teams and projects (required) :param str path: Path is the part of URLs that include service endpoints, suffixes, and parameters to use for the current proxy request to service. For example, the whole request URL is http://localhost/api/v1/namespaces/kube-system/services/elasticsearch-logging/_search?q=user:kimchy. Path is _search?q=user:kimchy. :return: str If the method is called asynchronously, returns the request thread.",2,0,0,1,3,1,0,0,1,2
"def patch_namespaced_horizontal_pod_autoscaler(self, name, namespace, body, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.patch_namespaced_horizontal_pod_autoscaler_with_http_info(name, namespace, body, **kwargs)
  else:
  (data) = self.patch_namespaced_horizontal_pod_autoscaler_with_http_info(name, namespace, body, **kwargs)
  return data","partially update the specified HorizontalPodAutoscaler This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.patch_namespaced_horizontal_pod_autoscaler(name, namespace, body, async_req=True) >>> result = thread.get() :param async_req bool :param str name: name of the HorizontalPodAutoscaler (required) :param str namespace: object name and auth scope, such as for teams and projects (required) :param object body: (required) :param str pretty: If 'true', then the output is pretty printed. :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint. This field is required for apply requests (application/apply-patch) but optional for non-apply patch types (JsonPatch, MergePatch, StrategicMergePatch). :param bool force: Force is going to \""force\"" Apply requests. It means user will re-acquire conflicting fields owned by other people. Force flag must be unset for non-apply patch requests. :return: V2beta1HorizontalPodAutoscaler If the method is called asynchronously, returns the request thread.",1,0,0,2,3,1,0,0,1,2
"def add_account_user_to_groups(self, account_id, user_id, body, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('asynchronous'):
  return self.add_account_user_to_groups_with_http_info(account_id, user_id, body, **kwargs)
  else:
  (data) = self.add_account_user_to_groups_with_http_info(account_id, user_id, body, **kwargs)
  return data","Add user to a list of groups. # noqa: E501 An endpoint for adding user to groups. **Example usage:** `curl -X POST https://api.us-east-1.mbedcloud.com/v3/accounts/{accountID}/users/{user-id}/groups -d '[0162056a9a1586f30242590700000000,0117056a9a1586f30242590700000000]' -H 'content-type: application/json' -H 'Authorization: Bearer API_KEY'` # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass asynchronous=True >>> thread = api.add_account_user_to_groups(account_id, user_id, body, asynchronous=True) >>> result = thread.get() :param asynchronous bool :param str account_id: Account ID. (required) :param str user_id: The ID of the user to be added to the group. (required) :param list[str] body: A list of IDs of the groups to be updated. (required) :return: UpdatedResponse If the method is called asynchronously, returns the request thread.",0,1,0,1,2,2,1,0,1,4
"def preprocess_user_variables(userinput):
  retstr = """"
  while '$' in userinput:
  text_before_variable , variable_delimiter, userinput = userinput.partition('$')
  retstr += text_before_variable
  if not userinput or userinput.startswith('$'):
  retstr += '$'
  userinput = userinput[1:]
  continue
  space_variable_length = userinput.find(' ')
  dollarsign_variable_length = userinput.find('$')
  if space_variable_length == -1:
  space_variable_length = len(userinput)
  if dollarsign_variable_length == -1:
  dollarsign_variable_length = len(userinput)
  variable_length = min(space_variable_length, dollarsign_variable_length)
  variable_name = userinput[:variable_length]
  userinput = userinput[variable_length + 1:]
  try:
  retstr += uservariables[variable_name]
  except KeyError:
  raise seash_exceptions.UserError(""Variable does not exist: ""+variable_name)
  if space_variable_length < dollarsign_variable_length:
  retstr += ' '
  else:
  retstr += userinput
  return retstr",<Purpose> Command parser for user variables. Takes the raw userinput and replaces each user variable with its set value. <Arguments> userinput: A raw user string <Side Effects> Each user variable will be replaced by the value that it was previously set to. <Exceptions> UserError: User typed an unrecognized or invalid variable name <Returns> The preprocessed string,1,0,0,1,2,1,0,0,1,2
"def run(self, dat=None):
  conf = self.conf
  dat = dat if dat else {}
  publish.single(topic=dat.get('topic', 'test_topic'),
  payload=dat.get('payload', 'just a test topic payload'),
  qos=dat.get('qos', 0),
  retain=dat.get('retain', False),
  hostname=conf['hostname'],
  port=conf['port'],
  auth=conf['auth'])"," -  : .. code:: python t = { 'hostname': '192.168.1.2', 'port': 1883, 'username': 'admin', 'password': 'admin', } Publisher(t).run({'payload': json.dumps(t)}) if pub else TopicConsumer(t).run() :param dat: :type dat: :return: :rtype:",0,0,0,0,0,1,0,0,1,2
"def download_saved_posts(self, max_count: int = None, fast_update: bool = False,
  post_filter: Optional[Callable[[Post], bool]] = None) -> None:
  self.context.log(""Retrieving saved posts..."")
  count = 1
  for post in Profile.from_username(self.context, self.context.username).get_saved_posts():
  if max_count is not None and count > max_count:
  break
  if post_filter is not None and not post_filter(post):
  self.context.log(""<{} skipped>"".format(post), flush=True)
  continue
  self.context.log(""[{:>3}] "".format(count), end=str(), flush=True)
  count += 1
  with self.context.error_catcher('Download saved posts'):
  downloaded = self.download_post(post, target=':saved')
  if fast_update and not downloaded:
  break","Download user's saved pictures. :param max_count: Maximum count of pictures to download :param fast_update: If true, abort when first already-downloaded picture is encountered :param post_filter: function(post), which returns True if given picture should be downloaded",2,0,0,0,2,1,0,0,1,2
"def get_percentage(a, b, i=False, r=False):
  if i is False and r is True:
  percentage = round(100.0 * (float(a) / b), 2)
  elif (i is True and r is True) or (i is True and r is False):
  percentage = int(round(100 * (float(a) / b)))
  if r is False:
  warnings.warn(
  ""If integer is set to True and Round is set to False, you will still get a rounded number if you pass floating point numbers as arguments.""
  )
  else:
  percentage = 100.0 * (float(a) / b)
  return percentage","Finds the percentage of one number over another. Args: a: The number that is a percent, int or float. b: The base number that a is a percent of, int or float. i: Optional boolean integer. True if the user wants the result returned as a whole number. Assumes False. r: Optional boolean round. True if the user wants the result rounded. Rounds to the second decimal point on floating point numbers. Assumes False. Returns: The argument a as a percentage of b. Throws a warning if integer is set to True and round is set to False.",1,0,0,1,2,1,0,0,1,2
"def select_locale_by_request(self, request, locales=()):
  default_locale = locales and locales[0] or self.cfg.default_locale
  if len(locales) == 1 or 'ACCEPT-LANGUAGE' not in request.headers:
  return default_locale
  ulocales = [
  (q, locale_delim_re.split(v)[0])
  for v, q in parse_accept_header(request.headers['ACCEPT-LANGUAGE'])
  ]
  ulocales.sort()
  ulocales.reverse()
  for locale in locales:
  for _, ulocale in ulocales:
  ulocale = locale_delim_re.split(ulocale)[0]
  if ulocale.lower() == locale.lower():
  return ulocale
  return ulocales[0][1]",Choose an user's locales by request.,0,0,0,1,1,1,0,0,1,2
"def require_login(self, view_func):
  @wraps(view_func)
  def decorated(*args, **kwargs):
  if g.oidc_id_token is None:
  return self.redirect_to_auth_server(request.url)
  return view_func(*args, **kwargs)
  return decorated","Use this to decorate view functions that require a user to be logged in. If the user is not already logged in, they will be sent to the Provider to log in, after which they will be returned. .. versionadded:: 1.0 This was :func:`check` before.",1,0,0,1,2,1,0,0,1,2
"def fetch_value(self, select, table_name, where=None, extra=None):
  try:
  self.verify_table_existence(table_name)
  except TableNotFoundError as e:
  logger.debug(e)
  return None
  result = self.execute_query(
  Select(select, table_name, where, extra), logging.getLogger().findCaller()
  )
  if result is None:
  return None
  fetch = result.fetchone()
  if fetch is None:
  return None
  return fetch[0]","Fetch a value from the table. Return |None| if no value matches the conditions, or the table not found in the database. :param str select: Attribute for SELECT query :param str table_name: Table name of executing the query. :param where: |arg_select_where| :type where: |arg_where_type| :return: Result of execution of the query. :raises simplesqlite.NullDatabaseConnectionError: |raises_check_connection| :raises simplesqlite.OperationalError: |raises_operational_error|",0,0,1,0,1,1,0,1,1,3
"def populate(self, ticket=None):
  ticket = ticket or AuthTicket.objects.get_any_active('wsfe')
  client = clients.get_client('wsfe', ticket.owner.is_sandboxed)
  service = getattr(client.service, self.__service_name)
  response_xml = service(serializers.serialize_ticket(ticket))
  check_response(response_xml)
  for result in getattr(response_xml.ResultGet, self.__type_name):
  self.get_or_create(
  code=result.Id,
  description=result.Desc,
  valid_from=parsers.parse_date(result.FchDesde),
  valid_to=parsers.parse_date(result.FchHasta),
  )","Populate the database with types retrieved from the AFIP. If no ticket is provided, the most recent available one will be used.",1,1,1,0,3,1,0,1,1,3
"def reader(queue=None, is_unicode=False):
  if queue is None:
  queue = BufferQueue(is_unicode)
  ctx = _HandlerContext(
  container=_C_TOP_LEVEL,
  queue=queue,
  field_name=None,
  annotations=None,
  depth=0,
  whence=None,
  value=None,
  ion_type=None,
  pending_symbol=None
  )
  return reader_trampoline(_skip_trampoline(_container_handler(None, ctx)), allow_flush=True)","Returns a raw binary reader co-routine. Args: queue (Optional[BufferQueue]): The buffer read data for parsing, if ``None`` a new one will be created. is_unicode (Optional[bool]): True if all input data to this reader will be of unicode text type; False if all input data to this reader will be of binary type. Yields: IonEvent: parse events, will have an event type of ``INCOMPLETE`` if data is needed in the middle of a value or ``STREAM_END`` if there is no data **and** the parser is not in the middle of parsing a value. Receives :class:`DataEvent`, with :class:`ReadEventType` of ``NEXT`` or ``SKIP`` to iterate over values; ``DATA`` or ``NEXT`` if the last event type was ``INCOMPLETE``; or ``DATA`` if the last event type was ``STREAM_END``. When the reader receives ``NEXT`` after yielding ``INCOMPLETE``, this signals to the reader that no further data is coming, and that any pending data should be flushed as either parse events or errors. This is **only** valid at the top-level, and will **only** result in a parse event if the last character encountered... * was a digit or a decimal point in a non-timestamp, non-keyword numeric value; OR * ended a valid partial timestamp; OR * ended a keyword value (special floats, booleans, ``null``, and typed nulls); OR * was part of an unquoted symbol token, or whitespace or the end of a comment following an unquoted symbol token (as long as no colons were encountered after the token); OR * was the closing quote of a quoted symbol token, or whitespace or the end of a comment following a quoted symbol token (as long as no colons were encountered after the token); OR * was the final closing quote of a long string, or whitespace or the end of a comment following a long string. If the reader successfully yields a parse event as a result of this, ``NEXT`` is the only input that may immediately follow. At that point, there are only two possible responses from the reader: * If the last character read was the closing quote of an empty symbol following a long string, the reader will emit a parse event representing a symbol value with empty text. The next reader input/output event pair must be (``NEXT``, ``STREAM_END``). * Otherwise, the reader will emit ``STREAM_END``. After that ``STREAM_END``, the user may later provide ``DATA`` to resume reading. If this occurs, the new data will be interpreted as if it were at the start of the stream (i.e. it can never continue the previous value), except that it occurs within the same symbol table context. This has the following implications (where ``<FLUSH>`` stands for the (``INCOMPLETE``, ``NEXT``) transaction): * If the previously-emitted value was a numeric value (``int``, ``float``, ``decimal``, ``timestamp``), the new data will never extend that value, even if it would be a valid continuation. For example, ``123<FLUSH>456`` will always be emitted as two parse events (ints ``123`` and ``456``), even though it would have been interpreted as ``123456`` without the ``<FLUSH>``. * If the previously-emitted value was a symbol value or long string, the new data will be interpreted as the start of a new value. For example, ``abc<FLUSH>::123`` will be emitted as the symbol value ``'abc'``, followed by an error upon encountering ':' at the start of a value, even though it would have been interpreted as the ``int`` ``123`` annotated with ``'abc'`` without the ``<FLUSH>``. The input ``abc<FLUSH>abc`` will be emitted as the symbol value ``'abc'`` (represented by a :class:`SymbolToken`), followed by another symbol value ``'abc'`` (represented by a ``SymbolToken`` with the same symbol ID), even though it would have been interpreted as ``'abcabc'`` without the ``<FLUSH>``. Similarly, ``'''abc'''<FLUSH>'''def'''`` will the interpreted as two strings (``'abc'`` and ``'def'``), even though it would have been interpreted as ``'abcdef'`` without the ``<FLUSH>``. ``SKIP`` is only allowed within a container. A reader is *in* a container when the ``CONTAINER_START`` event type is encountered and *not in* a container when the ``CONTAINER_END`` event type for that container is encountered.",1,0,0,1,2,1,0,0,1,2
"def get(cls, user_id, github_id=None, name=None, check_owner=True):
  repo = cls.query.filter((Repository.github_id == github_id) |
  (Repository.name == name)).one()
  if (check_owner and repo and repo.user_id and
  repo.user_id != int(user_id)):
  raise RepositoryAccessError(
  u'User {user} cannot access repository {repo}({repo_id}).'
  .format(user=user_id, repo=name, repo_id=github_id)
  )
  return repo",Return a repository. :param integer user_id: User identifier. :param integer github_id: GitHub repository identifier. :param str name: GitHub repository full name. :returns: The repository object. :raises: :py:exc:`~sqlalchemy.orm.exc.NoResultFound`: if the repository doesn't exist. :raises: :py:exc:`~sqlalchemy.orm.exc.MultipleResultsFound`: if multiple repositories with the specified GitHub id and/or name exist. :raises: :py:exc:`RepositoryAccessError`: if the user is not the owner of the repository.,1,0,1,1,3,1,0,1,1,3
"def user_entry(entry_int, num_inst, command):
  valid_entry = False
  if not entry_int:
  print(""{}aborting{} - {} instance\n"".
  format(C_ERR, C_NORM, command))
  sys.exit()
  elif entry_int >= 1 and entry_int <= num_inst:
  entry_idx = entry_int - 1
  valid_entry = True
  else:
  print(""{}Invalid entry:{} enter a number between 1""
  "" and {}."".format(C_ERR, C_NORM, num_inst))
  entry_idx = entry_int
  return (entry_idx, valid_entry)","Validate user entry and returns index and validity flag. Processes the user entry and take the appropriate action: abort if '0' entered, set validity flag and index is valid entry, else return invalid index and the still unset validity flag. Args: entry_int (int): a number entered or 999 if a non-int was entered. num_inst (int): the largest valid number that can be entered. command (str): program command to display in prompt. Returns: entry_idx(int): the dictionary index number of the targeted instance valid_entry (bool): specifies if entry_idx is valid. Raises: SystemExit: if the user enters 0 when they are choosing from the list it triggers the ""abort"" option offered to the user.",1,0,0,1,2,1,0,0,1,2
"def search_ipv6_environment(self, ipv6, id_environment):
  if not is_valid_int_param(id_environment):
  raise InvalidParameterError(
  u'Environment identifier is invalid or was not informed.')
  ipv6_map = dict()
  ipv6_map['ipv6'] = ipv6
  ipv6_map['id_environment'] = id_environment
  code, xml = self.submit(
  {'ipv6_map': ipv6_map}, 'POST', 'ipv6/environment/')
  return self.response(code, xml)","Get IPv6 with an associated environment. :param ipv6: IPv6 address in the format x1:x2:x3:x4:x5:x6:x7:x8. :param id_environment: Environment identifier. Integer value and greater than zero. :return: Dictionary with the following structure: :: {'ipv6': {'id': < id >, 'id_vlan': < id_vlan >, 'bloco1': < bloco1 >, 'bloco2': < bloco2 >, 'bloco3': < bloco3 >, 'bloco4': < bloco4 >, 'bloco5': < bloco5 >, 'bloco6': < bloco6 >, 'bloco7': < bloco7 >, 'bloco8': < bloco8 >, 'descricao': < descricao > }} :raise IpNaoExisteError: IPv6 is not registered or is not associated to the environment. :raise AmbienteNaoExisteError: Environment not found. :raise InvalidParameterError: Environment identifier and/or IPv6 string is/are none or invalid. :raise DataBaseError: Networkapi failed to access the database. :raise XMLError: Networkapi failed to generate the XML response.",2,0,1,1,4,2,0,0,1,3
"def get_user_permissions_on_view(view_name):
  _ret = list()
  if current_user.is_authenticated:
  _current_user = current_user
  elif current_user_jwt:
  _current_user = current_user_jwt
  else:
  return _ret
  for role in _current_user.roles:
  if role.permissions:
  for permission in role.permissions:
  if permission.view_menu.name == view_name:
  _ret.append(permission.permission.name)
  return _ret",Returns all current user permissions on a certain view/resource :param view_name: The name of the view/resource/menu :return: (list) with permissions,1,0,1,1,3,1,0,0,1,2
"def grant_permission(username, resource=None, resource_type='keyspace', permission=None, contact_points=None, port=None,
  cql_user=None, cql_pass=None):
  permission_cql = ""grant {0}"".format(permission) if permission else ""grant all permissions""
  resource_cql = ""on {0} {1}"".format(resource_type, resource) if resource else ""on all keyspaces""
  query = ""{0} {1} to {2}"".format(permission_cql, resource_cql, username)
  log.debug(""Attempting to grant permissions with query '%s'"", query)
  try:
  cql_query(query, contact_points, port, cql_user, cql_pass)
  except CommandExecutionError:
  log.critical('Could not grant permissions.')
  raise
  except BaseException as e:
  log.critical('Unexpected error while granting permissions: %s', e)
  raise
  return True","Grant permissions to a user. :param username: The name of the user to grant permissions to. :type username: str :param resource: The resource (keyspace or table), if None, permissions for all resources are granted. :type resource: str :param resource_type: The resource_type (keyspace or table), defaults to 'keyspace'. :type resource_type: str :param permission: A permission name (e.g. select), if None, all permissions are granted. :type permission: str :param contact_points: The Cassandra cluster addresses, can either be a string or a list of IPs. :type contact_points: str | list[str] :param cql_user: The Cassandra user if authentication is turned on. :type cql_user: str :param cql_pass: The Cassandra user password if authentication is turned on. :type cql_pass: str :param port: The Cassandra cluster port, defaults to None. :type port: int :return: :rtype: CLI Example: .. code-block:: bash salt 'minion1' cassandra_cql.grant_permission salt 'minion1' cassandra_cql.grant_permission username=joe resource=test_keyspace permission=select salt 'minion1' cassandra_cql.grant_permission username=joe resource=test_table resource_type=table \ permission=select contact_points=minion1",1,1,0,1,3,1,1,1,1,4
"def login_with_api_key(self, email, api_key, application='Default'):
  parameters = dict()
  parameters['email'] = BaseDriver.email = email
  parameters['apikey'] = BaseDriver.apikey = api_key
  parameters['appname'] = application
  response = self.request('midas.login', parameters)
  if 'token' in response:
  return response['token']
  if 'mfa_token_id':
  return response['mfa_token_id']","Login and get a token. If you do not specify a specific application, 'Default' will be used. :param email: Email address of the user :type email: string :param api_key: API key assigned to the user :type api_key: string :param application: (optional) Application designated for this API key :type application: string :returns: Token to be used for interaction with the API until expiration :rtype: string",2,0,0,1,3,2,0,0,1,3
"async def authorize_redirect(
  self,
  callback_uri: str = None,
  extra_params: Dict[str, Any] = None,
  http_client: httpclient.AsyncHTTPClient = None,
  ) -> None:
  if callback_uri and getattr(self, ""_OAUTH_NO_CALLBACKS"", False):
  raise Exception(""This service does not support oauth_callback"")
  if http_client is None:
  http_client = self.get_auth_http_client()
  assert http_client is not None
  if getattr(self, ""_OAUTH_VERSION"", ""1.0a"") == ""1.0a"":
  response = await http_client.fetch(
  self._oauth_request_token_url(
  callback_uri=callback_uri, extra_params=extra_params
  )
  )
  else:
  response = await http_client.fetch(self._oauth_request_token_url())
  url = self._OAUTH_AUTHORIZE_URL
  self._on_request_token(url, callback_uri, response)","Redirects the user to obtain OAuth authorization for this service. The ``callback_uri`` may be omitted if you have previously registered a callback URI with the third-party service. For some services, you must use a previously-registered callback URI and cannot specify a callback via this method. This method sets a cookie called ``_oauth_request_token`` which is subsequently used (and cleared) in `get_authenticated_user` for security purposes. This method is asynchronous and must be called with ``await`` or ``yield`` (This is different from other ``auth*_redirect`` methods defined in this module). It calls `.RequestHandler.finish` for you so you should not write any other response after it returns. .. versionchanged:: 3.1 Now returns a `.Future` and takes an optional callback, for compatibility with `.gen.coroutine`. .. versionchanged:: 6.0 The ``callback`` argument was removed. Use the returned awaitable object instead.",2,0,0,2,4,2,0,0,1,3
"def is_valid(self, instance):
  dbdata = instance.dbdata
  data = dbdata['cleaned_data'] = {}
  errors = dbdata['errors'] = {}
  for field, value in instance.fieldvalue_pairs():
  name = field.attname
  try:
  svalue = field.set_get_value(instance, value)
  except Exception as e:
  errors[name] = str(e)
  else:
  if (svalue is None or svalue is '') and field.required:
  errors[name] = (""Field '{0}' is required for '{1}'.""
  .format(name, self))
  else:
  if isinstance(svalue, dict):
  data.update(svalue)
  elif svalue is not None:
  data[name] = svalue
  return len(errors) == 0","Perform validation for *instance* and stores serialized data, indexes and errors into local cache. Return ``True`` if the instance is ready to be saved to database.",1,0,1,0,2,1,0,1,1,3
"def get_initial_featuring(statemaker, feature_rad, actual_rad=None,
  im_name=None, tile=None, invert=True, desc='', use_full_path=False,
  featuring_params={}, statemaker_kwargs={}, **kwargs):
  if actual_rad is None:
  actual_rad = feature_rad
  _, im_name = _pick_state_im_name('', im_name, use_full_path=use_full_path)
  im = util.RawImage(im_name, tile=tile)
  pos = locate_spheres(im, feature_rad, invert=invert, **featuring_params)
  if np.size(pos) == 0:
  msg = 'No particles found. Try using a smaller `feature_rad`.'
  raise ValueError(msg)
  rad = np.ones(pos.shape[0], dtype='float') * actual_rad
  s = statemaker(im, pos, rad, **statemaker_kwargs)
  RLOG.info('State Created.')
  if desc is not None:
  states.save(s, desc=desc+'initial')
  optimize_from_initial(s, invert=invert, desc=desc, **kwargs)
  return s","Completely optimizes a state from an image of roughly monodisperse particles. The user can interactively select the image. The state is periodically saved during optimization, with different filename for different stages of the optimization. Parameters ---------- statemaker : Function A statemaker function. Given arguments `im` (a :class:`~peri.util.Image`), `pos` (numpy.ndarray), `rad` (ndarray), and any additional `statemaker_kwargs`, must return a :class:`~peri.states.ImageState`. There is an example function in scripts/statemaker_example.py feature_rad : Int, odd The particle radius for featuring, as passed to locate_spheres. actual_rad : Float, optional The actual radius of the particles. Default is feature_rad im_name : string, optional The file name of the image to load. If not set, it is selected interactively through Tk. tile : :class:`peri.util.Tile`, optional The tile of the raw image to be analyzed. Default is None, the entire image. invert : Bool, optional Whether to invert the image for featuring, as passed to trackpy. Default is True. desc : String, optional A description to be inserted in saved state. The save name will be, e.g., '0.tif-peri-' + desc + 'initial-burn.pkl'. Default is '' use_full_path : Bool, optional Set to True to use the full path name for the image. Default is False. featuring_params : Dict, optional kwargs-like dict of any additional keyword arguments to pass to ``get_initial_featuring``, such as ``'use_tp'`` or ``'minmass'``. Default is ``{}``. statemaker_kwargs : Dict, optional kwargs-like dict of any additional keyword arguments to pass to the statemaker function. Default is ``{}``. Other Parameters ---------------- max_mem : Numeric The maximum additional memory to use for the optimizers, as passed to optimize.burn. Default is 1e9. min_rad : Float, optional The minimum particle radius, as passed to addsubtract.add_subtract. Particles with a fitted radius smaller than this are identified as fake and removed. Default is 0.5 * actual_rad. max_rad : Float, optional The maximum particle radius, as passed to addsubtract.add_subtract. Particles with a fitted radius larger than this are identified as fake and removed. Default is 1.5 * actual_rad, however you may find better results if you make this more stringent. rz_order : int, optional If nonzero, the order of an additional augmented rscl(z) parameter for optimization. Default is 0; i.e. no rscl(z) optimization. zscale : Float, optional The zscale of the image. Default is 1.0 Returns ------- s : :class:`peri.states.ImageState` The optimized state. See Also -------- feature_from_pos_rad : Using a previous state's globals and user-provided positions and radii as an initial guess, completely optimizes a state. get_particle_featuring : Using a previous state's globals and positions as an initial guess, completely optimizes a state. translate_featuring : Use a previous state's globals and centroids methods for an initial particle guess, completely optimizes a state. Notes ----- Proceeds by centroid-featuring the image for an initial guess of particle positions, then optimizing the globals + positions until termination as called in _optimize_from_centroid. The ``Other Parameters`` are passed to _optimize_from_centroid.",1,0,0,1,2,1,0,0,1,2
"def auth(self, request):
  callback_url = self.callback_url(request)
  twitter = Twython(self.consumer_key, self.consumer_secret)
  req_token = twitter.get_authentication_tokens(
  callback_url=callback_url)
  request.session['oauth_token'] = req_token['oauth_token']
  request.session['oauth_token_secret'] = req_token['oauth_token_secret']
  return req_token['auth_url']",build the request to access to the Twitter website with all its required parms :param request: makes the url to call Twitter + the callback url :return: go to the Twitter website to ask to the user to allow the access of TriggerHappy,2,0,0,1,3,2,0,0,1,3
"def find_username_from_user_id(session, user_id):
  comments_page = session.session.get(u'http://myanimelist.net/comments.php?' + urllib.urlencode({'id': int(user_id)})).text
  comments_page = bs4.BeautifulSoup(comments_page)
  username_elt = comments_page.find('h1')
  if ""'s Comments"" not in username_elt.text:
  raise InvalidUserError(user_id, message=""Invalid user ID given when looking up username"")
  return username_elt.text.replace(""'s Comments"", """")",Look up a MAL username's user ID. :type session: :class:`myanimelist.session.Session` :param session: A valid MAL session. :type user_id: int :param user_id: The user ID for which we want to look up a username. :raises: :class:`.InvalidUserError` :rtype: str :return: The given user's username.,1,0,0,1,2,1,0,0,1,2
"def energy(self, strand, dotparens, temp=37.0, pseudo=False, material=None,
  dangles='some', sodium=1.0, magnesium=0.0):
  material = self._set_material(strand, material)
  cmd_args = self._prep_cmd_args(temp, dangles, material, pseudo, sodium,
  magnesium, multi=False)
  lines = [str(strand), dotparens]
  stdout = self._run('energy', cmd_args, lines).split('\n')
  return float(stdout[-2])","Calculate the free energy of a given sequence structure. Runs the \'energy\' command. :param strand: Strand on which to run energy. Strands must be either coral.DNA or coral.RNA). :type strand: coral.DNA or coral.RNA :param dotparens: The structure in dotparens notation. :type dotparens: str :param temp: Temperature setting for the computation. Negative values are not allowed. :type temp: float :param pseudo: Enable pseudoknots. :type pseudo: bool :param material: The material setting to use in the computation. If set to None (the default), the material type is inferred from the strands. Other settings available: 'dna' for DNA parameters, 'rna' for RNA (1995) parameters, and 'rna1999' for the RNA 1999 parameters. :type material: str :param dangles: How to treat dangles in the computation. From the user guide: For \'none\': Dangle energies are ignored. For \'some\': \'A dangle energy is incorporated for each unpaired base flanking a duplex\'. For 'all': all dangle energy is considered. :type dangles: str :param sodium: Sodium concentration in solution (molar), only applies to DNA. :type sodium: float :param magnesium: Magnesium concentration in solution (molar), only applies to DNA> :type magnesium: float :returns: The free energy of the sequence with the specified secondary structure. :rtype: float",1,0,0,1,2,1,0,0,1,2
"def stop_pipeline(url, pipeline_id, auth, verify_ssl):
  stop_result = requests.post(url + '/' + pipeline_id + '/stop', headers=X_REQ_BY, auth=auth, verify=verify_ssl)
  stop_result.raise_for_status()
  logging.info(""Pipeline stop requested."")
  poll_pipeline_status(STATUS_STOPPED, url, pipeline_id, auth, verify_ssl)
  logging.info('Pipeline stopped.')
  return stop_result.json()","Stop a running pipeline. The API waits for the pipeline to be 'STOPPED' before returning. Args: url (str): the host url in the form 'http://host:port/'. pipeline_id (str): the ID of of the exported pipeline. auth (tuple): a tuple of username, and password. verify_ssl (bool): whether to verify ssl certificates Returns: dict: the response json",2,0,0,2,4,2,0,0,1,3
"def expand(self, name):
  if self._is_pattern(name):
  expr = re.compile(self._pattern_to_regex(name))
  for cfg_name in self._d.keys():
  if expr.match(cfg_name):
  self._expand(cfg_name)
  else:
  self._expand(name)","Expand config for `name` by adding a sub-configuration for every dot-separated collection ""below"" the given one (or all, if none given). For example, for a database 'mydb' with collections ['spiderman.amazing', 'spiderman.spectacular', 'spiderman2'] and a configuration {'host':'foo', 'database':'mydb', 'collection':'spiderman'} then `expand(""mydb.spiderman"")` would add keys for 'spiderman.amazing' and 'spiderman.spectacular', but *not* 'spiderman2'. :param name: Name, or glob-style pattern, for DB configurations. :type name: basestring :return: None :raises: KeyError (if no such configuration)",0,0,0,1,1,0,0,0,1,1
"def hostinterface_create(hostid, ip_, dns='', main=1, if_type=1, useip=1, port=None, **kwargs):
  conn_args = _login(**kwargs)
  ret = {}
  if not port:
  port = INTERFACE_DEFAULT_PORTS[if_type]
  try:
  if conn_args:
  method = 'hostinterface.create'
  params = {""hostid"": hostid,
  ""ip"": ip_,
  ""dns"": dns,
  ""main"": main,
  ""port"": port,
  ""type"": if_type,
  ""useip"": useip}
  params = _params_extend(params, **kwargs)
  ret = _query(method, params, conn_args['url'], conn_args['auth'])
  return ret['result']['interfaceids']
  else:
  raise KeyError
  except KeyError:
  return ret",".. versionadded:: 2016.3.0 Create new host interface .. note:: This function accepts all standard host group interface: keyword argument names differ depending on your zabbix version, see here__. .. __: https://www.zabbix.com/documentation/3.0/manual/api/reference/hostinterface/object :param hostid: ID of the host the interface belongs to :param ip_: IP address used by the interface :param dns: DNS name used by the interface :param main: whether the interface is used as default on the host (0 - not default, 1 - default) :param port: port number used by the interface :param type: Interface type (1 - agent; 2 - SNMP; 3 - IPMI; 4 - JMX) :param useip: Whether the connection should be made via IP (0 - connect using host DNS name; 1 - connect using host IP address for this host interface) :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring) :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring) :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring) :return: ID of the created host interface, False on failure. CLI Example: .. code-block:: bash salt '*' zabbix.hostinterface_create 10105 192.193.194.197",1,1,0,2,4,1,0,0,1,2
"def check_permission(self, obj_id, permissions):
  response = self._client.session.post(
  '{url}/{id}/permissioncheck'.format(
  url=self.endpoint_url, id=obj_id
  ),
  data={'permissions': permissions}
  )
  return self.process_response(response)",Get list of permissions for a user :param obj_id: int :param permissions: str|list|tuple :return: dict|str,2,0,0,1,3,2,0,0,1,3
"def convert_response(self):
  if hasattr(self.response, 'body_raw'):
  if self.response.body_raw is not None:
  to_type = re.sub('[^a-zA-Z_]', '_', self.type)
  to_type_method = 'to_' + to_type
  if hasattr(self, to_type_method):
  self.response.body = getattr(self, to_type_method)(
  self.response.body_raw)
  del self.response.body_raw",Finish filling the instance's response object so it's ready to be served to the client. This includes converting the body_raw property to the content type requested by the user if necessary.,0,0,0,1,1,0,0,0,1,1
"def fetch_projects(self, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('callback'):
  return self.fetch_projects_with_http_info(**kwargs)
  else:
  (data) = self.fetch_projects_with_http_info(**kwargs)
  return data","List projects owned Fetch projects that the currently authenticated user has access to because he or she is the owner of the project. This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please define a `callback` function to be invoked when receiving the response. >>> def callback_function(response): >>> pprint(response) >>> >>> thread = api.fetch_projects(callback=callback_function) :param callback function: The callback function for asynchronous request. (optional) :return: PaginatedProjectResults If the method is called asynchronously, returns the request thread.",2,0,0,1,3,2,0,0,1,3
"def img_url(obj, profile_app_name, profile_model_name):
  try:
  content_type = ContentType.objects.get(
  app_label=profile_app_name,
  model=profile_model_name.lower()
  )
  except ContentType.DoesNotExist:
  return """"
  except AttributeError:
  return """"
  Profile = content_type.model_class()
  fields = Profile._meta.get_fields()
  profile = content_type.model_class().objects.get(user=obj.user)
  for field in fields:
  if hasattr(field, ""upload_to""):
  return field.value_from_object(profile).url",returns url of profile image of a user,0,0,1,1,2,1,0,1,1,3
"def userinfo(access_token, scope_request=None, claims_request=None):
  handlers = HANDLERS['userinfo']
  claims_request_section = claims_request.get('userinfo', {}) if claims_request else {}
  if not scope_request and not claims_request_section:
  scope_request = provider.scope.to_names(access_token.scope)
  else:
  scope_request = scope_request
  scopes, claims = collect(
  handlers,
  access_token,
  scope_request=scope_request,
  claims_request=claims_request_section,
  )
  return IDToken(access_token, scopes, claims)","Returns data required for an OpenID Connect UserInfo response, according to: http://openid.net/specs/openid-connect-basic-1_0.html#UserInfoResponse Supports scope and claims request parameter as described in: - http://openid.net/specs/openid-connect-core-1_0.html#ScopeClaims - http://openid.net/specs/openid-connect-core-1_0.html#ClaimsParameter Arguments: access_token (:class:`AccessToken`): Associated access token. scope_request (list): Optional list of requested scopes. Only scopes authorized in the `access_token` will be considered. claims_request (dict): Optional dictionary with a claims request parameter. Information on the claims request parameter specification: - http://openid.net/specs/openid-connect-core-1_0.html#ClaimsParameter As a convinience, if neither `scope_request` or user_info claim is specified in the `claims_request`, it will return the claims for all the scopes in the `access_token`. Returns an :class:`IDToken` instance with the scopes from the `scope_request` and the corresponding claims. Claims in the `claims_request` paramater userinfo section will be included *in addition* to the ones corresponding to `scope_request`.",1,0,0,1,2,1,0,0,1,2
"def _call_post_with_user_override(self, sap_user_id, url, payload):
  SAPSuccessFactorsEnterpriseCustomerConfiguration = apps.get_model(
  'sap_success_factors',
  'SAPSuccessFactorsEnterpriseCustomerConfiguration'
  )
  oauth_access_token, _ = SAPSuccessFactorsAPIClient.get_oauth_access_token(
  self.enterprise_configuration.sapsf_base_url,
  self.enterprise_configuration.key,
  self.enterprise_configuration.secret,
  self.enterprise_configuration.sapsf_company_id,
  sap_user_id,
  SAPSuccessFactorsEnterpriseCustomerConfiguration.USER_TYPE_USER
  )
  response = requests.post(
  url,
  data=payload,
  headers={
  'Authorization': 'Bearer {}'.format(oauth_access_token),
  'content-type': 'application/json'
  }
  )
  return response.status_code, response.text",Make a post request with an auth token acquired for a specific user to a SuccessFactors endpoint. Args: sap_user_id (str): The user to use to retrieve an auth token. url (str): The url to post to. payload (str): The json encoded payload to post.,0,0,1,1,2,1,0,0,1,2
"def save(c, prefix, relative_paths=True):
  dirname = os.path.dirname(prefix)
  pybedtools.BedTool(c.features).saveas(prefix + '.intervals')
  def usepath(f):
  if relative_paths:
  return os.path.relpath(f, start=dirname)
  else:
  return os.path.abspath(f)
  with open(prefix + '.info', 'w') as fout:
  info = {
  'ip_bam': usepath(c.ip.fn),
  'control_bam': usepath(c.control.fn),
  'array_kwargs': c.array_kwargs,
  'dbfn': usepath(c.dbfn),
  'browser_local_coverage_kwargs': c.browser_local_coverage_kwargs,
  'relative_paths': relative_paths,
  }
  fout.write(yaml.dump(info, default_flow_style=False))
  np.savez(
  prefix,
  diffed_array=c.diffed_array,
  ip_array=c.ip_array,
  control_array=c.control_array
  )","Save data from a Chipseq object. Parameters ---------- c : Chipseq object Chipseq object, most likely after calling the `diffed_array` method prefix : str Prefix, including any leading directory paths, to save the data. relative_paths : bool If True (default), then the path names in the `prefix.info` file will be relative to `prefix`. Otherwise, they will be absolute. The following files will be created: :prefix.intervals: A BED file (or GFF, GTF, or VCF as appropriate) of the features used for the array :prefix.info: A YAML-format file indicating the IP and control BAM files, any array kwargs, the database filename, and any minibrowser local coverage args. These are all needed to reconstruct a new Chipseq object. Path names will be relative to `prefix`. :prefix.npz: A NumPy .npz file with keys 'diffed_array', 'ip_array', and 'control_array'",1,0,0,0,1,1,0,0,1,2
"def hostinterface_update(interfaceid, **kwargs):
  conn_args = _login(**kwargs)
  ret = {}
  try:
  if conn_args:
  method = 'hostinterface.update'
  params = {""interfaceid"": interfaceid}
  params = _params_extend(params, **kwargs)
  ret = _query(method, params, conn_args['url'], conn_args['auth'])
  return ret['result']['interfaceids']
  else:
  raise KeyError
  except KeyError:
  return ret",".. versionadded:: 2016.3.0 Update host interface .. note:: This function accepts all standard hostinterface: keyword argument names differ depending on your zabbix version, see here__. .. __: https://www.zabbix.com/documentation/2.4/manual/api/reference/hostinterface/object#host_interface :param interfaceid: ID of the hostinterface to update :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring) :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring) :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring) :return: ID of the updated host interface, False on failure. CLI Example: .. code-block:: bash salt '*' zabbix.hostinterface_update 6 ip_=0.0.0.2",2,1,1,2,6,1,0,0,1,2
"def update_user_entitlements(self, document, do_not_send_invite_for_new_users=None):
  query_parameters = {}
  if do_not_send_invite_for_new_users is not None:
  query_parameters['doNotSendInviteForNewUsers'] = self._serialize.query('do_not_send_invite_for_new_users', do_not_send_invite_for_new_users, 'bool')
  content = self._serialize.body(document, '[JsonPatchOperation]')
  response = self._send(http_method='PATCH',
  location_id='387f832c-dbf2-4643-88e9-c1aa94dbb737',
  version='5.0-preview.2',
  query_parameters=query_parameters,
  content=content,
  media_type='application/json-patch+json')
  return self._deserialize('UserEntitlementOperationReference', response)","UpdateUserEntitlements. [Preview API] Edit the entitlements (License, Extensions, Projects, Teams etc) for one or more users. :param :class:`<[JsonPatchOperation]> <azure.devops.v5_0.member_entitlement_management.models.[JsonPatchOperation]>` document: JsonPatchDocument containing the operations to perform. :param bool do_not_send_invite_for_new_users: Whether to send email invites to new users or not :rtype: :class:`<UserEntitlementOperationReference> <azure.devops.v5_0.member_entitlement_management.models.UserEntitlementOperationReference>`",1,0,0,2,3,1,0,0,1,2
"def display_window(self):
  if self.idx_chan.count() == 0:
  self.update()
  chan_name = self.idx_chan.currentText()
  lg.debug('Power spectrum for channel ' + chan_name)
  if chan_name:
  trial = 0
  data = self.parent.traces.data(trial=trial, chan=chan_name)
  self.display(data)
  else:
  self.scene.clear()","Read the channel name from QComboBox and plot its spectrum. This function is necessary it reads the data and it sends it to self.display. When the user selects a smaller chunk of data from the visible traces, then we don't need to call this function.",1,0,0,1,2,1,0,0,1,2
"def emit(self, event, *args, **kwargs):
  namespace = kwargs.pop('namespace', '/')
  room = kwargs.pop('room', None)
  include_self = kwargs.pop('include_self', True)
  skip_sid = kwargs.pop('skip_sid', None)
  if not include_self and not skip_sid:
  skip_sid = flask.request.sid
  callback = kwargs.pop('callback', None)
  self.server.emit(event, *args, namespace=namespace, room=room,
  skip_sid=skip_sid, callback=callback, **kwargs)","Emit a server generated SocketIO event. This function emits a SocketIO event to one or more connected clients. A JSON blob can be attached to the event as payload. This function can be used outside of a SocketIO event context, so it is appropriate to use when the server is the originator of an event, outside of any client context, such as in a regular HTTP request handler or a background task. Example:: @app.route('/ping') def ping(): socketio.emit('ping event', {'data': 42}, namespace='/chat') :param event: The name of the user event to emit. :param args: A dictionary with the JSON data to send as payload. :param namespace: The namespace under which the message is to be sent. Defaults to the global namespace. :param room: Send the message to all the users in the given room. If this parameter is not included, the event is sent to all connected users. :param skip_sid: The session id of a client to ignore when broadcasting or addressing a room. This is typically set to the originator of the message, so that everyone except that client receive the message. :param callback: If given, this function will be called to acknowledge that the client has received the message. The arguments that will be passed to the function are those provided by the client. Callback functions can only be used when addressing an individual client.",1,0,0,1,2,1,0,0,1,2
"def rename(self, *args, **kwargs):
  axes = validate_axis_style_args(self, args, kwargs, 'mapper', 'rename')
  kwargs.update(axes)
  kwargs.pop('axis', None)
  kwargs.pop('mapper', None)
  return super().rename(**kwargs)","Alter axes labels. Function / dict values must be unique (1-to-1). Labels not contained in a dict / Series will be left as-is. Extra labels listed don't throw an error. See the :ref:`user guide <basics.rename>` for more. Parameters ---------- mapper : dict-like or function Dict-like or functions transformations to apply to that axis' values. Use either ``mapper`` and ``axis`` to specify the axis to target with ``mapper``, or ``index`` and ``columns``. index : dict-like or function Alternative to specifying axis (``mapper, axis=0`` is equivalent to ``index=mapper``). columns : dict-like or function Alternative to specifying axis (``mapper, axis=1`` is equivalent to ``columns=mapper``). axis : int or str Axis to target with ``mapper``. Can be either the axis name ('index', 'columns') or number (0, 1). The default is 'index'. copy : bool, default True Also copy underlying data. inplace : bool, default False Whether to return a new DataFrame. If True then value of copy is ignored. level : int or level name, default None In case of a MultiIndex, only rename labels in the specified level. errors : {'ignore', 'raise'}, default 'ignore' If 'raise', raise a `KeyError` when a dict-like `mapper`, `index`, or `columns` contains labels that are not present in the Index being transformed. If 'ignore', existing keys will be renamed and extra keys will be ignored. Returns ------- DataFrame DataFrame with the renamed axis labels. Raises ------ KeyError If any of the labels is not found in the selected axis and ""errors='raise'"". See Also -------- DataFrame.rename_axis : Set the name of the axis. Examples -------- ``DataFrame.rename`` supports two calling conventions * ``(index=index_mapper, columns=columns_mapper, ...)`` * ``(mapper, axis={'index', 'columns'}, ...)`` We *highly* recommend using keyword arguments to clarify your intent. >>> df = pd.DataFrame({""A"": [1, 2, 3], ""B"": [4, 5, 6]}) >>> df.rename(index=str, columns={""A"": ""a"", ""B"": ""c""}) a c 0 1 4 1 2 5 2 3 6 >>> df.rename(index=str, columns={""A"": ""a"", ""C"": ""c""}) a B 0 1 4 1 2 5 2 3 6 >>> df.rename(index=str, columns={""A"": ""a"", ""C"": ""c""}, errors=""raise"") Traceback (most recent call last): KeyError: ['C'] not found in axis Using axis-style parameters >>> df.rename(str.lower, axis='columns') a b 0 1 4 1 2 5 2 3 6 >>> df.rename({1: 2, 2: 4}, axis='index') A B 0 1 4 2 2 5 4 3 6",1,0,0,1,2,1,0,0,1,2
"def failsafe_add(_session, _instance, **filters):
  if _instance in _session:
  _session.expunge(_instance)
  _session.begin_nested()
  try:
  _session.add(_instance)
  _session.commit()
  if filters:
  return _instance
  except IntegrityError as e:
  _session.rollback()
  if filters:
  try:
  return _session.query(_instance.__class__).filter_by(**filters).one()
  except NoResultFound:
  raise e","Add and commit a new instance in a nested transaction (using SQL SAVEPOINT), gracefully handling failure in case a conflicting entry is already in the database (which may occur due to parallel requests causing race conditions in a production environment with multiple workers). Returns the instance saved to database if no error occurred, or loaded from database using the provided filters if an error occurred. If the filters fail to load from the database, the original IntegrityError is re-raised, as it is assumed to imply that the commit failed because of missing or invalid data, not because of a duplicate entry. However, when no filters are provided, nothing is returned and IntegrityError is also suppressed as there is no way to distinguish between data validation failure and an existing conflicting record in the database. Use this option when failures are acceptable but the cost of verification is not. Usage: ``failsafe_add(db.session, instance, **filters)`` where filters are the parameters passed to ``Model.query.filter_by(**filters).one()`` to load the instance. You must commit the transaction as usual after calling ``failsafe_add``. :param _session: Database session :param _instance: Instance to commit :param filters: Filters required to load existing instance from the database in case the commit fails (required) :return: Instance that is in the database",0,1,1,0,2,0,1,1,1,3
"def load(self: T, **kwargs) -> T:
  lazy_data = {k: v._data for k, v in self.variables.items()
  if isinstance(v._data, dask_array_type)}
  if lazy_data:
  import dask.array as da
  evaluated_data = da.compute(*lazy_data.values(), **kwargs)
  for k, data in zip(lazy_data, evaluated_data):
  self.variables[k].data = data
  for k, v in self.variables.items():
  if k not in lazy_data:
  v.load()
  return self","Manually trigger loading of this dataset's data from disk or a remote source into memory and return this dataset. Normally, it should not be necessary to call this method in user code, because all xarray functions should either work on deferred data or load data automatically. However, this method can be necessary when working with many file objects on disk. Parameters ---------- **kwargs : dict Additional keyword arguments passed on to ``dask.array.compute``. See Also -------- dask.array.compute",1,0,0,0,1,1,0,0,1,2
"def dump(u, *args, **kwargs):
  return dict(
  id=u.id,
  email=u.email,
  password=u.password,
  password_salt=u.password_salt,
  note=u.note,
  full_name=u.full_name if hasattr(u, 'full_name') else '{0} {1}'.format(
  u.given_names, u.family_name),
  settings=u.settings,
  nickname=u.nickname,
  last_login=dt2iso_or_empty(u.last_login))",Dump the users as a list of dictionaries. :param u: User to be dumped. :type u: `invenio.modules.accounts.models.User [Invenio2.x]` or namedtuple. :returns: User serialized to dictionary. :rtype: dict,0,0,0,1,1,1,0,0,1,2
"def list_subcommand(vcard_list, parsable):
  if not vcard_list:
  if not parsable:
  print(""Found no contacts"")
  sys.exit(1)
  elif parsable:
  contact_line_list = []
  for vcard in vcard_list:
  if config.display_by_name() == ""first_name"":
  name = vcard.get_first_name_last_name()
  else:
  name = vcard.get_last_name_first_name()
  contact_line_list.append('\t'.join([vcard.get_uid(), name,
  vcard.address_book.name]))
  print('\n'.join(contact_line_list))
  else:
  list_contacts(vcard_list)",Print a user friendly contacts table. :param vcard_list: the vcards to print :type vcard_list: list of carddav_object.CarddavObject :param parsable: machine readable output: columns devided by tabulator (\t) :type parsable: bool :returns: None :rtype: None,1,0,0,1,2,1,0,0,1,2
"def send_reset_password_instructions(user):
  token = generate_reset_password_token(user)
  reset_link = url_for_security(
  'reset_password', token=token, _external=True
  )
  if config_value('SEND_PASSWORD_RESET_EMAIL'):
  _security.send_mail(config_value('EMAIL_SUBJECT_PASSWORD_RESET'),
  user.email, 'reset_instructions',
  user=user, reset_link=reset_link)
  reset_password_instructions_sent.send(
  app._get_current_object(), user=user, token=token
  )",Sends the reset password instructions email for the specified user. :param user: The user to send the instructions to,1,0,1,0,2,1,0,0,1,2
"def read_namespaced_deployment(self, name, namespace, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.read_namespaced_deployment_with_http_info(name, namespace, **kwargs)
  else:
  (data) = self.read_namespaced_deployment_with_http_info(name, namespace, **kwargs)
  return data","read the specified Deployment This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.read_namespaced_deployment(name, namespace, async_req=True) >>> result = thread.get() :param async_req bool :param str name: name of the Deployment (required) :param str namespace: object name and auth scope, such as for teams and projects (required) :param str pretty: If 'true', then the output is pretty printed. :param bool exact: Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. Deprecated. Planned for removal in 1.18. :param bool export: Should this value be exported. Export strips fields that a user can not specify. Deprecated. Planned for removal in 1.18. :return: V1Deployment If the method is called asynchronously, returns the request thread.",2,0,0,1,3,2,0,0,1,3
"def selection_error_control(self, form_info):
  keys, names = self.return_selected_form_items(form_info['ChannelList'])
  chosen_channels_number = len(keys)
  if form_info['new_channel'] and chosen_channels_number < 2:
  return False, _(
  u""You should choose at least two channel to merge operation at a new channel."")
  elif form_info['existing_channel'] and chosen_channels_number == 0:
  return False, _(
  u""You should choose at least one channel to merge operation with existing channel."")
  elif form_info['find_chosen_channel'] and chosen_channels_number != 1:
  return False, _(u""You should choose one channel for split operation."")
  return True, None","It controls the selection from the form according to the operations, and returns an error message if it does not comply with the rules. Args: form_info: Channel or subscriber form from the user Returns: True or False error message",1,0,0,1,2,1,0,0,1,2
"def cmdargs(mysqldump: str,
  username: str,
  password: str,
  database: str,
  verbose: bool,
  with_drop_create_database: bool,
  max_allowed_packet: str,
  hide_password: bool = False) -> List[str]:
  ca = [
  mysqldump,
  ""-u"", username,
  ""-p{}"".format(""*****"" if hide_password else password),
  ""--max_allowed_packet={}"".format(max_allowed_packet),
  ""--hex-blob"",
  ]
  if verbose:
  ca.append(""--verbose"")
  if with_drop_create_database:
  ca.extend([
  ""--add-drop-database"",
  ""--databases"",
  database
  ])
  else:
  ca.append(database)
  pass
  return ca",Returns command arguments for a ``mysqldump`` call. Args: mysqldump: ``mysqldump`` executable filename username: user name password: password database: database name verbose: verbose output? with_drop_create_database: produce commands to ``DROP`` the database and recreate it? max_allowed_packet: passed to ``mysqldump`` hide_password: obscure the password (will break the arguments but provide a safe version to show the user)? Returns: list of command-line arguments,1,0,1,1,3,1,1,0,1,3
"def query(cls, owner=None, name=None, offset=None, limit=None, api=None):
  api = api if api else cls._API
  query_params = {}
  if owner:
  url = cls._URL['query'].format(owner=owner)
  else:
  url = cls._URL['query'].format(owner='')
  if name:
  query_params['name'] = name
  return super(Project, cls)._query(
  url=url, offset=offset, limit=limit, fields='_all',
  api=api, **query_params
  )",Query (List) projects :param owner: Owner username. :param name: Project name :param offset: Pagination offset. :param limit: Pagination limit. :param api: Api instance. :return: Collection object.,0,0,1,1,2,2,0,0,1,3
"def get_organisations(self, service_desk_id=None, start=0, limit=50):
  url_without_sd_id = 'rest/servicedeskapi/organization'
  url_with_sd_id = 'rest/servicedeskapi/servicedesk/{}/organization'.format(service_desk_id)
  params = {}
  if start is not None:
  params['start'] = int(start)
  if limit is not None:
  params['limit'] = int(limit)
  if service_desk_id is None:
  return self.get(url_without_sd_id, headers=self.experimental_headers, params=params)
  else:
  return self.get(url_with_sd_id, headers=self.experimental_headers, params=params)","Returns a list of organizations in the Jira instance. If the user is not an agent, the resource returns a list of organizations the user is a member of. :param service_desk_id: OPTIONAL: str Get organizations from single Service Desk :param start: OPTIONAL: int The starting index of the returned objects. Base index: 0. See the Pagination section for more details. :param limit: OPTIONAL: int The maximum number of users to return per page. Default: 50. See the Pagination section for more details. :return:",2,0,0,1,3,2,0,0,1,3
"def _get_enrollments_list_page(self, params=None):
  req_url = urljoin(self.base_url, self.enrollment_list_url)
  resp = self.requester.get(req_url, params=params)
  resp.raise_for_status()
  resp_json = resp.json()
  results = resp_json['results']
  next_url_str = resp_json.get('next')
  cursor = None
  qstr_cursor = None
  if next_url_str:
  next_url = urlparse(next_url_str)
  qstr = parse_qs(next_url.query)
  qstr_cursor = qstr.get('cursor')
  if qstr_cursor and isinstance(qstr_cursor, list):
  cursor = qstr_cursor[0]
  return results, cursor",Submit request to retrieve enrollments list. Args: params (dict): Query parameters to use in the request. Valid parameters are: * course_id: Filters the result to course enrollments for the course corresponding to the given course ID. The value must be URL encoded. Optional. * username: username: List of comma-separated usernames. Filters the result to the course enrollments of the given users. Optional.,1,0,0,1,2,2,0,0,1,3
"def _GetTSKPartitionIdentifiers(self, scan_node):
  if not scan_node or not scan_node.path_spec:
  raise errors.ScannerError('Invalid scan node.')
  volume_system = tsk_volume_system.TSKVolumeSystem()
  volume_system.Open(scan_node.path_spec)
  volume_identifiers = self._source_scanner.GetVolumeIdentifiers(
  volume_system)
  if not volume_identifiers:
  return []
  if len(volume_identifiers) == 1:
  return volume_identifiers
  if not self._mediator:
  raise errors.ScannerError(
  'Unable to proceed. Partitions found but no mediator to determine '
  'how they should be used.')
  try:
  volume_identifiers = self._mediator.GetPartitionIdentifiers(
  volume_system, volume_identifiers)
  except KeyboardInterrupt:
  raise errors.UserAbort('File system scan aborted.')
  return self._NormalizedVolumeIdentifiers(
  volume_system, volume_identifiers, prefix='p')",Determines the TSK partition identifiers. Args: scan_node (SourceScanNode): scan node. Returns: list[str]: TSK partition identifiers. Raises: ScannerError: if the format of or within the source is not supported or the scan node is invalid or if the volume for a specific identifier cannot be retrieved. UserAbort: if the user requested to abort.,1,0,0,0,1,1,0,0,1,2
"def default_error_handler(socket, error_name, error_message, endpoint,
  msg_id, quiet):
  pkt = dict(type='event', name='error',
  args=[error_name, error_message],
  endpoint=endpoint)
  if msg_id:
  pkt['id'] = msg_id
  if not quiet:
  socket.send_packet(pkt)
  log.error(u""default_error_handler: {}, {} (endpoint={}, msg_id={})"".format(
  error_name, error_message, endpoint, msg_id
  ))","This is the default error handler, you can override this when calling :func:`socketio.socketio_manage`. It basically sends an event through the socket with the 'error' name. See documentation for :meth:`Socket.error`. :param quiet: if quiet, this handler will not send a packet to the user, but only log for the server developer.",1,0,0,1,2,1,0,0,1,2
"def get_next_step(self):
  if self.layer_purpose != layer_purpose_aggregation:
  subcategory = self.parent.step_kw_subcategory.\
  selected_subcategory()
  else:
  subcategory = {'key': None}
  if is_raster_layer(self.parent.layer):
  return self.parent.step_kw_source
  inasafe_fields = get_non_compulsory_fields(
  self.layer_purpose['key'], subcategory['key'])
  if not skip_inasafe_field(self.parent.layer, inasafe_fields):
  return self.parent.step_kw_inasafe_fields
  default_inasafe_fields = get_fields(
  self.layer_purpose['key'],
  subcategory['key'],
  replace_null=True,
  in_group=False
  )
  if default_inasafe_fields:
  return self.parent.step_kw_default_inasafe_fields
  return self.parent.step_kw_source",Find the proper step when user clicks the Next button. :returns: The step to be switched to. :rtype: WizardStep instance or None,1,0,0,1,2,1,0,0,1,2
"def get_getter(cls, prop_name,
  user_getter=None, getter_takes_name=False):
  if user_getter:
  if getter_takes_name:
  _deps = type(cls)._get_old_style_getter_deps(cls, prop_name,
  user_getter)
  def _getter(self, deps=_deps):
  return user_getter(self, prop_name)
  else:
  _getter = user_getter
  return _getter
  def _getter(self):
  return getattr(self, PROP_NAME % {'prop_name' : prop_name})
  return _getter","Returns a function wich is a getter for a property. prop_name is the name off the property. user_getter is an optional function doing the work. If specified, that function will be called instead of getting the attribute whose name is in 'prop_name'. If user_getter is specified with a False value for getter_takes_name (default), than the method is used to get the value of the property. If True is specified for getter_takes_name, then the user_getter is called by passing the property name (i.e. it is considered a general method which receive the property name whose value has to be returned.)",1,0,0,1,2,0,0,0,1,1
"def get_all_invitations(self, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('asynchronous'):
  return self.get_all_invitations_with_http_info(**kwargs)
  else:
  (data) = self.get_all_invitations_with_http_info(**kwargs)
  return data","Get the details of all the user invitations. # noqa: E501 An endpoint for retrieving the details of all the active user invitations sent for new or existing users to join the account. **Example usage:** `curl https://api.us-east-1.mbedcloud.com/v3/user-invitations -H 'Authorization: Bearer API_KEY'` # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass asynchronous=True >>> thread = api.get_all_invitations(asynchronous=True) >>> result = thread.get() :param asynchronous bool :param int limit: The number of results to return (2-1000), default is 50. :param str after: The entity ID to fetch after the given one. :param str order: The order of the records based on creation time, ASC or DESC; by default ASC :return: UserInvitationRespList If the method is called asynchronously, returns the request thread.",2,0,0,1,3,2,0,0,1,3
"def _get_registry_auth(registry_url, config_path):
  username = None
  password = None
  try:
  docker_config = json.load(open(config_path))
  except ValueError:
  return username, password
  if docker_config.get('auths'):
  docker_config = docker_config['auths']
  auth_key = docker_config.get(registry_url, {}).get('auth', None)
  if auth_key:
  username, password = base64.b64decode(auth_key).split(':', 1)
  return username, password","Retrieve from the config file the current authentication for a given URL, and return the username, password",1,0,0,1,2,1,0,0,1,2
"def get_stored_variation(self, experiment, user_profile):
  user_id = user_profile.user_id
  variation_id = user_profile.get_variation_for_experiment(experiment.id)
  if variation_id:
  variation = self.config.get_variation_from_id(experiment.key, variation_id)
  if variation:
  self.logger.info('Found a stored decision. User ""%s"" is in variation ""%s"" of experiment ""%s"".' % (
  user_id,
  variation.key,
  experiment.key
  ))
  return variation
  return None",Determine if the user has a stored variation available for the given experiment and return that. Args: experiment: Object representing the experiment for which user is to be bucketed. user_profile: UserProfile object representing the user's profile. Returns: Variation if available. None otherwise.,0,0,1,1,2,1,0,1,1,3
"def variant(self, case_id, variant_id):
  variant_id = int(variant_id)
  gemini_query = ""SELECT * from variants WHERE variant_id = {0}"".format(
  variant_id
  )
  individuals = []
  case_obj = self.case(case_id)
  for individual in case_obj.individuals:
  individuals.append(individual)
  self.db = case_obj.variant_source
  self.variant_type = case_obj.variant_type
  gq = GeminiQuery(self.db)
  gq.run(gemini_query)
  for gemini_variant in gq:
  variant = self._format_variant(
  case_id=case_id,
  gemini_variant=gemini_variant,
  individual_objs=individuals,
  index=gemini_variant['variant_id'],
  add_all_info = True
  )
  return variant
  return None",Return a specific variant. We solve this by building a gemini query and send it to _variants Args: case_id (str): Path to a gemini database variant_id (int): A gemini variant id Returns: variant_obj (dict): A puzzle variant,1,0,1,1,3,1,0,1,1,3
"def save_model(self, network=None, model_name='model', **kwargs):
  kwargs.update({'model_name': model_name})
  self._fill_project_info(kwargs)
  params = network.get_all_params()
  s = time.time()
  kwargs.update({'architecture': network.all_graphs, 'time': datetime.utcnow()})
  try:
  params_id = self.model_fs.put(self._serialization(params))
  kwargs.update({'params_id': params_id, 'time': datetime.utcnow()})
  self.db.Model.insert_one(kwargs)
  print(""[Database] Save model: SUCCESS, took: {}s"".format(round(time.time() - s, 2)))
  return True
  except Exception as e:
  exc_type, exc_obj, exc_tb = sys.exc_info()
  fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
  logging.info(""{} {} {} {} {}"".format(exc_type, exc_obj, fname, exc_tb.tb_lineno, e))
  print(""[Database] Save model: FAIL"")
  return False","Save model architecture and parameters into database, timestamp will be added automatically. Parameters ---------- network : TensorLayer layer TensorLayer layer instance. model_name : str The name/key of model. kwargs : other events Other events, such as name, accuracy, loss, step number and etc (optinal). Examples --------- Save model architecture and parameters into database. >>> db.save_model(net, accuracy=0.8, loss=2.3, name='second_model') Load one model with parameters from database (run this in other script) >>> net = db.find_top_model(sess=sess, accuracy=0.8, loss=2.3) Find and load the latest model. >>> net = db.find_top_model(sess=sess, sort=[(""time"", pymongo.DESCENDING)]) >>> net = db.find_top_model(sess=sess, sort=[(""time"", -1)]) Find and load the oldest model. >>> net = db.find_top_model(sess=sess, sort=[(""time"", pymongo.ASCENDING)]) >>> net = db.find_top_model(sess=sess, sort=[(""time"", 1)]) Get model information >>> net._accuracy ... 0.8 Returns --------- boolean : True for success, False for fail.",1,1,1,0,3,1,1,1,1,4
"def getProcList(self, fields=('pid', 'user', 'cmd',), threads=False,
  **kwargs):
  field_list = list(fields)
  for key in kwargs:
  col = re.sub('(_ic)?(_regex)?$', '', key)
  if not col in field_list:
  field_list.append(col)
  pinfo = self.parseProcCmd(field_list, threads)
  if pinfo:
  if len(kwargs) > 0:
  pfilter = util.TableFilter()
  pfilter.registerFilters(**kwargs)
  stats = pfilter.applyFilters(pinfo['headers'], pinfo['stats'])
  return {'headers': pinfo['headers'], 'stats': stats}
  else:
  return pinfo
  else:
  return None","Execute ps command with custom output format with columns columns from fields, select lines using the filters defined by kwargs and return result as a nested list. The Standard Format Specifiers from ps man page must be used for the fields parameter. @param fields: Fields included in the output. Default: pid, user, cmd @param threads: If True, include threads in output. @param **kwargs: Keyword variables are used for filtering the results depending on the values of the columns. Each keyword must correspond to a field name with an optional suffix: field: Field equal to value or in list of values. field_ic: Field equal to value or in list of values, using case insensitive comparison. field_regex: Field matches regex value or matches with any regex in list of values. field_ic_regex: Field matches regex value or matches with any regex in list of values using case insensitive match. @return: List of headers and list of rows and columns.",1,0,0,1,2,1,0,0,1,2
"def patch_priority_class(self, name, body, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.patch_priority_class_with_http_info(name, body, **kwargs)
  else:
  (data) = self.patch_priority_class_with_http_info(name, body, **kwargs)
  return data","partially update the specified PriorityClass This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.patch_priority_class(name, body, async_req=True) >>> result = thread.get() :param async_req bool :param str name: name of the PriorityClass (required) :param object body: (required) :param str pretty: If 'true', then the output is pretty printed. :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint. This field is required for apply requests (application/apply-patch) but optional for non-apply patch types (JsonPatch, MergePatch, StrategicMergePatch). :param bool force: Force is going to \""force\"" Apply requests. It means user will re-acquire conflicting fields owned by other people. Force flag must be unset for non-apply patch requests. :return: V1PriorityClass If the method is called asynchronously, returns the request thread.",1,0,0,1,2,1,0,0,1,2
"def _parseAsteriskConf(self):
  if os.path.isfile(confFileAMI):
  try:
  fp = open(confFileAMI, 'r')
  data = fp.read()
  fp.close()
  except:
  raise IOError('Failed reading Asterisk configuration file: %s'
  % confFileAMI)
  mobj = re.search('^\[(\w+)\]\s*\r{0,1}\nsecret\s*=\s*(\S+)\s*$',
  data, re.MULTILINE)
  if mobj:
  self._amiuser = mobj.group(1)
  self._amipass = mobj.group(2)
  return True
  return False",Parses Asterisk configuration file /etc/asterisk/manager.conf for user and password for Manager Interface. Returns True on success. @return: True if configuration file is found and parsed successfully.,0,0,0,1,1,1,0,0,1,2
"def delete(self, username, realm=None):
  if realm is None:
  name = username
  else:
  name = UrlEncoded(realm, encode_slash=True) + "":"" + UrlEncoded(username, encode_slash=True)
  if name[-1] is not "":"":
  name = name + "":""
  return Collection.delete(self, name)","Delete a storage password by username and/or realm. The identifier can be passed in through the username parameter as <username> or <realm>:<username>, but the preferred way is by passing in the username and realm parameters. :param username: The username for the credentials, or <realm>:<username> if the realm parameter is omitted. :type name: ``string`` :param realm: The credential realm. (optional) :type name: ``string`` :return: The `StoragePassword` collection. :rtype: ``self``",1,0,0,2,3,1,0,0,1,2
"def send_game(chat_id, game_name_short,
  disable_notification=False, reply_to_message_id=None, reply_markup=None, **kwargs):
  params = dict(chat_id=chat_id,
  game_name_short=game_name_short)
  params.update(
  _clean_params(
  disable_notification=disable_notification,
  reply_to_message_id=reply_to_message_id,
  reply_markup=reply_markup,
  )
  )
  return TelegramBotRPCRequest('sendGame', params=params,
  on_result=Message.from_result, **kwargs)","Use this method to send a game. :param chat_id: Unique identifier for the target chat or username of the target channel (in the format @channelusername) :param game_name_short: Short name of the game, serves as the unique identifier for the game. Set up your games via Botfather. :param disable_notification: Sends the message silently. iOS users will not receive a notification, Android users will receive a notification with no sound. :param reply_to_message_id: If the message is a reply, ID of the original message :param reply_markup: A JSON-serialized object for an inline keyboard. :param *\*kwargs: Args that get passed down to :class:`TelegramBotRPCRequest` :type chat_id: str :type game_name_short: str :type disable_notification: bool :type reply_to_message_id: int :type reply_markup: InlineKeyboardMarkup :return: On success, the sent Message is returned. :rtype: TelegramBotRPCRequest",1,0,0,2,3,2,0,0,2,4
"def create_ssh_template(auth, url, ssh_template ):
  f_url = url + ""/imcrs/plat/res/ssh/add""
  response = requests.post(f_url, data = json.dumps(ssh_template), auth=auth, headers=HEADERS)
  try:
  return response.status_code
  except requests.exceptions.RequestException as error:
  return ""Error:\n"" + str(error) + "" create_ssh_template: An Error has occured""","Function takes input of a dictionry containing the required key/value pair for the creation of a ssh template. :param auth: :param url: :param ssh: dictionary of valid JSON which complains to API schema :return: int value of HTTP response code 201 for proper creation or 404 for failed creation :rtype int Sample of proper KV pairs. Please see documentation for valid values for different fields. ssh_template = { ""type"": ""0"", ""name"": ""ssh_admin_template"", ""authType"": ""3"", ""authTypeStr"": ""Password + Super Password"", ""userName"": ""admin"", ""password"": ""password"", ""superPassword"": ""password"", ""port"": ""22"", ""timeout"": ""10"", ""retries"": ""3"", ""keyFileName"": """", ""keyPhrase"": """" }",1,0,0,2,3,2,0,0,2,4
"def delete_jobs(self,
  user_ids,
  job_ids,
  task_ids,
  labels,
  create_time_min=None,
  create_time_max=None):
  tasks = list(
  self.lookup_job_tasks(
  {'RUNNING'},
  user_ids=user_ids,
  job_ids=job_ids,
  task_ids=task_ids,
  labels=labels,
  create_time_min=create_time_min,
  create_time_max=create_time_max))
  print('Found %d tasks to delete.' % len(tasks))
  return google_base.cancel(self._service.new_batch_http_request,
  self._service.operations().cancel, tasks)","Kills the operations associated with the specified job or job.task. Args: user_ids: List of user ids who ""own"" the job(s) to cancel. job_ids: List of job_ids to cancel. task_ids: List of task-ids to cancel. labels: List of LabelParam, each must match the job(s) to be canceled. create_time_min: a timezone-aware datetime value for the earliest create time of a task, inclusive. create_time_max: a timezone-aware datetime value for the most recent create time of a task, inclusive. Returns: A list of tasks canceled and a list of error messages.",2,0,1,2,5,1,1,0,1,3
"def delete_metadata_for_uri(self, uri):
  hash_value = self.hash_for_datasource(uri)
  try:
  cursor = self.get_cursor()
  sql = 'delete from metadata where hash = \'' + hash_value + '\';'
  cursor.execute(sql)
  self.connection.commit()
  except sqlite.Error as e:
  LOGGER.debug(""SQLITE Error %s:"" % e.args[0])
  self.connection.rollback()
  except Exception as e:
  LOGGER.debug(""Error %s:"" % e.args[0])
  self.connection.rollback()
  raise
  finally:
  self.close_connection()","Delete metadata for a URI in the metadata database. A hash will be constructed from the supplied uri and a lookup made in a local SQLITE database for the metadata. If there is an existing record for the hash, the entire record will be erased. .. seealso:: write_metadata_for_uri, read_metadata_for_uri :param uri: A layer uri. e.g. ```dbname=\'osm\' host=localhost port=5432 user=\'foo\'password=\'bar\' sslmode=disable key=\'id\' srid=4326``` :type uri: str",0,1,1,0,2,1,1,1,1,4
"def get_db_prep_save(self, value, connection, prepared=False):
  log.debug(""get_db_prep_save: {}, {}"", value, type(value))
  if not value:
  return ''
  return python_localized_datetime_to_human_iso(value)",Convert Python value to database value for SAVING. We save with full timezone information.,0,1,0,0,1,1,1,1,1,4
"def adsSyncDelDeviceNotificationReqEx(port, adr, notification_handle, user_handle):
  adsSyncDelDeviceNotificationReqFct = _adsDLL.AdsSyncDelDeviceNotificationReqEx
  pAmsAddr = ctypes.pointer(adr.amsAddrStruct())
  nHNotification = ctypes.c_ulong(notification_handle)
  err_code = adsSyncDelDeviceNotificationReqFct(port, pAmsAddr, nHNotification)
  callback_store.pop(notification_handle, None)
  if err_code:
  raise ADSError(err_code)
  adsSyncWriteReqEx(port, adr, ADSIGRP_SYM_RELEASEHND, 0, user_handle, PLCTYPE_UDINT)",Remove a device notification. :param int port: local AMS port as returned by adsPortOpenEx() :param pyads.structs.AmsAddr adr: local or remote AmsAddr :param int notification_handle: Notification Handle :param int user_handle: User Handle,1,0,0,1,2,1,1,0,1,3
"def clean(self):
  cleaned_data = super(UserCredential, self).clean()
  if ""username"" in cleaned_data and ""password"" in cleaned_data:
  auth = utils.import_attr(settings.CAS_AUTH_CLASS)(cleaned_data[""username""])
  if auth.test_password(cleaned_data[""password""]):
  cleaned_data[""username""] = auth.username
  else:
  raise forms.ValidationError(
  _(u""The credentials you provided cannot be determined to be authentic."")
  )
  return cleaned_data",Validate that the submited :attr:`username` and :attr:`password` are valid :raises django.forms.ValidationError: if the :attr:`username` and :attr:`password` are not valid. :return: The cleaned POST data :rtype: dict,1,0,0,1,2,1,0,0,1,2
"def get_user_detail(self, req, account, user):
  path = quote('/v1/%s/%s/%s' % (self.auth_account, account, user))
  resp = self.make_pre_authed_request(
  req.environ, 'GET', path).get_response(self.app)
  if resp.status_int == 404:
  return None
  if resp.status_int // 100 != 2:
  raise Exception('Could not get user object: %s %s' %
  (path, resp.status))
  return resp.body","Returns the response body of a GET request for the specified user The body is in JSON format and contains all user information. :param req: The swob request :param account: the account the user is a member of :param user: the user :returns: A JSON response with the user detail information, None if the user doesn't exist",2,0,0,1,3,2,0,0,1,3
"def get_access_info(self, unscoped_auth):
  try:
  unscoped_auth_ref = base.BasePlugin.get_access_info(
  self, unscoped_auth)
  except exceptions.KeystoneAuthException as excp:
  msg = _('Service provider authentication failed. %s')
  raise exceptions.KeystoneAuthException(msg % str(excp))
  return unscoped_auth_ref",Get the access info object We attempt to get the auth ref. If it fails and if the K2K auth plugin was being used then we will prepend a message saying that the error was on the service provider side. :param: unscoped_auth: Keystone auth plugin for unscoped user :returns: keystoneclient.access.AccessInfo object,1,0,0,0,1,1,0,0,1,2
"def authenticate_ldap(self, username, password):
  user_map = dict()
  user_map['username'] = username
  user_map['password'] = password
  code, xml = self.submit(
  {'user': user_map}, 'POST', 'authenticate/ldap/')
  return self.response(code, xml)","Get user by username and password and their permissions. :param username: Username. String with a minimum 3 and maximum of 45 characters :param password: User password. String with a minimum 3 and maximum of 45 characters :return: Following dictionary: :: {'user': {'id': < id >} {'user': < user >} {'nome': < nome >} {'pwd': < pwd >} {'email': < email >} {'active': < active >} {'permission':[ {'<function>': { 'write': <value>, 'read': <value>}, ... more function ... ] } } } :raise InvalidParameterError: The value of username or password is invalid. :raise DataBaseError: Networkapi failed to access the database. :raise XMLError: Networkapi failed to generate the XML response.",2,0,0,2,4,2,0,0,2,4
"def user_data_dir(appname, roaming=False):
  if WINDOWS:
  const = roaming and ""CSIDL_APPDATA"" or ""CSIDL_LOCAL_APPDATA""
  path = os.path.join(os.path.normpath(_get_win_folder(const)), appname)
  elif sys.platform == ""darwin"":
  path = os.path.join(
  os.path.expanduser('~/Library/Application Support/'),
  appname,
  )
  else:
  path = os.path.join(
  os.getenv('XDG_DATA_HOME', os.path.expanduser(""~/.local/share"")),
  appname,
  )
  return path","Return full path to the user-specific data dir for this application. ""appname"" is the name of application. If None, just the system directory is returned. ""roaming"" (boolean, default False) can be set True to use the Windows roaming appdata directory. That means that for users on a Windows network setup for roaming profiles, this user data will be sync'd on login. See <http://technet.microsoft.com/en-us/library/cc766489(WS.10).aspx> for a discussion of issues. Typical user data directories are: Mac OS X: ~/Library/Application Support/<AppName> Unix: ~/.local/share/<AppName> # or in $XDG_DATA_HOME, if defined Win XP (not roaming): C:\Documents and Settings\<username>\ ... ...Application Data\<AppName> Win XP (roaming): C:\Documents and Settings\<username>\Local ... ...Settings\Application Data\<AppName> Win 7 (not roaming): C:\\Users\<username>\AppData\Local\<AppName> Win 7 (roaming): C:\\Users\<username>\AppData\Roaming\<AppName> For Unix, we follow the XDG spec and support $XDG_DATA_HOME. That means, by default ""~/.local/share/<AppName>"".",1,0,0,1,2,1,0,0,1,2
"def QueryUsers(self, database_link, query, options=None):
  if options is None:
  options = {}
  path = base.GetPathFromLink(database_link, 'users')
  database_id = base.GetResourceIdOrFullNameFromLink(database_link)
  def fetch_fn(options):
  return self.__QueryFeed(path,
  'users',
  database_id,
  lambda r: r['Users'],
  lambda _, b: b,
  query,
  options), self.last_response_headers
  return query_iterable.QueryIterable(self, query, options, fetch_fn)",Queries users in a database. :param str database_link: The link to the database. :param (str or dict) query: :param dict options: The request options for the request. :return: Query Iterable of Users. :rtype: query_iterable.QueryIterable,2,0,1,1,4,2,0,0,1,3
"def get(self, path_info):
  assert path_info[""scheme""] == ""local""
  path = path_info[""path""]
  if not os.path.exists(path):
  return None
  actual_mtime, actual_size = get_mtime_and_size(path)
  actual_inode = get_inode(path)
  existing_record = self.get_state_record_for_inode(actual_inode)
  if not existing_record:
  return None
  mtime, size, checksum, _ = existing_record
  if self._file_metadata_changed(actual_mtime, mtime, actual_size, size):
  return None
  self._update_state_record_timestamp_for_inode(actual_inode)
  return checksum",Gets the checksum for the specified path info. Checksum will be retrieved from the state database if available. Args: path_info (dict): path info to get the checksum for. Returns: str or None: checksum for the specified path info or None if it doesn't exist in the state database.,0,1,1,0,2,0,1,0,1,2
"def parse_require(self, env, keys, defaults={}):
  for k in keys:
  env[k] = getattr(self.options, k) or env.get(k, None)
  if env[k] is None:
  self.error(""config syntax error,""
  ""please set `%s` in your env: %s"" % (k, env))
  pass
  for k, v in defaults.items():
  env.setdefault(k, v)
  return env",check and get require config :param dict env: user config node :param list keys: check keys .. note:: option and key name must be same. :param dict defaults: default value for keys :return: dict.env with verified. .. exception:: will raise `ValueError` when some key missed.,1,0,0,1,2,1,0,0,1,2
"def pylab_activate(user_ns, gui=None, import_all=True, shell=None):
  gui, backend = find_gui_and_backend(gui)
  activate_matplotlib(backend)
  import_pylab(user_ns, import_all)
  if shell is not None:
  configure_inline_support(shell, backend, user_ns)
  print % backend
  sys.stdout.flush()
  return gui","Activate pylab mode in the user's namespace. Loads and initializes numpy, matplotlib and friends for interactive use. Parameters ---------- user_ns : dict Namespace where the imports will occur. gui : optional, string A valid gui name following the conventions of the %gui magic. import_all : optional, boolean If true, an 'import *' is done from numpy and pylab. Returns ------- The actual gui used (if not given as input, it was obtained from matplotlib itself, and will be needed next to configure IPython's gui integration.",1,0,0,1,2,1,0,0,1,2
"def hl_canvas2table(self, canvas, button, data_x, data_y):
  self.treeview.clear_selection()
  if self.maskhltag:
  try:
  canvas.delete_object_by_tag(self.maskhltag, redraw=True)
  except Exception:
  pass
  try:
  obj = canvas.get_object_by_tag(self.masktag)
  except Exception:
  return
  if obj.kind != 'compound':
  return
  if len(self._maskobjs) == 0:
  return
  for i, mobj in enumerate(self._maskobjs):
  mask1 = self._rgbtomask(mobj)
  if mask1[int(data_y), int(data_x)]:
  self._highlight_path(self._treepaths[i])",Highlight mask on table when user click on canvas.,1,0,0,1,2,1,0,0,1,2
"def token_setter(remote, token, secret='', token_type='', extra_data=None,
  user=None):
  session[token_session_key(remote.name)] = (token, secret)
  user = user or current_user
  if not user.is_anonymous:
  uid = user.id
  cid = remote.consumer_key
  t = RemoteToken.get(uid, cid, token_type=token_type)
  if t:
  t.update_token(token, secret)
  else:
  t = RemoteToken.create(
  uid, cid, token, secret,
  token_type=token_type, extra_data=extra_data
  )
  return t
  return None","Set token for user. :param remote: The remote application. :param token: The token to set. :param token_type: The token type. (Default: ``''``) :param extra_data: Extra information. (Default: ``None``) :param user: The user owner of the remote token. If it's not defined, the current user is used automatically. (Default: ``None``) :returns: A :class:`invenio_oauthclient.models.RemoteToken` instance or ``None``.",1,1,1,0,3,1,1,1,1,4
"def priceItems(items):
  retItems = []
  sendItems = []
  for item in items:
  sendItems.append(item.name)
  resp = CodexAPI.searchMany(sendItems)
  for respItem in resp:
  retItems[respItem['name']].price = respItem['price']
  return retItems","Takes a list of Item objects and returns a list of Item objects with respective prices modified Uses the given list of item objects to formulate a query to the item database. Uses the returned results to populate each item in the list with its respective price, then returns the modified list. Parameters: items (list[Item]) -- List of items to price Returns list[Item] - Priced list of items",1,0,0,1,2,2,0,0,1,3
"def get_impala_queries(self, start_time, end_time, filter_str="""", limit=100,
  offset=0):
  params = {
  'from': start_time.isoformat(),
  'to': end_time.isoformat(),
  'filter': filter_str,
  'limit': limit,
  'offset': offset,
  }
  return self._get(""impalaQueries"", ApiImpalaQueryResponse,
  params=params, api_version=4)",Returns a list of queries that satisfy the filter @type start_time: datetime.datetime. Note that the datetime must either be time zone aware or specified in the server time zone. See the python datetime documentation for more details about python's time zone handling. @param start_time: Queries must have ended after this time @type end_time: datetime.datetime. Note that the datetime must either be time zone aware or specified in the server time zone. See the python datetime documentation for more details about python's time zone handling. @param end_time: Queries must have started before this time @param filter_str: A filter to apply to the queries. For example: 'user = root and queryDuration > 5s' @param limit: The maximum number of results to return @param offset: The offset into the return list @since: API v4,1,0,1,1,3,2,0,0,1,3
"def get_flair(self, subreddit, redditor, **params):
  name = six.text_type(redditor)
  params.update(name=name)
  url = self.config['flairlist'].format(
  subreddit=six.text_type(subreddit))
  data = self.request_json(url, params=params)
  if not data['users'] or \
  data['users'][0]['user'].lower() != name.lower():
  return None
  return data['users'][0]","Return the flair for a user on the given subreddit. :param subreddit: Can be either a Subreddit object or the name of a subreddit. :param redditor: Can be either a Redditor object or the name of a redditor. :returns: None if the user doesn't exist, otherwise a dictionary containing the keys `flair_css_class`, `flair_text`, and `user`.",1,0,0,1,2,2,0,0,1,3
"def validate(self, messages):
  messages = self.validate_version(messages)
  messages = self.validate_data_lics(messages)
  messages = self.validate_name(messages)
  messages = self.validate_spdx_id(messages)
  messages = self.validate_namespace(messages)
  messages = self.validate_ext_document_references(messages)
  messages = self.validate_creation_info(messages)
  messages = self.validate_package(messages)
  messages = self.validate_extracted_licenses(messages)
  messages = self.validate_reviews(messages)
  return messages",Validate all fields of the document and update the messages list with user friendly error messages for display.,0,0,0,1,1,1,0,0,1,2
"def lookup_genome_alignment_index(index_fh, indexed_fh, out_fh=sys.stdout,
  key=None, verbose=False):
  bound_iter = functools.partial(genome_alignment_iterator,
  reference_species=""hg19"", index_friendly=True)
  hash_func = JustInTimeGenomeAlignmentBlock.build_hash
  idx = IndexedFile(record_iterator=bound_iter, record_hash_function=hash_func)
  idx.read_index(index_fh, indexed_fh)
  if key is None:
  while key is None or key.strip() != """":
  sys.stderr.write(""[WAITING FOR KEY ENTRY ON STDIN; "" +
  ""END WITH EMPTY LINE]\n"")
  key = raw_input()
  key = '\t'.join(key.split()).strip()
  if key != """":
  out_fh.write(str(idx[key]) + ""\n"")
  sys.stderr.write(""\n"")
  else:
  key = '\t'.join(key.split())
  out_fh.write(str(idx[key]) + ""\n"")","Load a GA index and its indexed file and extract one or more blocks. :param index_fh: the index file to load. Can be a filename or a stream-like object. :param indexed_fh: the file that the index was built for, :param key: A single key, iterable of keys, or None. This key will be used for lookup. If None, user is prompted to enter keys interactively.",1,0,0,1,2,1,0,0,1,2
"def closeEvent(self, event):
  if self.inimodel.get_edited():
  r = self.doc_modified_prompt()
  if r == QtGui.QMessageBox.Yes:
  event.accept()
  else:
  event.ignore()
  else:
  event.accept()","Handles closing of the window. If configs were edited, ask user to continue. :param event: the close event :type event: QCloseEvent :returns: None :rtype: None :raises: None",1,0,0,1,2,1,0,0,1,2
"def press_enter(multiple=False, silent=False):
  def f():
  try:
  while True:
  if silent:
  yield input()
  else:
  sys.stderr.write(""<press enter> "")
  sys.stderr.flush()
  yield input()
  if not multiple:
  break
  except (EOFError, KeyboardInterrupt):
  if not silent:
  sys.stderr.write(""\n"")
  sys.stderr.flush()
  return
  return f",Return a generator function which yields every time the user presses return.,1,0,0,1,2,1,0,0,1,2
"def table(self, name, database=None, schema=None):
  if database is not None and database != self.current_database:
  return self.database(name=database).table(name=name, schema=schema)
  else:
  alch_table = self._get_sqla_table(name, schema=schema)
  node = self.table_class(alch_table, self, self._schemas.get(name))
  return self.table_expr_class(node)","Create a table expression that references a particular a table called `name` in a MySQL database called `database`. Parameters ---------- name : str The name of the table to retrieve. database : str, optional The database in which the table referred to by `name` resides. If ``None`` then the ``current_database`` is used. schema : str, optional The schema in which the table resides. If ``None`` then the `public` schema is assumed. Returns ------- table : TableExpr A table expression.",1,0,1,1,3,1,0,1,1,3
"def create_session(self, user_agent, remote_address, client_version):
  self.session_counter += 1
  self.sessions[self.session_counter] = session = self.session_class()
  session.user_agent = user_agent
  session.remote_address = remote_address
  session.client_version = client_version
  invoke_hooks(self.hooks, ""session_created"", self.session_counter)
  return self.session_counter",Create a new session. :param str user_agent: Client user agent :param str remote_addr: Remote address of client :param str client_version: Remote client version :return: The new session id :rtype: int,1,0,0,1,2,1,0,0,1,2
"def upload_sticker_file(self, user_id, png_sticker):
  from pytgbot.api_types.sendable.files import InputFile
  assert_type_or_raise(user_id, int, parameter_name=""user_id"")
  assert_type_or_raise(png_sticker, InputFile, parameter_name=""png_sticker"")
  result = self.do(""uploadStickerFile"", user_id=user_id, png_sticker=png_sticker)
  if self.return_python_objects:
  logger.debug(""Trying to parse {data}"".format(data=repr(result)))
  from pytgbot.api_types.receivable.media import File
  try:
  return File.from_array(result)
  except TgApiParseException:
  logger.debug(""Failed parsing as api_type File"", exc_info=True)
  raise TgApiParseException(""Could not parse result."")
  return result","Use this method to upload a .png file with a sticker for later use in createNewStickerSet and addStickerToSet methods (can be used multiple times). Returns the uploaded File on success. https://core.telegram.org/bots/api#uploadstickerfile Parameters: :param user_id: User identifier of sticker file owner :type user_id: int :param png_sticker: Png image with the sticker, must be up to 512 kilobytes in size, dimensions must not exceed 512px, and either width or height must be exactly 512px. More info on Sending Files  :type png_sticker: pytgbot.api_types.sendable.files.InputFile Returns: :return: Returns the uploaded File on success :rtype: pytgbot.api_types.receivable.media.File",1,0,0,2,3,2,0,0,1,3
"def getPlainText( parent, title, caption, text = '' ):
  dlg = QDialog(parent)
  dlg.setWindowTitle(title)
  label = QLabel(dlg)
  label.setText(caption)
  edit = QTextEdit(dlg)
  edit.setText(text)
  edit.selectAll()
  opts = QDialogButtonBox.Ok | QDialogButtonBox.Cancel
  btns = QDialogButtonBox(opts, Qt.Horizontal, dlg)
  btns.accepted.connect(dlg.accept)
  btns.rejected.connect(dlg.reject)
  layout = QVBoxLayout()
  layout.addWidget(label)
  layout.addWidget(edit)
  layout.addWidget(btns)
  dlg.setLayout(layout)
  dlg.adjustSize()
  if ( dlg.exec_() ):
  return (edit.toPlainText(), True)
  return ('', False)","Prompts the user for more advanced text input. :param parent | <QWidget> || None title | <str> caption | <str> text | <str> :return (<str>, <bool> accepted)",1,0,0,1,2,1,0,0,1,2
"def is_draft(request):
  if PublishingMiddleware.is_admin_request(request):
  return True
  if PublishingMiddleware.is_api_request(request):
  return True
  if PublishingMiddleware.is_draft_only_view(request):
  return True
  if PublishingMiddleware.is_content_reviewer_user(request):
  return True
  if PublishingMiddleware.is_draft_request(request):
  if PublishingMiddleware.is_staff_user(request):
  return True
  if verify_draft_url(request.get_full_path()):
  return True
  return False","A request is considered to be in draft mode if: - it is for *any* admin resource, since the admin site deals only with draft objects and hides the published version from admin users - it is for *any* view in *any* app that deals only with draft objects - user is a member of the ""Content Reviewer"" group, since content reviewers' sole purpose is to review draft content and they need not see the published content - the user is a staff member and therefore can see draft versions of pages if they wish, and the 'preview' GET parameter flag is included to show the draft page is definitely wanted instead of a normal published page. - the 'preview' GET parameter flag is included with a valid HMAC for the requested URL, regardless of authenticated permissions.",1,0,0,0,1,1,0,0,1,2
"def main(target_device):
  jlink = pylink.JLink()
  print(""connecting to JLink..."")
  jlink.open()
  print(""connecting to %s..."" % target_device)
  jlink.set_tif(pylink.enums.JLinkInterfaces.SWD)
  jlink.connect(target_device)
  print(""connected, starting RTT..."")
  jlink.rtt_start()
  while True:
  try:
  num_up = jlink.rtt_get_num_up_buffers()
  num_down = jlink.rtt_get_num_down_buffers()
  print(""RTT started, %d up bufs, %d down bufs."" % (num_up, num_down))
  break
  except pylink.errors.JLinkRTTException:
  time.sleep(0.1)
  try:
  thread.start_new_thread(read_rtt, (jlink,))
  thread.start_new_thread(write_rtt, (jlink,))
  while jlink.connected():
  time.sleep(1)
  print(""JLink disconnected, exiting..."")
  except KeyboardInterrupt:
  print(""ctrl-c detected, exiting..."")
  pass","Creates an interactive terminal to the target via RTT. The main loop opens a connection to the JLink, and then connects to the target device. RTT is started, the number of buffers is presented, and then two worker threads are spawned: one for read, and one for write. The main loops sleeps until the JLink is either disconnected or the user hits ctrl-c. Args: target_device (string): The target CPU to connect to. Returns: Always returns ``0`` or a JLinkException. Raises: JLinkException on error.",2,0,0,2,4,1,0,0,1,2
"def remove_from_groups(self, groups=None, all_groups=False, group_type=None):
  if all_groups:
  if groups or group_type:
  raise ArgumentError(""When removing from all groups, do not specify specific groups or types"")
  glist = ""all""
  else:
  if not groups:
  raise ArgumentError(""You must specify groups from which to remove the user"")
  if not group_type:
  group_type = GroupTypes.product
  elif group_type in GroupTypes.__members__:
  group_type = GroupTypes[group_type]
  if group_type not in GroupTypes:
  raise ArgumentError(""You must specify a GroupType value for argument group_type"")
  glist = {group_type.name: [group for group in groups]}
  return self.append(remove=glist)","Remove user from some PLC groups, or all of them. :param groups: list of group names the user should be removed from :param all_groups: a boolean meaning remove from all (don't specify groups or group_type in this case) :param group_type: the type of group (defaults to ""product"") :return: the User, so you can do User(...).remove_from_groups(...).add_role(...)",1,0,0,1,2,1,1,0,1,3
"def superuser_api_key_required(f):
  @functools.wraps(f)
  def wrapped(*args, **kwargs):
  api_key = current_api_key()
  g.api_key = api_key
  utils.jsonify_assert(
  api_key.superuser,
  'API key=%r must be a super user' % api_key.id,
  403)
  return f(*args, **kwargs)
  return wrapped",Decorator ensures only superuser API keys can request this function.,2,0,0,0,2,1,0,0,1,2
"def share_pull_request(self, user_message, repository_id, pull_request_id, project=None):
  route_values = {}
  if project is not None:
  route_values['project'] = self._serialize.url('project', project, 'str')
  if repository_id is not None:
  route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')
  if pull_request_id is not None:
  route_values['pullRequestId'] = self._serialize.url('pull_request_id', pull_request_id, 'int')
  content = self._serialize.body(user_message, 'ShareNotificationContext')
  self._send(http_method='POST',
  location_id='696f3a82-47c9-487f-9117-b9d00972ca84',
  version='5.0-preview.1',
  route_values=route_values,
  content=content)",SharePullRequest. [Preview API] Sends an e-mail notification about a specific pull request to a set of recipients :param :class:`<ShareNotificationContext> <azure.devops.v5_0.git.models.ShareNotificationContext>` user_message: :param str repository_id: ID of the git repository. :param int pull_request_id: ID of the pull request. :param str project: Project ID or project name,1,0,0,1,2,2,0,0,1,3
"async def get_oauth_verifier(oauth_token):
  url = ""https://api.twitter.com/oauth/authorize?oauth_token="" + oauth_token
  try:
  browser = webbrowser.open(url)
  await asyncio.sleep(2)
  if not browser:
  raise RuntimeError
  except RuntimeError:
  print(""could not open a browser\ngo here to enter your PIN: "" + url)
  verifier = input(""\nEnter your PIN: "")
  return verifier","Open authorize page in a browser, print the url if it didn't work Arguments --------- oauth_token : str The oauth token received in :func:`get_oauth_token` Returns ------- str The PIN entered by the user",1,0,0,1,2,2,0,0,1,3
"def add(self, instance, modified=True, **params):
  sm = self.model(instance)
  instance.session = self
  o = sm.add(instance, modified=modified, **params)
  if modified and not self.transaction:
  transaction = self.begin()
  return transaction.commit(lambda: o)
  else:
  return o","Add an ``instance`` to the session. If the session is not in a :ref:`transactional state <transactional-state>`, this operation commits changes to the back-end server immediately. :parameter instance: a :class:`Model` instance. It must be registered with the :attr:`router` which created this :class:`Session`. :parameter modified: a boolean flag indicating if the instance was modified. :return: the ``instance``. If the instance is persistent (it is already stored in the database), an updated will be performed, otherwise a new entry will be created once the :meth:`commit` method is invoked.",1,1,0,0,2,1,1,1,1,4
"def get_airport_details(self, iata, page=1, limit=100):
  url = AIRPORT_DATA_BASE.format(iata, str(self.AUTH_TOKEN), page, limit)
  details = self._fr24.get_airport_details(url)
  weather = self._fr24.get_airport_weather(url)
  details['position']['elevation'] = weather['elevation']
  return details","Retrieve the details of an airport Given the IATA code of an airport, this method returns the detailed information like lat lon, full name, URL, codes etc. Args: iata (str): The IATA code for an airport, e.g. HYD page (int): Optional page number; for users who are on a plan with flightradar24 they can pass in higher page numbers to get more data limit (int): Optional limit on number of records returned Returns: A list of dicts with the data; one dict for each row of data from flightradar24 Example:: from pyflightdata import FlightData f=FlightData() #optional login f.login(myemail,mypassword) f.get_airport_details('HYD') f.get_airport_details('HYD',page=1,limit=10)",2,0,0,1,3,2,0,0,1,3
"def decode_state(cls, state, param='user_state'):
  if state and cls.supports_user_state:
  return json.loads(base64.urlsafe_b64decode(
  unquote(str(state))).decode('utf-8'))[param]
  else:
  return state if param == 'csrf' else ''",Decode state and return param. :param str state: state parameter passed through by provider :param str param: key to query from decoded state variable. Options include 'csrf' and 'user_state'. :returns: string value from decoded state,0,0,0,1,1,1,0,0,1,2
"def GetAllUsers(self, pagination_size=10):
  next_page_token, accounts = self.rpc_helper.DownloadAccount(
  None, pagination_size)
  while accounts:
  for account in accounts:
  yield GitkitUser.FromApiResponse(account)
  next_page_token, accounts = self.rpc_helper.DownloadAccount(
  next_page_token, pagination_size)","Gets all user info from Gitkit server. Args: pagination_size: int, how many users should be returned per request. The account info are retrieved in pagination. Yields: A generator to iterate all users.",2,0,1,1,4,2,0,0,1,3
"def isSane(self):
  if self.host == 'localhost':
  return True
  host_parts = self.host.split('.')
  if self.wildcard:
  assert host_parts[0] == '', host_parts
  del host_parts[0]
  if host_parts and not host_parts[-1]:
  del host_parts[-1]
  if not host_parts:
  return False
  if '' in host_parts:
  return False
  tld = host_parts[-1]
  if tld not in _top_level_domains:
  return False
  if len(host_parts) == 1:
  return False
  if self.wildcard:
  if len(tld) == 2 and len(host_parts[-2]) <= 3:
  return len(host_parts) > 2
  return True","This method checks the to see if a trust root represents a reasonable (sane) set of URLs. 'http://*.com/', for example is not a reasonable pattern, as it cannot meaningfully specify the site claiming it. This function attempts to find many related examples, but it can only work via heuristics. Negative responses from this method should be treated as advisory, used only to alert the user to examine the trust root carefully. @return: Whether the trust root is sane @rtype: C{bool}",1,0,0,1,2,1,0,0,1,2
"def list_(
  pkg=None,
  user=None,
  installed=False,
  env=None):
  cmd = ['cabal list --simple-output']
  if installed:
  cmd.append('--installed')
  if pkg:
  cmd.append('""{0}""'.format(pkg))
  result = __salt__['cmd.run_all'](' '.join(cmd), runas=user, env=env)
  packages = {}
  for line in result['stdout'].splitlines():
  data = line.split()
  package_name = data[0]
  package_version = data[1]
  packages[package_name] = package_version
  return packages","List packages matching a search string. pkg Search string for matching package names user The user to run cabal list with installed If True, only return installed packages. env Environment variables to set when invoking cabal. Uses the same ``env`` format as the :py:func:`cmd.run <salt.modules.cmdmod.run>` execution function CLI example: .. code-block:: bash salt '*' cabal.list salt '*' cabal.list ShellCheck",1,0,0,1,2,1,0,0,1,2
"def send_error(self, code, message=None):
  try:
  short, long = self.responses[code]
  except KeyError:
  short, long = '???', '???'
  if message is None:
  message = short
  explain = long
  self.log_error(""code %d, message %s"", code, message)
  content = (self.error_message_format %
  {'code': code, 'message': _quote_html(message), 'explain': explain})
  self.send_response(code, message)
  self.send_header(""Content-Type"", self.error_content_type)
  self.send_header('Connection', 'close')
  self.end_headers()
  if self.command != 'HEAD' and code >= 200 and code not in (204, 304):
  self.wfile.write(content)","Send and log an error reply. Arguments are the error code, and a detailed message. The detailed message defaults to the short entry matching the response code. This sends an error response (so it must be called before any output has been generated), logs the error, and finally sends a piece of HTML explaining the error to the user.",1,0,0,1,2,1,0,0,1,2
"def upload_bel_namespace(self, update: bool = False) -> Namespace:
  if not self.is_populated():
  self.populate()
  namespace = self._get_default_namespace()
  if namespace is None:
  log.info('making namespace for %s', self._get_namespace_name())
  return self._make_namespace()
  if update:
  self._update_namespace(namespace)
  return namespace",Upload the namespace to the PyBEL database. :param update: Should the namespace be updated first?,0,1,1,0,2,1,1,0,1,3
"def login(self, client_id, username, password, connection, id_token=None,
  grant_type='password', device=None, scope='openid'):
  warnings.warn(""/oauth/ro will be deprecated in future releases"", DeprecationWarning)
  return self.post(
  'https://{}/oauth/ro'.format(self.domain),
  data={
  'client_id': client_id,
  'username': username,
  'password': password,
  'id_token': id_token,
  'connection': connection,
  'device': device,
  'grant_type': grant_type,
  'scope': scope,
  },
  headers={'Content-Type': 'application/json'}
  )","Login using username and password Given the user credentials and the connection specified, it will do the authentication on the provider and return a dict with the access_token and id_token. This endpoint only works for database connections, passwordless connections, Active Directory/LDAP, Windows Azure AD and ADFS.",2,0,0,2,4,2,0,0,1,3
"def precision_at_k(
  model,
  test_interactions,
  train_interactions=None,
  k=10,
  user_features=None,
  item_features=None,
  preserve_rows=False,
  num_threads=1,
  check_intersections=True,
 ):
  if num_threads < 1:
  raise ValueError(""Number of threads must be 1 or larger."")
  ranks = model.predict_rank(
  test_interactions,
  train_interactions=train_interactions,
  user_features=user_features,
  item_features=item_features,
  num_threads=num_threads,
  check_intersections=check_intersections,
  )
  ranks.data = np.less(ranks.data, k, ranks.data)
  precision = np.squeeze(np.array(ranks.sum(axis=1))) / k
  if not preserve_rows:
  precision = precision[test_interactions.getnnz(axis=1) > 0]
  return precision","Measure the precision at k metric for a model: the fraction of known positives in the first k positions of the ranked list of results. A perfect score is 1.0. Parameters ---------- model: LightFM instance the fitted model to be evaluated test_interactions: np.float32 csr_matrix of shape [n_users, n_items] Non-zero entries representing known positives in the evaluation set. train_interactions: np.float32 csr_matrix of shape [n_users, n_items], optional Non-zero entries representing known positives in the train set. These will be omitted from the score calculations to avoid re-recommending known positives. k: integer, optional The k parameter. user_features: np.float32 csr_matrix of shape [n_users, n_user_features], optional Each row contains that user's weights over features. item_features: np.float32 csr_matrix of shape [n_items, n_item_features], optional Each row contains that item's weights over features. preserve_rows: boolean, optional When False (default), the number of rows in the output will be equal to the number of users with interactions in the evaluation set. When True, the number of rows in the output will be equal to the number of users. num_threads: int, optional Number of parallel computation threads to use. Should not be higher than the number of physical cores. check_intersections: bool, optional, True by default, Only relevant when train_interactions are supplied. A flag that signals whether the test and train matrices should be checked for intersections to prevent optimistic ranks / wrong evaluation / bad data split. Returns ------- np.array of shape [n_users with interactions or n_users,] Numpy array containing precision@k scores for each user. If there are no interactions for a given user the returned precision will be 0.",1,0,0,1,2,1,0,0,1,2
"def acquire_code(args, session, session3):
  serial_number = find_mfa_for_user(args.serial_number, session, session3)
  if not serial_number:
  print(""There are no MFA devices associated with this user."",
  file=sys.stderr)
  return None, None, USER_RECOVERABLE_ERROR
  token_code = args.token_code
  if token_code is None:
  while token_code is None or len(token_code) != 6:
  token_code = getpass.getpass(""MFA Token Code: "")
  return serial_number, token_code, OK","returns the user's token serial number, MFA token code, and an error code.",2,0,0,1,3,1,0,0,1,2
"def netmiko_config(*config_commands, **kwargs):
  netmiko_kwargs = netmiko_args()
  kwargs.update(netmiko_kwargs)
  return __salt__['netmiko.send_config'](config_commands=config_commands,
  **kwargs)",".. versionadded:: 2019.2.0 Load a list of configuration commands on the remote device, via Netmiko. .. warning:: Please remember that ``netmiko`` does not have any rollback safeguards and any configuration change will be directly loaded into the running config if the platform doesn't have the concept of ``candidate`` config. On Junos, or other platforms that have this capability, the changes will not be loaded into the running config, and the user must set the ``commit`` argument to ``True`` to transfer the changes from the candidate into the running config before exiting. config_commands A list of configuration commands to be loaded on the remote device. config_file Read the configuration commands from a file. The file can equally be a template that can be rendered using the engine of choice (see ``template_engine``). This can be specified using the absolute path to the file, or using one of the following URL schemes: - ``salt://``, to fetch the file from the Salt fileserver. - ``http://`` or ``https://`` - ``ftp://`` - ``s3://`` - ``swift://`` exit_config_mode: ``True`` Determines whether or not to exit config mode after complete. delay_factor: ``1`` Factor to adjust delays. max_loops: ``150`` Controls wait time in conjunction with delay_factor (default: ``150``). strip_prompt: ``False`` Determines whether or not to strip the prompt (default: ``False``). strip_command: ``False`` Determines whether or not to strip the command (default: ``False``). config_mode_command The command to enter into config mode. commit: ``False`` Commit the configuration changes before exiting the config mode. This option is by default disabled, as many platforms don't have this capability natively. CLI Example: .. code-block:: bash salt '*' napalm.netmiko_config 'set system ntp peer 1.2.3.4' commit=True salt '*' napalm.netmiko_config https://bit.ly/2sgljCB",1,0,0,1,2,1,0,0,1,2
"def sign_out(entry, time_out=None, forgot=False):
  if time_out is None:
  time_out = datetime.today().time()
  if forgot:
  entry.forgot_sign_out = True
  logger.info(
  '{} forgot to sign out on {}.'.format(entry.user_id, entry.date)
  )
  else:
  entry.time_out = time_out
  logger.info('{} ({}) signed out.'.format(entry.user_id, entry.user_type))
  return entry","Sign out of an existing entry in the timesheet. If the user forgot to sign out, flag the entry. :param entry: `models.Entry` object. The entry to sign out. :param time_out: (optional) `datetime.time` object. Specify the sign out time. :param forgot: (optional) If true, user forgot to sign out. Entry will be flagged as forgotten. :return: The signed out entry.",1,1,0,1,3,1,1,0,1,3
"def _launch_editor(starting_text=''):
  ""Launch editor, let user write text, then return that text.""
  editor = os.environ.get('EDITOR', 'vim')
  with tempfile.TemporaryDirectory() as dirname:
  filename = pathlib.Path(dirname) / 'metadata.yml'
  with filename.open(mode='wt') as handle:
  handle.write(starting_text)
  subprocess.call([editor, filename])
  with filename.open(mode='rt') as handle:
  text = handle.read()
  return text","Launch editor, let user write text, then return that text.",1,0,0,1,2,1,0,0,1,2
"def send_message(self, device_type, device_id, user_id, content):
  content = to_text(base64.b64encode(to_binary(content)))
  return self._post(
  'transmsg',
  data={
  'device_type': device_type,
  'device_id': device_id,
  'open_id': user_id,
  'content': content
  }
  )",  https://iot.weixin.qq.com/wiki/new/index.html?page=3-4-3 :param device_type: ID :param device_id: ID :param user_id: openid :param content: BASE64  :return:  JSON ,1,0,0,2,3,1,0,0,1,2
"def asHumanly(self, tzinfo=None, now=None, precision=Precision.MINUTES):
  try:
  timeFormat = Time._timeFormat[precision]
  except KeyError:
  raise InvalidPrecision(
  'Use Time.Precision.MINUTES or Time.Precision.SECONDS')
  if now is None:
  now = Time().asDatetime(tzinfo)
  else:
  now = now.asDatetime(tzinfo)
  dtime = self.asDatetime(tzinfo)
  if dtime.date() == now.date():
  if self.isAllDay():
  return 'all day'
  return dtime.strftime(timeFormat).lower()
  else:
  res = str(dtime.date().day) + dtime.strftime(' %b')
  if not dtime.date().year == now.date().year:
  res += dtime.strftime(' %Y')
  if not self.isAllDay():
  res += dtime.strftime(', %s' % (timeFormat,)).lower()
  return res","Return this time as a short string, tailored to the current time. Parts of the date that can be assumed are omitted. Consequently, the output string depends on the current time. This is the format used for displaying dates in most user visible places in the quotient web UI. By default, the current time is determined by the system clock. The current time used for formatting the time can be changed by providing a Time instance as the parameter 'now'. @param precision: The smallest unit of time that will be represented in the returned string. Valid values are L{Time.Precision.MINUTES} and L{Time.Precision.SECONDS}. @raise InvalidPrecision: if the specified precision is not either L{Time.Precision.MINUTES} or L{Time.Precision.SECONDS}.",1,0,0,1,2,1,0,0,1,2
"def post_mortem(trace_back=None, exc_info=None):
  if not ikpdb:
  return ""Error: IKP3db must be launched before calling ikpd.post_mortem().""
  if exc_info:
  trace_back = exc_info[2]
  elif trace_back and not exc_info:
  if sys.exc_info()[2] == trace_back:
  exc_info = sys.exc_info()
  else:
  return ""missing parameter trace_back or exc_info""
  pm_traceback = trace_back
  while pm_traceback.tb_next:
  pm_traceback = pm_traceback.tb_next
  ikpdb._line_tracer(pm_traceback.tb_frame, exc_info=exc_info)
  _logger.g_info(""Post mortem processing finished."")
  return None","Breaks on a traceback and send all execution information to the debugger client. If the interpreter is handling an exception at this traceback, exception information is sent to _line_tracer() which will transmit it to the debugging client. Caller can also pass an *exc_info* that will be used to extract exception information. If passed exc_info has precedence over traceback. This method is useful for integrating with systems that manage exceptions. Using it, you can setup a developer mode where unhandled exceptions are sent to the developer. Once user resumes execution, control is returned to caller. IKP3db is just used to ""pretty"" display the execution environment. To call post_mortem() use: .. code-block:: python import ikp3db ... ikp3db.postmortem(any_traceback) :param trace_back: The traceback at which to break on. :type trace_back: traceback :param exc_info: Complete description of the raised Exception as returned by sys.exc_info. :type exc_info: tuple :return: An error message or None is everything went fine. :rtype: str or None",1,0,0,1,2,1,0,0,1,2
"def find_message(current):
  current.output = {
  'results': [],
  'status': 'OK',
  'code': 201
  }
  query_set = Message(current).objects.search_on(['msg_title', 'body', 'url'],
  contains=current.input['query'])
  if current.input['channel_key']:
  query_set = query_set.filter(channel_id=current.input['channel_key'])
  else:
  subscribed_channels = Subscriber.objects.filter(user_id=current.user_id).values_list(
  ""channel_id"", flatten=True)
  query_set = query_set.filter(channel_id__in=subscribed_channels)
  query_set, pagination_data = _paginate(current_page=current.input['page'], query_set=query_set)
  current.output['pagination'] = pagination_data
  for msg in query_set:
  current.output['results'].append(msg.serialize(current.user))","Search in messages. If ""channel_key"" given, search will be limited to that channel, otherwise search will be performed on all of user's subscribed channels. .. code-block:: python # request: { 'view':'_zops_search_unit, 'channel_key': key, 'query': string, 'page': int, } # response: { 'results': [MSG_DICT, ], 'pagination': { 'page': int, # current page 'total_pages': int, 'total_objects': int, 'per_page': int, # object per page }, 'status': 'OK', 'code': 200 }",1,0,1,1,3,1,0,1,1,3
"def CheckPasswordReuse(skipUserInput):
  goodlogging.Log.Info(""EXTRACT"", ""RAR files needs password to extract"")
  if skipUserInput is False:
  prompt = ""Enter 't' to reuse the last password for just this file, "" \
  ""'a' to reuse for all subsequent files, "" \
  ""'n' to enter a new password for this file "" \
  ""or 's' to enter a new password for all files: ""
  response = goodlogging.Log.Input(""EXTRACT"", prompt)
  response = util.ValidUserResponse(response, ('t','a','n','s'))
  else:
  response = 'a'
  if response.lower() == 's':
  return -1
  if response.lower() == 'n':
  return 0
  elif response.lower() == 't':
  return 1
  elif response.lower() == 'a':
  return 2",Check with user for password reuse. Parameters ---------- skipUserInput : boolean Set to skip user input. Returns ---------- int Integer from -1 to 2 depending on user response.,1,0,0,1,2,1,0,0,1,2
"def update_access_key(self, access_key_id, status, user_name=None):
  params = {'AccessKeyId' : access_key_id,
  'Status' : status}
  if user_name:
  params['UserName'] = user_name
  return self.get_response('UpdateAccessKey', params)","Changes the status of the specified access key from Active to Inactive or vice versa. This action can be used to disable a user's key as part of a key rotation workflow. If the user_name is not specified, the user_name is determined implicitly based on the AWS Access Key ID used to sign the request. :type access_key_id: string :param access_key_id: The ID of the access key. :type status: string :param status: Either Active or Inactive. :type user_name: string :param user_name: The username of user (optional).",1,0,0,1,2,2,0,0,1,3
"def export_chat_invite_link(
  self,
  chat_id: Union[int, str]
  ) -> str:
  peer = self.resolve_peer(chat_id)
  if isinstance(peer, types.InputPeerChat):
  return self.send(
  functions.messages.ExportChatInvite(
  peer=peer.chat_id
  )
  ).link
  elif isinstance(peer, types.InputPeerChannel):
  return self.send(
  functions.channels.ExportInvite(
  channel=peer
  )
  ).link","Use this method to generate a new invite link for a chat; any previously generated link is revoked. You must be an administrator in the chat for this to work and have the appropriate admin rights. Args: chat_id (``int`` | ``str``): Unique identifier for the target chat or username of the target channel/supergroup (in the format @username). Returns: On success, the exported invite link as string is returned. Raises: :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.",1,0,0,1,2,1,0,0,1,2
"def find_nameid(self, userid, **kwargs):
  res = []
  try:
  _vals = self.db[userid]
  except KeyError:
  logger.debug(""failed to find userid %s in IdentDB"", userid)
  return res
  for val in _vals.split("" ""):
  nid = decode(val)
  if kwargs:
  for key, _val in kwargs.items():
  if getattr(nid, key, None) != _val:
  break
  else:
  res.append(nid)
  else:
  res.append(nid)
  return res",Find a set of NameID's that matches the search criteria. :param userid: User id :param kwargs: The search filter a set of attribute/value pairs :return: a list of NameID instances,0,0,1,0,1,1,0,1,1,3
"def patch_mutating_webhook_configuration(self, name, body, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.patch_mutating_webhook_configuration_with_http_info(name, body, **kwargs)
  else:
  (data) = self.patch_mutating_webhook_configuration_with_http_info(name, body, **kwargs)
  return data","partially update the specified MutatingWebhookConfiguration This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.patch_mutating_webhook_configuration(name, body, async_req=True) >>> result = thread.get() :param async_req bool :param str name: name of the MutatingWebhookConfiguration (required) :param object body: (required) :param str pretty: If 'true', then the output is pretty printed. :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint. This field is required for apply requests (application/apply-patch) but optional for non-apply patch types (JsonPatch, MergePatch, StrategicMergePatch). :param bool force: Force is going to \""force\"" Apply requests. It means user will re-acquire conflicting fields owned by other people. Force flag must be unset for non-apply patch requests. :return: V1beta1MutatingWebhookConfiguration If the method is called asynchronously, returns the request thread.",1,0,0,2,3,1,0,0,1,2
"def _swap_database(self, query):
  database = _query_db(query)
  if database == self.database:
  return query
  if self._subclassed(peewee.PostgresqlDatabase, database,
  self.database):
  can_swap = True
  elif self._subclassed(peewee.MySQLDatabase, database,
  self.database):
  can_swap = True
  else:
  can_swap = False
  if can_swap:
  query = query.clone()
  query._database = self.database
  return query
  assert False, (
  ""Error, query's database and manager's database are ""
  ""different. Query: %s Manager: %s"" % (database, self.database)
  )
  return None",Swap database for query if swappable. Return **new query** with swapped database. This is experimental feature which allows us to have multiple managers configured against different databases for single model definition. The essential limitation though is that database backend have to be **the same type** for model and manager!,0,1,1,0,2,0,1,1,1,3
"def authorize(self, request, response, environ, scopes):
  if self.site_adapter.user_has_denied_access(request) is True:
  raise OAuthInvalidError(error=""access_denied"",
  explanation=""Authorization denied by user"")
  try:
  result = self.site_adapter.authenticate(request, environ, scopes,
  self.client)
  return self.sanitize_return_value(result)
  except UserNotAuthenticated:
  return self.site_adapter.render_auth_page(request, response,
  environ, scopes,
  self.client)","Controls all steps to authorize a request by a user. :param request: The incoming :class:`oauth2.web.Request` :param response: The :class:`oauth2.web.Response` that will be returned eventually :param environ: The environment variables of this request :param scopes: The scopes requested by an application :return: A tuple containing (`dict`, user_id) or the response.",1,0,0,1,2,1,0,0,1,2
"def list_liked_topics(self, user_alias=None, start=0):
  user_alias = user_alias or self.api.user_alias
  xml = self.api.xml(API_GROUP_LIST_USER_LIKED_TOPICS % user_alias, params={'start': start})
  return build_list_result(self._parse_topic_table(xml, 'title,comment,time,group'), xml)", :param user_alias:  :param start:  :return: ,2,0,0,1,3,2,0,0,1,3
"def list_notifications(self, client=None):
  client = self._require_client(client)
  path = self.path + ""/notificationConfigs""
  iterator = page_iterator.HTTPIterator(
  client=client,
  api_request=client._connection.api_request,
  path=path,
  item_to_value=_item_to_notification,
  )
  iterator.bucket = self
  return iterator","List Pub / Sub notifications for this bucket. See: https://cloud.google.com/storage/docs/json_api/v1/notifications/list If :attr:`user_project` is set, bills the API request to that project. :type client: :class:`~google.cloud.storage.client.Client` or ``NoneType`` :param client: Optional. The client to use. If not passed, falls back to the ``client`` stored on the current bucket. :rtype: list of :class:`.BucketNotification` :returns: notification instances",1,0,0,1,2,2,0,0,1,3
"def model_builders(self, algo=None, timeoutSecs=10, **kwargs):
  params_dict = {}
  h2o_methods.check_params_update_kwargs(params_dict, kwargs, 'model_builders', False)
  request = '3/ModelBuilders.json'
  if algo:
  request += ""/"" + algo
  result = self.do_json_request(request, timeout=timeoutSecs, params=params_dict)
  h2o_sandbox.check_sandbox_for_errors()
  return result","Return a model builder or all of the model builders known to the h2o cluster. The model builders are contained in a dictionary called ""model_builders"" at the top level of the result. The dictionary maps algorithm names to parameters lists. Each of the parameters contains all the metdata required by a client to present a model building interface to the user. if parameters = True, return the parameters?",1,0,0,1,2,2,0,0,1,3
"def send_request_email(
  authorised_text, authorised_role, authorised_persons, application,
  link, is_secret):
  context = CONTEXT.copy()
  context['requester'] = application.applicant
  context['link'] = link
  context['is_secret'] = is_secret
  context['application'] = application
  context['authorised_text'] = authorised_text
  _send_request_email(
  context,
  authorised_role, authorised_persons,
  ""common_request"")",Sends an email to admin asking to approve user application,1,0,0,0,1,1,0,0,1,2
"def set_user_variable(input_dict, environment_dict):
  command_key = input_dict.keys()[0]
  while input_dict[command_key]['name'] is not 'variable_name':
  input_dict = input_dict[command_key]['children']
  variable_name = command_key = input_dict.keys()[0]
  try:
  while input_dict[command_key]['name'] is not 'variable_value':
  input_dict = input_dict[command_key]['children']
  variable_value = command_key = input_dict.keys()[0]
  except IndexError, e:
  raise seash_exceptions.UserError(""Error, expected a value to assign to variable"")
  uservariables[variable_name] = variable_value.strip()","<Purpose> Seash callback to allow user to define a custom variable and assign a value to it. <Arguments> input_dict: Input dictionary generated by seash_dictionary.parse_command(). environment_dict: Dictionary describing the current seash environment. For more information, see command_callbacks.py's module docstring. <Side Effects> A new variable will be added to the uservariables dictionary. <Exceptions> UserError: The user did not provide a value to assign to the variable <Return> None",0,0,0,1,1,1,0,0,1,2
"def auth_none(self, username):
  if (not self.active) or (not self.initial_kex_done):
  raise SSHException('No existing session')
  my_event = threading.Event()
  self.auth_handler = AuthHandler(self)
  self.auth_handler.auth_none(username, my_event)
  return self.auth_handler.wait_for_response(my_event)","Try to authenticate to the server using no authentication at all. This will almost always fail. It may be useful for determining the list of authentication types supported by the server, by catching the L{BadAuthenticationType} exception raised. @param username: the username to authenticate as @type username: string @return: list of auth types permissible for the next stage of authentication (normally empty) @rtype: list @raise BadAuthenticationType: if ""none"" authentication isn't allowed by the server for this user @raise SSHException: if the authentication failed due to a network error @since: 1.5",1,0,0,1,2,1,0,0,1,2
"def get_link_page_text(link_page):
  text = ''
  for i, link in enumerate(link_page):
  capped_link_text = (link['text'] if len(link['text']) <= 20
  else link['text'][:19] + '')
  text += '[{}] [{}]({})\n'.format(i, capped_link_text, link['href'])
  return text",Construct the dialog box to display a list of links to the user.,0,0,0,1,1,1,0,0,1,2
"def _crossmatch_transients_against_catalogues(
  self,
  transientsMetadataListIndex,
  colMaps):
  global theseBatches
  self.log.debug(
  'starting the ``_crossmatch_transients_against_catalogues`` method')
  transientsMetadataList = theseBatches[transientsMetadataListIndex]
  dbConn = database(
  log=self.log,
  dbSettings=self.settings[""database settings""][""static catalogues""]
  ).connect()
  self.allClassifications = []
  cm = transient_catalogue_crossmatch(
  log=self.log,
  dbConn=dbConn,
  transients=transientsMetadataList,
  settings=self.settings,
  colMaps=colMaps
  )
  crossmatches = cm.match()
  self.log.debug(
  'completed the ``_crossmatch_transients_against_catalogues`` method')
  return crossmatches",run the transients through the crossmatch algorithm in the settings file **Key Arguments:** - ``transientsMetadataListIndex`` -- the list of transient metadata lifted from the database. - ``colMaps`` -- dictionary of dictionaries with the name of the database-view (e.g. `tcs_view_agn_milliquas_v4_5`) as the key and the column-name dictary map as value (`{view_name: {columnMap}}`). **Return:** - ``crossmatches`` -- a list of dictionaries of the associated sources crossmatched from the catalogues database .. todo :: - update key arguments values and definitions with defaults - update return values and definitions - update usage examples and text - update docstring text - check sublime snippet exists - clip any useful text to docs mindmap - regenerate the docs and check redendering of this docstring,0,0,1,1,2,1,0,1,1,3
"def forward_message(self, chat_id, from_chat_id, message_id, disable_notification=False):
  assert_type_or_raise(chat_id, (int, unicode_type), parameter_name=""chat_id"")
  assert_type_or_raise(from_chat_id, (int, unicode_type), parameter_name=""from_chat_id"")
  assert_type_or_raise(message_id, int, parameter_name=""message_id"")
  assert_type_or_raise(disable_notification, None, bool, parameter_name=""disable_notification"")
  result = self.do(
  ""forwardMessage"", chat_id=chat_id, from_chat_id=from_chat_id, message_id=message_id,
  disable_notification=disable_notification
  )
  if self.return_python_objects:
  logger.debug(""Trying to parse {data}"".format(data=repr(result)))
  from pytgbot.api_types.receivable.updates import Message
  try:
  return Message.from_array(result)
  except TgApiParseException:
  logger.debug(""Failed parsing as api_type Message"", exc_info=True)
  raise TgApiParseException(""Could not parse result."")
  return result","Use this method to forward messages of any kind. On success, the sent Message is returned. https://core.telegram.org/bots/api#forwardmessage Parameters: :param chat_id: Unique identifier for the target chat (chat id of user chat or group chat) or username of the target channel (in the format @channelusername) :type chat_id: int | str|unicode :param from_chat_id: Unique identifier for the chat where the original message was sent (id for chats or the channel's username in the format @channelusername) :type from_chat_id: int | str|unicode :param message_id: Message identifier in the chat specified in from_chat_id :type message_id: int Optional keyword parameters: :param disable_notification: Sends the message silently. Users will receive a notification with no sound. :type disable_notification: bool Returns: :return: On success, the sent Message is returned :rtype: pytgbot.api_types.receivable.updates.Message",1,0,0,1,2,1,0,0,1,2
"def generate_user_agent(os=None, navigator=None, platform=None,
  device_type=None):
  return generate_navigator(os=os, navigator=navigator,
  platform=platform,
  device_type=device_type)['user_agent']","Generates HTTP User-Agent header :param os: limit list of os for generation :type os: string or list/tuple or None :param navigator: limit list of browser engines for generation :type navigator: string or list/tuple or None :param device_type: limit possible oses by device type :type device_type: list/tuple or None, possible values: ""desktop"", ""smartphone"", ""tablet"", ""all"" :return: User-Agent string :rtype: string :raises InvalidOption: if could not generate user-agent for any combination of allowed oses and navigators :raise InvalidOption: if any of passed options is invalid",1,0,0,1,2,1,0,0,1,2
"def list(cls, **kwargs):
  conn = Qubole.agent()
  params = {}
  for k in kwargs:
  if kwargs[k]:
  params[k] = kwargs[k]
  params = None if not params else params
  return conn.get(cls.rest_entity_path, params=params)","List a command by issuing a GET request to the /command endpoint Args: `**kwargs`: Various parameters can be used to filter the commands such as: * command_type - HiveQuery, PrestoQuery, etc. The types should be in title case. * status - failed, success, etc * name * command_id * qbol_user_id * command_source * page * cluster_label * session_id, etc For example - Command.list(command_type = ""HiveQuery"", status = ""success"")",1,0,0,1,2,1,0,0,1,2
"def create_user(self, user_name, initial_password):
  res = self._make_ocs_request(
  'POST',
  self.OCS_SERVICE_CLOUD,
  'users',
  data={'password': initial_password, 'userid': user_name}
  )
  if res.status_code == 200:
  tree = ET.fromstring(res.content)
  self._check_ocs_status(tree, [100])
  return True
  raise HTTPResponseError(res)","Create a new user with an initial password via provisioning API. It is not an error, if the user already existed before. If you get back an error 999, then the provisioning API is not enabled. :param user_name: name of user to be created :param initial_password: password for user being created :returns: True on success :raises: HTTPResponseError in case an HTTP error status was returned",1,1,0,2,4,2,0,0,1,3
"def finish_oauth(self, code):
  r = requests.post(self._login_uri(""/oauth/token""), data={
  ""code"": code,
  ""client_id"": self.client_id,
  ""client_secret"": self.client_secret
  })
  if r.status_code != 200:
  raise ApiError(""OAuth token exchange failed"", status=r.status_code, json=r.json())
  token = r.json()[""access_token""]
  scopes = OAuthScopes.parse(r.json()[""scopes""])
  expiry = datetime.now() + timedelta(seconds=r.json()['expires_in'])
  refresh_token = r.json()['refresh_token']
  return token, scopes, expiry, refresh_token","Given an OAuth Exchange Code, completes the OAuth exchange with the authentication server. This should be called once the user has already been directed to the login_uri, and has been sent back after successfully authenticating. For example, in `Flask`_, this might be implemented as a route like this:: @app.route(""/oauth-redirect"") def oauth_redirect(): exchange_code = request.args.get(""code"") login_client = LinodeLoginClient(client_id, client_secret) token, scopes = login_client.finish_oauth(exchange_code) # store the user's OAuth token in their session for later use # and mark that they are logged in. return redirect(""/"") .. _Flask: http://flask.pocoo.org :param code: The OAuth Exchange Code returned from the authentication server in the query string. :type code: str :returns: The new OAuth token, and a list of scopes the token has, when the token expires, and a refresh token that can generate a new valid token when this one is expired. :rtype: tuple(str, list) :raise ApiError: If the OAuth exchange fails.",2,0,0,1,3,2,0,0,2,4
"def open_external_editor(filename=None, sql=None):
  message = None
  filename = filename.strip().split(' ', 1)[0] if filename else None
  sql = sql or ''
  MARKER = '
  query = click.edit(u'{sql}\n\n{marker}'.format(sql=sql, marker=MARKER),
  filename=filename, extension='.sql')
  if filename:
  try:
  with open(filename, encoding='utf-8') as f:
  query = f.read()
  except IOError:
  message = 'Error reading file: %s.' % filename
  if query is not None:
  query = query.split(MARKER, 1)[0].rstrip('\n')
  else:
  query = sql
  return (query, message)","Open external editor, wait for the user to type in their query, return the query. :return: list with one tuple, query as first element.",1,0,0,1,2,1,0,0,1,2
"def login(username=None, password=None):
  global user
  user = UserConfig()
  if username:
  user.username = username
  else:
  user.username = user.input_username()
  if password:
  user.password = password
  else:
  user.password = user.input_password()
  if (not ( user.username and user.password) ):
  logging.error(""Username and password must be provided for login"")
  return;
  user.retrieve_keycloak_token()
  user.apiclient = user.create_api_client()
  save()",Log in to PNC using the supplied username and password. The keycloak token will be saved for all subsequent pnc-cli operations until login is called again :return:,2,0,0,0,2,1,0,0,1,2
"def iter_pulls(self, state=None, head=None, base=None, sort='created',
  direction='desc', number=-1, etag=None):
  url = self._build_url('pulls', base_url=self._api)
  params = {}
  if state and state.lower() in ('all', 'open', 'closed'):
  params['state'] = state.lower()
  params.update(head=head, base=base, sort=sort, direction=direction)
  self._remove_none(params)
  return self._iter(int(number), url, PullRequest, params, etag)","List pull requests on repository. .. versionchanged:: 0.9.0 - The ``state`` parameter now accepts 'all' in addition to 'open' and 'closed'. - The ``sort`` parameter was added. - The ``direction`` parameter was added. :param str state: (optional), accepted values: ('all', 'open', 'closed') :param str head: (optional), filters pulls by head user and branch name in the format ``user:ref-name``, e.g., ``seveas:debian`` :param str base: (optional), filter pulls by base branch name. Example: ``develop``. :param str sort: (optional), Sort pull requests by ``created``, ``updated``, ``popularity``, ``long-running``. Default: 'created' :param str direction: (optional), Choose the direction to list pull requests. Accepted values: ('desc', 'asc'). Default: 'desc' :param int number: (optional), number of pulls to return. Default: -1 returns all available pull requests :param str etag: (optional), ETag from a previous request to the same endpoint :returns: generator of :class:`PullRequest <github3.pulls.PullRequest>`\ s",2,0,0,1,3,2,0,0,1,3
"def activation_key_expired(self):
  import utils
  expiration_date = datetime.timedelta(
  days=utils.get_settings('REGISTRATION_API_ACCOUNT_ACTIVATION_DAYS'))
  return self.activation_key == self.ACTIVATED or \
  (self.user.date_joined + expiration_date <= datetime_now())","Determine whether this ``RegistrationProfile``'s activation key has expired, returning a boolean -- ``True`` if the key has expired. Key expiration is determined by a two-step process: 1. If the user has already activated, the key will have been reset to the string constant ``ACTIVATED``. Re-activating is not permitted, and so this method returns ``True`` in this case. 2. Otherwise, the date the user signed up is incremented by the number of days specified in the setting ``REGISTRATION_API_ACCOUNT_ACTIVATION_DAYS`` (which should be the number of days after signup during which a user is allowed to activate their account); if the result is less than or equal to the current date, the key has expired and this method returns ``True``.",1,0,0,1,2,1,0,0,1,2
"def get_var(self, key):
  'Retrieve one saved variable from the database.'
  vt = quote(self.__vars_table)
  data = self.execute(u'SELECT * FROM %s WHERE `key` = ?' % vt, [key], commit = False)
  if data == []:
  raise NameError(u'The DumpTruck variables table doesn\'t have a value for %s.' % key)
  else:
  tmp = quote(self.__vars_table_tmp)
  row = data[0]
  self.execute(u'DROP TABLE IF EXISTS %s' % tmp, commit = False)
  self.execute(u'CREATE TEMPORARY TABLE %s (`value` %s)' % (tmp, row['type']), commit = False)
  self.execute(u'INSERT INTO %s (`value`) VALUES (?)' % tmp, [row['value']], commit = False)
  value = self.dump(tmp)[0]['value']
  self.execute(u'DROP TABLE %s' % tmp, commit = False)
  return value",Retrieve one saved variable from the database.,0,1,1,0,2,1,1,1,1,4
"async def create(source_id: str, name: str, requested_attrs: list, revocation_interval: dict, requested_predicates: list = []):
  constructor_params = (source_id,)
  c_source_id = c_char_p(source_id.encode('utf-8'))
  c_name = c_char_p(name.encode('utf-8'))
  c_req_predicates = c_char_p(json.dumps(requested_predicates).encode('utf-8'))
  c_req_attrs = c_char_p(json.dumps(requested_attrs).encode('utf-8'))
  c_revocation_interval = c_char_p(json.dumps(revocation_interval).encode('utf-8'))
  c_params = (c_source_id, c_req_attrs, c_req_predicates, c_revocation_interval, c_name)
  return await Proof._create(""vcx_proof_create"",
  constructor_params,
  c_params)","Builds a generic proof object :param source_id: Tag associated by user of sdk :param name: Name of the Proof :param requested_attrs: Attributes associated with the Proof :param revocation_interval: interval applied to all requested attributes indicating when the claim must be valid (NOT revoked) Example: name = ""proof name"" requested_attrs = [{""name"": ""age"", ""restrictions"": [{""schema_id"": ""6XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11"", ""schema_name"":""Faber Student Info"", ""schema_version"":""1.0"", ""schema_issuer_did"":""6XFh8yBzrpJQmNyZzgoTqB"", ""issuer_did"":""8XFh8yBzrpJQmNyZzgoTqB"", ""cred_def_id"": ""8XFh8yBzrpJQmNyZzgoTqB:3:CL:1766"" }, { ""schema_id"": ""5XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11"", ""schema_name"":""BYU Student Info"", ""schema_version"":""1.0"", ""schema_issuer_did"":""5XFh8yBzrpJQmNyZzgoTqB"", ""issuer_did"":""66Fh8yBzrpJQmNyZzgoTqB"", ""cred_def_id"": ""66Fh8yBzrpJQmNyZzgoTqB:3:CL:1766"" } ] }, { ""name"":""name"", ""restrictions"": [ { ""schema_id"": ""6XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11"", ""schema_name"":""Faber Student Info"", ""schema_version"":""1.0"", ""schema_issuer_did"":""6XFh8yBzrpJQmNyZzgoTqB"", ""issuer_did"":""8XFh8yBzrpJQmNyZzgoTqB"", ""cred_def_id"": ""8XFh8yBzrpJQmNyZzgoTqB:3:CL:1766"" }, { ""schema_id"": ""5XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11"", ""schema_name"":""BYU Student Info"", ""schema_version"":""1.0"", ""schema_issuer_did"":""5XFh8yBzrpJQmNyZzgoTqB"", ""issuer_did"":""66Fh8yBzrpJQmNyZzgoTqB"", ""cred_def_id"": ""66Fh8yBzrpJQmNyZzgoTqB:3:CL:1766""}]}] revocation_interval = {""from"": 1, ""to"": 2} // Both values are optional proof = await Proof.create(source_id, name, requested_attrs) :return: Proof Object",1,0,0,1,2,1,0,0,1,2
"def factory(cls, config, db):
  if not hasattr(db, 'register_script'):
  LOG.debug(""Redis client does not support register_script()"")
  return GetBucketKeyByLock(config, db)
  info = db.info()
  if version_greater('2.6', info['redis_version']):
  LOG.debug(""Redis server supports register_script()"")
  return GetBucketKeyByScript(config, db)
  LOG.debug(""Redis server does not support register_script()"")
  return GetBucketKeyByLock(config, db)","Given a configuration and database, select and return an appropriate instance of a subclass of GetBucketKey. This will ensure that both client and server support are available for the Lua script feature of Redis, and if not, a lock will be used. :param config: A dictionary of compactor options. :param db: A database handle for the Redis database. :returns: An instance of a subclass of GetBucketKey, dependent on the support for the Lua script feature of Redis.",1,0,0,0,1,0,0,1,1,2
"def process_user_info_response(self, response):
  mapping = (
  ('username', 'preferred_username'),
  ('email', 'email'),
  ('last_name', 'family_name'),
  ('first_name', 'given_name'),
  )
  return {dest: response[source] for dest, source in mapping}","Process the user info response data. By default, this simply maps the edX user info key-values (example below) to Django-friendly names. If your provider returns different fields, you should sub-class this class and override this method. .. code-block:: python { ""username"": ""jdoe"", ""email"": ""jdoe@example.com"", ""first_name"": ""Jane"", ""last_name"": ""Doe"" } Arguments: response (dict): User info data Returns: dict",1,0,0,1,2,1,0,0,1,2
"def get_auth_uri(self, state, scopes=None, implicit=False):
  if state is None or state == '':
  raise AttributeError('""state"" must be non empty, non None string')
  scopes_list = [] if not scopes else scopes
  response_type = 'code' if not implicit else 'token'
  auth_uri = '%s?response_type=%s&redirect_uri=%s&client_id=%s%s%s' % (
  self.oauth_authorize,
  response_type,
  quote(self.redirect_uri, safe=''),
  self.client_id,
  '&scope=%s' % '+'.join(scopes_list) if scopes else '',
  '&state=%s' % state
  )
  if self.secret_key is None and not implicit:
  auth_uri += '&code_challenge_method=S256&code_challenge=%s' % (
  generate_code_challenge(self.code_verifier)
  )
  return auth_uri",Constructs the full auth uri and returns it.. Parameters ---------- state : string The state to pass through the auth process scopes : list (optional) The list of scope to have implicit : Boolean (optional) Activate or not the implicit flow Returns ------- String The generated URL for the user to log into EVE SSO,1,0,0,1,2,1,0,0,1,2
"def request_ligodotorg(url, debug=False):
  debug = int(debug)
  httpsHandler = urllib2.HTTPSHandler(debuglevel = debug)
  jar = cookielib.LWPCookieJar()
  if os.path.exists(COOKIE_JAR):
  os.chmod(COOKIE_JAR, stat.S_IRUSR | stat.S_IWUSR)
  jar.load(COOKIE_JAR, ignore_discard = True)
  cookie_handler = urllib2.HTTPCookieProcessor(jar)
  redirectHandler = urllib2.HTTPRedirectHandler()
  auth_handler = HTTPNegotiateAuthHandler(service_principal='HTTP@%s'
  % (LIGO_LOGIN_URL))
  opener = urllib2.build_opener(auth_handler, cookie_handler, httpsHandler,
  redirectHandler)
  request = urllib2.Request(url)
  response = opener.open(request)
  jar.save(COOKIE_JAR, ignore_discard=True)
  return response","Request the given URL using LIGO.ORG SAML authentication. This requires an active Kerberos ticket for the user, to get one: $ kinit albert.einstein@LIGO.ORG Parameters ---------- url : `str` URL path for request debug : `bool`, optional Query in verbose debuggin mode, default `False` Returns ------- urllib.addinfourl file object containing output data, use .read() to extract text content",2,0,0,1,3,1,0,0,1,2
"def run_as_wait(user, domain, password, filename, logon_flag=1, work_dir="""",
  show_flag=Properties.SW_SHOWNORMAL):
  ret = AUTO_IT.AU3_RunAsWait(
  LPCWSTR(user), LPCWSTR(domain), LPCWSTR(password), INT(logon_flag),
  LPCWSTR(filename), LPCWSTR(work_dir), INT(show_flag)
  )
  return ret","Runs an external program. :param user: username The user name to use. :param domain: The domain name to use. :param password: The password to use. :param logon_flag: 0 = do not load the user profile, 1 = (default) load the user profile, 2 = use for net credentials only :param filename: The name of the executable (EXE, BAT, COM, or PIF) to run. :param work_dir: The working directory. :param show_flag: The ""show"" flag of the executed program: SW_HIDE = Hidden window SW_MINIMIZE = Minimized window SW_MAXIMIZE = Maximized window :return:",1,0,0,0,1,1,0,0,1,2
"def get_value(self, section, option,
  default=None):
  ret = default
  try:
  ret = self.parser.get(section, option)
  except NoOptionError:
  pass
  if ret is not None:
  try:
  match = self.re_pattern.findall(ret)
  for m in match:
  ret = ret.replace(m, system_exec(m[1:-1]))
  except TypeError:
  pass
  return ret","Get the value of an option, if it exists. If it did not exist, then return the default value. It allows user to define dynamic configuration key (see issue#1204) Dynamic vlaue should starts and end with the ` char Example: prefix=`hostname`",0,0,0,1,1,1,0,0,1,2
"def authenticatedUserForKey(self, key):
  session = self.store.findFirst(
  PersistentSession, PersistentSession.sessionKey == key)
  if session is None:
  return None
  else:
  session.renew()
  return session.authenticatedAs","Find a persistent session for a user. @type key: L{bytes} @param key: The persistent session identifier. @rtype: L{bytes} or C{None} @return: The avatar ID the session belongs to, or C{None} if no such session exists.",0,0,1,0,1,1,0,1,1,3
"def admin_cmd(argv=sys.argv[1:]):
  arguments = docopt(admin_cmd.__doc__, argv=argv)
  initialize_config(__mode__='fit')
  if arguments['activate']:
  activate(model_version=int(arguments['<version>']))
  elif arguments['delete']:
  delete(model_version=int(arguments['<version>']))",\ Activate or delete models. Models are usually made active right after fitting (see command pld-fit). The 'activate' command allows you to explicitly set the currently active model. Use 'pld-list' to get an overview of all available models along with their version identifiers. Deleting a model will simply remove it from the database. Usage: pld-admin activate <version> [options] pld-admin delete <version> [options] Options: -h --help Show this screen.,1,1,0,0,2,1,0,0,1,2
"def double_prompt_for_plaintext_password():
  password = 1
  password_repeat = 2
  while password != password_repeat:
  password = getpass.getpass('Enter password: ')
  password_repeat = getpass.getpass('Repeat password: ')
  if password != password_repeat:
  sys.stderr.write('Passwords do not match, try again.\n')
  return password",Get the desired password from the user through a double prompt.,1,0,0,0,1,1,0,0,1,2
"def return_single_convert_numpy_base(dbpath, folder_path, set_object, object_id, converter, add_args=None):
  engine = create_engine('sqlite:////' + dbpath)
  session_cl = sessionmaker(bind=engine)
  session = session_cl()
  tmp_object = session.query(set_object).get(object_id)
  session.close()
  if add_args is None:
  return converter(join(folder_path, tmp_object.path))
  else:
  return converter(join(folder_path, tmp_object.path), add_args)","Generic function which converts an object specified by the object_id into a numpy array and returns the array, the conversion is done by the 'converter' function Parameters ---------- dbpath : string, path to SQLite database file folder_path : string, path to folder where the files are stored set_object : object (either TestSet or TrainSet) which is stored in the database object_id : int, id of object in database converter : function, which takes the path of a data point and *args as parameters and returns a numpy array add_args : optional arguments for the converter (list/dictionary/tuple/whatever). if None, the converter should take only one input argument - the file path. default value: None Returns ------- result : ndarray",0,0,1,0,1,1,1,1,1,4
"async def read(self, *_id):
 if not _id:
 return {""error"":400,
 ""reason"":""Missed required fields""}
 result = []
 for i in _id:
 document = await self.collection.find_one({""id"":i})
 try:
 result.append({i:document[i] for i in document
 if i != ""_id""})
 except:
 continue
 return result","Read data from database table. Accepts ids of entries. Returns list of results if success or string with error code and explanation. read(*id) => [(result), (result)] (if success) read(*id) => [] (if missed) read() => {""error"":400, ""reason"":""Missed required fields""}",1,0,1,1,3,1,0,1,1,3
"def delete(self, request, id):
  if self.readonly:
  return HTTPMethodNotAllowed(headers={'Allow': 'GET, HEAD'})
  session = self.Session()
  obj = session.query(self.mapped_class).get(id)
  if obj is None:
  return HTTPNotFound()
  if self.before_delete is not None:
  self.before_delete(request, obj)
  session.delete(obj)
  return Response(status_int=204)",Remove the targeted feature from the database,1,1,1,1,4,1,0,1,1,3
"def render_pdf(html, stylesheets=None,
  download_filename=None, automatic_download=True):
  if not hasattr(html, 'write_pdf'):
  html = HTML(html)
  pdf = html.write_pdf(stylesheets=stylesheets)
  response = current_app.response_class(pdf, mimetype='application/pdf')
  if download_filename:
  if automatic_download:
  value = 'attachment'
  else:
  value = 'inline'
  response.headers.add('Content-Disposition', value,
  filename=download_filename)
  return response","Render a PDF to a response with the correct ``Content-Type`` header. :param html: Either a :class:`weasyprint.HTML` object or an URL to be passed to :func:`flask_weasyprint.HTML`. The latter case requires a request context. :param stylesheets: A list of user stylesheets, passed to :meth:`~weasyprint.HTML.write_pdf` :param download_filename: If provided, the ``Content-Disposition`` header is set so that most web browser will show the ""Save as"" dialog with the value as the default filename. :param automatic_download: If True, the browser will automatic download file. :returns: a :class:`flask.Response` object.",1,0,0,1,2,1,0,0,1,2
"def iter_starred(self, login=None, sort=None, direction=None, number=-1,
  etag=None):
  if login:
  return self.user(login).iter_starred(sort, direction)
  params = {'sort': sort, 'direction': direction}
  self._remove_none(params)
  url = self._build_url('user', 'starred')
  return self._iter(int(number), url, Repository, params, etag)","Iterate over repositories starred by ``login`` or the authenticated user. .. versionchanged:: 0.5 Added sort and direction parameters (optional) as per the change in GitHub's API. :param str login: (optional), name of user whose stars you want to see :param str sort: (optional), either 'created' (when the star was created) or 'updated' (when the repository was last pushed to) :param str direction: (optional), either 'asc' or 'desc'. Default: 'desc' :param int number: (optional), number of repositories to return. Default: -1 returns all repositories :param str etag: (optional), ETag from a previous request to the same endpoint :returns: generator of :class:`Repository <github3.repos.Repository>`",2,0,0,1,3,2,0,0,1,3
"def run(self):
  response = None
  root = tkinter.Tk()
  root.withdraw()
  while response is not True:
  response = tkinter.messagebox.askokcancel(title=self.title, message=self.pre_message)
  if self.post_message:
  print(self.post_message)
  self.exit_time = time.time()",pop up a dialog box and return when the user has closed it,1,0,0,1,2,1,0,0,1,2
"def _define_range(self, sequences):
  sequence_count = 0
  total_sequence = 0
  for record in SeqIO.parse(open(sequences), 'fasta'):
  total_sequence+=1
  sequence_count+=len(record.seq)
  max_range = (sequence_count/total_sequence)*1.5
  return max_range","define_range - define the maximum range within which two hits in a db search can be linked. This is defined as 1.5X the average length of all reads in the database. Parameters ---------- sequences : str A path to the sequences in FASTA format. This fasta file is assumed to be in the correct format. i.e. headers start with '>' Returns ------- max_range : int As described above, 1.5X the size of the average length of genes within the database",1,0,0,0,1,0,0,0,1,1
"def get_user(self, username):
  User = get_user_model()
  try:
  user = User.objects.get(**{
  User.USERNAME_FIELD: username,
  })
  if user.is_active:
  raise ActivationError(
  self.ALREADY_ACTIVATED_MESSAGE,
  code='already_activated'
  )
  return user
  except User.DoesNotExist:
  raise ActivationError(
  self.BAD_USERNAME_MESSAGE,
  code='bad_username'
  )","Given the verified username, look up and return the corresponding user account if it exists, or raising ``ActivationError`` if it doesn't.",2,0,1,1,4,1,0,1,1,3
"def normalize_message(message_text):
  for mark in PUNCTUATION_MARKS:
  message_text = message_text.replace(mark, ' ')
  message_text = ' ' + message_text + ' '
  for article in ARTICLES:
  message_text = message_text.replace(article, ' ')
  message_text = ' '.join(message_text.split())
  return message_text","Remove punctuation marks, articles and bot appeals (like 'hey bot') :param message_text: user's message text :return: normalized message text",1,0,0,1,2,1,0,0,1,2
"def manage_profile_requests_view(request):
  page_name = ""Admin - Manage Profile Requests""
  profile_requests = ProfileRequest.objects.all()
  return render_to_response('manage_profile_requests.html', {
  'page_name': page_name,
  'choices': UserProfile.STATUS_CHOICES,
  'profile_requests': profile_requests
  }, context_instance=RequestContext(request))",The page to manage user profile requests.,1,0,1,1,3,1,0,1,1,3
"def _cli_main(args=None):
  arguments = _parse_arguments(args)
  _remove_none_values(arguments)
  verbosity = min(arguments.pop('verbose'), 4)
  levels = [logging.ERROR,
  logging.WARNING,
  logging.INFO,
  logging.DEBUG,
  TRACE_LEVEL]
  arguments.setdefault('debug_level', levels[verbosity])
  with open_tunnel(**arguments) as tunnel:
  if tunnel.is_alive:
  input_()","Pass input arguments to open_tunnel Mandatory: ssh_address, -R (remote bind address list) Optional: -U (username) we may gather it from SSH_CONFIG_FILE or current username -p (server_port), defaults to 22 -P (password) -L (local_bind_address), default to 0.0.0.0:22 -k (ssh_host_key) -K (private_key_file), may be gathered from SSH_CONFIG_FILE -S (private_key_password) -t (threaded), allow concurrent connections over tunnels -v (verbose), up to 3 (-vvv) to raise loglevel from ERROR to DEBUG -V (version) -x (proxy), ProxyCommand's IP:PORT, may be gathered from config file -c (ssh_config), ssh configuration file (defaults to SSH_CONFIG_FILE) -z (compress) -n (noagent), disable looking for keys from an Agent -d (host_pkey_directories), look for keys on these folders",1,0,0,1,2,1,0,0,1,2
"def fetch(engine, tablename, columns=None, selection=None, **kwargs):
  import pandas as pd
  if columns is None:
  columnstr = '*'
  else:
  columnstr = ', '.join('""%s""' % c for c in columns)
  selectionstr = format_db_selection(selection, engine=engine)
  qstr = 'SELECT %s FROM %s %s' % (columnstr, tablename, selectionstr)
  tab = pd.read_sql(qstr, engine, **kwargs)
  types = tab.apply(lambda x: pd.api.types.infer_dtype(x.values))
  if not tab.empty:
  for col in types[types == 'unicode'].index:
  tab[col] = tab[col].astype(str)
  return Table.from_pandas(tab).filled()","Fetch data from an SQL table into an `EventTable` Parameters ---------- engine : `sqlalchemy.engine.Engine` the database engine to use when connecting table : `str`, The name of table you are attempting to receive triggers from. selection other filters you would like to supply underlying reader method for the given format .. note:: For now it will attempt to automatically connect you to a specific DB. In the future, this may be an input argument. Returns ------- table : `GravitySpyTable`",1,0,1,1,3,1,0,1,1,3
"def get_shape_points2(cur, shape_id):
  cur.execute(, (shape_id,))
  shape_points = {'seqs': [], 'lats': [], 'lons': [], 'd': []}
  for row in cur:
  shape_points['seqs'].append(row[0])
  shape_points['lats'].append(row[1])
  shape_points['lons'].append(row[2])
  shape_points['d'].append(row[3])
  return shape_points","Given a shape_id, return its shape-sequence (as a dict of lists). get_shape_points function returns them as a list of dicts Parameters ---------- cur: sqlite3.Cursor cursor to a GTFS database shape_id: str id of the route Returns ------- shape_points: dict of lists dict contains keys 'seq', 'lat', 'lon', and 'd'(istance) of the shape",1,0,1,1,3,1,0,0,1,2
"def save_task(self, patient_id,
  task_type,
  target_user,
  work_object_id,
  comments,
  subject):
  magic = self._magic_json(
  action=TouchWorksMagicConstants.ACTION_SAVE_TASK,
  patient_id=patient_id,
  parameter1=task_type,
  parameter2=target_user,
  parameter3=work_object_id,
  parameter4=comments,
  parameter5=subject)
  response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)
  result = self._get_results_or_raise_if_magic_invalid(
  magic,
  response,
  TouchWorksMagicConstants.RESULT_GET_ENCOUNTER_LIST_FOR_PATIENT)
  return result","invokes TouchWorksMagicConstants.ACTION_SAVE_TASK action :param patient_id :param task_type - EntryMnemonic value from IDX_TASK_ACTION_DE. Dictionary values can be looked up using the GetDictionary action. :param target_user - TargetUser Pass in the username of the individual who will be assigned the task. Typical delegates can be found by calling GetDelegates. It is also possible to assign a task to a team by passing in 'Team'+the ID of the corresponding team from the Team_DE dictionary. The team can be looked up using the GetDictionary action. If the LoginUser is the same as the TargetUser, the task will be marked as delegated (and therefore no longer available in GetTask for that LoginUser). :param work_object_id - The ID of the item to link to the task, such as the medication or note ID. If not needed, 0 can be passed instead. :param comments - A comment to set for the task. :return: JSON response",1,0,0,1,2,1,0,0,1,2
"def mfe_multi(self, strands, permutation=None, degenerate=False, temp=37.0,
  pseudo=False, material=None, dangles='some', sodium=1.0,
  magnesium=0.0):
  material = self._set_material(strands, material, multi=True)
  cmd_args = self._prep_cmd_args(temp, dangles, material, pseudo, sodium,
  magnesium, multi=True)
  if degenerate:
  cmd_args.append('-degenerate')
  if permutation is None:
  permutation = range(1, len(strands) + 1)
  lines = self._multi_lines(strands, permutation)
  self._run('mfe', cmd_args, lines)
  structures = self._process_mfe(self._read_tempfile('mfe.mfe'))
  if degenerate:
  return structures
  else:
  return structures[0]","Compute the MFE for an ordered complex of strands. Runs the \'mfe\' command. :param strands: Strands on which to run mfe. Strands must be either coral.DNA or coral.RNA). :type strands: list :param permutation: The circular permutation of strands to test in complex. e.g. to test in the order that was input for 4 strands, the permutation would be [1,2,3,4]. If set to None, defaults to the order of the input strands. :type permutation: list :param temp: Temperature setting for the computation. Negative values are not allowed. :type temp: float :param pseudo: Enable pseudoknots. :type pseudo: bool :param material: The material setting to use in the computation. If set to None (the default), the material type is inferred from the strands. Other settings available: 'dna' for DNA parameters, 'rna' for RNA (1995) parameters, and 'rna1999' for the RNA 1999 parameters. :type material: str :param dangles: How to treat dangles in the computation. From the user guide: For \'none\': Dangle energies are ignored. For \'some\': \'A dangle energy is incorporated for each unpaired base flanking a duplex\'. For 'all': all dangle energy is considered. :type dangles: str :param sodium: Sodium concentration in solution (molar), only applies to DNA. :type sodium: float :param magnesium: Magnesium concentration in solution (molar), only applies to DNA> :type magnesium: float :param degenerate: Setting to True will result in returning a list of dictionaries associated with structures having the same, minimal MFE value. :type degenerate: bool :returns: A dictionary with keys for 'mfe' (a float), 'dotparens' (dot-parens notation of the MFE structure), and 'pairlist' (a pair list notation of the MFE structure). Note that the pair list will be an empty list if the MFE is unstructured. :rtype: dict",1,0,0,1,2,1,0,0,1,2
"def update_peer(self,
  current_name,
  new_name, new_url, username, password, peer_type=""REPLICATION""):
  if self._get_resource_root().version < 11:
  peer_type = None
  peer = ApiCmPeer(self._get_resource_root(),
  name=new_name,
  url=new_url,
  username=username,
  password=password,
  type=peer_type)
  return self._put(""peers/"" + current_name, ApiCmPeer, data=peer, api_version=3)",Update a replication peer. @param current_name: The name of the peer to updated. @param new_name: The new name for the peer. @param new_url: The new url for the peer. @param username: The admin username to use to setup the remote side of the peer connection. @param password: The password of the admin user. @param peer_type: Added in v11. The type of the peer. Defaults to 'REPLICATION'. @return: The updated peer. @since: API v3,1,0,0,2,3,1,0,0,1,2
"def __exportUsers(self, sort, limit=0):
  position = 1
  dataUsers = self.getSortedUsers(sort)
  if limit:
  dataUsers = dataUsers[:limit]
  exportedUsers = []
  for u in dataUsers:
  userExported = u.export()
  userExported[""position""] = position
  exportedUsers.append(userExported)
  if position < len(dataUsers):
  userExported[""comma""] = True
  position += 1
  return exportedUsers",Export the users to a dictionary. :param sort: field to sort the users :type sort: str. :return: exported users. :rtype: dict.,1,0,1,0,2,1,0,0,1,2
"def create(self, display_name, content=None):
  activity = {
  ""verb"": ""create"",
  ""object"": {
  ""objectType"": ""collection"",
  ""objectTypes"": [self.membertype],
  ""displayName"": display_name,
  ""content"": content
  }
  }
  if self._post_activity(activity, unserialize=False):
  return self[display_name]","Create a new user list :class:`collection <pypump.models.collection.Collection>`. :param display_name: List title. :param content: (optional) List description. Example: >>> pump.me.lists.create(display_name='Friends', content='List of friends') >>> myfriends = pump.me.lists['Friends'] >>> print(myfriends) Friends",2,0,0,1,3,1,0,0,1,2
"def function(self, text=None, entry_name=None, limit=None, as_df=False):
  q = self.session.query(models.Function)
  model_queries_config = (
  (text, models.Function.text),
  )
  q = self.get_model_queries(q, model_queries_config)
  q = self.get_one_to_many_queries(q, ((entry_name, models.Entry.name),))
  return self._limit_and_df(q, limit, as_df)","Method to query :class:`.models.Function` objects in database :param text: description(s) of function(s) :type text: str or tuple(str) or None :param entry_name: name(s) in :class:`.models.Entry` :type entry_name: str or tuple(str) or None :param limit: - if `isinstance(limit,int)==True` -> limit - if `isinstance(limit,tuple)==True` -> format:= tuple(page_number, results_per_page) - if limit == None -> all results :type limit: int or tuple(int) or None :param bool as_df: if `True` results are returned as :class:`pandas.DataFrame` :return: - if `as_df == False` -> list(:class:`.models.Function`) - if `as_df == True` -> :class:`pandas.DataFrame` :rtype: list(:class:`.models.Function`) or :class:`pandas.DataFrame`",1,0,1,1,3,1,0,1,1,3
"def init_mysql_db(username, host, database, port='', password='', initTime=False):
  mysql_base_url = 'mysql://'
  if password != '':
  password = ':%s' % password
  if port != '':
  port = ':%s' % port
  sqlalchemy_url = '%s%s%s@%s%s/%s' % (
  mysql_base_url,
  username,
  password,
  host,
  port,
  database
  )
  init_time = init_db(sqlalchemy_url)
  if initTime:
  print('TIME: {0} seconds'.format(init_time))
  return sqlalchemy_url","Initialize MySQL Database .. note:: mysql-python or similar driver required Args: username(str): Database username. host(str): Database host URL. database(str): Database name. port(Optional[int,str]): Database port. password(Optional[str]): Database password. initTime(Optional[bool]): If True, it will print the amount of time to generate database. Example:: from gsshapy.lib.db_tools import init_mysql_db, create_session sqlalchemy_url = init_mysql_db(username='gsshapy', host='localhost', database='gsshapy_mysql_tutorial', port='5432', password='pass') db_work_sessionmaker = get_sessionmaker(sqlalchemy_url) db_work_session = db_work_sessionmaker() ##DO WORK db_work_session.close()",1,1,0,0,2,1,1,0,1,3
"def from_data(self, time, value, series_id=None, key=None, tz=None):
  t = check_time_param(time)
  if type(value) in [float, int]:
  v = value
  else:
  raise ValueError('Values must be int or float. Got ""%s"".' %
  str(value))
  j = {
  't': t,
  'v': v,
  'id': series_id,
  'key': key
  }
  return DataPoint(j, None, tz=tz)","Create a DataPoint object from data, rather than a JSON object or string. This should be used by user code to construct DataPoints from Python-based data like Datetime objects and floats. The series_id and key arguments are only necessary if you are doing a multi write, in which case those arguments can be used to specify which series the DataPoint belongs to. If needed, the tz argument should be an Olsen database compliant string specifying the time zone for this DataPoint. This argument is most often used internally when reading data from TempoDB. :param time: the point in time for this reading :type time: ISO8601 string or Datetime :param value: the value for this reading :type value: int or float :param string series_id: (optional) a series ID for this point :param string key: (optional) a key for this point :param string tz: (optional) a timezone for this point :rtype: :class:`DataPoint`",1,0,0,1,2,1,0,0,1,2
"def kernel_output(self, user_name, kernel_slug, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.kernel_output_with_http_info(user_name, kernel_slug, **kwargs)
  else:
  (data) = self.kernel_output_with_http_info(user_name, kernel_slug, **kwargs)
  return data","Download the latest output from a kernel # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.kernel_output(user_name, kernel_slug, async_req=True) >>> result = thread.get() :param async_req bool :param str user_name: Kernel owner (required) :param str kernel_slug: Kernel name (required) :return: Result If the method is called asynchronously, returns the request thread.",2,0,0,1,3,2,0,0,1,3
"def action_logging(f):
  @functools.wraps(f)
  def wrapper(*args, **kwargs):
  assert args
  assert isinstance(args[0], Namespace), \
  ""1st positional argument should be argparse.Namespace instance, "" \
  ""but {}"".format(args[0])
  metrics = _build_metrics(f.__name__, args[0])
  cli_action_loggers.on_pre_execution(**metrics)
  try:
  return f(*args, **kwargs)
  except Exception as e:
  metrics['error'] = e
  raise
  finally:
  metrics['end_datetime'] = datetime.utcnow()
  cli_action_loggers.on_post_execution(**metrics)
  return wrapper","Decorates function to execute function at the same time submitting action_logging but in CLI context. It will call action logger callbacks twice, one for pre-execution and the other one for post-execution. Action logger will be called with below keyword parameters: sub_command : name of sub-command start_datetime : start datetime instance by utc end_datetime : end datetime instance by utc full_command : full command line arguments user : current user log : airflow.models.log.Log ORM instance dag_id : dag id (optional) task_id : task_id (optional) execution_date : execution date (optional) error : exception instance if there's an exception :param f: function instance :return: wrapped function",1,0,0,1,2,1,0,0,1,2
"def ssh_directory_for_unit(application_name, user=None):
  if user:
  application_name = ""{}_{}"".format(application_name, user)
  _dir = os.path.join(NOVA_SSH_DIR, application_name)
  for d in [NOVA_SSH_DIR, _dir]:
  if not os.path.isdir(d):
  os.mkdir(d)
  for f in ['authorized_keys', 'known_hosts']:
  f = os.path.join(_dir, f)
  if not os.path.isfile(f):
  open(f, 'w').close()
  return _dir",Return the directory used to store ssh assets for the application. :param application_name: Name of application eg nova-compute-something :type application_name: str :param user: The user that the ssh asserts are for. :type user: str :returns: Fully qualified directory path. :rtype: str,1,0,0,0,1,1,0,0,1,2
"def to_sql(self, name, con, schema=None, if_exists='fail', index=True,
  index_label=None, chunksize=None, dtype=None, method=None):
  from pandas.io import sql
  sql.to_sql(self, name, con, schema=schema, if_exists=if_exists,
  index=index, index_label=index_label, chunksize=chunksize,
  dtype=dtype, method=method)","Write records stored in a DataFrame to a SQL database. Databases supported by SQLAlchemy [1]_ are supported. Tables can be newly created, appended to, or overwritten. Parameters ---------- name : string Name of SQL table. con : sqlalchemy.engine.Engine or sqlite3.Connection Using SQLAlchemy makes it possible to use any DB supported by that library. Legacy support is provided for sqlite3.Connection objects. schema : string, optional Specify the schema (if database flavor supports this). If None, use default schema. if_exists : {'fail', 'replace', 'append'}, default 'fail' How to behave if the table already exists. * fail: Raise a ValueError. * replace: Drop the table before inserting new values. * append: Insert new values to the existing table. index : bool, default True Write DataFrame index as a column. Uses `index_label` as the column name in the table. index_label : string or sequence, default None Column label for index column(s). If None is given (default) and `index` is True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex. chunksize : int, optional Rows will be written in batches of this size at a time. By default, all rows will be written at once. dtype : dict, optional Specifying the datatype for columns. The keys should be the column names and the values should be the SQLAlchemy types or strings for the sqlite3 legacy mode. method : {None, 'multi', callable}, default None Controls the SQL insertion clause used: * None : Uses standard SQL ``INSERT`` clause (one per row). * 'multi': Pass multiple values in a single ``INSERT`` clause. * callable with signature ``(pd_table, conn, keys, data_iter)``. Details and a sample callable implementation can be found in the section :ref:`insert method <io.sql.method>`. .. versionadded:: 0.24.0 Raises ------ ValueError When the table already exists and `if_exists` is 'fail' (the default). See Also -------- read_sql : Read a DataFrame from a table. Notes ----- Timezone aware datetime columns will be written as ``Timestamp with timezone`` type with SQLAlchemy if supported by the database. Otherwise, the datetimes will be stored as timezone unaware timestamps local to the original timezone. .. versionadded:: 0.24.0 References ---------- .. [1] http://docs.sqlalchemy.org .. [2] https://www.python.org/dev/peps/pep-0249/ Examples -------- Create an in-memory SQLite database. >>> from sqlalchemy import create_engine >>> engine = create_engine('sqlite://', echo=False) Create a table from scratch with 3 rows. >>> df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']}) >>> df name 0 User 1 1 User 2 2 User 3 >>> df.to_sql('users', con=engine) >>> engine.execute(""SELECT * FROM users"").fetchall() [(0, 'User 1'), (1, 'User 2'), (2, 'User 3')] >>> df1 = pd.DataFrame({'name' : ['User 4', 'User 5']}) >>> df1.to_sql('users', con=engine, if_exists='append') >>> engine.execute(""SELECT * FROM users"").fetchall() [(0, 'User 1'), (1, 'User 2'), (2, 'User 3'), (0, 'User 4'), (1, 'User 5')] Overwrite the table with just ``df1``. >>> df1.to_sql('users', con=engine, if_exists='replace', ... index_label='id') >>> engine.execute(""SELECT * FROM users"").fetchall() [(0, 'User 4'), (1, 'User 5')] Specify the dtype (especially useful for integers with missing values). Notice that while pandas is forced to store the data as floating point, the database supports nullable integers. When fetching the data with Python, we get back integer scalars. >>> df = pd.DataFrame({""A"": [1, None, 2]}) >>> df A 0 1.0 1 NaN 2 2.0 >>> from sqlalchemy.types import Integer >>> df.to_sql('integers', con=engine, index=False, ... dtype={""A"": Integer()}) >>> engine.execute(""SELECT * FROM integers"").fetchall() [(1,), (None,), (2,)]",0,0,0,1,1,1,1,0,1,3
"def _create_user_agent(self):
  user_agent = '{}/{} {}'.format(pyspacegdn.__title__,
  pyspacegdn.__version__,
  default_user_agent())
  if self.client_name:
  user_agent = '{}/{} {}'.format(self.client_name,
  self.client_version, user_agent)
  return user_agent",Create the user agent and return it as a string.,0,0,0,0,0,1,0,0,1,2
"def send_inline_bot_result(
  self,
  chat_id: Union[int, str],
  query_id: int,
  result_id: str,
  disable_notification: bool = None,
  reply_to_message_id: int = None,
  hide_via: bool = None
  ):
  return self.send(
  functions.messages.SendInlineBotResult(
  peer=self.resolve_peer(chat_id),
  query_id=query_id,
  id=result_id,
  random_id=self.rnd_id(),
  silent=disable_notification or None,
  reply_to_msg_id=reply_to_message_id,
  hide_via=hide_via or None
  )
  )","Use this method to send an inline bot result. Bot results can be retrieved using :obj:`get_inline_bot_results <pyrogram.Client.get_inline_bot_results>` Args: chat_id (``int`` | ``str``): Unique identifier (int) or username (str) of the target chat. For your personal cloud (Saved Messages) you can simply use ""me"" or ""self"". For a contact that exists in your Telegram address book you can use his phone number (str). query_id (``int``): Unique identifier for the answered query. result_id (``str``): Unique identifier for the result that was chosen. disable_notification (``bool``, *optional*): Sends the message silently. Users will receive a notification with no sound. reply_to_message_id (``bool``, *optional*): If the message is a reply, ID of the original message. hide_via (``bool``): Sends the message with *via @bot* hidden. Returns: On success, the sent Message is returned. Raises: :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.",2,0,0,2,4,1,0,0,1,2
"def compare_tags(self, tags):
  all_tags = []
  for task in self._tasks:
  all_tags.extend(task.tags)
  all_tags_set = set(all_tags)
  tags_set = set(tags)
  matched_tags = all_tags_set & tags_set
  unmatched_tags = all_tags_set - tags_set
  return matched_tags, unmatched_tags","given a list of tags that the user has specified, return two lists: matched_tags: tags were found within the current play and match those given by the user unmatched_tags: tags that were found within the current play but do not match any provided by the user",0,0,0,1,1,1,0,0,1,2
"def send_file(self, sender, receiver_type, receiver_id, media_id):
  data = {
  'receiver': {
  'type': receiver_type,
  'id': receiver_id,
  },
  'sender': sender,
  'msgtype': 'file',
  'file': {
  'media_id': media_id,
  }
  }
  return self._post('chat/send', data=data)","  https://qydev.weixin.qq.com/wiki/index.php?title= :param sender:  :param receiver_type: single|group| :param receiver_id: userid|chatidid|id :param media_id: id  , 4 :return:  JSON ",1,0,0,1,2,1,0,0,1,2
"def save_data(self,session, exp_id, content):
  from expfactory.database.models import (
  Participant,
  Result
  )
  subid = session.get('subid')
  bot.info('Saving data for subid %s' % subid)
  if subid is not None:
  p = Participant.query.filter(Participant.id == subid).first()
  if ""data"" in content:
  content = content['data']
  if isinstance(content,dict):
  content = json.dumps(content)
  result = Result(data=content,
  exp_id=exp_id,
  participant_id=p.id)
  self.session.add(result)
  p.results.append(result)
  self.session.commit()
  bot.info(""Participant: %s"" %p)
  bot.info(""Result: %s"" %result)","save data will obtain the current subid from the session, and save it depending on the database type.",0,1,1,1,3,1,1,1,1,4
"def parse_keytab(keytab):
  try:
  out = subprocess.check_output(['klist', '-k', keytab],
  stderr=subprocess.PIPE)
  except OSError:
  raise KerberosError(""Failed to locate klist, cannot read keytab"")
  except subprocess.CalledProcessError:
  raise KerberosError(""Cannot read keytab {!r}"".format(keytab))
  principals = []
  for line in out.splitlines():
  if isinstance(line, bytes):
  line = line.decode('utf-8')
  try:
  kvno, principal, = re.split(r'\s+', line.strip(' '), 1)
  except ValueError:
  continue
  else:
  if not kvno.isdigit():
  continue
  principals.append(tuple(principal.split('@')) + (int(kvno),))
  return list(OrderedDict.fromkeys(principals).keys())","Read the contents of a KRB5 keytab file, returning a list of credentials listed within Parameters ---------- keytab : `str` path to keytab file Returns ------- creds : `list` of `tuple` the (unique) list of `(username, realm, kvno)` as read from the keytab file Examples -------- >>> from gwpy.io.kerberos import parse_keytab >>> print(parse_keytab(""creds.keytab"")) [('albert.einstein', 'LIGO.ORG', 1)]",0,0,0,1,1,1,0,0,1,2
"def menu(self, choices, prompt='Please choose from the provided options:',
  error='Invalid choice', intro=None, strict=True, default=None,
  numerator=lambda x: [i + 1 for i in range(x)],
  formatter=lambda x, y: '{0:>3}) {1}'.format(x, y),
  clean=utils.safeint):
  numbers = list(numerator(len(choices)))
  labels = (label for _, label in choices)
  values = [value for value, _ in choices]
  if intro:
  self.pstd('\n' + utils.rewrap_long(intro))
  for n, label in zip(numbers, labels):
  self.pstd(formatter(n, label))
  validator = lambda x: x in numbers
  val = self.rvpl(prompt, error=error, validator=validator, clean=clean,
  strict=strict, default=default)
  if not strict and val == default:
  return val
  return values[numbers.index(val)]","Print a menu The choices must be an iterable of two-tuples where the first value is the value of the menu item, and the second is the label for that matches the value. The menu will be printed with numeric choices. For example:: 1) foo 2) bar Formatting of the number is controlled by the formatter function which can be overridden by passing the ``formatter`` argument. The numbers used for the menu are generated using the numerator function which can be specified using the ``numerator`` function. This function must take the number of choices and return the same number of items that will be used as choice characters as a list. The cleaner function is passed to ``pvpl()`` method can be customized using ``clean`` argument. This function should generally be customized whenever ``numerator`` is customized, as default cleaner converts input to integers to match the default numerator. Optional ``intro`` argument can be passed to print a message above the menu. The return value of this method is the value user has chosen. The prompt will keep asking the user for input until a valid choice is selected. Each time an invalid selection is made, error message is printed. This message can be customized using ``error`` argument. If ``strict`` argument is set, then only values in choices are allowed, otherwise any value will be allowed. The ``default`` argument can be used to define what value is returned in case user select an invalid value when strict checking is off.",1,0,0,1,2,1,0,0,1,2
"def login(self, username='admin', password='admin'):
  self.session = requests.Session()
  login = self.session.post(self.url+'login',
  data={'username': username,
  'password': password})
  if login.text == 'Ok.':
  self._is_authenticated = True
  else:
  return login.text","Method to authenticate the qBittorrent Client. Declares a class attribute named ``session`` which stores the authenticated session if the login is correct. Else, shows the login error. :param username: Username. :param password: Password. :return: Response to login request to the API.",2,0,0,2,4,2,0,0,1,3
"def _post_request(self, url, data=None, files=None):
  import requests
  request_kwargs = {
  'url': url
  }
  if data:
  request_kwargs['data'] = data
  if files:
  request_kwargs['files'] = files
  try:
  response = requests.post(**request_kwargs)
  except Exception:
  if self.requests_handler:
  request_kwargs['method'] = 'POST'
  request_object = requests.Request(**request_kwargs)
  return self.requests_handler(request_object)
  else:
  raise
  response_details = self.telegram_handler.handle(response)
  return response_details",a helper method for sending post requests to telegram api https://core.telegram.org/bots/api#making-requests https://requests.readthedocs.io/en/master/user/quickstart/ :param url: string with url for post request :param data: [optional] dictionary with data to add to request :param files: [optional] byte data to add to request :return: dictionary with response details,1,0,0,1,2,2,0,0,1,3
"def has_site_permission(user):
  mw = ""yacms.core.middleware.SitePermissionMiddleware""
  if mw not in get_middleware_setting():
  from warnings import warn
  warn(mw + "" missing from settings.MIDDLEWARE - per site""
  ""permissions not applied"")
  return user.is_staff and user.is_active
  return getattr(user, ""has_site_permission"", False)","Checks if a staff user has staff-level access for the current site. The actual permission lookup occurs in ``SitePermissionMiddleware`` which then marks the request with the ``has_site_permission`` flag, so that we only query the db once per request, so this function serves as the entry point for everything else to check access. We also fall back to an ``is_staff`` check if the middleware is not installed, to ease migration.",1,0,1,1,3,1,0,0,1,2
"def kick_chat_member(chat_id, user_id, until_date=None, **kwargs):
  params = dict(
  chat_id=chat_id,
  user_id=user_id,
  )
  params.update(
  _clean_params(
  until_date=until_date
  )
  )
  return TelegramBotRPCRequest('kickChatMember', params=params, on_result=lambda result: result, **kwargs)","Use this method to kick a user from a group or a supergroup. In the case of supergroups, the user will not be able to return to the group on their own using invite links, etc., unless unbanned first. The bot must be an administrator in the group for this to work. Returns True on success. Note: This will method only work if the All Members Are Admins setting is off in the target group. Otherwise members may only be removed by the group's creator or by the member that added them. :param chat_id: Unique identifier for the target chat or username of the target channel (in the format @channelusername) :param user_id: Unique identifier of the target user :param until_date: *Optional.* Date when the user will be unbanned, unix time. If user is banned for more than 366 days or less than 30 seconds from the current time they are considered to be banned forever :param kwargs: Args that get passed down to :class:`TelegramBotRPCRequest` :type chat_id: int or str :type user_id: int :returns: Returns True on success. :rtype: bool",1,0,0,1,2,2,0,0,1,3
"def create_user_id(self, user_id, app_id, cidr_block=None, mount_point='app-id', **kwargs):
  if isinstance(app_id, (list, set, tuple)):
  app_id = ','.join(app_id)
  params = {
  'value': app_id
  }
  if cidr_block:
  params['cidr_block'] = cidr_block
  params.update(kwargs)
  return self._adapter.post('/v1/auth/{}/map/user-id/{}'.format(mount_point, user_id), json=params)",POST /auth/<mount point>/map/user-id/<user_id> :param user_id: :type user_id: :param app_id: :type app_id: :param cidr_block: :type cidr_block: :param mount_point: :type mount_point: :param kwargs: :type kwargs: :return: :rtype:,1,0,0,1,2,1,0,0,1,2
"def filter_paths(pathnames,
  included_patterns=None,
  excluded_patterns=None,
  case_sensitive=True):
  included = [""*""] if included_patterns is None else included_patterns
  excluded = [] if excluded_patterns is None else excluded_patterns
  for pathname in pathnames:
  if _match_path(pathname, included, excluded, case_sensitive):
  yield pathname","Filters from a set of paths based on acceptable patterns and ignorable patterns. :param pathnames: A list of path names that will be filtered based on matching and ignored patterns. :param included_patterns: Allow filenames matching wildcard patterns specified in this list. If no pattern list is specified, [""*""] is used as the default pattern, which matches all files. :param excluded_patterns: Ignores filenames matching wildcard patterns specified in this list. If no pattern list is specified, no files are ignored. :param case_sensitive: ``True`` if matching should be case-sensitive; ``False`` otherwise. :returns: A list of pathnames that matched the allowable patterns and passed through the ignored patterns. Doctests:: >>> pathnames = set([""/users/gorakhargosh/foobar.py"", ""/var/cache/pdnsd.status"", ""/etc/pdnsd.conf"", ""/usr/local/bin/python""]) >>> set(filter_paths(pathnames)) == pathnames True >>> set(filter_paths(pathnames, case_sensitive=False)) == pathnames True >>> set(filter_paths(pathnames, [""*.py"", ""*.conf""], [""*.status""], case_sensitive=True)) == set([""/users/gorakhargosh/foobar.py"", ""/etc/pdnsd.conf""]) True",0,0,0,1,1,1,0,0,1,2
"def set_metrics_params(self, enable=None, store_dir=None, restore=None, no_cores=None):
  self._set('enable-metrics', enable, cast=bool)
  self._set('metrics-dir', self._section.replace_placeholders(store_dir))
  self._set('metrics-dir-restore', restore, cast=bool)
  self._set('metrics-no-cores', no_cores, cast=bool)
  return self._section","Sets basic Metrics subsystem params. uWSGI metrics subsystem allows you to manage ""numbers"" from your apps. When enabled, the subsystem configures a vast amount of metrics (like requests per-core, memory usage, etc) but, in addition to this, you can configure your own metrics, such as the number of active users or, say, hits of a particular URL, as well as the memory consumption of your app or the whole server. * http://uwsgi.readthedocs.io/en/latest/Metrics.html * SNMP Integration - http://uwsgi.readthedocs.io/en/latest/Metrics.html#snmp-integration :param bool enable: Enables the subsystem. :param str|unicode store_dir: Directory to store metrics. The metrics subsystem can expose all of its metrics in the form of text files in a directory. The content of each file is the value of the metric (updated in real time). .. note:: Placeholders can be used to build paths, e.g.: {project_runtime_dir}/metrics/ See ``Section.project_name`` and ``Section.runtime_dir``. :param bool restore: Restore previous metrics from ``store_dir``. When you restart a uWSGI instance, all of its metrics are reset. Use the option to force the metric subsystem to read-back the values from the metric directory before starting to collect values. :param bool no_cores: Disable generation of cores-related metrics.",1,0,0,0,1,1,0,0,1,2
"def locateChild(self, context, segments):
  request = IRequest(context)
  hostname = request.getHeader('host')
  info = self.subdomain(hostname)
  if info is not None:
  username, domain = info
  index = UserIndexPage(IRealm(self.siteStore),
  self.webViewer)
  resource = index.locateChild(None, [username])[0]
  return resource, segments
  return self.wrapped.locateChild(context, segments)","Delegate dispatch to a sharing resource if the request is for a user subdomain, otherwise fall back to the wrapped resource's C{locateChild} implementation.",1,0,0,1,2,1,0,0,1,2
"def _BuildOobLink(self, param, mode):
  code = self.rpc_helper.GetOobCode(param)
  if code:
  parsed = list(parse.urlparse(self.widget_url))
  query = dict(parse.parse_qsl(parsed[4]))
  query.update({'mode': mode, 'oobCode': code})
  try:
  parsed[4] = parse.urlencode(query)
  except AttributeError:
  parsed[4] = urllib.urlencode(query)
  return code, parse.urlunparse(parsed)
  raise errors.GitkitClientError('invalid request')","Builds out-of-band URL. Gitkit API GetOobCode() is called and the returning code is combined with Gitkit widget URL to building the out-of-band url. Args: param: dict of request. mode: string, Gitkit widget mode to handle the oob action after user clicks the oob url in the email. Raises: GitkitClientError: if oob code is not returned. Returns: A string of oob url.",1,0,0,2,3,1,0,0,1,2
"def maybe_parse_user_type(t):
  is_type = isinstance(t, type)
  is_preserved = isinstance(t, type) and issubclass(t, _preserved_iterable_types)
  is_string = isinstance(t, string_types)
  is_iterable = isinstance(t, Iterable)
  if is_preserved:
  return [t]
  elif is_string:
  return [t]
  elif is_type and not is_iterable:
  return [t]
  elif is_iterable:
  ts = t
  return tuple(e for t in ts for e in maybe_parse_user_type(t))
  else:
  raise TypeError(
  'Type specifications must be types or strings. Input: {}'.format(t)
  )","Try to coerce a user-supplied type directive into a list of types. This function should be used in all places where a user specifies a type, for consistency. The policy for what defines valid user input should be clear from the implementation.",1,0,0,1,2,1,0,0,1,2
"def with_bundler(self):
  def gemfile_exists():
  return os.path.exists('Gemfile')
  if 'GIT_UP_BUNDLER_CHECK' in os.environ:
  print(colored(
  , 'yellow'))
  if self.settings['bundler.check']:
  return gemfile_exists()
  if ('GIT_UP_BUNDLER_CHECK' in os.environ
  and os.environ['GIT_UP_BUNDLER_CHECK'] == 'true'):
  return gemfile_exists()
  return False","Check, if bundler check is requested. Check, if the user wants us to check for new gems and return True in this case. :rtype : bool",1,0,0,1,2,1,0,0,1,2
"def expect_file_to_exist(self, filepath=None, result_format=None, include_config=False,
  catch_exceptions=None, meta=None):
  if filepath is not None and os.path.isfile(filepath):
  success = True
  elif self._path is not None and os.path.isfile(self._path):
  success = True
  else:
  success = False
  return {""success"":success}","Checks to see if a file specified by the user actually exists Args: filepath (str or None): \ The filepath to evalutate. If none, will check the currently-configured path object of this FileDataAsset. Keyword Args: result_format (str or None): \ Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`. For more detail, see :ref:`result_format <result_format>`. include_config (boolean): \ If True, then include the expectation config as part of the result object. \ For more detail, see :ref:`include_config`. catch_exceptions (boolean or None): \ If True, then catch exceptions and include them as part of the result object. \ For more detail, see :ref:`catch_exceptions`. meta (dict or None): \ A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification. For more detail, see :ref:`meta`. Returns: A JSON-serializable expectation result object. Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.",1,0,0,1,2,1,0,0,1,2
"def get_summary(self, timezone_offset, first_date, start=0.0, end=0.0):
  title = '%s.get_summary' % self.__class__.__name__
  if 'activity' not in self.service_scope:
  raise ValueError('%s requires service scope to contain ""activity"".' % title)
  url_string = '%s/user/summary/daily' % self.endpoint
  parameters = self._process_dates(timezone_offset, first_date, start, end, title)
  response_details = self._get_request(url_string, params=parameters)
  return response_details","a method to retrieve summary details for a period of time NOTE: start and end must be no more than 30 days, 1 second apart :param timezone_offset: integer with timezone offset from user profile details :param first_date: string with ISO date from user profile details firstDate :param start: [optional] float with starting datetime for daily summaries :param end: [optional] float with ending datetime for daily summaries :return: dictionary of response details with summary list inside json key { 'headers': { ... }, 'code': 200, 'error': '', 'url': 'https://api.moves-app.com/api/1.1/user/summary/daily' 'json': [ SEE RESPONSE in https://dev.moves-app.com/docs/api_summaries ] }",1,0,0,1,2,2,0,0,1,3
"def read(calc_id, username=None):
  if isinstance(calc_id, str) or calc_id < 0 and not username:
  return datastore.read(calc_id)
  job = logs.dbcmd('get_job', calc_id, username)
  if job:
  return datastore.read(job.ds_calc_dir + '.hdf5')
  else:
  return datastore.read(calc_id)",":param calc_id: a calculation ID :param username: if given, restrict the search to the user's calculations :returns: the associated DataStore instance",1,0,1,1,3,1,0,1,1,3
"def edit( plugins, parent = None, default = None, modal = True ):
  if ( XConfigDialog._instance ):
  XConfigDialog._instance.show()
  XConfigDialog._instance.activateWindow()
  return True
  dlg = XConfigDialog( parent )
  dlg.setPlugins(plugins)
  dlg.setCurrentPlugin(default)
  if ( not modal ):
  XConfigDialog._instance = dlg
  dlg.setAttribute(Qt.WA_DeleteOnClose)
  dlg.show()
  return True
  if ( dlg.exec_() ):
  return True
  return False","Prompts the user to edit the config settings for the inputed config \ plugins. :param plugins | [<XConfigPlugin>, ..] parent | <QWidget> default | <XConfigPlugin> || None :return <bool> success",1,0,0,0,1,1,0,0,1,2
"def grant_user_permission(self, id, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.grant_user_permission_with_http_info(id, **kwargs)
  else:
  (data) = self.grant_user_permission_with_http_info(id, **kwargs)
  return data","Grants a specific user permission # noqa: E501 # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.grant_user_permission(id, async_req=True) >>> result = thread.get() :param async_req bool :param str id: (required) :param str group: Permission group to grant to this user. Please note that 'host_tag_management' is the equivalent of the 'Source Tag Management' permission :return: UserModel If the method is called asynchronously, returns the request thread.",1,0,0,2,3,2,0,0,1,3
"def assign(self, login):
  if not login:
  return False
  number = self.milestone.number if self.milestone else None
  labels = [str(l) for l in self.labels]
  return self.edit(self.title, self.body, login, self.state, number,
  labels)",Assigns user ``login`` to this issue. This is a short cut for ``issue.edit``. :param str login: username of the person to assign this issue to :returns: bool,1,0,0,1,2,1,0,0,1,2
"def get_user(self, request):
  try:
  return User.objects.get(username=request.data.get('username'),
  is_active=True)
  except User.DoesNotExist:
  return None",return active user or ``None``,1,0,1,1,3,1,0,1,1,3
"def scan(self, cursor=0, pattern=None, count=None):
  def format_response(value):
  return int(value[0]), value[1]
  command = [b'SCAN', ascii(cursor).encode('ascii')]
  if pattern:
  command += [b'MATCH', pattern]
  if count:
  command += [b'COUNT', ascii(count).encode('ascii')]
  return self._execute(command, format_callback=format_response)","The :meth:`~tredis.RedisClient.scan` command and the closely related commands :meth:`~tredis.RedisClient.sscan`, :meth:`~tredis.RedisClient.hscan` and :meth:`~tredis.RedisClient.zscan` are used in order to incrementally iterate over a collection of elements. - :meth:`~tredis.RedisClient.scan` iterates the set of keys in the currently selected Redis database. - :meth:`~tredis.RedisClient.sscan` iterates elements of Sets types. - :meth:`~tredis.RedisClient.hscan` iterates fields of Hash types and their associated values. - :meth:`~tredis.RedisClient.zscan` iterates elements of Sorted Set types and their associated scores. **Basic usage** :meth:`~tredis.RedisClient.scan` is a cursor based iterator. This means that at every call of the command, the server returns an updated cursor that the user needs to use as the cursor argument in the next call. An iteration starts when the cursor is set to ``0``, and terminates when the cursor returned by the server is ``0``. For more information on :meth:`~tredis.RedisClient.scan`, visit the `Redis docs on scan <http://redis.io/commands/scan>`_. .. note:: **Time complexity**: ``O(1)`` for every call. ``O(N)`` for a complete iteration, including enough command calls for the cursor to return back to ``0``. ``N`` is the number of elements inside the collection. :param int cursor: The server specified cursor value or ``0`` :param pattern: An optional pattern to apply for key matching :type pattern: :class:`str`, :class:`bytes` :param int count: An optional amount of work to perform in the scan :rtype: int, list :returns: A tuple containing the cursor and the list of keys :raises: :exc:`~tredis.exceptions.RedisError`",1,0,0,1,2,1,0,0,1,2
"def make_posix(path):
  if not isinstance(path, six.string_types):
  raise TypeError(""Expected a string for path, received {0!r}..."".format(path))
  starts_with_sep = path.startswith(os.path.sep)
  separated = normalize_path(path).split(os.path.sep)
  if isinstance(separated, (list, tuple)):
  path = posixpath.join(*separated)
  if starts_with_sep:
  path = ""/{0}"".format(path)
  return path","Convert a path with possible windows-style separators to a posix-style path (with **/** separators instead of **\\** separators). :param Text path: A path to convert. :return: A converted posix-style path :rtype: Text >>> make_posix(""c:/users/user/venvs/some_venv\\Lib\\site-packages"") ""c:/users/user/venvs/some_venv/Lib/site-packages"" >>> make_posix(""c:\\users\\user\\venvs\\some_venv"") ""c:/users/user/venvs/some_venv""",1,0,0,1,2,1,0,0,1,2
"def is_authorized(self, request):
  if self._is_request_in_include_path(request):
  if self._is_request_in_exclude_path(request):
  return True
  else:
  auth = request.authorization
  if auth and auth[0] == 'Basic':
  credentials = b64decode(auth[1]).decode('UTF-8')
  username, password = credentials.split(':', 1)
  return self._users.get(username) == password
  else:
  return False
  else:
  return True",Check if the user is authenticated for the given request. The include_paths and exclude_paths are first checked. If authentication is required then the Authorization HTTP header is checked against the credentials.,1,0,0,0,1,1,0,0,1,2
"def email_has_role(self, email, role_name, uuid=None):
  mbr_data = self.get_membership(uuid=uuid)
  docs = []
  try:
  docs = mbr_data['response']['docs']
  except KeyError:
  failure_message = ('KeyError in membership data - '
  'got {0}'.format(mbr_data))
  log.exception(failure_message)
  raise PyLmodUnexpectedData(failure_message)
  if len(docs) == 0:
  return False
  has_role = any(
  (x.get('email') == email and x.get('roleType') == role_name)
  for x in docs
  )
  if has_role:
  return True
  return False",Determine if an email is associated with a role. Args: email (str): user email role_name (str): user role uuid (str): optional uuid. defaults to self.cuuid Raises: PyLmodUnexpectedData: Unexpected data was returned. requests.RequestException: Exception connection error Returns: bool: True or False if email has role_name,1,0,0,1,2,2,0,0,1,3
"def remark_user(self, user_id, remark):
  return self.post(
  url=""https://api.weixin.qq.com/cgi-bin/user/info/updateremark"",
  data={
  ""openid"": user_id,
  ""remark"": remark
  }
  )", :param user_id:  ID :param remark: 30 :return:  JSON ,1,0,0,2,3,2,0,0,1,3
"def create_templates(self, templates):
  count = 0
  for template in templates:
  if not self.template_exists_db(template):
  name, location, description, language = template
  text = self.open_file(location)
  html_content = self.get_html_content(text)
  data = {
  'name': utils.camel_to_snake(name).upper(),
  'html_content': html_content,
  'content': self.text_version(html_content),
  'subject': self.get_subject(text),
  'description': description,
  'language': language
  }
  if models.EmailTemplate.objects.create(**data):
  count += 1
  return count",Gets a list of templates to insert into the database,1,1,1,1,4,1,1,1,1,4
"def validate_key(self, activation_key):
  try:
  username = signing.loads(
  activation_key,
  salt=self.key_salt,
  max_age=conf.get('ACCOUNT_ACTIVATION_DAYS') * 86400
  )
  return username
  except signing.BadSignature:
  return None","Verify that the activation key is valid and within the permitted activation time window, returning the username if valid or ``None`` if not.",0,0,0,1,1,1,0,0,1,2
"def add_user(self, username, email, **kwargs):
  api = self._get_api(iam.AccountAdminApi)
  kwargs.update({'username': username, 'email': email})
  user = User._create_request_map(kwargs)
  body = iam.UserUpdateReq(**user)
  return User(api.create_user(body))","Create a new user with provided details. Add user example: .. code-block:: python account_management_api = AccountManagementAPI() # Add user user = { ""username"": ""test_user"", ""email"": ""test@gmail.com"", ""phone_number"": ""0123456789"" } new_user = account_management_api.add_user(**user) :param str username: The unique username of the user (Required) :param str email: The unique email of the user (Required) :param str full_name: The full name of the user :param list groups: List of group IDs (`str`) which this user belongs to :param str password: The password string of the user :param str phone_number: Phone number of the user :param bool terms_accepted: 'General Terms & Conditions' have been accepted :param bool marketing_accepted: Marketing Information opt-in :returns: the new user object :rtype: User",1,1,0,2,4,1,0,0,1,2
"def populate_model(model_or_inst, excludes=None, only=None):
  inst = model_or_inst if _is_inst(model_or_inst) else model_or_inst()
  parser = make_request_parser(model_or_inst, excludes, only, for_populate=True)
  req_args = parser.parse_args()
  for key, value in req_args.items():
  setattr(inst, key, value)
  return inst","Call `make_request_parser()` to build a `RequestParser`, use it extract user request data, and padding the data into model instance. If user passed a model class, instead of model instance, create a new instance use the extracted data.",1,0,0,0,1,1,0,1,1,3
"def login_with_google(self, email, oauth2_token, **kwargs):
  params = {
  'email': email,
  'oauth2_token': oauth2_token
  }
  req_func = self._get
  if kwargs.get('auto_signup', 0) == 1:
  req_func = self._post
  return req_func('login_with_google', params, **kwargs)","Login to Todoist using Google's oauth2 authentication. :param email: The user's Google email address. :type email: str :param oauth2_token: The user's Google oauth2 token. :type oauth2_token: str :param auto_signup: If ``1`` register an account automatically. :type auto_signup: int :param full_name: The full name to use if the account is registered automatically. If no name is given an email based nickname is used. :type full_name: str :param timezone: The timezone to use if the account is registered automatically. If no timezone is given one is chosen based on the user's IP address. :type timezone: str :param lang: The user's language. :type lang: str :return: The HTTP response to the request. :rtype: :class:`requests.Response` >>> from pytodoist.api import TodoistAPI >>> api = TodoistAPI() >>> oauth2_token = 'oauth2_token' # Get this from Google. >>> response = api.login_with_google('john.doe@gmail.com', ... oauth2_token) >>> user_info = response.json() >>> full_name = user_info['full_name'] >>> print(full_name) John Doe",2,0,0,2,4,2,0,0,1,3
"def write_to_file(filename, content):
  if not config[""destdir""]:
  print(""{destdir} config variable not present. Did you forget to run init()?"")
  sys.exit(8)
  abs_filename = os.path.abspath(config[""destdir""] + ""/"" + filename)
  abs_filepath = os.path.dirname(abs_filename)
  if not os.path.exists(abs_filepath):
  try:
  os.makedirs(abs_filepath)
  except OSError as e:
  print(""Cannot create directory "" + abs_filepath)
  print(""Error %d: %s"" % (e.errno, e.strerror))
  sys.exit(6)
  with codecs.open(abs_filename, ""w"", ""utf-8"") as out:
  if isinstance(content, str): content = [content]
  for line in content:
  if line is not None:
  out.write(line)
  out.write(""\n"")","Writes content to the given file. The file's directory will be created if needed. :param filename: name of the output file, relative to the ""destination folder"" provided by the user :param content: iterable (line-by-line) that should be written to the file. Either a list or a generator. Each line will be appended with a ""\n"". Lines containing None will be skipped.",1,0,0,0,1,1,0,0,1,2
"def get_values(self):
  return_dict = {}
  for i,ctrl in enumerate(self.list_ctrls):
  if hasattr(self.parse_funcs,'__getitem__') and len(self.parse_funcs)>i and hasattr(self.parse_funcs[i],'__call__'):
  try: return_dict[self.inputs[i]] = self.parse_funcs[i](ctrl.GetValue())
  except: return_dict[self.inputs[i]] = ctrl.GetValue()
  else:
  return_dict[self.inputs[i]] = ctrl.GetValue()
  return ('' not in list(return_dict.values()), return_dict)",Applies parsing functions to each input as specified in init before returning a tuple with first entry being a boolean which specifies if the user entered all values and a second entry which is a dictionary of input names to parsed values.,1,0,0,1,2,1,0,0,1,2
"def get_user_subadmin_groups(self, user_name):
  res = self._make_ocs_request(
  'GET',
  self.OCS_SERVICE_CLOUD,
  'users/' + user_name + '/subadmins',
  )
  if res.status_code == 200:
  tree = ET.fromstring(res.content)
  self._check_ocs_status(tree, [100])
  groups = tree.find('data')
  return groups
  raise HTTPResponseError(res)",Get a list of subadmin groups associated to a user. :param user_name: name of user :returns: list of subadmin groups :raises: HTTPResponseError in case an HTTP error status was returned,2,0,0,1,3,2,0,0,1,3
"def get(datasets_identifiers, identifier_type='hid', history_id=None):
  history_id = history_id or os.environ['HISTORY_ID']
  gi = get_galaxy_connection(history_id=history_id, obj=False)
  for dataset_identifier in datasets_identifiers:
  file_path = '/import/%s' % dataset_identifier
  log.debug('Downloading gx=%s history=%s dataset=%s', gi, history_id, dataset_identifier)
  if not os.path.exists(file_path):
  hc = HistoryClient(gi)
  dc = DatasetClient(gi)
  history = hc.show_history(history_id, contents=True)
  datasets = {ds[identifier_type]: ds['id'] for ds in history}
  if identifier_type == 'hid':
  dataset_identifier = int(dataset_identifier)
  dc.download_dataset(datasets[dataset_identifier], file_path=file_path, use_default_filename=False)
  else:
  log.debug('Cached, not re-downloading')
  return file_path","Given the history_id that is displayed to the user, this function will download the file[s] from the history and stores them under /import/ Return value[s] are the path[s] to the dataset[s] stored under /import/",0,1,0,1,2,1,0,0,1,2
"def load_exons(self, exons, genes=None, build='37'):
  genes = genes or self.ensembl_genes(build)
  for exon in exons:
  exon_obj = build_exon(exon, genes)
  if not exon_obj:
  continue
  res = self.exon_collection.insert_one(exon_obj)",Create exon objects and insert them into the database Args: exons(iterable(dict)),0,1,1,0,2,1,1,1,1,4
"def onStart(self, *args, **kwarg):
  with transactUI(self):
  config = self.navbar.getActiveConfig()
  config.resetErrors()
  if config.isValid():
  self.clientRunner.run(self.buildCliString())
  self.showConsole()
  else:
  config.displayErrors()
  self.Layout()",Verify user input and kick off the client's program if valid,1,0,0,1,2,1,0,0,1,2
"def get_content_curation_token(args_token):
  if args_token != ""
  if os.path.isfile(args_token):
  with open(args_token, 'r') as fobj:
  return fobj.read().strip()
  else:
  return args_token
  else:
  token = get_env('STUDIO_TOKEN') or get_env('CONTENT_CURATION_TOKEN')
  if token is not None:
  return token
  else:
  return prompt_token(config.DOMAIN)","Get the token through one of four possible ways. Input `args_token` can be 1. path to a token-containing file (path) 2. actual token (str) in which case there's nothing to get just pass along 3. `#` (default value when no --token is given on command line) 3a. if environment variable CONTENT_CURATION_TOKEN exists, we'll use that 3b. else we prompt the user interactively",1,0,0,1,2,1,0,0,1,2
"def update_thumbnail(api_key, api_secret, video_key, position=7.0, **kwargs):
  jwplatform_client = jwplatform.Client(api_key, api_secret)
  logging.info(""Updating video thumbnail."")
  try:
  response = jwplatform_client.videos.thumbnails.update(
  video_key=video_key,
  position=position,
  **kwargs)
  except jwplatform.errors.JWPlatformError as e:
  logging.error(""Encountered an error updating thumbnail.\n{}"".format(e))
  sys.exit(e.message)
  return response","Function which updates the thumbnail for an EXISTING video utilizing position parameter. This function is useful for selecting a new thumbnail from with the already existing video content. Instead of position parameter, user may opt to utilize thumbnail_index parameter. Please eee documentation for further information. :param api_key: <string> JWPlatform api-key :param api_secret: <string> JWPlatform shared-secret :param video_key: <string> Video's object ID. Can be found within JWPlayer Dashboard. :param position: <float> Represents seconds into the duration of a video, for thumbnail extraction. :param kwargs: Arguments conforming to standards found @ https://developer.jwplayer.com/jw-platform/reference/v1/methods/videos/thumbnails/update.html :return: <dict> Dict which represents the JSON response.",2,0,0,2,4,2,0,0,1,3
"def generate_mfa_token(self, user_id, expires_in=259200, reusable=False):
  self.clean_error()
  try:
  url = self.get_url(Constants.GENERATE_MFA_TOKEN_URL, user_id)
  data = {
  'expires_in': expires_in,
  'reusable': reusable
  }
  response = self.execute_call('post', url, json=data)
  if response.status_code == 201:
  json_data = response.json()
  if json_data:
  return MFAToken(json_data)
  else:
  self.error = self.extract_status_code_from_response(response)
  self.error_description = self.extract_error_message_from_response(response)
  except Exception as e:
  self.error = 500
  self.error_description = e.args[0]","Use to generate a temporary MFA token that can be used in place of other MFA tokens for a set time period. For example, use this token for account recovery. :param user_id: Id of the user :type user_id: int :param expires_in: Set the duration of the token in seconds. (default: 259200 seconds = 72h) 72 hours is the max value. :type expires_in: int :param reusable: Defines if the token reusable. (default: false) If set to true, token can be used for multiple apps, until it expires. :type reusable: bool Returns a mfa token :return: return the object if success :rtype: MFAToken See https://developers.onelogin.com/api-docs/1/multi-factor-authentication/generate-mfa-token Generate MFA Token documentation",2,0,0,2,4,2,0,0,2,4
"def update_summary_file():
  global g_summary_text_filename
  global g_output_filename_failed_tests
  global g_output_filename_passed_tests
  with open(g_summary_text_filename,'a') as tempfile:
  write_file_content(tempfile,g_output_filename_failed_tests)
  write_file_content(tempfile,g_output_filename_passed_tests)",Concatecate all log file into a summary text file to be sent to users at the end of a daily log scraping. :return: none,0,0,0,1,1,1,0,0,0,1
"def ask_bool(self, question, default=True):
  if self.script.options.yes:
  return True
  elif self.script.options.dry_run or not self.script.options.interactive:
  return default
  else:
  choice = '*'
  while choice not in ""YNAQ"":
  choice = raw_input(""%s? [%s)es, %s)o, a)ll yes, q)uit]: "" % (
  fmt.to_console(question), ""yY""[int(default)], ""Nn""[int(default)],
  ))
  choice = choice[:1].upper() or ""NY""[int(default)]
  if choice == 'Q':
  self.quit()
  if choice == 'A':
  self.script.options.yes = True
  choice = 'Y'
  return choice == 'Y'","Ask the user for Y)es / N)o / Q)uit. If ""Q"" ist entered, this method will exit with RC=3. Else, the user's choice is returned. Note that the options --non-interactive and --defaults also influence the outcome.",1,0,0,1,2,1,0,0,1,2
"def callback(self, request, **kwargs):
  code = request.GET.get('code', '')
  redirect_uri = '%s://%s%s' % (request.scheme, request.get_host(), reverse(""reddit_callback""))
  reddit = RedditApi(client_id=self.consumer_key,
  client_secret=self.consumer_secret,
  redirect_uri=redirect_uri,
  user_agent=self.user_agent)
  token = reddit.auth.authorize(code)
  UserService.objects.filter(user=request.user, name='ServiceReddit').update(token=token)
  return 'reddit/callback.html'",Called from the Service when the user accept to activate it the url to go back after the external service call :param request: contains the current session :param kwargs: keyword args :type request: dict :type kwargs: dict :rtype: string,2,1,0,1,4,2,0,1,1,4
"def construct_gene_object(ensembl, transcript_id):
  (chrom, start, end, strand, genomic_sequence) = ensembl.get_genomic_seq_for_transcript(transcript_id, expand=10)
  cds_sequence = ensembl.get_cds_seq_for_transcript(transcript_id)
  cds_ranges = ensembl.get_cds_ranges_for_transcript(transcript_id)
  exon_ranges = ensembl.get_exon_ranges_for_transcript(transcript_id)
  transcript = Transcript(transcript_id, chrom, start, end, strand)
  transcript.set_exons(exon_ranges, cds_ranges)
  transcript.set_cds(cds_ranges)
  transcript.add_cds_sequence(cds_sequence)
  transcript.add_genomic_sequence(genomic_sequence, offset=10)
  return transcript","creates an Transcript object for a gene from ensembl databases Args: ensembl: EnsemblRequest object to request data from ensembl transcript_id: string for an Ensembl transcript ID Returns: a Transcript object, containing transcript coordinates and gene and transcript sequence. Raises: ValueError if CDS from genomic sequence given gene coordinates and CDS retrieved from Ensembl do not match.",1,0,4,0,5,1,0,0,1,2
"def permission_delete_link(context, perm):
  user = context['request'].user
  if user.is_authenticated():
  if (user.has_perm('authority.delete_foreign_permissions') or
  user.pk == perm.creator.pk):
  return base_link(context, perm, 'authority-delete-permission')
  return {'url': None}",Renders a html link to the delete view of the given permission. Returns no content if the request-user has no permission to delete foreign permissions.,1,0,1,1,3,1,0,1,1,3
"def setTypingStatus(self, status, thread_id=None, thread_type=None):
  thread_id, thread_type = self._getThread(thread_id, thread_type)
  data = {
  ""typ"": status.value,
  ""thread"": thread_id,
  ""to"": thread_id if thread_type == ThreadType.USER else """",
  ""source"": ""mercury-chat"",
  }
  j = self._post(self.req_url.TYPING, data, fix_request=True, as_json=True)",Sets users typing status in a thread :param status: Specify the typing status :param thread_id: User/Group ID to change status in. See :ref:`intro_threads` :param thread_type: See :ref:`intro_threads` :type status: models.TypingStatus :type thread_type: models.ThreadType :raises: FBchatException if request failed,1,0,0,1,2,1,0,0,2,3
"def user_addmedia(userids, active, mediatypeid, period, sendto, severity, **kwargs):
  conn_args = _login(**kwargs)
  ret = {}
  try:
  if conn_args:
  method = 'user.addmedia'
  params = {""users"": []}
  if not isinstance(userids, list):
  userids = [userids]
  for user in userids:
  params['users'].append({""userid"": user})
  params['medias'] = [{""active"": active, ""mediatypeid"": mediatypeid, ""period"": period,
  ""sendto"": sendto, ""severity"": severity}, ]
  ret = _query(method, params, conn_args['url'], conn_args['auth'])
  return ret['result']['mediaids']
  else:
  raise KeyError
  except KeyError:
  return ret","Add new media to multiple users. .. versionadded:: 2016.3.0 :param userids: ID of the user that uses the media :param active: Whether the media is enabled (0 enabled, 1 disabled) :param mediatypeid: ID of the media type used by the media :param period: Time when the notifications can be sent as a time period :param sendto: Address, user name or other identifier of the recipient :param severity: Trigger severities to send notifications about :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring) :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring) :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring) :return: IDs of the created media. CLI Example: .. code-block:: bash salt '*' zabbix.user_addmedia 4 active=0 mediatypeid=1 period='1-7,00:00-24:00' sendto='support2@example.com' severity=63",1,0,0,1,2,1,0,0,1,2
"def buscar_por_ip_ambiente(self, ip, id_environment):
  if not is_valid_int_param(id_environment):
  raise InvalidParameterError(
  u'Environment identifier is invalid or was not informed.')
  if not is_valid_ip(ip):
  raise InvalidParameterError(u'IP is invalid or was not informed.')
  url = 'ip/' + str(ip) + '/ambiente/' + str(id_environment) + '/'
  code, xml = self.submit(None, 'GET', url)
  return self.response(code, xml)","Get IP with an associated environment. :param ip: IP address in the format x1.x2.x3.x4. :param id_environment: Identifier of the environment. Integer value and greater than zero. :return: Dictionary with the following structure: :: {'ip': {'id': < id >, 'id_vlan': < id_vlan >, 'oct4': < oct4 >, 'oct3': < oct3 >, 'oct2': < oct2 >, 'oct1': < oct1 >, 'descricao': < descricao > }} :raise IpNaoExisteError: IP is not registered or not associated with environment. :raise InvalidParameterError: The environment identifier and/or IP is/are null or invalid. :raise DataBaseError: Networkapi failed to access the database.",2,0,1,1,4,1,0,0,1,2
"def add_library(self, name):
  libdoc = LibraryDocumentation(name)
  if len(libdoc.keywords) > 0:
  collection_id = self.add_collection(None, libdoc.name, libdoc.type,
  libdoc.doc, libdoc.version,
  libdoc.scope, libdoc.named_args,
  libdoc.doc_format)
  self._load_keywords(collection_id, libdoc=libdoc)","Add a library to the database This method is for adding a library by name (eg: ""BuiltIn"") rather than by a file.",1,0,0,1,2,1,0,0,1,2
"def accel_quit(self, *args):
  procs = self.notebook_manager.get_running_fg_processes_count()
  tabs = self.notebook_manager.get_n_pages()
  notebooks = self.notebook_manager.get_n_notebooks()
  prompt_cfg = self.settings.general.get_boolean('prompt-on-quit')
  prompt_tab_cfg = self.settings.general.get_int('prompt-on-close-tab')
  if prompt_cfg or (prompt_tab_cfg == 1 and procs > 0) or (prompt_tab_cfg == 2):
  log.debug(""Remaining procs=%r"", procs)
  if PromptQuitDialog(self.window, procs, tabs, notebooks).quit():
  log.info(""Quitting Guake"")
  Gtk.main_quit()
  else:
  log.info(""Quitting Guake"")
  Gtk.main_quit()",Callback to prompt the user whether to quit Guake or not.,1,0,0,0,1,1,0,0,1,2
"def _download_ned_source_metadata(
  self):
  self.log.debug('starting the ``_download_ned_source_metadata`` method')
  self.dbTableName = ""tcs_cat_ned_stream""
  total, batches = self._count_ned_sources_in_database_requiring_metadata()
  self.log.info(
  ""%(total)s galaxies require metadata. Need to send %(batches)s batch requests to NED."" % locals())
  totalBatches = self.batches
  thisCount = 0
  while self.total:
  thisCount += 1
  self._get_ned_sources_needing_metadata()
  self._do_ned_namesearch_queries_and_add_resulting_metadata_to_database(
  thisCount)
  self._count_ned_sources_in_database_requiring_metadata()
  self.log.debug(
  'completed the ``_download_ned_source_metadata`` method')
  return None",*Query NED using the names of the NED sources in our local database to retrieve extra metadata* *Usage:* .. code-block:: python stream._download_ned_source_metadata(),1,0,1,1,3,1,1,1,1,4
"async def set_chat_sticker_set(self, chat_id: typing.Union[base.Integer, base.String],
  sticker_set_name: base.String) -> base.Boolean:
  payload = generate_payload(**locals())
  result = await self.request(api.Methods.SET_CHAT_STICKER_SET, payload)
  return result","Use this method to set a new group sticker set for a supergroup. The bot must be an administrator in the chat for this to work and must have the appropriate admin rights. Use the field can_set_sticker_set optionally returned in getChat requests to check if the bot can use this method. Source: https://core.telegram.org/bots/api#setchatstickerset :param chat_id: Unique identifier for the target chat or username of the target supergroup :type chat_id: :obj:`typing.Union[base.Integer, base.String]` :param sticker_set_name: Name of the sticker set to be set as the group sticker set :type sticker_set_name: :obj:`base.String` :return: Returns True on success :rtype: :obj:`base.Boolean`",1,0,0,1,2,2,0,0,2,4
"def apiinfo_version(**kwargs):
  conn_args = _login(**kwargs)
  ret = {}
  try:
  if conn_args:
  method = 'apiinfo.version'
  params = {}
  ret = _query(method, params, conn_args['url'], conn_args['auth'])
  return ret['result']
  else:
  raise KeyError
  except KeyError:
  return False","Retrieve the version of the Zabbix API. .. versionadded:: 2016.3.0 :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring) :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring) :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring) :return: On success string with Zabbix API version, False on failure. CLI Example: .. code-block:: bash salt '*' zabbix.apiinfo_version",2,0,0,1,3,2,0,0,1,3
"def __get_interests(self):
  repos_of_interest = itertools.chain(
  self.user_starred_repositories,
  self.user_following_starred_repositories,
  )
  repo_descriptions = [repo.description for repo in repos_of_interest]
  return list(set(repo_descriptions))",Method to procure description of repositories the authenticated user is interested in. We currently attribute interest to: 1. The repositories the authenticated user has starred. 2. The repositories the users the authenticated user follows have starred. :return: List of repository descriptions.,1,0,0,1,2,1,0,0,1,2
"def request(self, url, method=None, data=None, headers=None, parser=None):
  assert self.access_token is not None
  parser = parser or loads
  if not method:
  method = 'GET' if not data else 'POST'
  req = self.token_transport('{0}{1}'.format(self.resource_endpoint,
  url), self.access_token, data=data, method=method, headers=headers)
  resp = urlopen(req)
  data = resp.read()
  try:
  return parser(data.decode(resp.info().get_content_charset() or
  'utf-8'))
  except UnicodeDecodeError:
  return parser(data)",Request user data from the resource endpoint :param url: The path to the resource and querystring if required :param method: HTTP method. Defaults to ``GET`` unless data is not None in which case it defaults to ``POST`` :param data: Data to be POSTed to the resource endpoint :param parser: Parser callback to deal with the returned data. Defaults to ``json.loads`.`,2,0,0,1,3,2,0,0,1,3
"def insert_object_into_db_pk_known(self,
  obj: Any,
  table: str,
  fieldlist: Sequence[str]) -> None:
  pkvalue = getattr(obj, fieldlist[0])
  if pkvalue is None:
  raise AssertionError(""insert_object_intoto_db_pk_known called ""
  ""without PK"")
  valuelist = []
  for f in fieldlist:
  valuelist.append(getattr(obj, f))
  self.db_exec(
  get_sql_insert(table, fieldlist, self.get_delims()),
  *valuelist
  )","Inserts object into database table, with PK (first field) already known.",0,1,0,0,1,1,1,1,1,4
"def console_input(default, validation=None, allow_empty=False):
  value = raw_input(""> "") or default
  if value == """" and not allow_empty:
  print ""Invalid: Empty value is not permitted.""
  return console_input(default, validation)
  if validation:
  try:
  return validation(value)
  except ValidationError, e:
  print ""Invalid: "", e
  return console_input(default, validation)
  return value",Get user input value from stdin Parameters ---------- default : string A default value. It will be used when user input nothing. validation : callable A validation function. The validation function must raise an error when validation has failed. Returns ------- string or any A user input string or validated value,1,0,0,1,2,1,0,0,1,2
"def has_perm(self, user, perm, obj=None, *args, **kwargs):
  try:
  if not self._obj_ok(obj):
  if hasattr(obj, 'get_permissions_object'):
  obj = obj.get_permissions_object(perm)
  else:
  raise InvalidPermissionObjectException
  return user.permset_tree.allow(Action(perm), obj)
  except ObjectDoesNotExist:
  return False",Test user permissions for a single action and object. :param user: The user to test. :type user: ``User`` :param perm: The action to test. :type perm: ``str`` :param obj: The object path to test. :type obj: ``tutelary.engine.Object`` :returns: ``bool`` -- is the action permitted?,1,0,1,1,3,1,0,1,1,3
"def connect(server=None, url=None, ip=None, port=None, https=None, verify_ssl_certificates=None, auth=None,
  proxy=None, cookies=None, verbose=True, config=None):
  global h2oconn
  if config:
  if ""connect_params"" in config:
  h2oconn = _connect_with_conf(config[""connect_params""])
  else:
  h2oconn = _connect_with_conf(config)
  else:
  h2oconn = H2OConnection.open(server=server, url=url, ip=ip, port=port, https=https,
  auth=auth, verify_ssl_certificates=verify_ssl_certificates,
  proxy=proxy, cookies=cookies,
  verbose=verbose)
  if verbose:
  h2oconn.cluster.show_status()
  return h2oconn","Connect to an existing H2O server, remote or local. There are two ways to connect to a server: either pass a `server` parameter containing an instance of an H2OLocalServer, or specify `ip` and `port` of the server that you want to connect to. :param server: An H2OLocalServer instance to connect to (optional). :param url: Full URL of the server to connect to (can be used instead of `ip` + `port` + `https`). :param ip: The ip address (or host name) of the server where H2O is running. :param port: Port number that H2O service is listening to. :param https: Set to True to connect via https:// instead of http://. :param verify_ssl_certificates: When using https, setting this to False will disable SSL certificates verification. :param auth: Either a (username, password) pair for basic authentication, an instance of h2o.auth.SpnegoAuth or one of the requests.auth authenticator objects. :param proxy: Proxy server address. :param cookies: Cookie (or list of) to add to request :param verbose: Set to False to disable printing connection status messages. :param connection_conf: Connection configuration object encapsulating connection parameters. :returns: the new :class:`H2OConnection` object.",1,0,0,1,2,1,0,0,1,2
"def selected_canvas_hazlayer(self):
  if self.lstCanvasHazLayers.selectedItems():
  item = self.lstCanvasHazLayers.currentItem()
  else:
  return None
  try:
  layer_id = item.data(Qt.UserRole)
  except (AttributeError, NameError):
  layer_id = None
  layer = QgsProject.instance().mapLayer(layer_id)
  return layer",Obtain the canvas layer selected by user. :returns: The currently selected map layer in the list. :rtype: QgsMapLayer,1,0,1,0,2,1,0,0,1,2
"def requestCheckDockerIo(origAppliance, imageName, tag):
  if '/' not in imageName:
  imageName = 'library/' + imageName
  token_url = 'https://auth.docker.io/token?service=registry.docker.io&scope=repository:{repo}:pull'.format(repo=imageName)
  requests_url = 'https://registry-1.docker.io/v2/{repo}/manifests/{tag}'.format(repo=imageName, tag=tag)
  token = requests.get(token_url)
  jsonToken = token.json()
  bearer = jsonToken[""token""]
  response = requests.head(requests_url, headers={'Authorization': 'Bearer {}'.format(bearer)})
  if not response.ok:
  raise ApplianceImageNotFound(origAppliance, requests_url, response.status_code)
  else:
  return origAppliance","Checks docker.io to see if an image exists using the requests library. URL is based on the docker v2 schema. Requires that an access token be fetched first. :param str origAppliance: The full url of the docker image originally specified by the user (or the default). e.g. ""ubuntu:latest"" :param str imageName: The image, including path and excluding the tag. e.g. ""ubuntu"" :param str tag: The tag used at that docker image's registry. e.g. ""latest"" :return: Return True if match found. Raise otherwise.",1,0,0,1,2,2,0,0,1,3
"def modify(self, **kwargs):
  for key in kwargs:
  if key not in ['email', 'cellphone', 'countrycode', 'countryiso',
  'defaultsmsprovider', 'directtwitter',
  'twitteruser', 'name']:
  sys.stderr.write(""'%s'"" % key + ' is not a valid argument ' +
  'of <PingdomContact>.modify()\n')
  response = self.pingdom.request('PUT', 'notification_contacts/%s' % self.id, kwargs)
  return response.json()['message']","Modify a contact. Returns status message Optional Parameters: * name -- Contact name Type: String * email -- Contact email address Type: String * cellphone -- Cellphone number, without the country code part. In some countries you are supposed to exclude leading zeroes. (Requires countrycode and countryiso) Type: String * countrycode -- Cellphone country code (Requires cellphone and countryiso) Type: String * countryiso -- Cellphone country ISO code. For example: US (USA), GB (Britain) or SE (Sweden) (Requires cellphone and countrycode) Type: String * defaultsmsprovider -- Default SMS provider Type: String ['clickatell', 'bulksms', 'esendex', 'cellsynt'] * directtwitter -- Send tweets as direct messages Type: Boolean Default: True * twitteruser -- Twitter user Type: String",1,1,0,2,4,1,0,0,1,2
"def delete(request, message_id, success_url=None):
  user = request.user
  now = timezone.now()
  message = get_object_or_404(Message, id=message_id)
  deleted = False
  if success_url is None:
  success_url = reverse('messages_inbox')
  if 'next' in request.GET:
  success_url = request.GET['next']
  if message.sender == user:
  message.sender_deleted_at = now
  deleted = True
  if message.recipient == user:
  message.recipient_deleted_at = now
  deleted = True
  if deleted:
  message.save()
  messages.info(request, _(u""Message successfully deleted.""))
  if notification:
  notification.send([user], ""messages_deleted"", {'message': message,})
  return HttpResponseRedirect(success_url)
  raise Http404","Marks a message as deleted by sender or recipient. The message is not really removed from the database, because two users must delete a message before it's save to remove it completely. A cron-job should prune the database and remove old messages which are deleted by both users. As a side effect, this makes it easy to implement a trash with undelete. You can pass ?next=/foo/bar/ via the url to redirect the user to a different page (e.g. `/foo/bar/`) than ``success_url`` after deletion of the message.",1,1,1,1,4,1,1,1,1,4
"def load_from_remote(remote_name, owner=None):
  from .. import GMQLDataset
  pmg = get_python_manager()
  remote_manager = get_remote_manager()
  parser = remote_manager.get_dataset_schema(remote_name, owner)
  source_table = get_source_table()
  id = source_table.search_source(remote=remote_name)
  if id is None:
  id = source_table.add_source(remote=remote_name, parser=parser)
  index = pmg.read_dataset(str(id), parser.get_gmql_parser())
  remote_sources = [id]
  return GMQLDataset.GMQLDataset(index=index, location=""remote"", path_or_name=remote_name,
  remote_sources=remote_sources)","Loads the data from a remote repository. :param remote_name: The name of the dataset in the remote repository :param owner: (optional) The owner of the dataset. If nothing is provided, the current user is used. For public datasets use 'public'. :return: A new GMQLDataset or a GDataframe",1,1,1,1,4,1,0,1,1,3
"def push(cwd,
  remote=None,
  ref=None,
  opts='',
  git_opts='',
  user=None,
  password=None,
  identity=None,
  ignore_retcode=False,
  saltenv='base',
  output_encoding=None,
  **kwargs):
  kwargs = salt.utils.args.clean_kwargs(**kwargs)
  if kwargs:
  salt.utils.args.invalid_kwargs(kwargs)
  cwd = _expand_path(cwd, user)
  command = ['git'] + _format_git_opts(git_opts)
  command.append('push')
  command.extend(_format_opts(opts))
  command.extend([remote, ref])
  return _git_run(command,
  cwd=cwd,
  user=user,
  password=password,
  identity=identity,
  ignore_retcode=ignore_retcode,
  saltenv=saltenv,
  output_encoding=output_encoding)['stdout']","Interface to `git-push(1)`_ cwd The path to the git checkout remote Name of the remote to which the ref should being pushed .. versionadded:: 2015.8.0 ref : master Name of the ref to push .. note:: Being a refspec_, this argument can include a colon to define local and remote ref names. opts Any additional options to add to the command line, in a single string .. note:: On the Salt CLI, if the opts are preceded with a dash, it is necessary to precede them with ``opts=`` (as in the CLI examples below) to avoid causing errors with Salt's own argument parsing. git_opts Any additional options to add to git command itself (not the ``push`` subcommand), in a single string. This is useful for passing ``-c`` to run git with temporary changes to the git configuration. .. versionadded:: 2017.7.0 .. note:: This is only supported in git 1.7.2 and newer. user User under which to run the git command. By default, the command is run by the user under which the minion is running. password Windows only. Required when specifying ``user``. This parameter will be ignored on non-Windows platforms. .. versionadded:: 2016.3.4 identity Path to a private key to use for ssh URLs .. warning:: Unless Salt is invoked from the minion using ``salt-call``, the key(s) must be passphraseless. For greater security with passphraseless private keys, see the `sshd(8)`_ manpage for information on securing the keypair from the remote side in the ``authorized_keys`` file. .. _`sshd(8)`: http://www.man7.org/linux/man-pages/man8/sshd.8.html#AUTHORIZED_KEYS_FILE_FORMAT .. versionchanged:: 2015.8.7 Salt will no longer attempt to use passphrase-protected keys unless invoked from the minion using ``salt-call``, to prevent blocking waiting for user input. Key can also be specified as a SaltStack file server URL, eg. salt://location/identity_file .. versionchanged:: 2016.3.0 ignore_retcode : False If ``True``, do not log an error to the minion log if the git command returns a nonzero exit status. .. versionadded:: 2015.8.0 saltenv The default salt environment to pull sls files from .. versionadded:: 2016.3.1 output_encoding Use this option to specify which encoding to use to decode the output from any git commands which are run. This should not be needed in most cases. .. note:: This should only be needed if the files in the repository were created with filenames using an encoding other than UTF-8 to handle Unicode characters. .. versionadded:: 2018.3.1 .. _`git-push(1)`: http://git-scm.com/docs/git-push .. _refspec: http://git-scm.com/book/en/v2/Git-Internals-The-Refspec CLI Example: .. code-block:: bash # Push master as origin/master salt myminion git.push /path/to/repo origin master # Push issue21 as upstream/develop salt myminion git.push /path/to/repo upstream issue21:develop # Delete remote branch 'upstream/temp' salt myminion git.push /path/to/repo upstream :temp",1,0,0,2,3,1,0,0,1,2
"def create_session(self, alias, url, headers={}, cookies={},
  auth=None, timeout=None, proxies=None,
  verify=False, debug=0, max_retries=3, backoff_factor=0.10, disable_warnings=0):
  auth = requests.auth.HTTPBasicAuth(*auth) if auth else None
  logger.info('Creating Session using : alias=%s, url=%s, headers=%s, \
  cookies=%s, auth=%s, timeout=%s, proxies=%s, verify=%s, \
  debug=%s ' % (alias, url, headers, cookies, auth, timeout,
  proxies, verify, debug))
  return self._create_session(
  alias,
  url,
  headers,
  cookies,
  auth,
  timeout,
  max_retries,
  backoff_factor,
  proxies,
  verify,
  debug,
  disable_warnings)",Create Session: create a HTTP session to a server ``url`` Base url of the server ``alias`` Robot Framework alias to identify the session ``headers`` Dictionary of default headers ``cookies`` Dictionary of cookies ``auth`` List of username & password for HTTP Basic Auth ``timeout`` Connection timeout ``proxies`` Dictionary that contains proxy urls for HTTP and HTTPS communication ``verify`` Whether the SSL cert will be verified. A CA_BUNDLE path can also be provided. Defaults to False. ``debug`` Enable http verbosity option more information https://docs.python.org/2/library/httplib.html#httplib.HTTPConnection.set_debuglevel ``max_retries`` The maximum number of retries each connection should attempt. ``backoff_factor`` The pause between for each retry ``disable_warnings`` Disable requests warning useful when you have large number of testcases,2,0,0,0,2,1,0,0,1,2
"def iter_issues(self,
  milestone=None,
  state=None,
  assignee=None,
  mentioned=None,
  labels=None,
  sort=None,
  direction=None,
  since=None,
  number=-1,
  etag=None):
  url = self._build_url('issues', base_url=self._api)
  params = {'assignee': assignee, 'mentioned': mentioned}
  if milestone in ('*', 'none') or isinstance(milestone, int):
  params['milestone'] = milestone
  self._remove_none(params)
  params.update(
  issue_params(None, state, labels, sort, direction,
  since)
  )
  return self._iter(int(number), url, Issue, params, etag)","Iterate over issues on this repo based upon parameters passed. .. versionchanged:: 0.9.0 The ``state`` parameter now accepts 'all' in addition to 'open' and 'closed'. :param int milestone: (optional), 'none', or '*' :param str state: (optional), accepted values: ('all', 'open', 'closed') :param str assignee: (optional), 'none', '*', or login name :param str mentioned: (optional), user's login name :param str labels: (optional), comma-separated list of labels, e.g. 'bug,ui,@high' :param sort: (optional), accepted values: ('created', 'updated', 'comments', 'created') :param str direction: (optional), accepted values: ('asc', 'desc') :param since: (optional), Only issues after this date will be returned. This can be a `datetime` or an `ISO8601` formatted date string, e.g., 2012-05-20T23:10:27Z :type since: datetime or string :param int number: (optional), Number of issues to return. By default all issues are returned :param str etag: (optional), ETag from a previous request to the same endpoint :returns: generator of :class:`Issue <github3.issues.issue.Issue>`\ s",1,0,0,1,2,2,0,0,1,3
"def createPage(self, title, author_name=None, author_url=None,
  content=None, html_content=None, return_content=False):
  if content is None:
  content = html_to_nodes(html_content)
  return self.make_method(""createPage"", {
  ""access_token"": self.access_token,
  ""title"": title,
  ""author_name"": author_name,
  ""author_url"": author_url,
  ""content"": json.dumps(content),
  ""return_content"": return_content
  })","Use this method to create a new Telegraph page. :param title: Required. Page title. :type title: str :param author_name: Optional. Author name, displayed below the article's title. :type author_name: str :param author_url: Optional. Profile link, opened when users click on the author's name below the title. Can be any link, not necessarily to a Telegram profile or channel. :type author_url: str :param content: Required. Content of the page in NODES format. :param html_content: Optional. Content of the page in HTML format. :type html_content: str :param return_content: Optional. If true, a content field will be returned in the Page object :type return_content: bool :returns: Page object.",1,0,0,1,2,2,0,0,1,3
"def list_credentials(self, user=None):
  if not user:
  user = self.user
  user_id = utils.get_id(user)
  uri = ""users/%s/OS-KSADM/credentials"" % user_id
  resp, resp_body = self.method_get(uri)
  return resp_body.get(""credentials"")","Returns a user's non-password credentials. If no user is specified, the credentials for the currently authenticated user are returned. You cannot retrieve passwords by this or any other means.",2,0,1,1,4,2,0,0,1,3
"def get_erasure_profile(service, name):
  try:
  out = check_output(['ceph', '--id', service,
  'osd', 'erasure-code-profile', 'get',
  name, '--format=json'])
  if six.PY3:
  out = out.decode('UTF-8')
  return json.loads(out)
  except (CalledProcessError, OSError, ValueError):
  return None",:param service: six.string_types. The Ceph user name to run the command under :param name: :return:,1,0,1,1,3,1,0,0,1,2
"def _apply_auth(self, view_func):
  @functools.wraps(view_func)
  def decorated(*args, **kwargs):
  if not self.auth:
  return view_func(*args, **kwargs)
  auth_data = self.auth.get_authorization()
  if auth_data is None:
  return self._handle_authentication_error()
  if not self._authentication_callback or not self._authentication_callback(auth_data):
  return self._handle_authentication_error()
  return view_func(*args, **kwargs)
  return decorated",Apply decorator to authenticate the user who is making the request. :param view_func: The flask view func.,0,0,0,1,1,1,0,0,1,2
"def _ExpandUsersHomeDirectoryPathSegments(
  cls, path_segments, path_separator, user_accounts):
  if not path_segments:
  return []
  user_paths = []
  first_path_segment = path_segments[0].lower()
  if first_path_segment not in ('%%users.homedir%%', '%%users.userprofile%%'):
  if cls._IsWindowsDrivePathSegment(path_segments[0]):
  path_segments[0] = ''
  user_path = path_separator.join(path_segments)
  user_paths.append(user_path)
  else:
  for user_account in user_accounts:
  user_path_segments = user_account.GetUserDirectoryPathSegments()
  if not user_path_segments:
  continue
  if cls._IsWindowsDrivePathSegment(user_path_segments[0]):
  user_path_segments[0] = ''
  if not user_path_segments[-1]:
  user_path_segments.pop()
  user_path_segments.extend(path_segments[1:])
  user_path = path_separator.join(user_path_segments)
  user_paths.append(user_path)
  return user_paths","Expands a path to contain all users home or profile directories. Expands the artifacts path variable ""%%users.homedir%%"" or ""%%users.userprofile%%"". Args: path_segments (list[str]): path segments. path_separator (str): path segment separator. user_accounts (list[UserAccountArtifact]): user accounts. Returns: list[str]: paths returned for user accounts without a drive indicator.",1,0,0,1,2,1,0,0,1,2
"def send_keys(self, cmd, enter=True, suppress_history=True, literal=False):
  prefix = ' ' if suppress_history else ''
  if literal:
  self.cmd('send-keys', '-l', prefix + cmd)
  else:
  self.cmd('send-keys', prefix + cmd)
  if enter:
  self.enter()","``$ tmux send-keys`` to the pane. A leading space character is added to cmd to avoid polluting the user's history. Parameters ---------- cmd : str Text or input into pane enter : bool, optional Send enter after sending the input, default True. suppress_history : bool, optional Don't add these keys to the shell history, default True. literal : bool, optional Send keys literally, default True.",1,0,0,0,1,1,0,0,1,2
"def genUserCert(self, name, signas=None, outp=None, csr=None):
  pkey, cert = self._genBasePkeyCert(name, pkey=csr)
  cert.add_extensions([
  crypto.X509Extension(b'nsCertType', False, b'client'),
  crypto.X509Extension(b'keyUsage', False, b'digitalSignature'),
  crypto.X509Extension(b'extendedKeyUsage', False, b'clientAuth'),
  crypto.X509Extension(b'basicConstraints', False, b'CA:FALSE'),
  ])
  if signas is not None:
  self.signCertAs(cert, signas)
  else:
  self.selfSignCert(cert, pkey)
  crtpath = self._saveCertTo(cert, 'users', '%s.crt' % name)
  if outp is not None:
  outp.printf('cert saved: %s' % (crtpath,))
  if not pkey._only_public:
  keypath = self._savePkeyTo(pkey, 'users', '%s.key' % name)
  if outp is not None:
  outp.printf('key saved: %s' % (keypath,))
  return pkey, cert","Generates a user keypair. Args: name (str): The name of the user keypair. signas (str): The CA keypair to sign the new user keypair with. outp (synapse.lib.output.Output): The output buffer. csr (OpenSSL.crypto.PKey): The CSR public key when generating the keypair from a CSR. Examples: Generate a user cert for the user ""myuser"": myuserkey, myusercert = cdir.genUserCert('myuser') Returns: ((OpenSSL.crypto.PKey, OpenSSL.crypto.X509)): Tuple containing the key and certificate objects.",1,0,0,1,2,1,0,0,1,2
"def get_metadata_display(self, field_formats = {}, escape=True):
  def field_format(field):
  if field in field_formats:
  return field_formats[field]
  return u'%s'
  t = join_formatted('', self.title, format=field_format('title'), escape=escape)
  t = join_formatted(t, self.description, u'%s: %s', escape=escape)
  if self.publish_author:
  t = join_formatted(t, self.author, u'%s' + u'  ' + u'Author: %s', u'%s' + u'Author: %s', escape=escape)
  if self.publish_copyright:
  t = join_formatted(t, self.copyright, u'%s, %s', escape=escape)
  if self.publish_date_time and self.date_time:
  date_time_formatted = dateformat.format(self.date_time, get_format('DATE_FORMAT'))
  t = join_formatted(t, date_time_formatted, u'%s (%s)', '%s%s', escape=escape)
  return t","Returns object metadata that has been selected to be displayed to users, compiled as a string.",0,0,0,1,1,1,0,0,1,2
"def run(self, args):
  email = args.email
  username = args.username
  force_send = args.resend
  auth_role = args.auth_role
  msg_file = args.msg_file
  message = read_argument_file_contents(msg_file)
  print(""Sharing project."")
  to_user = self.remote_store.lookup_or_register_user_by_email_or_username(email, username)
  try:
  project = self.fetch_project(args, must_exist=True, include_children=False)
  dest_email = self.service.share(project, to_user, force_send, auth_role, message)
  print(""Share email message sent to "" + dest_email)
  except D4S2Error as ex:
  if ex.warning:
  print(ex.message)
  else:
  raise",Gives user permission based on auth_role arg and sends email to that user. :param args Namespace arguments parsed from the command line,1,1,1,1,4,1,0,1,1,3
"def dump(self, password, db, format_='zip'):
  args = [password, db]
  if v(self._odoo.version)[0] >= 9:
  args.append(format_)
  data = self._odoo.json(
  '/jsonrpc',
  {'service': 'db',
  'method': 'dump',
  'args': args})
  result = encode2bytes(data['result'])
  content = base64.standard_b64decode(result)
  return io.BytesIO(content)","Backup the `db` database. Returns the dump as a binary ZIP file containing the SQL dump file alongside the filestore directory (if any). >>> dump = odoo.db.dump('super_admin_passwd', 'prod') # doctest: +SKIP .. doctest:: :hide: >>> dump = odoo.db.dump(SUPER_PWD, DB) If you get a timeout error, increase this one before performing the request: >>> timeout_backup = odoo.config['timeout'] >>> odoo.config['timeout'] = 600 # Timeout set to 10 minutes >>> dump = odoo.db.dump('super_admin_passwd', 'prod') # doctest: +SKIP >>> odoo.config['timeout'] = timeout_backup Write it on the file system: .. doctest:: :options: +SKIP >>> with open('dump.zip', 'wb') as dump_zip: ... dump_zip.write(dump.read()) ... .. doctest:: :hide: >>> with open('dump.zip', 'wb') as dump_zip: ... fileno = dump_zip.write(dump.read()) # Python 3 ... You can manipulate the file with the `zipfile` module for instance: .. doctest:: :options: +SKIP >>> import zipfile >>> zipfile.ZipFile('dump.zip').namelist() ['dump.sql', 'filestore/ef/ef2c882a36dbe90fc1e7e28d816ad1ac1464cfbb', 'filestore/dc/dcf00aacce882bbfd117c0277e514f829b4c5bf0', ...] .. doctest:: :hide: >>> import zipfile >>> zipfile.ZipFile('dump.zip').namelist() # doctest: +NORMALIZE_WHITESPACE ['dump.sql'...'filestore/...'...] The super administrator password is required to perform this method. *Python 2:* :return: `io.BytesIO` :raise: :class:`odoorpc.error.RPCError` (access denied / wrong database) :raise: `urllib2.URLError` (connection error) *Python 3:* :return: `io.BytesIO` :raise: :class:`odoorpc.error.RPCError` (access denied / wrong database) :raise: `urllib.error.URLError` (connection error)",2,0,1,1,4,1,0,0,1,2
"def restrict_chat_member(chat_id, user_id, until_date=None,
  can_send_messages=None, can_send_media_messages=None, can_send_other_messages=None, can_add_web_page_previews=None, **kwargs):
  params = dict(
  chat_id=chat_id,
  user_id=user_id,
  )
  params.update(
  _clean_params(
  until_date=until_date,
  can_send_messages=can_send_messages,
  can_send_media_messages=can_send_media_messages,
  can_send_other_messages=can_send_other_messages,
  can_add_web_page_previews=can_add_web_page_previews
  )
  )
  return TelegramBotRPCRequest('restrictChatMember', params=params, on_result=lambda result: result, **kwargs)","Use this method to kick a user from a group or a supergroup. In the case of supergroups, the user will not be able to return to the group on their own using invite links, etc., unless unbanned first. The bot must be an administrator in the group for this to work. Returns True on success. Note: This will method only work if the All Members Are Admins setting is off in the target group. Otherwise members may only be removed by the group's creator or by the member that added them. :param chat_id: Unique identifier for the target chat or username of the target channel (in the format @channelusername) :param user_id: Unique identifier of the target user :param until_date: *Optional.* Date when the user will be unbanned, unix time. If user is banned for more than 366 days or less than 30 seconds from the current time they are considered to be banned forever :param can_send_messages: *Optional.* Pass True, if the user can send text messages, contacts, locations and venues :param can_send_media_message: *Optional.* Pass True, if the user can send audios, documents, photos, videos, video notes and voice notes, implies can_send_messages :param can_send_other_messages: *Optional.* Pass True, if the user can send animations, games, stickers and use inline bots, implies can_send_media_messages :param can_add_web_page_previews: *Optional.* Pass True, if the user may add web page previews to their messages, implies can_send_media_messages :param kwargs: Args that get passed down to :class:`TelegramBotRPCRequest` :type chat_id: int or str :type user_id: int :returns: Returns True on success. :rtype: bool",1,0,0,1,2,1,0,0,1,2
"def is_http_request_sender(self):
  try:
  request = threadlocals.request()
  if request and request.user and request.user.is_authenticated:
  requesting_user_id = request.user.id
  return str(requesting_user_id) == str(self.user.id)
  except (AttributeError, KeyError) as e:
  logger.error(""Could not check request sender: {}"".format(e))
  return False
  return False",Checks if a user the HTTP request sender (accessing own info) Used primarily to load private personal information from the cache. (A student should see all info on his or her own profile regardless of how the permissions are set.) Returns: Boolean,1,0,0,0,1,1,0,0,1,2
"def alter_db(name, character_set=None, collate=None, **connection_args):
  dbc = _connect(**connection_args)
  if dbc is None:
  return []
  cur = dbc.cursor()
  existing = db_get(name, **connection_args)
  qry = 'ALTER DATABASE `{0}` CHARACTER SET {1} COLLATE {2};'.format(
  name.replace('%', r'\%').replace('_', r'\_'),
  character_set or existing.get('character_set'),
  collate or existing.get('collate'))
  args = {}
  _execute(cur, qry, args)",Modify database using ``ALTER DATABASE %(dbname)s CHARACTER SET %(charset)s COLLATE %(collation)s;`` query. CLI Example: .. code-block:: bash salt '*' mysql.alter_db testdb charset='latin1',1,1,1,0,3,1,1,1,1,4
"def get_display_name(self, mxit_id, scope='profile/public'):
  display_name = _get(
  token=self.oauth.get_app_token(scope),
  uri='/user/public/displayname/' + urllib.quote(mxit_id)
  )
  if display_name.startswith('""') and display_name.endswith('""'):
  display_name = display_name[1:-1]
  return display_name",Retrieve the Mxit user's display name No user authentication required,2,0,0,1,3,2,0,0,1,3
"def user_variables(self, names):
  out = {}
  user_ns = self.user_ns
  for varname in names:
  try:
  value = repr(user_ns[varname])
  except:
  value = self._simple_error()
  out[varname] = value
  return out","Get a list of variable names from the user's namespace. Parameters ---------- names : list of strings A list of names of variables to be read from the user namespace. Returns ------- A dict, keyed by the input names and with the repr() of each value.",1,0,0,1,2,1,0,0,1,2
"def get_structure_from_mp(formula):
  m = MPRester()
  entries = m.get_entries(formula, inc_structure=""final"")
  if len(entries) == 0:
  raise ValueError(""No structure with formula %s in Materials Project!"" %
  formula)
  elif len(entries) > 1:
  warnings.warn(""%d structures with formula %s found in Materials ""
  ""Project. The lowest energy structure will be returned."" %
  (len(entries), formula))
  return min(entries, key=lambda e: e.energy_per_atom).structure",Convenience method to get a crystal from the Materials Project database via the API. Requires PMG_MAPI_KEY to be set. Args: formula (str): A formula Returns: (Structure) The lowest energy structure in Materials Project with that formula.,1,0,1,1,3,1,0,0,1,2
"def get_show_function_result(self, ddoc_id, show_name, doc_id):
  ddoc = DesignDocument(self, ddoc_id)
  headers = {'Content-Type': 'application/json'}
  resp = get_docs(self.r_session,
  '/'.join([ddoc.document_url, '_show', show_name, doc_id]),
  self.client.encoder,
  headers)
  return resp.text","Retrieves a formatted document from the specified database based on the show function provided. Show functions, for example, are used when you want to access Cloudant directly from a browser, and need data to be returned in a different format, such as HTML. For example: .. code-block:: python # Assuming that 'view001' exists as part of the # 'ddoc001' design document in the remote database... # Retrieve a formatted 'doc001' document where the show function is 'show001' resp = db.get_show_function_result('ddoc001', 'show001', 'doc001') for row in resp['rows']: # Process data (in text format). For more detail on show functions, refer to the `Cloudant show documentation <https://console.bluemix.net/docs/services/Cloudant/api/ design_documents.html#show-functions>`_. :param str ddoc_id: Design document id used to get the result. :param str show_name: Name used in part to identify the show function. :param str doc_id: The ID of the document to show. :return: Formatted document result data in text format",2,0,1,1,4,1,0,0,1,2
"def auth_userpass(self, username, password, mount_point='userpass', use_token=True, **kwargs):
  params = {
  'password': password,
  }
  params.update(kwargs)
  return self.login('/v1/auth/{0}/login/{1}'.format(mount_point, username), json=params, use_token=use_token)",POST /auth/<mount point>/login/<username> :param username: :type username: :param password: :type password: :param mount_point: :type mount_point: :param use_token: :type use_token: :param kwargs: :type kwargs: :return: :rtype:,1,0,0,1,2,2,0,0,1,3
"def calc_pvalues(query, gene_sets, background=20000, **kwargs):
  k = len(query)
  query = set(query)
  vals = []
  if isinstance(background, set):
  bg = len(background)
  query = query.intersection(background)
  elif isinstance(background, int):
  bg = background
  else:
  raise ValueError(""background should be set or int object"")
  subsets = sorted(gene_sets.keys())
  for s in subsets:
  category = gene_sets.get(s)
  m = len(category)
  hits = query.intersection(set(category))
  x = len(hits)
  if x < 1 : continue
  vals.append((s, hypergeom.sf(x-1, bg, m, k), x, m, hits))
  return zip(*vals)","calculate pvalues for all categories in the graph :param set query: set of identifiers for which the p value is calculated :param dict gene_sets: gmt file dict after background was set :param set background: total number of genes in your annotated database. :returns: pvalues x: overlapped gene number n: length of gene_set which belongs to each terms hits: overlapped gene names. For 2*2 contingency table: ============================================================================= | in query | not in query | row total => in gene_set | a | b | a+b => not in gene_set | c | d | c+d column total | a+b+c+d = anno database ============================================================================= background genes number = a + b + c + d. Then, in R x=a the number of white balls drawn without replacement from an urn which contains both black and white balls. m=a+b the number of white balls in the urn n=c+d the number of black balls in the urn k=a+c the number of balls drawn from the urn In Scipy: for args in scipy.hypergeom.sf(k, M, n, N, loc=0): M: the total number of objects, n: the total number of Type I objects. k: the random variate represents the number of Type I objects in N drawn without replacement from the total population. Therefore, these two functions are the same when using parameters from 2*2 table: R: > phyper(x-1, m, n, k, lower.tail=FALSE) Scipy: >>> hypergeom.sf(x-1, m+n, m, k)",0,0,0,1,1,1,0,0,1,2
"def read_namespaced_secret(self, name, namespace, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.read_namespaced_secret_with_http_info(name, namespace, **kwargs)
  else:
  (data) = self.read_namespaced_secret_with_http_info(name, namespace, **kwargs)
  return data","read_namespaced_secret # noqa: E501 read the specified Secret # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.read_namespaced_secret(name, namespace, async_req=True) >>> result = thread.get() :param async_req bool :param str name: name of the Secret (required) :param str namespace: object name and auth scope, such as for teams and projects (required) :param str pretty: If 'true', then the output is pretty printed. :param bool exact: Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. :param bool export: Should this value be exported. Export strips fields that a user can not specify. :return: V1Secret If the method is called asynchronously, returns the request thread.",0,0,1,1,2,2,0,0,1,3
"def restore_from_cluster_snapshot(ClusterIdentifier=None, SnapshotIdentifier=None, SnapshotClusterIdentifier=None, Port=None, AvailabilityZone=None, AllowVersionUpgrade=None, ClusterSubnetGroupName=None, PubliclyAccessible=None, OwnerAccount=None, HsmClientCertificateIdentifier=None, HsmConfigurationIdentifier=None, ElasticIp=None, ClusterParameterGroupName=None, ClusterSecurityGroups=None, VpcSecurityGroupIds=None, PreferredMaintenanceWindow=None, AutomatedSnapshotRetentionPeriod=None, KmsKeyId=None, NodeType=None, EnhancedVpcRouting=None, AdditionalInfo=None, IamRoles=None):
  pass","Creates a new cluster from a snapshot. By default, Amazon Redshift creates the resulting cluster with the same configuration as the original cluster from which the snapshot was created, except that the new cluster is created with the default cluster security and parameter groups. After Amazon Redshift creates the cluster, you can use the ModifyCluster API to associate a different security group and different parameter group with the restored cluster. If you are using a DS node type, you can also choose to change to another DS node type of the same size during restore. If you restore a cluster into a VPC, you must provide a cluster subnet group where you want the cluster restored. For more information about working with snapshots, go to Amazon Redshift Snapshots in the Amazon Redshift Cluster Management Guide . See also: AWS API Documentation :example: response = client.restore_from_cluster_snapshot( ClusterIdentifier='string', SnapshotIdentifier='string', SnapshotClusterIdentifier='string', Port=123, AvailabilityZone='string', AllowVersionUpgrade=True|False, ClusterSubnetGroupName='string', PubliclyAccessible=True|False, OwnerAccount='string', HsmClientCertificateIdentifier='string', HsmConfigurationIdentifier='string', ElasticIp='string', ClusterParameterGroupName='string', ClusterSecurityGroups=[ 'string', ], VpcSecurityGroupIds=[ 'string', ], PreferredMaintenanceWindow='string', AutomatedSnapshotRetentionPeriod=123, KmsKeyId='string', NodeType='string', EnhancedVpcRouting=True|False, AdditionalInfo='string', IamRoles=[ 'string', ] ) :type ClusterIdentifier: string :param ClusterIdentifier: [REQUIRED] The identifier of the cluster that will be created from restoring the snapshot. Constraints: Must contain from 1 to 63 alphanumeric characters or hyphens. Alphabetic characters must be lowercase. First character must be a letter. Cannot end with a hyphen or contain two consecutive hyphens. Must be unique for all clusters within an AWS account. :type SnapshotIdentifier: string :param SnapshotIdentifier: [REQUIRED] The name of the snapshot from which to create the new cluster. This parameter isn't case sensitive. Example: my-snapshot-id :type SnapshotClusterIdentifier: string :param SnapshotClusterIdentifier: The name of the cluster the source snapshot was created from. This parameter is required if your IAM user has a policy containing a snapshot resource element that specifies anything other than * for the cluster name. :type Port: integer :param Port: The port number on which the cluster accepts connections. Default: The same port as the original cluster. Constraints: Must be between 1115 and 65535 . :type AvailabilityZone: string :param AvailabilityZone: The Amazon EC2 Availability Zone in which to restore the cluster. Default: A random, system-chosen Availability Zone. Example: us-east-1a :type AllowVersionUpgrade: boolean :param AllowVersionUpgrade: If true , major version upgrades can be applied during the maintenance window to the Amazon Redshift engine that is running on the cluster. Default: true :type ClusterSubnetGroupName: string :param ClusterSubnetGroupName: The name of the subnet group where you want to cluster restored. A snapshot of cluster in VPC can be restored only in VPC. Therefore, you must provide subnet group name where you want the cluster restored. :type PubliclyAccessible: boolean :param PubliclyAccessible: If true , the cluster can be accessed from a public network. :type OwnerAccount: string :param OwnerAccount: The AWS customer account used to create or copy the snapshot. Required if you are restoring a snapshot you do not own, optional if you own the snapshot. :type HsmClientCertificateIdentifier: string :param HsmClientCertificateIdentifier: Specifies the name of the HSM client certificate the Amazon Redshift cluster uses to retrieve the data encryption keys stored in an HSM. :type HsmConfigurationIdentifier: string :param HsmConfigurationIdentifier: Specifies the name of the HSM configuration that contains the information the Amazon Redshift cluster can use to retrieve and store keys in an HSM. :type ElasticIp: string :param ElasticIp: The elastic IP (EIP) address for the cluster. :type ClusterParameterGroupName: string :param ClusterParameterGroupName: The name of the parameter group to be associated with this cluster. Default: The default Amazon Redshift cluster parameter group. For information about the default parameter group, go to Working with Amazon Redshift Parameter Groups . Constraints: Must be 1 to 255 alphanumeric characters or hyphens. First character must be a letter. Cannot end with a hyphen or contain two consecutive hyphens. :type ClusterSecurityGroups: list :param ClusterSecurityGroups: A list of security groups to be associated with this cluster. Default: The default cluster security group for Amazon Redshift. Cluster security groups only apply to clusters outside of VPCs. (string) -- :type VpcSecurityGroupIds: list :param VpcSecurityGroupIds: A list of Virtual Private Cloud (VPC) security groups to be associated with the cluster. Default: The default VPC security group is associated with the cluster. VPC security groups only apply to clusters in VPCs. (string) -- :type PreferredMaintenanceWindow: string :param PreferredMaintenanceWindow: The weekly time range (in UTC) during which automated cluster maintenance can occur. Format: ddd:hh24:mi-ddd:hh24:mi Default: The value selected for the cluster from which the snapshot was taken. For more information about the time blocks for each region, see Maintenance Windows in Amazon Redshift Cluster Management Guide. Valid Days: Mon | Tue | Wed | Thu | Fri | Sat | Sun Constraints: Minimum 30-minute window. :type AutomatedSnapshotRetentionPeriod: integer :param AutomatedSnapshotRetentionPeriod: The number of days that automated snapshots are retained. If the value is 0, automated snapshots are disabled. Even if automated snapshots are disabled, you can still create manual snapshots when you want with CreateClusterSnapshot . Default: The value selected for the cluster from which the snapshot was taken. Constraints: Must be a value from 0 to 35. :type KmsKeyId: string :param KmsKeyId: The AWS Key Management Service (KMS) key ID of the encryption key that you want to use to encrypt data in the cluster that you restore from a shared snapshot. :type NodeType: string :param NodeType: The node type that the restored cluster will be provisioned with. Default: The node type of the cluster from which the snapshot was taken. You can modify this if you are using any DS node type. In that case, you can choose to restore into another DS node type of the same size. For example, you can restore ds1.8xlarge into ds2.8xlarge, or ds2.xlarge into ds1.xlarge. If you have a DC instance type, you must restore into that same instance type and size. In other words, you can only restore a dc1.large instance type into another dc1.large instance type. For more information about node types, see About Clusters and Nodes in the Amazon Redshift Cluster Management Guide :type EnhancedVpcRouting: boolean :param EnhancedVpcRouting: An option that specifies whether to create the cluster with enhanced VPC routing enabled. To create a cluster that uses enhanced VPC routing, the cluster must be in a VPC. For more information, see Enhanced VPC Routing in the Amazon Redshift Cluster Management Guide. If this option is true , enhanced VPC routing is enabled. Default: false :type AdditionalInfo: string :param AdditionalInfo: Reserved. :type IamRoles: list :param IamRoles: A list of AWS Identity and Access Management (IAM) roles that can be used by the cluster to access other AWS services. You must supply the IAM roles in their Amazon Resource Name (ARN) format. You can supply up to 10 IAM roles in a single request. A cluster can have up to 10 IAM roles associated at any time. (string) -- :rtype: dict :return: { 'Cluster': { 'ClusterIdentifier': 'string', 'NodeType': 'string', 'ClusterStatus': 'string', 'ModifyStatus': 'string', 'MasterUsername': 'string', 'DBName': 'string', 'Endpoint': { 'Address': 'string', 'Port': 123 }, 'ClusterCreateTime': datetime(2015, 1, 1), 'AutomatedSnapshotRetentionPeriod': 123, 'ClusterSecurityGroups': [ { 'ClusterSecurityGroupName': 'string', 'Status': 'string' }, ], 'VpcSecurityGroups': [ { 'VpcSecurityGroupId': 'string', 'Status': 'string' }, ], 'ClusterParameterGroups': [ { 'ParameterGroupName': 'string', 'ParameterApplyStatus': 'string', 'ClusterParameterStatusList': [ { 'ParameterName': 'string', 'ParameterApplyStatus': 'string', 'ParameterApplyErrorDescription': 'string' }, ] }, ], 'ClusterSubnetGroupName': 'string', 'VpcId': 'string', 'AvailabilityZone': 'string', 'PreferredMaintenanceWindow': 'string', 'PendingModifiedValues': { 'MasterUserPassword': 'string', 'NodeType': 'string', 'NumberOfNodes': 123, 'ClusterType': 'string', 'ClusterVersion': 'string', 'AutomatedSnapshotRetentionPeriod': 123, 'ClusterIdentifier': 'string', 'PubliclyAccessible': True|False, 'EnhancedVpcRouting': True|False }, 'ClusterVersion': 'string', 'AllowVersionUpgrade': True|False, 'NumberOfNodes': 123, 'PubliclyAccessible': True|False, 'Encrypted': True|False, 'RestoreStatus': { 'Status': 'string', 'CurrentRestoreRateInMegaBytesPerSecond': 123.0, 'SnapshotSizeInMegaBytes': 123, 'ProgressInMegaBytes': 123, 'ElapsedTimeInSeconds': 123, 'EstimatedTimeToCompletionInSeconds': 123 }, 'HsmStatus': { 'HsmClientCertificateIdentifier': 'string', 'HsmConfigurationIdentifier': 'string', 'Status': 'string' }, 'ClusterSnapshotCopyStatus': { 'DestinationRegion': 'string', 'RetentionPeriod': 123, 'SnapshotCopyGrantName': 'string' }, 'ClusterPublicKey': 'string', 'ClusterNodes': [ { 'NodeRole': 'string', 'PrivateIPAddress': 'string', 'PublicIPAddress': 'string' }, ], 'ElasticIpStatus': { 'ElasticIp': 'string', 'Status': 'string' }, 'ClusterRevisionNumber': 'string', 'Tags': [ { 'Key': 'string', 'Value': 'string' }, ], 'KmsKeyId': 'string', 'EnhancedVpcRouting': True|False, 'IamRoles': [ { 'IamRoleArn': 'string', 'ApplyStatus': 'string' }, ] } } :returns: available creating deleting final-snapshot hardware-failure incompatible-hsm incompatible-network incompatible-parameters incompatible-restore modifying rebooting renaming resizing rotating-keys storage-full updating-hsm",1,0,0,2,3,1,0,0,2,3
"def get_accounts_from_file(filename):
  accounts = []
  cfgparser = __import__('configparser', {}, {}, [''])
  parser = cfgparser.RawConfigParser()
  parser.optionxform = str
  parser.read(filename)
  for user, password in parser.items('account-pool'):
  password = base64.decodebytes(password.encode('latin1'))
  accounts.append(Account(user, password.decode('latin1')))
  return accounts","Reads a list of user/password combinations from the given file and returns a list of Account instances. The file content has the following format:: [account-pool] user1 = cGFzc3dvcmQ= user2 = cGFzc3dvcmQ= Note that ""cGFzc3dvcmQ="" is a base64 encoded password. If the input file contains extra config sections other than ""account-pool"", they are ignored. Each password needs to be base64 encrypted. To encrypt a password, you may use the following command:: python -c 'import base64; print(base64.b64encode(""thepassword""))' :type filename: string :param filename: The name of the file containing the list of accounts. :rtype: list[Account] :return: The newly created account instances.",1,0,0,1,2,1,0,0,1,2
"def _collect_editable_booleans(self):
  if hasattr(self, '_ajax_editable_booleans'):
  return
  self._ajax_editable_booleans = {}
  for field in self.list_display:
  item = getattr(self.__class__, field, None)
  if not item:
  continue
  attr = getattr(item, 'editable_boolean_field', None)
  if attr:
  def _fn(self, instance):
  return [ajax_editable_boolean_cell(instance, _fn.attr)]
  _fn.attr = attr
  result_func = getattr(item, 'editable_boolean_result', _fn)
  self._ajax_editable_booleans[attr] = result_func",Collect all fields marked as editable booleans. We do not want the user to be able to edit arbitrary fields by crafting an AJAX request by hand.,0,0,1,0,1,1,0,0,1,2
"def list(self, *sids):
  if sids == ():
  sids = [sid for (sid,) in self.dbcur.execute(SQL_SENSOR_ALL)]
  slist = []
  for sid in sids:
  tlist = []
  for tmpo in self.dbcur.execute(SQL_TMPO_ALL, (sid,)):
  tlist.append(tmpo)
  sid, rid, lvl, bid, ext, ctd, blk = tmpo
  self._dprintf(DBG_TMPO_WRITE, ctd, sid, rid, lvl, bid, len(blk))
  slist.append(tlist)
  return slist","List all tmpo-blocks in the database Parameters ---------- sids : list of str SensorID's for which to list blocks Optional, leave empty to get them all Returns ------- list[list[tuple]]",1,0,1,1,3,1,1,1,1,4
"def authenticate(self, *, scopes, **kwargs):
  kwargs.setdefault('token_backend', self.con.token_backend)
  return oauth_authentication_flow(*self.con.auth, scopes=scopes,
  protocol=self.protocol, **kwargs)",Performs the oauth authentication flow resulting in a stored token It uses the credentials passed on instantiation :param list[str] scopes: list of protocol user scopes to be converted by the protocol or scope helpers :param kwargs: other configurations to be passed to the Connection instance :return: Success / Failure :rtype: bool,2,0,0,1,3,1,0,0,1,2
"def facebook_request(self, method, callback, **args):
  self.require_setting(""facebook_api_key"", ""Facebook Connect"")
  self.require_setting(""facebook_secret"", ""Facebook Connect"")
  if not method.startswith(""facebook.""):
  method = ""facebook."" + method
  args[""api_key""] = self.settings[""facebook_api_key""]
  args[""v""] = ""1.0""
  args[""method""] = method
  args[""call_id""] = str(long(time.time() * 1e6))
  args[""format""] = ""json""
  args[""sig""] = self._signature(args)
  url = ""http://api.facebook.com/restserver.php?"" + \
  urllib.urlencode(args)
  http = httpclient.AsyncHTTPClient()
  http.fetch(url, callback=self.async_callback(
  self._parse_response, callback))","Makes a Facebook API REST request. We automatically include the Facebook API key and signature, but it is the callers responsibility to include 'session_key' and any other required arguments to the method. The available Facebook methods are documented here: http://wiki.developers.facebook.com/index.php/API Here is an example for the stream.get() method:: class MainHandler(tornado.web.RequestHandler, tornado.auth.FacebookMixin): @tornado.web.authenticated @tornado.web.asynchronous def get(self): self.facebook_request( method=""stream.get"", callback=self.async_callback(self._on_stream), session_key=self.current_user[""session_key""]) def _on_stream(self, stream): if stream is None: # Not authorized to read the stream yet? self.redirect(self.authorize_redirect(""read_stream"")) return self.render(""stream.html"", stream=stream)",2,0,0,1,3,2,0,0,1,3
"def validate_broker_ids_subset(broker_ids, subset_ids):
  all_ids = set(broker_ids)
  valid = True
  for subset_id in subset_ids:
  valid = valid and subset_id in all_ids
  if subset_id not in all_ids:
  print(""Error: user specified broker id {0} does not exist in cluster."".format(subset_id))
  return valid",Validate that user specified broker ids to restart exist in the broker ids retrieved from cluster config. :param broker_ids: all broker IDs in a cluster :type broker_ids: list of integers :param subset_ids: broker IDs specified by user :type subset_ids: list of integers :returns: bool,1,0,0,1,2,1,0,0,1,2
"def create_continuous_query(self, name, select, database=None,
  resample_opts=None):
  r
  query_string = (
  ""CREATE CONTINUOUS QUERY {0} ON {1}{2} BEGIN {3} END""
  ).format(quote_ident(name), quote_ident(database or self._database),
  ' RESAMPLE ' + resample_opts if resample_opts else '', select)
  self.query(query_string)","r""""""Create a continuous query for a database. :param name: the name of continuous query to create :type name: str :param select: select statement for the continuous query :type select: str :param database: the database for which the continuous query is created. Defaults to current client's database :type database: str :param resample_opts: resample options :type resample_opts: str :Example: :: >> select_clause = 'SELECT mean(""value"") INTO ""cpu_mean"" ' \ ... 'FROM ""cpu"" GROUP BY time(1m)' >> client.create_continuous_query( ... 'cpu_mean', select_clause, 'db_name', 'EVERY 10s FOR 2m' ... ) >> client.get_list_continuous_queries() [ { 'db_name': [ { 'name': 'cpu_mean', 'query': 'CREATE CONTINUOUS QUERY ""cpu_mean"" ' 'ON ""db_name"" ' 'RESAMPLE EVERY 10s FOR 2m ' 'BEGIN SELECT mean(""value"") ' 'INTO ""cpu_mean"" FROM ""cpu"" ' 'GROUP BY time(1m) END' } ] } ]",0,1,0,0,1,1,1,1,1,4
"def input_by_key():
  usr_inp = ''
  input_valid = True
  input_flush()
  with term.cbreak():
  while input_valid:
  ui_print(""\033[?25h"")
  key_raw = term.inkey()
  if key_raw.name == ""KEY_ENTER"":
  input_valid = False
  ui_print(""\033[?25l"")
  break
  if key_raw.name == 'KEY_DELETE':
  ui_del_char(len(usr_inp))
  usr_inp = usr_inp[:-1]
  if not key_raw.is_sequence:
  usr_inp += key_raw
  ui_print(key_raw)
  if not usr_inp:
  ui_print(""\033[D"")
  return usr_inp",Get user input using term.inkey to prevent /n printing at end.,1,0,0,1,2,1,0,0,1,2
"def foreign_key(*args,
  fk_col: Optional[str] = None,
  primary_key: bool = False,
  nullable: bool = False,
  **kwargs,
  ) -> Column:
  return Column(*_get_fk_col_args(args, fk_col, _default_col_type=BigInteger),
  primary_key=primary_key, nullable=nullable, **kwargs)","Helper method to add a foreign key column to a model. For example:: class Post(db.Model): category_id = db.foreign_key('Category') category = db.relationship('Category', back_populates='posts') Is equivalent to:: class Post(db.Model): category_id = db.Column(db.BigInteger, db.ForeignKey('category.id'), nullable=False) category = db.relationship('Category', back_populates='posts') Customizing all the things:: class Post(db.Model): _category_id = db.foreign_key('category_id', # db column name db.String, # db column type 'categories', # foreign table name fk_col='pk') # foreign key col name Is equivalent to:: class Post(db.Model): _category_id = db.Column('category_id', db.String, db.ForeignKey('categories.pk'), nullable=False) :param args: :func:`foreign_key` takes up to three positional arguments. Most commonly, you will only pass one argument, which should be the model name, the model class, or table name you're linking to. If you want to customize the column name the foreign key gets stored in the database under, then *it must be the first string argument*, and you must *also* supply the model name, class or table name. You can also customize the column type (eg ``sa.Integer`` or ``sa.String(36)``) by passing it as an arg. :param str fk_col: The column name of the primary key on the *opposite* side of the relationship (defaults to :attr:`sqlalchemy_unchained._ModelRegistry.default_primary_key_column`). :param bool primary_key: Whether or not this :class:`~sqlalchemy.Column` is a primary key. :param bool nullable: Whether or not this :class:`~sqlalchemy.Column` should be nullable. :param kwargs: Any other kwargs to pass the :class:`~sqlalchemy.Column` constructor.",1,0,0,0,1,1,0,1,1,3
"def find_magic_file(self, fname, system_only=False, user_only=False):
  loc = None
  if not system_only:
  fpath = self._user_path(self.BINWALK_MAGIC_DIR, fname)
  if os.path.exists(fpath) and common.file_size(fpath) > 0:
  loc = fpath
  if loc is None and not user_only:
  fpath = self._system_path(self.BINWALK_MAGIC_DIR, fname)
  if os.path.exists(fpath) and common.file_size(fpath) > 0:
  loc = fpath
  return fpath","Finds the specified magic file name in the system / user magic file directories. @fname - The name of the magic file. @system_only - If True, only the system magic file directory will be searched. @user_only - If True, only the user magic file directory will be searched. If system_only and user_only are not set, the user directory is always searched first. Returns the path to the file on success; returns None on failure.",1,0,0,1,2,1,0,0,1,2
"def load_variants(adapter, vcf_obj, case_obj, skip_case_id=False, gq_treshold=None,
  max_window=3000, variant_type='snv'):
  if variant_type == 'snv':
  nr_variants = case_obj['nr_variants']
  else:
  nr_variants = case_obj['nr_sv_variants']
  nr_inserted = 0
  case_id = case_obj['case_id']
  if skip_case_id:
  case_id = None
  with click.progressbar(vcf_obj, label=""Inserting variants"",length=nr_variants) as bar:
  variants = (build_variant(variant,case_obj,case_id, gq_treshold) for variant in bar)
  if variant_type == 'sv':
  for sv_variant in variants:
  if not sv_variant:
  continue
  adapter.add_structural_variant(variant=sv_variant, max_window=max_window)
  nr_inserted += 1
  if variant_type == 'snv':
  nr_inserted = adapter.add_variants(variants)
  LOG.info(""Inserted %s variants of type %s"", nr_inserted, variant_type)
  return nr_inserted",Load variants for a family into the database. Args: adapter (loqusdb.plugins.Adapter): initialized plugin case_obj(Case): dict with case information nr_variants(int) skip_case_id (bool): whether to include the case id on variant level or not gq_treshold(int) max_window(int): Specify the max size for sv windows variant_type(str): 'sv' or 'snv' Returns: nr_inserted(int),0,2,0,1,3,1,1,0,1,3
"def set_location(request):
  next = request.GET.get('next', None) or request.POST.get('next', None)
  if not next:
  next = request.META.get('HTTP_REFERER', None)
  if not next:
  next = '/'
  response = http.HttpResponseRedirect(next)
  if request.method == 'POST':
  location_id = request.POST.get('location_id', None) or request.POST.get('location', None)
  if location_id:
  try:
  location = get_class(settings.GEOIP_LOCATION_MODEL).objects.get(pk=location_id)
  storage_class(request=request, response=response).set(location=location, force=True)
  except (ValueError, ObjectDoesNotExist):
  pass
  return response","Redirect to a given url while setting the chosen location in the cookie. The url and the location_id need to be specified in the request parameters. Since this view changes how the user will see the rest of the site, it must only be accessed as a POST request. If called as a GET request, it will redirect to the page in the request (the 'next' parameter) without changing any state.",1,0,1,1,3,1,0,1,1,3
"def get(self, request, *args, **kwargs):
  serializer = self.serializer_reader_class
  if request.user.is_authenticated():
  return Response(serializer(request.user, context=self.get_serializer_context()).data)
  else:
  return Response({'detail': _('Authentication credentials were not provided')}, status=401)",return profile of current user if authenticated otherwise 401,1,0,0,1,2,1,0,1,1,3
"def Notify(self, message_type, subject, msg, source):
  pending = self.Get(self.Schema.PENDING_NOTIFICATIONS)
  if pending is None:
  pending = self.Schema.PENDING_NOTIFICATIONS()
  if message_type.split(
  "":"", 2)[0] not in rdf_flows.Notification.notification_types:
  raise TypeError(""Invalid notification type %s"" % message_type)
  pending.Append(
  type=message_type,
  subject=subject,
  message=msg,
  source=source,
  timestamp=int(time.time() * 1e6))
  while len(pending) > 50:
  pending.Pop(0)
  self.Set(self.Schema.PENDING_NOTIFICATIONS, pending)","Send an AFF4-based notification to the user in the UI. Args: message_type: One of aff4_grr.Notification.notification_types e.g. ""ViewObject"", ""HostInformation"", ""GrantAccess"" or the same with an added "":[new-style notification type] suffix, e.g. ""ViewObject:TYPE_CLIENT_INTERROGATED"". subject: The subject to use, normally a URN. msg: The message to display. source: The class doing the notification. Raises: TypeError: On invalid message_type.",0,1,1,1,3,1,0,0,1,2
"def get_items(self, page=1, order_by=None, filters=None):
  start = (page-1)*self.per_page
  query = self.get_query()
  if order_by is not None:
  query = query.order_by(self._get_field(order_by))
  if filters is not None:
  query = self._filter(query, filters)
  return query.offset(start).limit(self.per_page), self.count(query)","Fetch database for items matching. Args: page (int): which page will be sliced slice size is ``self.per_page``. order_by (str): a field name to order query by. filters (dict): a ``filter name``: ``value`` dict. Returns: tuple with: items, sliced by page*self.per_page total items without slice",2,0,1,1,4,1,0,1,1,3
"def can_create_repository_with_record_types(self, repository_record_types):
  if self._catalog_session is not None:
  return self._catalog_session.can_create_catalog_with_record_types(catalog_record_types=repository_record_types)
  return True","Tests if this user can create a single ``Repository`` using the desired record types. While ``RepositoryManager.getRepositoryRecordTypes()`` can be used to examine which records are supported, this method tests which record(s) are required for creating a specific ``Repository``. Providing an empty array tests if a ``Repository`` can be created with no records. arg: repository_record_types (osid.type.Type[]): array of repository record types return: (boolean) - ``true`` if ``Repository`` creation using the specified ``Types`` is supported, ``false`` otherwise raise: NullArgument - ``repository_record_types`` is ``null`` *compliance: mandatory -- This method must be implemented.*",1,0,0,1,2,2,0,0,1,3
"def showPerformance(self):
  if len(self.inputs) == 0:
  print('no patterns to test')
  return
  self.setContext()
  while True:
  BackpropNetwork.showPerformance(self)
  if self.quitFromSweep:
  return",SRN.showPerformance() Clears the context layer(s) and then repeatedly cycles through training patterns until the user decides to quit.,1,0,0,1,2,1,0,0,1,2
"def can_invite_others(self, user):
  if self.is_managed:
  return False
  elif self.is_admin(user):
  return True
  elif self.subscription_policy != SubscriptionPolicy.CLOSED:
  return True
  else:
  return False","Determine if user can invite people to a group. Be aware that this check is independent from the people (users) which are going to be invited. The checked user is the one who invites someone, NOT who is going to be invited. :param user: User to be checked. :returns: True or False.",1,0,1,1,3,1,0,0,1,2
"def start_login_server(self, ):
  self.login_server = oauth.LoginServer(session=self)
  target = self.login_server.serve_forever
  self.login_thread = threading.Thread(target=target)
  self.login_thread.setDaemon(True)
  log.debug('Starting login server thread.')
  self.login_thread.start()","Start a server that will get a request from a user logging in. This uses the Implicit Grant Flow of OAuth2. The user is asked to login to twitch and grant PyTwitcher authorization. Once the user agrees, he is redirected to an url. This server will respond to that url and get the oauth token. The server serves in another thread. To shut him down, call :meth:`TwitchSession.shutdown_login_server`. This sets the :data:`TwitchSession.login_server`, :data:`TwitchSession.login_thread` variables. :returns: The created server :rtype: :class:`BaseHTTPServer.HTTPServer` :raises: None",1,0,0,0,1,1,0,0,1,2
"def get_keeper_token(host, username, password):
  token_endpoint = urljoin(host, '/token')
  r = requests.get(token_endpoint, auth=(username, password))
  if r.status_code != 200:
  raise KeeperError('Could not authenticate to {0}: error {1:d}\n{2}'.
  format(host, r.status_code, r.json()))
  return r.json()['token']","Get a temporary auth token from LTD Keeper. Parameters ---------- host : `str` Hostname of the LTD Keeper API (e.g., ``'https://keeper.lsst.codes'``). username : `str` Username. password : `str` Password. Returns ------- token : `str` LTD Keeper API token. Raises ------ KeeperError Raised if the LTD Keeper API cannot return a token.",2,0,0,0,2,2,0,0,1,3
"def DeleteUser(self, user_link, options=None):
  if options is None:
  options = {}
  path = base.GetPathFromLink(user_link)
  user_id = base.GetResourceIdOrFullNameFromLink(user_link)
  return self.DeleteResource(path,
  'users',
  user_id,
  None,
  options)",Deletes a user. :param str user_link: The link to the user entity. :param dict options: The request options for the request. :return: The deleted user. :rtype: dict,2,1,1,2,6,2,1,0,1,4
"def get_auth_provider_affiliates(self, auth_provider_id, full_name_contains=None, email=None, username=None):
  data = {}
  if full_name_contains:
  data['full_name_contains'] = full_name_contains
  if email:
  data['email'] = email
  if username:
  data['username'] = username
  return self._get_collection(""/auth_providers/{}/affiliates/"".format(auth_provider_id), data)",List affiliates for a specific auth provider. :param auth_provider_id: str: uuid of the auth provider to list affiliates of :param full_name_contains: str: filters affiliates for this name :param email: str: filters affiliates for this email address :param username: str: filters affiliates for this username :return: requests.Response containing the successful result,2,0,0,1,3,2,0,0,1,3
"def delegates():
  qry = DbCursor().execute_and_fetchall()
  Delegate = namedtuple(
  'delegate',
  'username pubkey timestamp address transactionId')
  res = []
  for i in qry:
  registration = Delegate(
  username=i[0],
  pubkey=binascii.hexlify(i[4]).decode(""utf-8""),
  timestamp=i[2],
  address=i[3],
  transactionId=i[1]
  )
  res.append(registration)
  return res","returns a list of named tuples of all delegates. {username: {'pubkey':pubkey, 'timestamp':timestamp, 'address':address}}",0,0,1,1,2,1,0,1,1,3
"def send_video_message(self, user_id, media_id, title=None, description=None):
  video_data = {
  'media_id': media_id,
  }
  if title:
  video_data['title'] = title
  if description:
  video_data['description'] = description
  return self.request.post(
  url='https://api.weixin.qq.com/cgi-bin/message/custom/send',
  data={
  'touser': user_id,
  'msgtype': 'video',
  'video': video_data,
  }
  )","  http://mp.weixin.qq.com/wiki/7/12a5a320ae96fecdf0e15cb06123de9f.html :param user_id:  ID,   WechatMessage  source :param media_id: ID  :func:`upload_media`   :param title:   :param description:  :return:  JSON ",1,0,0,2,3,2,0,0,1,3
"def _CreateNewSeasonDir(self, seasonNum):
  seasonDirName = ""Season {0}"".format(seasonNum)
  goodlogging.Log.Info(""RENAMER"", ""Generated directory name: '{0}'"".format(seasonDirName))
  if self._skipUserInput is False:
  response = goodlogging.Log.Input(""RENAMER"", ""Enter 'y' to accept this directory, 'b' to use base show directory, 'x' to skip this file or enter a new directory name to use: "")
  response = util.CheckEmptyResponse(response)
  else:
  response = 'y'
  if response.lower() == 'b':
  return ''
  elif response.lower() == 'y':
  return seasonDirName
  elif response.lower() == 'x':
  return None
  else:
  return response","Creates a new season directory name in the form 'Season <NUM>'. If skipUserInput is True this will be accepted by default otherwise the user can choose to accept this, use the base show directory or enter a different name. Parameters ---------- seasonNum : int Season number. Returns ---------- string or None If the user accepts the generated directory name or gives a new name this will be returned. If it the user chooses to use the base directory an empty string is returned. If the user chooses to skip at this input stage None is returned.",1,0,0,1,2,1,0,0,1,2
"def scp_put(files,
  remote_path=None,
  recursive=False,
  preserve_times=False,
  saltenv='base',
  **kwargs):
  conn_args = netmiko_args(**kwargs)
  conn_args['hostname'] = conn_args['host']
  kwargs.update(conn_args)
  return __salt__['scp.put'](files,
  remote_path=remote_path,
  recursive=recursive,
  preserve_times=preserve_times,
  saltenv=saltenv,
  **kwargs)",".. versionadded:: 2019.2.0 Transfer files and directories to remote network device. .. note:: This function is only available only when the underlying library `scp <https://github.com/jbardin/scp.py>`_ is installed. See :mod:`scp module <salt.modules.scp_mod>` for more details. files A single path or a list of paths to be transferred. remote_path The path on the remote device where to store the files. recursive: ``True`` Transfer files and directories recursively. preserve_times: ``False`` Preserve ``mtime`` and ``atime`` of transferred files and directories. saltenv: ``base`` The name of the Salt environment. Ignored when ``files`` is not a ``salt://`` URL. hostname The hostname of the remote device. port: ``22`` The port of the remote device. username The username required for SSH authentication on the device. password Used for password authentication. It is also used for private key decryption if ``passphrase`` is not given. passphrase Used for decrypting private keys. pkey An optional private key to use for authentication. key_filename The filename, or list of filenames, of optional private key(s) and/or certificates to try for authentication. timeout An optional timeout (in seconds) for the TCP connect. socket_timeout: ``10`` The channel socket timeout in seconds. buff_size: ``16384`` The size of the SCP send buffer. allow_agent: ``True`` Set to ``False`` to disable connecting to the SSH agent. look_for_keys: ``True`` Set to ``False`` to disable searching for discoverable private key files in ``~/.ssh/`` banner_timeout An optional timeout (in seconds) to wait for the SSH banner to be presented. auth_timeout An optional timeout (in seconds) to wait for an authentication response. auto_add_policy: ``False`` Automatically add the host to the ``known_hosts``. CLI Example: .. code-block:: bash salt '*' napalm.scp_put /path/to/file /var/tmp/file auto_add_policy=True",1,0,0,1,2,1,0,0,1,2
"def get_user_data(self, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('callback'):
  return self.get_user_data_with_http_info(**kwargs)
  else:
  (data) = self.get_user_data_with_http_info(**kwargs)
  return data","Get user data Return profile information for the currently authenticated user. This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please define a `callback` function to be invoked when receiving the response. >>> def callback_function(response): >>> pprint(response) >>> >>> thread = api.get_user_data(callback=callback_function) :param callback function: The callback function for asynchronous request. (optional) :return: UserDataResponse If the method is called asynchronously, returns the request thread.",2,0,0,1,3,2,0,0,1,3
"def connect(servers=None,
  timeout=None,
  client=None,
  verify_ssl_cert=False,
  ca_cert=None,
  error_trace=False,
  cert_file=None,
  key_file=None,
  username=None,
  password=None,
  schema=None):
  return Connection(servers=servers,
  timeout=timeout,
  client=client,
  verify_ssl_cert=verify_ssl_cert,
  ca_cert=ca_cert,
  error_trace=error_trace,
  cert_file=cert_file,
  key_file=key_file,
  username=username,
  password=password,
  schema=schema)","Create a :class:Connection object :param servers: either a string in the form of '<hostname>:<port>' or a list of servers in the form of ['<hostname>:<port>', '...'] :param timeout: (optional) define the retry timeout for unreachable servers in seconds :param client: (optional - for testing) client used to communicate with crate. :param verify_ssl_cert: if set to ``True`` verify the servers SSL server certificate. defaults to ``False`` :param ca_cert: a path to a CA certificate to use when verifying the SSL server certificate. :param error_trace: if set to ``True`` return a whole stacktrace of any server error if one occurs :param cert_file: a path to the client certificate to present to the server. :param key_file: a path to the client key to use when communicating with the server. :param username: the username in the database. :param password: the password of the user in the database. >>> connect(['host1:4200', 'host2:4200']) <Connection <Client ['http://host1:4200', 'http://host2:4200']>>",1,0,0,1,2,1,0,0,1,2
"def rvs(self, random_state=None):
  r
  if self.dist is None:
  return self.value
  rs = check_random_state(random_state)
  samples = self.dist.rvs(size=self.shape, random_state=rs)
  samples = self.bounds.clip(samples)
  return samples","r"""""" Draw a random value from this Parameter's distribution. If ``value`` was not initialised with a ``scipy.stats`` object, then the scalar/ndarray value is returned. Parameters ---------- random_state : None, int or RandomState, optional random seed Returns ------- ndarray : of size ``self.shape``, a random draw from the distribution, or ``self.value`` if not initialised with a ``scipy.stats`` object. Note ---- Random draws are *clipped* to the bounds, and so it is up to the user to input a sensible sampling distribution!",0,0,0,1,1,1,0,0,1,2
"def install_visa_handler(self, session, event_type, handler, user_handle=None):
  try:
  new_handler = self.install_handler(session, event_type, handler, user_handle)
  except TypeError as e:
  raise errors.VisaTypeError(str(e))
  self.handlers[session].append(new_handler + (event_type,))
  return new_handler[1]",Installs handlers for event callbacks. :param session: Unique logical identifier to a session. :param event_type: Logical event identifier. :param handler: Interpreted as a valid reference to a handler to be installed by a client application. :param user_handle: A value specified by an application that can be used for identifying handlers uniquely for an event type. :returns: user handle (a ctypes object),0,0,0,1,1,1,0,0,1,2
"def get_or_set_hash(name,
  length=8,
  chars='abcdefghijklmnopqrstuvwxyz0123456789!@
  ret = get(name, None)
  if ret is None:
  val = ''.join([random.SystemRandom().choice(chars) for _ in range(length)])
  if DEFAULT_TARGET_DELIM in name:
  root, rest = name.split(DEFAULT_TARGET_DELIM, 1)
  curr = get(root, _infinitedict())
  val = _dict_from_path(rest, val)
  curr.update(val)
  setval(root, curr)
  else:
  setval(name, val)
  return get(name)","Perform a one-time generation of a hash and write it to the local grains. If that grain has already been set return the value instead. This is useful for generating passwords or keys that are specific to a single minion that don't need to be stored somewhere centrally. State Example: .. code-block:: yaml some_mysql_user: mysql_user: - present - host: localhost - password: {{ salt['grains.get_or_set_hash']('mysql:some_mysql_user') }} CLI Example: .. code-block:: bash salt '*' grains.get_or_set_hash 'django:SECRET_KEY' 50 .. warning:: This function could return strings which may contain characters which are reserved as directives by the YAML parser, such as strings beginning with ``%``. To avoid issues when using the output of this function in an SLS file containing YAML+Jinja, surround the call with single quotes.",0,0,0,0,0,1,0,0,1,2
"def reply_location(
  self,
  latitude: float,
  longitude: float,
  quote: bool = None,
  disable_notification: bool = None,
  reply_to_message_id: int = None,
  reply_markup: Union[
  ""pyrogram.InlineKeyboardMarkup"",
  ""pyrogram.ReplyKeyboardMarkup"",
  ""pyrogram.ReplyKeyboardRemove"",
  ""pyrogram.ForceReply""
  ] = None
  ) -> ""Message"":
  if quote is None:
  quote = self.chat.type != ""private""
  if reply_to_message_id is None and quote:
  reply_to_message_id = self.message_id
  return self._client.send_location(
  chat_id=self.chat.id,
  latitude=latitude,
  longitude=longitude,
  disable_notification=disable_notification,
  reply_to_message_id=reply_to_message_id,
  reply_markup=reply_markup
  )","Bound method *reply_location* of :obj:`Message <pyrogram.Message>`. Use as a shortcut for: .. code-block:: python client.send_location( chat_id=message.chat.id, latitude=41.890251, longitude=12.492373 ) Example: .. code-block:: python message.reply_location(41.890251, 12.492373) Args: latitude (``float``): Latitude of the location. longitude (``float``): Longitude of the location. quote (``bool``, *optional*): If ``True``, the message will be sent as a reply to this message. If *reply_to_message_id* is passed, this parameter will be ignored. Defaults to ``True`` in group chats and ``False`` in private chats. disable_notification (``bool``, *optional*): Sends the message silently. Users will receive a notification with no sound. reply_to_message_id (``int``, *optional*): If the message is a reply, ID of the original message reply_markup (:obj:`InlineKeyboardMarkup` | :obj:`ReplyKeyboardMarkup` | :obj:`ReplyKeyboardRemove` | :obj:`ForceReply`, *optional*): Additional interface options. An object for an inline keyboard, custom reply keyboard, instructions to remove reply keyboard or to force a reply from the user. Returns: On success, the sent :obj:`Message <pyrogram.Message>` is returned. Raises: :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.",1,0,0,2,3,1,0,0,1,2
"def execute_actioncollection(obj, actioncollection, confirm=True):
  actioncollection.execute(obj)
  status = actioncollection.status()
  if status.value == ActionStatus.SUCCESS or not confirm:
  return status
  ard = ActionReportDialog(actioncollection)
  confirmed = ard.exec_()
  if confirmed:
  msg = ""User confirmed to continue although the status was: %s"" % status.message,
  s = ActionStatus.SUCCESS
  tb = status.traceback
  else:
  s = status.value
  msg = ""User aborted the actions because the status was: %s"" % status.message,
  tb = status.traceback
  return ActionStatus(s, msg, tb)","Execute the given actioncollection with the given object :param obj: the object to be processed :param actioncollection: :type actioncollection: :class:`ActionCollection` :param confirm: If True, ask the user to continue, if actions failed. :type confirm: :class:`bool` :returns: An action status. If the execution fails but the user confirms, the status will be successful. :rtype: :class:`ActionStatus` :raises: None",1,0,0,1,2,1,0,1,1,3
"def create_claims_set(self, request, userid, extra_claims=None):
  claims_set = {self.userid_claim: userid}
  now = timegm(datetime.utcnow().utctimetuple())
  if self.expiration_delta is not None:
  claims_set['exp'] = now + self.expiration_delta
  if self.issuer is not None:
  claims_set['iss'] = self.issuer
  if self.allow_refresh:
  if self.refresh_delta is not None:
  claims_set['refresh_until'] = now + self.refresh_delta
  if self.refresh_nonce_handler is not None:
  claims_set['nonce'] = self.refresh_nonce_handler(request,
  userid)
  if extra_claims is not None:
  claims_set.update(extra_claims)
  return claims_set","Create the claims set based on the userid of the claimed identity, the settings and the extra_claims dictionary. The userid will be stored in settings.jwtauth.userid_claim (default: ""sub""). If settings.jwtauth.expiration_delta is set it will be added to the current time and stored in the ""exp"" claim. If settings.jwtauth.issuer is set, it get stored in the ""iss"" claim. If settings.jwtauth.refresh_delta is set it will be added to the current time and stored in the ""refresh_until"" claim and the return value of settings.jwtauth.refresh_nonce_handler called with ""user_id"" as argument will be stored in the ""nonce"" claim. With the extra_claims dictionary you can provide additional claims. This can be registered claims like ""nbf"" (the time before which the token should not be processed) and/or claims containing extra info about the identity, which will be stored in the Identity object. :param request: current request object. :type request: :class:`morepath.Request` :param userid: the userid of the claimed identity. :param extra_claims: dictionary, containing additional claims or None.",1,0,0,1,2,1,0,0,1,2
"def get_all_knoreq_user_objects(self, include_machine = False):
 logger.debug('Polling AD for all user objects, machine accounts included: %s'% include_machine)
 if include_machine == True:
 ldap_filter = r'(userAccountControl:1.2.840.113556.1.4.803:=4194304)'
 else:
 ldap_filter = r'(&(userAccountControl:1.2.840.113556.1.4.803:=4194304)(!(sAMAccountName = *$)))'
 attributes = MSADUser.ATTRS
 for entry in self.pagedsearch(ldap_filter, attributes):
 yield MSADUser.from_ldap(entry, self._ldapinfo)
 logger.debug('Finished polling for entries!')","Fetches all user objects with useraccountcontrol DONT_REQ_PREAUTH flag set from the AD, and returns MSADUser object.",0,0,1,0,1,1,0,0,1,2
"def get_db_prep_value(self, value, connection, prepared=False):
  log.debug(""get_db_prep_value: {}, {}"", value, type(value))
  value = super().get_db_prep_value(value, connection, prepared)
  if value is None:
  return value
  if connection.settings_dict['ENGINE'] == 'django.db.backends.sqlite3':
  return python_utc_datetime_to_sqlite_strftime_string(value)
  return value","Further conversion of Python value to database value for QUERYING. This follows ``get_prep_value()``, and is for backend-specific stuff. See notes above.",0,0,0,0,0,1,1,1,1,4
"def command(self, command, value=1, callback=None,
  check=True, allowable_errors=[], **kwargs):
  if isinstance(command, basestring):
  command = SON([(command, value)])
  command.update(kwargs)
  self.connection(""$cmd"").find_one(command,callback=callback,
  _must_use_master=True,
  _is_command=True)","Issue a MongoDB command. Send command `command` to the database and return the response. If `command` is an instance of :class:`basestring` then the command {`command`: `value`} will be sent. Otherwise, `command` must be an instance of :class:`dict` and will be sent as is. Any additional keyword arguments will be added to the final command document before it is sent. For example, a command like ``{buildinfo: 1}`` can be sent using: >>> db.command(""buildinfo"") For a command where the value matters, like ``{collstats: collection_name}`` we can do: >>> db.command(""collstats"", collection_name) For commands that take additional arguments we can use kwargs. So ``{filemd5: object_id, root: file_root}`` becomes: >>> db.command(""filemd5"", object_id, root=file_root) :Parameters: - `command`: document representing the command to be issued, or the name of the command (for simple commands only). .. note:: the order of keys in the `command` document is significant (the ""verb"" must come first), so commands which require multiple keys (e.g. `findandmodify`) should use an instance of :class:`~bson.son.SON` or a string and kwargs instead of a Python `dict`. - `value` (optional): value to use for the command verb when `command` is passed as a string - `**kwargs` (optional): additional keyword arguments will be added to the command document before it is sent .. mongodoc:: commands",1,0,1,1,3,1,0,1,1,3
"def delete_servers(*servers, **options):
  test = options.pop('test', False)
  commit = options.pop('commit', True)
  return __salt__['net.load_template']('delete_ntp_servers',
  servers=servers,
  test=test,
  commit=commit,
  inherit_napalm_device=napalm_device)","Removes NTP servers configured on the device. :param servers: list of IP Addresses/Domain Names to be removed as NTP servers :param test (bool): discard loaded config. By default ``test`` is False (will not dicard the changes) :param commit (bool): commit loaded config. By default ``commit`` is True (will commit the changes). Useful when the user does not want to commit after each change, but after a couple. By default this function will commit the config changes (if any). To load without committing, use the ``commit`` option. For dry run use the ``test`` argument. CLI Example: .. code-block:: bash salt '*' ntp.delete_servers 8.8.8.8 time.apple.com salt '*' ntp.delete_servers 172.17.17.1 test=True # only displays the diff salt '*' ntp.delete_servers 192.168.0.1 commit=False # preserves the changes, but does not commit",0,1,0,1,2,1,0,0,1,2
"def get_menu_checked(self, request):
  checked_id = []
  qd = request.GET
  query_dict = dict(qd.items())
  if query_dict:
  app_label = query_dict['app_label']
  model_name = query_dict['model_name']
  pk = query_dict['pk']
  model = get_model(app_label, model_name)
  object = model.objects.get(pk=pk)
  checked_id = object.menus_checked.split(',')
  return checked_id","checked usermenu_form.html  usermenu modelmenus_checkedmenus_show groupmenu @return eg. ['1', '8', '9', '10' ] check_ids,app_label, model_name, pk eg. /easyui/menulistview/?app_label=easyui&model_name=UserMenu&pk=1",1,0,1,1,3,1,0,1,1,3
"def _notify_ms_vc_exception(self, event):
  dwType = event.get_exception_information(0)
  if dwType == 0x1000:
  pszName = event.get_exception_information(1)
  dwThreadId = event.get_exception_information(2)
  dwFlags = event.get_exception_information(3)
  aProcess = event.get_process()
  szName = aProcess.peek_string(pszName, fUnicode = False)
  if szName:
  if dwThreadId == -1:
  dwThreadId = event.get_tid()
  if aProcess.has_thread(dwThreadId):
  aThread = aProcess.get_thread(dwThreadId)
  else:
  aThread = Thread(dwThreadId)
  aProcess._add_thread(aThread)
  aThread.set_name(szName)
  return True","Notify of a Microsoft Visual C exception. @warning: This method is meant to be used internally by the debugger. @note: This allows the debugger to understand the Microsoft Visual C thread naming convention. @see: U{http://msdn.microsoft.com/en-us/library/xcb2z8hs.aspx} @type event: L{ExceptionEvent} @param event: Microsoft Visual C exception event. @rtype: bool @return: C{True} to call the user-defined handle, C{False} otherwise.",0,0,0,1,1,1,0,0,1,2
"def reply_sticker(
  self,
  sticker: str,
  quote: bool = None,
  disable_notification: bool = None,
  reply_to_message_id: int = None,
  reply_markup: Union[
  ""pyrogram.InlineKeyboardMarkup"",
  ""pyrogram.ReplyKeyboardMarkup"",
  ""pyrogram.ReplyKeyboardRemove"",
  ""pyrogram.ForceReply""
  ] = None,
  progress: callable = None,
  progress_args: tuple = ()
  ) -> ""Message"":
  if quote is None:
  quote = self.chat.type != ""private""
  if reply_to_message_id is None and quote:
  reply_to_message_id = self.message_id
  return self._client.send_sticker(
  chat_id=self.chat.id,
  sticker=sticker,
  disable_notification=disable_notification,
  reply_to_message_id=reply_to_message_id,
  reply_markup=reply_markup,
  progress=progress,
  progress_args=progress_args
  )","Bound method *reply_sticker* of :obj:`Message <pyrogram.Message>`. Use as a shortcut for: .. code-block:: python client.send_sticker( chat_id=message.chat.id, sticker=sticker ) Example: .. code-block:: python message.reply_sticker(sticker) Args: sticker (``str``): Sticker to send. Pass a file_id as string to send a sticker that exists on the Telegram servers, pass an HTTP URL as a string for Telegram to get a .webp sticker file from the Internet, or pass a file path as string to upload a new sticker that exists on your local machine. quote (``bool``, *optional*): If ``True``, the message will be sent as a reply to this message. If *reply_to_message_id* is passed, this parameter will be ignored. Defaults to ``True`` in group chats and ``False`` in private chats. disable_notification (``bool``, *optional*): Sends the message silently. Users will receive a notification with no sound. reply_to_message_id (``int``, *optional*): If the message is a reply, ID of the original message. reply_markup (:obj:`InlineKeyboardMarkup` | :obj:`ReplyKeyboardMarkup` | :obj:`ReplyKeyboardRemove` | :obj:`ForceReply`, *optional*): Additional interface options. An object for an inline keyboard, custom reply keyboard, instructions to remove reply keyboard or to force a reply from the user. progress (``callable``, *optional*): Pass a callback function to view the upload progress. The function must take *(client, current, total, \*args)* as positional arguments (look at the section below for a detailed description). progress_args (``tuple``, *optional*): Extra custom arguments for the progress callback function. Useful, for example, if you want to pass a chat_id and a message_id in order to edit a message with the updated progress. Other Parameters: client (:obj:`Client <pyrogram.Client>`): The Client itself, useful when you want to call other API methods inside the callback function. current (``int``): The amount of bytes uploaded so far. total (``int``): The size of the file. *args (``tuple``, *optional*): Extra custom arguments as defined in the *progress_args* parameter. You can either keep *\*args* or add every single extra argument in your function signature. Returns: On success, the sent :obj:`Message <pyrogram.Message>` is returned. In case the upload is deliberately stopped with :meth:`stop_transmission`, None is returned instead. Raises: :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.",1,0,0,1,2,1,0,0,1,2
"def import_ed25519_privatekey_from_file(filepath, password=None, prompt=False):
  securesystemslib.formats.PATH_SCHEMA.check_match(filepath)
  if password and prompt:
  raise ValueError(""Passing 'password' and 'prompt' True is not allowed."")
  if password is not None:
  securesystemslib.formats.PASSWORD_SCHEMA.check_match(password)
  if not len(password):
  raise ValueError('Password must be 1 or more characters')
  elif prompt:
  password = get_password('Enter a password for an encrypted RSA'
  ' file \'' + Fore.RED + filepath + Fore.RESET + '\': ',
  confirm=False)
  if len(password) == 0:
  password = None
  with open(filepath, 'rb') as file_object:
  json_str = file_object.read()
  return securesystemslib.keys.\
  import_ed25519key_from_private_json(json_str, password=password)","<Purpose> Import the encrypted ed25519 key file in 'filepath', decrypt it, and return the key object in 'securesystemslib.formats.ED25519KEY_SCHEMA' format. The private key (may also contain the public part) is encrypted with AES 256 and CTR the mode of operation. The password is strengthened with PBKDF2-HMAC-SHA256. <Arguments> filepath: <filepath> file, an RSA encrypted key file. password: The password, or passphrase, to import the private key (i.e., the encrypted key file 'filepath' must be decrypted before the ed25519 key object can be returned. prompt: If True the user is prompted for a passphrase to decrypt 'filepath'. Default is False. <Exceptions> securesystemslib.exceptions.FormatError, if the arguments are improperly formatted or the imported key object contains an invalid key type (i.e., not 'ed25519'). securesystemslib.exceptions.CryptoError, if 'filepath' cannot be decrypted. <Side Effects> 'password' is used to decrypt the 'filepath' key file. <Returns> An ed25519 key object of the form: 'securesystemslib.formats.ED25519KEY_SCHEMA'.",1,0,0,0,1,1,0,0,1,2
"def mark_as_nsfw(self, unmark_nsfw=False):
  def mark_as_nsfw_helper(self):
  url = self.reddit_session.config['unmarknsfw' if unmark_nsfw else
  'marknsfw']
  data = {'id': self.fullname}
  return self.reddit_session.request_json(url, data=data)
  is_author = (self.reddit_session.is_logged_in() and self.author ==
  self.reddit_session.user)
  if is_author:
  return mark_as_nsfw_helper(self)
  else:
  return restrict_access('modposts')(mark_as_nsfw_helper)(self)","Mark as Not Safe For Work. Requires that the currently authenticated user is the author of the submission, has the modposts oauth scope or has user/password authentication as a mod of the subreddit. :returns: The json response from the server.",1,0,0,2,3,2,0,0,1,3
"def _build_pools(self):
  if self.level >= Topic:
  self.topics_pool = set(self.topic() for i in range(self.pool_size))
  if self.level >= Fact:
  self.facts_pool = set(self.fact() for i in range(self.pool_size))
  if self.level >= Theory:
  self.theories_pool = set(self.theory() for i in range(self.pool_size))
  if self.level >= Text:
  self.propositions_pool = set(chain.from_iterable((self.topics_pool, self.facts_pool, self.theories_pool)))","Slow method, retrieve all the terms from the database. :return:",0,0,0,0,0,1,0,0,1,2
"def watson_request(text, synth_args):
  params = {
  'text': text,
  'accept': 'audio/wav'
  }
  if synth_args is not None:
  params.update(synth_args)
  if 'username' in params:
  username = params.pop('username')
  else:
  raise Warning('The IBM Watson API requires credentials that should be passed as ""username"" and ""password"" in ""synth_args""')
  if 'password' in params:
  password = params.pop('password')
  else:
  raise Warning('The IBM Watson API requires credentials that should be passed as ""username"" and ""password"" in ""synth_args""')
  return requests.get(watson_url, auth=(username, password), params=params)",Makes a single request to the IBM Watson text-to-speech API. :param text: The text that will be synthesized to audio. :param synth_args: A dictionary of arguments to add to the request. These should include username and password for authentication.,2,0,0,1,3,2,0,0,1,3
"def parse_auth(header):
  try:
  method, data = header.split(None, 1)
  if method.lower() == 'basic':
  name, pwd = base64.b64decode(data).split(':', 1)
  return name, pwd
  except (KeyError, ValueError, TypeError):
  return None","Parse rfc2617 HTTP authentication header string (basic) and return (user,pass) tuple or None",0,0,0,1,1,1,0,0,1,2
"def is_user_attempt_whitelisted(request: AxesHttpRequest, credentials: dict = None) -> bool:
  username_field = getattr(get_user_model(), 'USERNAME_FIELD', 'username')
  username_value = get_client_username(request, credentials)
  kwargs = {
  username_field: username_value
  }
  user_model = get_user_model()
  try:
  user = user_model.objects.get(**kwargs)
  return user.nolockout
  except (user_model.DoesNotExist, AttributeError):
  pass
  return False","Check if the given request or credentials refer to a whitelisted username. A whitelisted user has the magic ``nolockout`` property set. If the property is unknown or False or the user can not be found, this implementation fails gracefully and returns True.",1,0,1,0,2,1,0,1,1,3
"def GrantApproval(self,
  requestor_username,
  approval_id,
  grantor_username,
  cursor=None):
  self._GrantApproval(
  requestor_username, _ApprovalIDToInt(approval_id), grantor_username,
  cursor)",Grants approval for a given request using given username.,1,1,0,0,2,1,0,0,1,2
"def calc_bootstrap(func, returns, *args, **kwargs):
  n_samples = kwargs.pop('n_samples', 1000)
  out = np.empty(n_samples)
  factor_returns = kwargs.pop('factor_returns', None)
  for i in range(n_samples):
  idx = np.random.randint(len(returns), size=len(returns))
  returns_i = returns.iloc[idx].reset_index(drop=True)
  if factor_returns is not None:
  factor_returns_i = factor_returns.iloc[idx].reset_index(drop=True)
  out[i] = func(returns_i, factor_returns_i,
  *args, **kwargs)
  else:
  out[i] = func(returns_i,
  *args, **kwargs)
  return out","Performs a bootstrap analysis on a user-defined function returning a summary statistic. Parameters ---------- func : function Function that either takes a single array (commonly returns) or two arrays (commonly returns and factor returns) and returns a single value (commonly a summary statistic). Additional args and kwargs are passed as well. returns : pd.Series Daily returns of the strategy, noncumulative. - See full explanation in tears.create_full_tear_sheet. factor_returns : pd.Series, optional Daily noncumulative returns of the benchmark factor to which betas are computed. Usually a benchmark such as market returns. - This is in the same style as returns. n_samples : int, optional Number of bootstrap samples to draw. Default is 1000. Increasing this will lead to more stable / accurate estimates. Returns ------- numpy.ndarray Bootstrapped sampling distribution of passed in func.",0,0,0,1,1,0,0,0,1,1
"def create_connection(url, timeout=None, class_=WebSocket, **options):
  sockopt = options.pop(""sockopt"", [])
  sslopt = options.pop(""sslopt"", {})
  fire_cont_frame = options.pop(""fire_cont_frame"", False)
  enable_multithread = options.pop(""enable_multithread"", False)
  skip_utf8_validation = options.pop(""skip_utf8_validation"", False)
  websock = class_(sockopt=sockopt, sslopt=sslopt,
  fire_cont_frame=fire_cont_frame,
  enable_multithread=enable_multithread,
  skip_utf8_validation=skip_utf8_validation, **options)
  websock.settimeout(timeout if timeout is not None else getdefaulttimeout())
  websock.connect(url, **options)
  return websock","connect to url and return websocket object. Connect to url and return the WebSocket object. Passing optional timeout parameter will set the timeout on the socket. If no timeout is supplied, the global default timeout setting returned by getdefauttimeout() is used. You can customize using 'options'. If you set ""header"" list object, you can set your own custom header. >>> conn = create_connection(""ws://echo.websocket.org/"", ... header=[""User-Agent: MyProgram"", ... ""x-custom: header""]) timeout: socket timeout time. This value is integer. if you set None for this value, it means ""use default_timeout value"" class_: class to instantiate when creating the connection. It has to implement settimeout and connect. It's __init__ should be compatible with WebSocket.__init__, i.e. accept all of it's kwargs. options: ""header"" -> custom http header list or dict. ""cookie"" -> cookie value. ""origin"" -> custom origin url. ""suppress_origin"" -> suppress outputting origin header. ""host"" -> custom host header string. ""http_proxy_host"" - http proxy host name. ""http_proxy_port"" - http proxy port. If not set, set to 80. ""http_no_proxy"" - host names, which doesn't use proxy. ""http_proxy_auth"" - http proxy auth information. tuple of username and password. default is None ""enable_multithread"" -> enable lock for multithread. ""redirect_limit"" -> number of redirects to follow. ""sockopt"" -> socket options ""sslopt"" -> ssl option ""subprotocols"" - array of available sub protocols. default is None. ""skip_utf8_validation"" - skip utf8 validation. ""socket"" - pre-initialized stream socket.",2,0,0,1,3,1,0,0,1,2
"def showpath(path):
  try:
  retval = os.path.relpath(path, os.getcwd())
  except ValueError:
  retval = path
  else:
  if retval.startswith('..'):
  retval = path
  return retval","Return path in form most convenient for user to read. Return relative path when input path is within the current working directory, otherwise return same (absolute) path passed in. :param path: file system path :type path: str or unicode :returns: file system path :rtype: str",0,0,0,1,1,1,0,0,1,2
"def statuses_home_timeline(self, count=None, since_id=None, max_id=None,
  trim_user=None, exclude_replies=None,
  contributor_details=None,
  include_entities=None):
  params = {}
  set_int_param(params, 'count', count)
  set_str_param(params, 'since_id', since_id)
  set_str_param(params, 'max_id', max_id)
  set_bool_param(params, 'trim_user', trim_user)
  set_bool_param(params, 'exclude_replies', exclude_replies)
  set_bool_param(params, 'contributor_details', contributor_details)
  set_bool_param(params, 'include_entities', include_entities)
  return self._get_api('statuses/home_timeline.json', params)","Returns a collection of the most recent Tweets and retweets posted by the authenticating user and the users they follow. https://dev.twitter.com/docs/api/1.1/get/statuses/home_timeline :param int count: Specifies the number of tweets to try and retrieve, up to a maximum of 200. :param str since_id: Returns results with an ID greater than (that is, more recent than) the specified ID. Tweets newer than this may not be returned due to certain API limits. :param str max_id: Returns results with an ID less than (that is, older than) or equal to the specified ID. :param bool trim_user: When set to ``True``, the tweet's user object includes only the status author's numerical ID. :param bool exclude_replies: When set to ``True``, replies will not appear in the timeline. :param bool contributor_details: This parameter enhances the contributors element of the status response to include the screen_name of the contributor. By default only the user_id of the contributor is included. :param bool include_entities: When set to ``False``, the ``entities`` node will not be included. :returns: A list of tweet dicts.",2,0,0,1,3,2,0,0,1,3
"def isAuthorized(self, request):
  restrictions = self.get_view_restrictions()
  if restrictions and request is None:
  return False
  else:
  return all(restriction.accept_request(request)
  for restriction in restrictions)",Is the user authorized for the requested action with this event?,1,0,0,1,2,1,0,0,1,2
"def subscribe(self, topics=(), pattern=None, listener=None):
  if self._user_assignment or (topics and pattern):
  raise IllegalStateError(self._SUBSCRIPTION_EXCEPTION_MESSAGE)
  assert topics or pattern, 'Must provide topics or pattern'
  if pattern:
  log.info('Subscribing to pattern: /%s/', pattern)
  self.subscription = set()
  self.subscribed_pattern = re.compile(pattern)
  else:
  self.change_subscription(topics)
  if listener and not isinstance(listener, ConsumerRebalanceListener):
  raise TypeError('listener must be a ConsumerRebalanceListener')
  self.listener = listener","Subscribe to a list of topics, or a topic regex pattern. Partitions will be dynamically assigned via a group coordinator. Topic subscriptions are not incremental: this list will replace the current assignment (if there is one). This method is incompatible with assign_from_user() Arguments: topics (list): List of topics for subscription. pattern (str): Pattern to match available topics. You must provide either topics or pattern, but not both. listener (ConsumerRebalanceListener): Optionally include listener callback, which will be called before and after each rebalance operation. As part of group management, the consumer will keep track of the list of consumers that belong to a particular group and will trigger a rebalance operation if one of the following events trigger: * Number of partitions change for any of the subscribed topics * Topic is created or deleted * An existing member of the consumer group dies * A new member is added to the consumer group When any of these events are triggered, the provided listener will be invoked first to indicate that the consumer's assignment has been revoked, and then again when the new assignment has been received. Note that this listener will immediately override any listener set in a previous call to subscribe. It is guaranteed, however, that the partitions revoked/assigned through this interface are from topics subscribed in this call.",1,0,0,0,1,1,0,0,1,2
"def get_groups_of_user(self, user_id, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('asynchronous'):
  return self.get_groups_of_user_with_http_info(user_id, **kwargs)
  else:
  (data) = self.get_groups_of_user_with_http_info(user_id, **kwargs)
  return data","Get groups of the user. # noqa: E501 An endpoint for retrieving groups of the user. **Example usage:** `curl https://api.us-east-1.mbedcloud.com/v3/users/{user-id}/groups -H 'Authorization: Bearer API_KEY'` # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass asynchronous=True >>> thread = api.get_groups_of_user(user_id, asynchronous=True) >>> result = thread.get() :param asynchronous bool :param str user_id: The ID of the user whose details are retrieved. (required) :param int limit: The number of results to return (2-1000), default is 50. :param str after: The entity ID to fetch after the given one. :param str order: The order of the records based on creation time, ASC or DESC; by default ASC :param str include: Comma separated additional data to return. Currently supported: total_count :return: GroupSummaryList If the method is called asynchronously, returns the request thread.",2,0,0,1,3,2,0,0,1,3
"def import_users(self, users):
  if len(users) <= 0:
  return
  known_attrs = ('login', 'fullName', 'email', 'jabber')
  xml = '<list>\n'
  for u in users:
  xml += ' <user ' + """".join(k + '=' + quoteattr(u[k]) + ' ' for k in u if k in known_attrs) + '/>\n'
  xml += '</list>'
  if isinstance(xml, str):
  xml = xml.encode('utf-8')
  return self._req_xml('PUT', '/import/users', xml, 400).toxml()","Import users, returns import result (http://confluence.jetbrains.net/display/YTD2/Import+Users) Example: importUsers([{'login':'vadim', 'fullName':'vadim', 'email':'eee@ss.com', 'jabber':'fff@fff.com'}, {'login':'maxim', 'fullName':'maxim', 'email':'aaa@ss.com', 'jabber':'www@fff.com'}])",1,1,0,1,3,1,0,0,1,2
"def can_create_bin_with_record_types(self, bin_record_types):
  if self._catalog_session is not None:
  return self._catalog_session.can_create_catalog_with_record_types(catalog_record_types=bin_record_types)
  return True","Tests if this user can create a single ``Bin`` using the desired record types. While ``ResourceManager.getBinRecordTypes()`` can be used to examine which records are supported, this method tests which record(s) are required for creating a specific ``Bin``. Providing an empty array tests if a ``Bin`` can be created with no records. arg: bin_record_types (osid.type.Type[]): array of bin record types return: (boolean) - ``true`` if ``Bin`` creation using the specified ``Types`` is supported, ``false`` otherwise raise: NullArgument - ``bin_record_types`` is ``null`` *compliance: mandatory -- This method must be implemented.*",2,0,0,1,3,1,0,0,1,2
"def delete_posix_account(
  self,
  name,
  retry=google.api_core.gapic_v1.method.DEFAULT,
  timeout=google.api_core.gapic_v1.method.DEFAULT,
  metadata=None,
  ):
  if ""delete_posix_account"" not in self._inner_api_calls:
  self._inner_api_calls[
  ""delete_posix_account""
  ] = google.api_core.gapic_v1.method.wrap_method(
  self.transport.delete_posix_account,
  default_retry=self._method_configs[""DeletePosixAccount""].retry,
  default_timeout=self._method_configs[""DeletePosixAccount""].timeout,
  client_info=self._client_info,
  )
  request = oslogin_pb2.DeletePosixAccountRequest(name=name)
  self._inner_api_calls[""delete_posix_account""](
  request, retry=retry, timeout=timeout, metadata=metadata
  )","Deletes a POSIX account. Example: >>> from google.cloud import oslogin_v1 >>> >>> client = oslogin_v1.OsLoginServiceClient() >>> >>> name = client.project_path('[USER]', '[PROJECT]') >>> >>> client.delete_posix_account(name) Args: name (str): A reference to the POSIX account to update. POSIX accounts are identified by the project ID they are associated with. A reference to the POSIX account is in format ``users/{user}/projects/{project}``. retry (Optional[google.api_core.retry.Retry]): A retry object used to retry requests. If ``None`` is specified, requests will not be retried. timeout (Optional[float]): The amount of time, in seconds, to wait for the request to complete. Note that if ``retry`` is specified, the timeout applies to each individual attempt. metadata (Optional[Sequence[Tuple[str, str]]]): Additional metadata that is provided to the method. Raises: google.api_core.exceptions.GoogleAPICallError: If the request failed for any reason. google.api_core.exceptions.RetryError: If the request failed due to a retryable error and retry attempts failed. ValueError: If the parameters are invalid.",1,0,0,1,2,1,0,0,1,2
"def get_char(prompt=None):
  while True:
  s = get_string(prompt)
  if s is None:
  return None
  if len(s) == 1:
  return s[0]
  if prompt is None:
  print(""Retry: "", end="""")","Read a line of text from standard input and return the equivalent char; if text is not a single char, user is prompted to retry. If line can't be read, return None.",1,0,0,1,2,1,0,0,1,2
"def read_json(self, file_path,
  *args, **kwargs):
  def json_file_to_df(files):
  for _, contents in files:
  yield pandas.read_json(sio(contents), *args, **kwargs)
  return self.from_pandas_rdd(self.spark_ctx.wholeTextFiles(file_path)
  .mapPartitions(json_file_to_df))","Read a json file in and parse it into Pandas DataFrames. If no names is provided we use the first row for the names. Currently, it is not possible to skip the first n rows of a file. Headers are provided in the json file and not specified separately. Parameters ---------- file_path: string Path to input. Any valid file path in Spark works here, eg: 'my/path/in/local/file/system' or 'hdfs:/user/juliet/' Other than skipRows, all additional parameters available in pandas.read_csv() are usable here. Returns ------- A SparklingPandas DataFrame that contains the data from the specified file.",1,0,0,1,2,1,0,0,1,2
"def has_permission(self, request, view):
  url_username = request.parser_context.get('kwargs', {}).get('username', '')
  if request.user.username.lower() != url_username.lower():
  if request.user.is_staff:
  return False
  raise Http404()
  return True",Returns true if the current request is by the user themselves. Note: a 404 is returned for non-staff instead of a 403. This is to prevent users from being able to detect the existence of accounts.,0,0,0,1,1,1,0,0,1,2
"def main(guess_a=1., guess_b=0., power=3, savetxt='None', verbose=False):
  x, sol = solve(guess_a, guess_b, power)
  assert sol.success
  if savetxt != 'None':
  np.savetxt(x, savetxt)
  else:
  if verbose:
  print(sol)
  else:
  print(x)",Example demonstrating how to solve a system of non-linear equations defined as SymPy expressions. The example shows how a non-linear problem can be given a command-line interface which may be preferred by end-users who are not familiar with Python.,1,0,0,1,2,1,0,0,1,2
"def state(self, state):
  assert state != 'build_bundle'
  self.buildstate.state.current = state
  self.buildstate.state[state] = time()
  self.buildstate.state.lasttime = time()
  self.buildstate.state.error = False
  self.buildstate.state.exception = None
  self.buildstate.state.exception_type = None
  self.buildstate.commit()
  if state in (self.STATES.NEW, self.STATES.CLEANED, self.STATES.BUILT, self.STATES.FINALIZED,
  self.STATES.SOURCE):
  state = state if state != self.STATES.CLEANED else self.STATES.NEW
  self.dstate = state","Set the current build state and record the time to maintain history. Note! This is different from the dataset state. Setting the build set is commiteed to the progress table/database immediately. The dstate is also set, but is not committed until the bundle is committed. So, the dstate changes more slowly.",0,1,0,0,1,0,0,0,0,0
"def is_analysis_edition_allowed(self, analysis_brain):
  if not self.context_active:
  return False
  analysis_obj = api.get_object(analysis_brain)
  if analysis_obj.getPointOfCapture() == 'field':
  if not self.has_permission(EditFieldResults, analysis_obj):
  return False
  elif not self.has_permission(EditResults, analysis_obj):
  return False
  if not self.has_permission(FieldEditAnalysisResult, analysis_obj):
  return False
  if not self.is_analysis_instrument_valid(analysis_brain):
  return analysis_obj.getManualEntryOfResults()
  return True","Returns if the analysis passed in can be edited by the current user :param analysis_brain: Brain that represents an analysis :return: True if the user can edit the analysis, otherwise False",2,0,0,2,4,1,0,1,1,3
"def setGroups(self, *args, **kwargs):
  try:
  groups = self.mambugroupsclass(creditOfficerUsername=self['username'], *args, **kwargs)
  except AttributeError as ae:
  from .mambugroup import MambuGroups
  self.mambugroupsclass = MambuGroups
  groups = self.mambugroupsclass(creditOfficerUsername=self['username'], *args, **kwargs)
  self['groups'] = groups
  return 1",Adds the groups assigned to this user to a 'groups' field. Returns the number of requests done to Mambu.,2,0,1,0,3,1,0,0,1,2
"def change_password(self, previous, new_password):
  if not self.verify_password(previous):
  raise exceptions.Unauthorized('Incorrect password')
  if len(new_password) < options.min_length_password:
  msg = ('Passwords must be at least {} characters'
  .format(options.min_length_password))
  raise exceptions.ValidationError(msg)
  if len(new_password) > options.max_length_password:
  msg = ('Passwords must be at no more than {} characters'
  .format(options.max_length_password))
  raise exceptions.ValidationError(msg)
  self.password = self.hash_password(new_password)
  yield self._save()",Change the user's password and save to the database :param previous: plain text previous password :param new_password: plain text new password :raises: ValidationError,1,0,0,1,2,1,1,0,1,3
"def create_session(self, username, prekeybundle, autotrust=False):
  logger.debug(""create_session(username=%s, prekeybunder=[omitted], autotrust=%s)"" % (username, autotrust))
  session_builder = SessionBuilder(self._store, self._store, self._store, self._store, username, 1)
  try:
  session_builder.processPreKeyBundle(prekeybundle)
  except UntrustedIdentityException as ex:
  if autotrust:
  self.trust_identity(ex.getName(), ex.getIdentityKey())
  else:
  raise exceptions.UntrustedIdentityException(ex.getName(), ex.getIdentityKey())",:param username: :type username: str :param prekeybundle: :type prekeybundle: PreKeyBundle :return: :rtype:,0,0,0,0,0,1,0,0,0,1
"def libravatar_url(email=None, openid=None, size=64, default=""retro""):
  params = collections.OrderedDict([(""s"", size), (""d"", default)])
  query = parse.urlencode(params)
  if email:
  value = email
  elif openid:
  value = openid
  else:
  raise ValueError(""You must provide either the email or the openid."")
  idhash = sha256(value.encode(""utf-8"")).hexdigest()
  return ""https://seccdn.libravatar.org/avatar/%s?%s"" % (idhash, query)","Get the URL to an avatar from libravatar. Either the user's email or openid must be provided. If you want to use Libravatar federation (through DNS), you should install and use the ``libravatar`` library instead. Check out the ``libravatar.libravatar_url()`` function. Args: email (str): The user's email openid (str): The user's OpenID size (int): Size of the avatar in pixels (it's a square). default (str): Default avatar to return if not found. Returns: str: The URL to the avatar image. Raises: ValueError: If neither email nor openid are provided.",2,0,0,1,3,1,0,0,1,2
"def add_ipv4(self, id_network_ipv4, id_equipamento, descricao):
  ip_map = dict()
  ip_map['id_network_ipv4'] = id_network_ipv4
  ip_map['description'] = descricao
  ip_map['id_equipment'] = id_equipamento
  code, xml = self.submit({'ip': ip_map}, 'POST', 'ipv4/')
  return self.response(code, xml)","Allocate an IP on a network to an equipment. Insert new IP for network and associate to the equipment :param id_network_ipv4: ID for NetworkIPv4. :param id_equipamento: ID for Equipment. :param descricao: Description for IP. :return: Following dictionary: :: {'ip': {'id': < id_ip >, 'id_network_ipv4': < id_network_ipv4 >, 'oct1: < oct1 >, 'oct2': < oct2 >, 'oct3': < oct3 >, 'oct4': < oct4 >, 'descricao': < descricao >}} :raise InvalidParameterError: Invalid ID for NetworkIPv4 or Equipment. :raise InvalidParameterError: The value of description is invalid. :raise EquipamentoNaoExisteError: Equipment not found. :raise RedeIPv4NaoExisteError: NetworkIPv4 not found. :raise IPNaoDisponivelError: There is no network address is available to create the VLAN. :raise ConfigEnvironmentInvalidError: Invalid Environment Configuration or not registered :raise DataBaseError: Networkapi failed to access the database. :raise XMLError: Networkapi failed to generate the XML response.",1,0,0,2,3,2,0,0,2,4
"def forward(
  self,
  chat_id: Union[int, str],
  disable_notification: bool = None,
  as_copy: bool = False,
  remove_caption: bool = False
  ):
  forwarded_messages = []
  for message in self.messages:
  forwarded_messages.append(
  message.forward(
  chat_id=chat_id,
  as_copy=as_copy,
  disable_notification=disable_notification,
  remove_caption=remove_caption
  )
  )
  return Messages(
  total_count=len(forwarded_messages),
  messages=forwarded_messages,
  client=self._client
  )","Bound method *forward* of :obj:`Message <pyrogram.Messages>`. Args: chat_id (``int`` | ``str``): Unique identifier (int) or username (str) of the target chat. For your personal cloud (Saved Messages) you can simply use ""me"" or ""self"". For a contact that exists in your Telegram address book you can use his phone number (str). disable_notification (``bool``, *optional*): Sends messages silently. Users will receive a notification with no sound. as_copy (``bool``, *optional*): Pass True to forward messages without the forward header (i.e.: send a copy of the message content). Defaults to False. remove_caption (``bool``, *optional*): If set to True and *as_copy* is enabled as well, media captions are not preserved when copying the message. Has no effect if *as_copy* is not enabled. Defaults to False. Returns: On success, a :class:`Messages <pyrogram.Messages>` containing forwarded messages is returned. Raises: :class:`RPCError <pyrogram.RPCError>`",1,0,0,1,2,1,0,0,1,2
"def _friendlyAuthError(fn):
  @functools.wraps(fn)
  def wrapped(*args, **kwargs):
  try:
  return fn(*args, **kwargs)
  except requests.exceptions.HTTPError as e:
  if e.response.status_code == requests.codes.unauthorized:
  logger.error('insufficient permission')
  elif e.response.status_code == requests.codes.bad and 'jwt has expired' in e.response.text.lower():
  logger.error('server returned status %s: %s', e.response.status_code, e.response.text)
  logger.error('Check that your system clock is set accurately!')
  else:
  logger.error('server returned status %s: %s', e.response.status_code, e.response.text)
  raise
  return wrapped",Decorator to print a friendly you-are-not-authorised message. Use **outside** the _handleAuth decorator to only print the message after the user has been given a chance to login.,1,0,0,1,2,1,0,0,1,2
"def sync_networks(self):
  nets = self.neutronclient.list_networks()
  for net in nets.get(""networks""):
  LOG.info(""Syncing network %s"", net[""id""])
  self.network_create_func(net)
  subnets = self.neutronclient.list_subnets()
  for subnet in subnets.get(""subnets""):
  LOG.info(""Syncing subnet %s"", subnet[""id""])
  self.create_subnet(subnet)",sync networks. It will retrieve networks from neutron and populate them in dfa database and dcnm,1,1,0,1,3,1,0,0,1,2
"def user_remove_prj(self, *args, **kwargs):
  if not self.cur_user:
  return
  i = self.user_prj_tablev.currentIndex()
  item = i.internalPointer()
  if item:
  prj = item.internal_data()
  prj.users.remove(self.cur_user)
  item.set_parent(None)",Remove the selected project from the user :returns: None :rtype: None :raises: None,1,0,0,1,2,1,0,0,1,2
"def menu(self, prompt, choices):
  menu = [prompt] + [
  ""{0}. {1}"".format(*choice) for choice in enumerate(choices, start=1)
  ]
  command = 'inputlist({})'.format(repr(menu))
  choice = int(self._vim.eval(command))
  if not 0 < choice < len(menu):
  return
  return choices[choice - 1]","Presents a selection menu and returns the user's choice. Args: prompt (str): Text to ask the user what to select. choices (Sequence[str]): Values for the user to select from. Returns: The value selected by the user, or ``None``. Todo: Nice opportunity to provide a hook for Unite.vim, etc. here.",1,0,0,1,2,1,0,0,1,2
"def user_get_messages(self, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.user_get_messages_with_http_info(**kwargs)
  else:
  (data) = self.user_get_messages_with_http_info(**kwargs)
  return data","Gets messages applicable to the current user, i.e. within time range and distribution scope # noqa: E501 # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.user_get_messages(async_req=True) >>> result = thread.get() :param async_req bool :param int offset: :param int limit: :param bool unread_only: :return: ResponseContainerPagedMessage If the method is called asynchronously, returns the request thread.",2,0,0,1,3,2,0,0,1,3
"def saved_from_user(cls, user, include_pending=False):
  url = 'https://gameclipsmetadata.xboxlive.com/users/xuid(%s)/clips/saved'
  resp = xbox.client._get(url % user.xuid)
  data = resp.json()
  for clip in data['gameClips']:
  if clip['state'] != 'PendingUpload' or include_pending:
  yield cls(user, clip)",Gets all clips 'saved' by a user. :param user: :class:`~xbox.GamerProfile` instance :param bool include_pending: whether to ignore clips that are not yet uploaded. These clips will have thumbnails and media_url set to ``None`` :returns: Iterator of :class:`~xbox.Clip` instances,1,0,0,1,2,2,0,0,1,3
"def get_feed_changes(self, include_deleted=None, continuation_token=None, batch_size=None):
  query_parameters = {}
  if include_deleted is not None:
  query_parameters['includeDeleted'] = self._serialize.query('include_deleted', include_deleted, 'bool')
  if continuation_token is not None:
  query_parameters['continuationToken'] = self._serialize.query('continuation_token', continuation_token, 'long')
  if batch_size is not None:
  query_parameters['batchSize'] = self._serialize.query('batch_size', batch_size, 'int')
  response = self._send(http_method='GET',
  location_id='29ba2dad-389a-4661-b5d3-de76397ca05b',
  version='5.0-preview.1',
  query_parameters=query_parameters)
  return self._deserialize('FeedChangesResponse', response)","GetFeedChanges. [Preview API] Query to determine which feeds have changed since the last call, tracked through the provided continuationToken. Only changes to a feed itself are returned and impact the continuationToken, not additions or alterations to packages within the feeds. :param bool include_deleted: If true, get changes for all feeds including deleted feeds. The default value is false. :param long continuation_token: A continuation token which acts as a bookmark to a previously retrieved change. This token allows the user to continue retrieving changes in batches, picking up where the previous batch left off. If specified, all the changes that occur strictly after the token will be returned. If not specified or 0, iteration will start with the first change. :param int batch_size: Number of package changes to fetch. The default value is 1000. The maximum value is 2000. :rtype: :class:`<FeedChangesResponse> <azure.devops.v5_0.feed.models.FeedChangesResponse>`",1,0,0,1,2,2,0,0,1,3
"def add_coeffs(self, Tmin, Tmax, coeffs):
  self.n += 1
  if not self.Ts:
  self.Ts = [Tmin, Tmax]
  self.coeff_sets = [coeffs]
  else:
  for ind, T in enumerate(self.Ts):
  if Tmin < T:
  self.Ts.insert(ind, Tmin)
  self.coeff_sets.insert(ind, coeffs)
  return
  self.Ts.append(Tmax)
  self.coeff_sets.append(coeffs)","Called internally during the parsing of the Zabransky database, to add coefficients as they are read one per line",0,0,1,0,1,0,0,0,1,1
"def create_307_response(self):
  request = get_current_request()
  msg_mb = UserMessageMember(self.message)
  coll = request.root['_messages']
  coll.add(msg_mb)
  qs = self.__get_new_query_string(request.query_string,
  self.message.slug)
  resubmit_url = ""%s?%s"" % (request.path_url, qs)
  headers = [('Warning', '299 %s' % self.message.text),
  ]
  http_exc = HttpWarningResubmit(location=resubmit_url,
  detail=self.message.text,
  headers=headers)
  return request.get_response(http_exc)","Creates a 307 ""Temporary Redirect"" response including a HTTP Warning header with code 299 that contains the user message received during processing the request.",1,0,0,1,2,1,0,1,1,3
"def user_exists(self,
  user,
  note=None,
  loglevel=logging.DEBUG):
 shutit = self.shutit
 shutit.handle_note(note)
 exists = False
 if user == '':
 return exists
 ret = self.send(ShutItSendSpec(self,
  send=' command id %s && echo E""""XIST || echo N""""XIST' % user,
  expect=['NXIST', 'EXIST'],
  echo=False,
  loglevel=loglevel,
  ignore_background=True))
 if ret:
 exists = True
 self.expect(self.default_expect)
 shutit.handle_note_after(note=note)
 return exists",Returns true if the specified username exists. @param user: username to check for @param note: See send() @type user: string @rtype: boolean,1,0,0,0,1,1,0,0,1,2
"def get_enterprise_customer_user_queryset(self, request, search_keyword, customer_uuid, page_size=PAGE_SIZE):
  page = request.GET.get('page', 1)
  learners = EnterpriseCustomerUser.objects.filter(enterprise_customer__uuid=customer_uuid)
  user_ids = learners.values_list('user_id', flat=True)
  matching_users = User.objects.filter(pk__in=user_ids)
  if search_keyword is not None:
  matching_users = matching_users.filter(
  Q(email__icontains=search_keyword) | Q(username__icontains=search_keyword)
  )
  matching_user_ids = matching_users.values_list('pk', flat=True)
  learners = learners.filter(user_id__in=matching_user_ids)
  return paginated_list(learners, page, page_size)",Get the list of EnterpriseCustomerUsers we want to render. Arguments: request (HttpRequest): HTTP Request instance. search_keyword (str): The keyword to search for in users' email addresses and usernames. customer_uuid (str): A unique identifier to filter down to only users linked to a particular EnterpriseCustomer. page_size (int): Number of learners displayed in each paginated set.,1,0,1,1,3,1,0,1,1,3
"def _run_svn(cmd, cwd, user, username, password, opts, **kwargs):
  cmd = ['svn', '--non-interactive', cmd]
  options = list(opts)
  if username:
  options.extend(['--username', username])
  if password:
  options.extend(['--password', password])
  cmd.extend(options)
  result = __salt__['cmd.run_all'](cmd, python_shell=False, cwd=cwd, runas=user, **kwargs)
  retcode = result['retcode']
  if retcode == 0:
  return result['stdout']
  raise CommandExecutionError(result['stderr'] + '\n\n' + ' '.join(cmd))",Execute svn return the output of the command cmd The command to run. cwd The path to the Subversion repository user Run svn as a user other than what the minion runs as username Connect to the Subversion server as another user password Connect to the Subversion server with this password .. versionadded:: 0.17.0 opts Any additional options to add to the command line kwargs Additional options to pass to the run-cmd,1,0,0,1,2,1,0,0,1,2
"def get_statements(subject=None, object=None, agents=None, stmt_type=None,
  use_exact_type=False, persist=True, timeout=None,
  simple_response=False, ev_limit=10, best_first=True, tries=2,
  max_stmts=None):
  processor = IndraDBRestProcessor(subject, object, agents, stmt_type,
  use_exact_type, persist, timeout,
  ev_limit, best_first, tries, max_stmts)
  if simple_response:
  ret = processor.statements
  else:
  ret = processor
  return ret","Get a processor for the INDRA DB web API matching given agents and type. There are two types of responses available. You can just get a list of INDRA Statements, or you can get an IndraDBRestProcessor object, which allow Statements to be loaded in a background thread, providing a sample of the best* content available promptly in the sample_statements attribute, and populates the statements attribute when the paged load is complete. The latter should be used in all new code, and where convenient the prior should be converted to use the processor, as this option may be removed in the future. * In the sense of having the most supporting evidence. Parameters ---------- subject/object : str Optionally specify the subject and/or object of the statements in you wish to get from the database. By default, the namespace is assumed to be HGNC gene names, however you may specify another namespace by including `@<namespace>` at the end of the name string. For example, if you want to specify an agent by chebi, you could use `CHEBI:6801@CHEBI`, or if you wanted to use the HGNC id, you could use `6871@HGNC`. agents : list[str] A list of agents, specified in the same manner as subject and object, but without specifying their grammatical position. stmt_type : str Specify the types of interactions you are interested in, as indicated by the sub-classes of INDRA's Statements. This argument is *not* case sensitive. If the statement class given has sub-classes (e.g. RegulateAmount has IncreaseAmount and DecreaseAmount), then both the class itself, and its subclasses, will be queried, by default. If you do not want this behavior, set use_exact_type=True. Note that if max_stmts is set, it is possible only the exact statement type will be returned, as this is the first searched. The processor then cycles through the types, getting a page of results for each type and adding it to the quota, until the max number of statements is reached. use_exact_type : bool If stmt_type is given, and you only want to search for that specific statement type, set this to True. Default is False. persist : bool Default is True. When False, if a query comes back limited (not all results returned), just give up and pass along what was returned. Otherwise, make further queries to get the rest of the data (which may take some time). timeout : positive int or None If an int, block until the work is done and statements are retrieved, or until the timeout has expired, in which case the results so far will be returned in the response object, and further results will be added in a separate thread as they become available. If simple_response is True, all statements available will be returned. Otherwise (if None), block indefinitely until all statements are retrieved. Default is None. simple_response : bool If True, a simple list of statements is returned (thus block should also be True). If block is False, only the original sample will be returned (as though persist was False), until the statements are done loading, in which case the rest should appear in the list. This behavior is not encouraged. Default is False (which breaks backwards compatibility with usage of INDRA versions from before 1/22/2019). WE ENCOURAGE ALL NEW USE-CASES TO USE THE PROCESSOR, AS THIS FEATURE MAY BE REMOVED AT A LATER DATE. ev_limit : int or None Limit the amount of evidence returned per Statement. Default is 10. best_first : bool If True, the preassembled statements will be sorted by the amount of evidence they have, and those with the most evidence will be prioritized. When using `max_stmts`, this means you will get the ""best"" statements. If False, statements will be queried in arbitrary order. tries : int > 0 Set the number of times to try the query. The database often caches results, so if a query times out the first time, trying again after a timeout will often succeed fast enough to avoid a timeout. This can also help gracefully handle an unreliable connection, if you're willing to wait. Default is 2. max_stmts : int or None Select the maximum number of statements to return. When set less than 1000 the effect is much the same as setting persist to false, and will guarantee a faster response. Default is None. Returns ------- processor : :py:class:`IndraDBRestProcessor` An instance of the IndraDBRestProcessor, which has an attribute `statements` which will be populated when the query/queries are done. This is the default behavior, and is encouraged in all future cases, however a simple list of statements may be returned using the `simple_response` option described above.",2,0,1,1,4,1,0,0,1,2
"def rev_parse(cwd,
  rev=None,
  opts='',
  git_opts='',
  user=None,
  password=None,
  ignore_retcode=False,
  output_encoding=None):
  cwd = _expand_path(cwd, user)
  command = ['git'] + _format_git_opts(git_opts)
  command.append('rev-parse')
  command.extend(_format_opts(opts))
  if rev is not None:
  command.append(rev)
  return _git_run(command,
  cwd=cwd,
  user=user,
  password=password,
  ignore_retcode=ignore_retcode,
  output_encoding=output_encoding)['stdout']",".. versionadded:: 2015.8.0 Interface to `git-rev-parse(1)`_ cwd The path to the git checkout rev Revision to parse. See the `SPECIFYING REVISIONS`_ section of the `git-rev-parse(1)`_ manpage for details on how to format this argument. This argument is optional when using the options in the `Options for Files` section of the `git-rev-parse(1)`_ manpage. opts Any additional options to add to the command line, in a single string git_opts Any additional options to add to git command itself (not the ``rev-parse`` subcommand), in a single string. This is useful for passing ``-c`` to run git with temporary changes to the git configuration. .. versionadded:: 2017.7.0 .. note:: This is only supported in git 1.7.2 and newer. user User under which to run the git command. By default, the command is run by the user under which the minion is running. password Windows only. Required when specifying ``user``. This parameter will be ignored on non-Windows platforms. .. versionadded:: 2016.3.4 ignore_retcode : False If ``True``, do not log an error to the minion log if the git command returns a nonzero exit status. output_encoding Use this option to specify which encoding to use to decode the output from any git commands which are run. This should not be needed in most cases. .. note:: This should only be needed if the files in the repository were created with filenames using an encoding other than UTF-8 to handle Unicode characters. .. versionadded:: 2018.3.1 .. _`git-rev-parse(1)`: http://git-scm.com/docs/git-rev-parse .. _`SPECIFYING REVISIONS`: http://git-scm.com/docs/git-rev-parse#_specifying_revisions .. _`Options for Files`: http://git-scm.com/docs/git-rev-parse#_options_for_files CLI Examples: .. code-block:: bash # Get the full SHA1 for HEAD salt myminion git.rev_parse /path/to/repo HEAD # Get the short SHA1 for HEAD salt myminion git.rev_parse /path/to/repo HEAD opts='--short' # Get the develop branch's upstream tracking branch salt myminion git.rev_parse /path/to/repo 'develop@{upstream}' opts='--abbrev-ref' # Get the SHA1 for the commit corresponding to tag v1.2.3 salt myminion git.rev_parse /path/to/repo 'v1.2.3^{commit}' # Find out whether or not the repo at /path/to/repo is a bare repository salt myminion git.rev_parse /path/to/repo opts='--is-bare-repository'",1,0,0,1,2,1,0,0,1,2
"def handle_unsubscribe_request(cls, request, message, dispatch, hash_is_valid, redirect_to):
  if hash_is_valid:
  Subscription.cancel(
  dispatch.recipient_id or dispatch.address, cls.alias, dispatch.messenger
  )
  signal = sig_unsubscribe_success
  else:
  signal = sig_unsubscribe_failed
  signal.send(cls, request=request, message=message, dispatch=dispatch)
  return redirect(redirect_to)",Handles user subscription cancelling request. :param Request request: Request instance :param Message message: Message model instance :param Dispatch dispatch: Dispatch model instance :param bool hash_is_valid: Flag indicating that user supplied request signature is correct :param str redirect_to: Redirection URL :rtype: list,1,1,0,1,3,1,0,0,1,2
"def generate_token(self, user, password):
  logger.debug('(TOKEN_CREATE) :: User: %s' % user)
  session = OAuth2Session(client=LegacyApplicationClient(client_id=self.client_id))
  try:
  return dict(session.fetch_token(token_url=self.token_url,
  username=user,
  password=password,
  client_id=self.client_id,
  client_secret=self.client_secret))
  except OAuth2Error as exception:
  raise TokenCreateError('Error creating user token', exception.description, exception.status_code)",Takes user and password credentials and generates a new token :param user: user :param password: password :return: - dictionary containing token data :raises: - TokenCreateError: If there was an error generating the new token,2,0,0,2,4,2,0,0,1,3
"def list_faxes(self, user_id, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async'):
  return self.list_faxes_with_http_info(user_id, **kwargs)
  else:
  (data) = self.list_faxes_with_http_info(user_id, **kwargs)
  return data","Get fax records # noqa: E501 With this API call you will be able to retrieve a collection of faxes (either sent or received or spam based on the category selected). If you want to filter your archive please provide the `category` parameter # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async=True >>> thread = api.list_faxes(user_id, async=True) >>> result = thread.get() :param async bool :param str user_id: 'self' or user id of a corporate member (required) :param str category: Category parameter can be one of these values: **inbox**, **sent**, **spam** :param str after: Start date to get records from that date :param str before: End date to get records before that date :param int limit: Limit of fax records you want to get per request :return: ResponseArchive If the method is called asynchronously, returns the request thread.",2,0,0,1,3,2,0,0,1,3
"def add_user(username, password):
  user_model = Query()
  if db.search(user_model.username == username):
  return {
  'error': 'User {0} already exists'.format(username)
  }
  salt = hashlib.sha512(str(os.urandom(64)).encode('utf-8')).hexdigest()
  password = hash_password(password, salt)
  api_key = gen_api_key(username)
  user = {
  'username': username,
  'password': password,
  'salt': salt,
  'api_key': api_key
  }
  user_id = db.insert(user)
  return {
  'result': 'success',
  'eid': user_id,
  'user_created': user
  }",CLI Parameter to add a user to the database :param username: :param password: :return: JSON status output,1,1,1,1,4,1,1,1,1,4
"def refreshUserMembership(self, users):
  params = {
  ""f"" : ""json"",
  ""users"" : users
  }
  url = self._url + ""/users/refreshMembership""
  return self._post(url=url,
  param_dict=params,
  proxy_port=self._proxy_port,
  proxy_url=self._proxy_url)","This operation iterates over every enterprise group configured in the portal and determines if the input user accounts belong to any of the configured enterprise groups. If there is any change in membership, the database and the indexes are updated for each user account. While portal automatically refreshes the memberships during a user login and during a periodic refresh (configured through the Update Identity Store operation), this operation allows an administrator to force a refresh. Parameters: users - comma seperated list of user names",1,0,0,1,2,2,0,0,2,4
"def editPage(self, path, title, content=None, html_content=None,
  author_name=None, author_url=None, return_content=False):
  if content is None:
  content = html_to_nodes(html_content)
  if path is None:
  raise TelegraphAPIException(""Error while executing editPage: ""
  ""PAGE_NOT_FOUND"")
  r = requests.post(BASE_URL + ""editPage/"" + path, data={
  ""access_token"": self.access_token,
  ""title"": title,
  ""content"": json.dumps(content),
  ""author_name"": author_name,
  ""author_url"": author_url,
  ""return_content"": return_content
  })
  if r.json()['ok'] is not True:
  raise TelegraphAPIException(""Error while executing editPage: "" +
  r.json()['error'])
  return r.json()['result']","Use this method to edit an existing Telegraph page. :param path: Required. Path to the page. :type path: str :param title: Required. Page title. :type title: str :param content: Required. Content of the page in NODES format. :param html_content: Optional. Content of the page in HTML format. :type html_content: str :param author_name: Optional. Author name, displayed below the article's title. :type author_name: str :param author_url: Optional. Profile link, opened when users click on the author's name below the title. Can be any link, not necessarily to a Telegram profile or channel. :type author_url: str :param return_content: Optional. If true, a content field will be returned in the Page object. :type return_content: bool :returns: Page object.",2,1,0,2,5,2,0,0,1,3
"def grant_privilege(self, privilege, database, username):
  text = ""GRANT {0} ON {1} TO {2}"".format(privilege,
  quote_ident(database),
  quote_ident(username))
  self.query(text, method=""POST"")","Grant a privilege on a database to a user. :param privilege: the privilege to grant, one of 'read', 'write' or 'all'. The string is case-insensitive :type privilege: str :param database: the database to grant the privilege on :type database: str :param username: the username to grant the privilege to :type username: str",1,1,0,0,2,1,0,0,1,2
"def iter_all_posts(self, limit=None):
  feed = self.get_feed(limit=999999, offset=0)
  cids = [post['id'] for post in feed[""feed""]]
  if limit is not None:
  cids = cids[:limit]
  for cid in cids:
  yield self.get_post(cid)","Get all posts visible to the current user This grabs you current feed and ids of all posts from it; each post is then individually fetched. This method does not go against a bulk endpoint; it retrieves each post individually, so a caution to the user when using this. :type limit: int|None :param limit: If given, will limit the number of posts to fetch before the generator is exhausted and raises StopIteration. No special consideration is given to `0`; provide `None` to retrieve all posts. :returns: An iterator which yields all posts which the current user can view :rtype: generator",2,0,0,1,3,2,0,1,1,4
"def set_flair(self, subreddit, item, flair_text='', flair_css_class=''):
  data = {'r': six.text_type(subreddit),
  'text': flair_text or '',
  'css_class': flair_css_class or ''}
  if isinstance(item, objects.Submission):
  data['link'] = item.fullname
  evict = item.permalink
  else:
  data['name'] = six.text_type(item)
  evict = self.config['flairlist'].format(
  subreddit=six.text_type(subreddit))
  response = self.request_json(self.config['flair'], data=data)
  self.evict(evict)
  return response","Set flair for the user in the given subreddit. `item` can be a string, Redditor object, or Submission object. If `item` is a string it will be treated as the name of a Redditor. This method can only be called by a subreddit moderator with flair permissions. To set flair on yourself or your own links use :meth:`~praw.__init__.AuthenticatedReddit.select_flair`. :returns: The json response from the server.",1,0,0,2,3,1,0,0,1,2
"def authenticate(self, request):
  auth = get_authorization_header(request).split()
  if not auth or auth[0].lower() != b'token':
  return None
  if len(auth) == 1:
  msg = _('Invalid auth token header. No credentials provided.')
  raise AuthenticationFailed(msg)
  elif len(auth) > 2:
  msg = _('Invalid auth token.')
  raise AuthenticationFailed(msg)
  try:
  token = urlsafe_b64decode(auth[1])
  except ValueError:
  msg = _('Invalid auth token.')
  raise AuthenticationFailed(msg)
  return self.authenticate_credentials(token, request)","Authenticate the request and return a two-tuple of (user, token).",0,0,0,1,1,1,0,0,1,2
"def auth_required(method):
  @functools.wraps(method)
  def wrapper(*args, **kwargs):
  _verify_and_add_jwt()
  try:
  return method(*args, **kwargs)
  finally:
  remove_jwt_data_from_app_context()
  return wrapper",This decorator is used to ensure that a user is authenticated before being able to access a flask route. It also adds the current user to the current flask context.,1,0,0,1,2,1,0,0,0,1
"def menuconfig(kconf):
  global _kconf
  global _conf_filename
  global _conf_changed
  global _minconf_filename
  global _show_all
  _kconf = kconf
  _conf_changed = _load_config()
  _conf_filename = standard_config_filename()
  _minconf_filename = ""defconfig""
  _show_all = False
  if not _shown_nodes(kconf.top_node):
  _show_all = True
  if not _shown_nodes(kconf.top_node):
  print(""Empty configuration -- nothing to configure.\n""
  ""Check that environment variables are set properly."")
  return
  kconf.disable_warnings()
  locale.setlocale(locale.LC_ALL, """")
  if _CONVERT_C_LC_CTYPE_TO_UTF8:
  _convert_c_lc_ctype_to_utf8()
  os.environ.setdefault(""ESCDELAY"", ""0"")
  print(curses.wrapper(_menuconfig))","Launches the configuration interface, returning after the user exits. kconf: Kconfig instance to be configured",1,0,0,1,2,1,0,0,1,2
"def handleCheckIDRequest(request, openid_request):
  if not openid_request.idSelect():
  id_url = getViewURL(request, idPage)
  if id_url != openid_request.identity:
  error_response = ProtocolError(
  openid_request.message,
  ""This server cannot verify the URL %r"" %
  (openid_request.identity,))
  return displayResponse(request, error_response)
  if openid_request.immediate:
  openid_response = openid_request.answer(False)
  return displayResponse(request, openid_response)
  else:
  setRequest(request, openid_request)
  return showDecidePage(request, openid_request)","Handle checkid_* requests. Get input from the user to find out whether she trusts the RP involved. Possibly, get intput about what Simple Registration information, if any, to send in the response.",1,0,0,1,2,1,0,0,1,2
"def _get_perm_obj_or_404(self, pk=None):
  if pk:
  obj = get_object_or_none(self.core.model, pk=pk)
  else:
  try:
  obj = self.get_obj(False)
  except Http404:
  obj = get_object_or_none(self.core.model, **self.get_obj_filters())
  if not obj:
  raise Http404
  return obj","If is send parameter pk is returned object according this pk, else is returned object from get_obj method, but it search only inside filtered values for current user, finally if object is still None is returned according the input key from all objects. If object does not exist is raised Http404",1,0,1,1,3,0,0,1,1,2
"async def get_authenticated_user(
  self, redirect_uri: str, code: str
  ) -> Dict[str, Any]:
  handler = cast(RequestHandler, self)
  http = self.get_auth_http_client()
  body = urllib.parse.urlencode(
  {
  ""redirect_uri"": redirect_uri,
  ""code"": code,
  ""client_id"": handler.settings[self._OAUTH_SETTINGS_KEY][""key""],
  ""client_secret"": handler.settings[self._OAUTH_SETTINGS_KEY][""secret""],
  ""grant_type"": ""authorization_code"",
  }
  )
  response = await http.fetch(
  self._OAUTH_ACCESS_TOKEN_URL,
  method=""POST"",
  headers={""Content-Type"": ""application/x-www-form-urlencoded""},
  body=body,
  )
  return escape.json_decode(response.body)","Handles the login for the Google user, returning an access token. The result is a dictionary containing an ``access_token`` field ([among others](https://developers.google.com/identity/protocols/OAuth2WebServer#handlingtheresponse)). Unlike other ``get_authenticated_user`` methods in this package, this method does not return any additional information about the user. The returned access token can be used with `OAuth2Mixin.oauth2_request` to request additional information (perhaps from ``https://www.googleapis.com/oauth2/v2/userinfo``) Example usage: .. testcode:: class GoogleOAuth2LoginHandler(tornado.web.RequestHandler, tornado.auth.GoogleOAuth2Mixin): async def get(self): if self.get_argument('code', False): access = await self.get_authenticated_user( redirect_uri='http://your.site.com/auth/google', code=self.get_argument('code')) user = await self.oauth2_request( ""https://www.googleapis.com/oauth2/v1/userinfo"", access_token=access[""access_token""]) # Save the user and access token with # e.g. set_secure_cookie. else: await self.authorize_redirect( redirect_uri='http://your.site.com/auth/google', client_id=self.settings['google_oauth']['key'], scope=['profile', 'email'], response_type='code', extra_params={'approval_prompt': 'auto'}) .. testoutput:: :hide: .. versionchanged:: 6.0 The ``callback`` argument was removed. Use the returned awaitable object instead.",2,0,0,1,3,2,0,0,1,3
"def has_object_permission(self, request, view, obj):
  if self.protected_methods and request.method not in self.protected_methods:
  return True
  user = getattr(request, ""user"", None)
  if not user or user.is_anonymous():
  return False
  if self.require_staff and not user.is_staff:
  return False
  if user.has_perms(self.permissions):
  return True
  authors_field = getattr(obj, self.authors_field, None)
  if not authors_field:
  return False
  if self.author_permissions and not user.has_perms(self.author_permissions):
  return False
  return user in authors_field.all()",determines if requesting user has permissions for the object :param request: WSGI request object - where we get the user from :param view: the view calling for permission :param obj: the object in question :return: `bool`,1,0,1,1,3,1,0,1,1,3
"def FullJournalName(self):
  global abbrevDict
  if abbrevDict is None:
  abbrevDict = getj9dict()
  if self.isJournal():
  return abbrevDict[self.journal][0]
  else:
  return None","Returns the full name of the Citation's journal field. Requires the [j9Abbreviations](../modules/journalAbbreviations.html#metaknowledge.journalAbbreviations.backend.getj9dict) database file. **Note**: Requires the [j9Abbreviations](../modules/journalAbbreviations.html#metaknowledge.journalAbbreviations.backend.getj9dict) database file and will raise an error if it cannot be found. # Returns `str` > The first full name given for the journal of the Citation (or the first name in the WOS list if multiple names exist), if there is not one then `None` is returned",1,0,1,1,3,0,0,0,1,1
"def texture_map_to_plane(dataset, origin=None, point_u=None, point_v=None,
  inplace=False, name='Texture Coordinates'):
  alg = vtk.vtkTextureMapToPlane()
  if origin is None or point_u is None or point_v is None:
  alg.SetAutomaticPlaneGeneration(True)
  else:
  alg.SetOrigin(origin)
  alg.SetPoint1(point_u)
  alg.SetPoint2(point_v)
  alg.SetInputDataObject(dataset)
  alg.Update()
  output = _get_output(alg)
  if not inplace:
  return output
  t_coords = output.GetPointData().GetTCoords()
  t_coords.SetName(name)
  otc = dataset.GetPointData().GetTCoords()
  dataset.GetPointData().SetTCoords(t_coords)
  dataset.GetPointData().AddArray(t_coords)
  dataset.GetPointData().AddArray(otc)
  return","Texture map this dataset to a user defined plane. This is often used to define a plane to texture map an image to this dataset. The plane defines the spatial reference and extent of that image. Parameters ---------- origin : tuple(float) Length 3 iterable of floats defining the XYZ coordinates of the BOTTOM LEFT CORNER of the plane point_u : tuple(float) Length 3 iterable of floats defining the XYZ coordinates of the BOTTOM RIGHT CORNER of the plane point_v : tuple(float) Length 3 iterable of floats defining the XYZ coordinates of the TOP LEFT CORNER of the plane inplace : bool, optional If True, the new texture coordinates will be added to the dataset inplace. If False (default), a new dataset is returned with the textures coordinates name : str, optional The string name to give the new texture coordinates if applying the filter inplace.",0,0,0,1,1,1,0,0,1,2
"def _convert_timestr_to_seconds(time_str, rec_start):
  if not CHECK_TIME_STR.match(time_str):
  raise ValueError('Input can only contain digits and colons')
  if ':' in time_str:
  time_split = [int(x) for x in time_str.split(':')]
  if len(time_split) == 2:
  time_split.append(0)
  clock_time = time(*time_split)
  chosen_start = datetime.combine(rec_start.date(), clock_time)
  if clock_time < rec_start.time():
  chosen_start += timedelta(days=1)
  window_start = int((chosen_start - rec_start).total_seconds())
  else:
  window_start = int(time_str)
  return window_start","Convert input from user about time string to an absolute time for the recordings. Parameters ---------- time_str : str time information as '123' or '22:30' or '22:30:22' rec_start: instance of datetime absolute start time of the recordings. Returns ------- int start time of the window, in s, from the start of the recordings Raises ------ ValueError if it cannot convert the string",1,0,0,1,2,1,0,0,1,2
"def ynbox(msg=""Shall I continue?"", title="" "",
  choices=(""[<F1>]Yes"", ""[<F2>]No""), image=None,
  default_choice='[<F1>]Yes', cancel_choice='[<F2>]No'):
  return boolbox(msg=msg,
  title=title,
  choices=choices,
  image=image,
  default_choice=default_choice,
  cancel_choice=cancel_choice)","Display a msgbox with choices of Yes and No. The returned value is calculated this way:: if the first choice (""Yes"") is chosen, or if the dialog is cancelled: return True else: return False If invoked without a msg argument, displays a generic request for a confirmation that the user wishes to continue. So it can be used this way:: if ynbox(): pass # continue else: sys.exit(0) # exit the program :param msg: the msg to be displayed :type msg: str :param str title: the window title :param list choices: a list or tuple of the choices to be displayed :param str image: Filename of image to display :param str default_choice: The choice you want highlighted when the gui appears :param str cancel_choice: If the user presses the 'X' close, which button should be pressed :return: True if 'Yes' or dialog is cancelled, False if 'No'",1,0,0,1,2,1,0,0,1,2
"def did_you_mean(message: str, user_input: str, choices: Sequence[str]) -> str:
  if not choices:
  return message
  else:
  result = {
  difflib.SequenceMatcher(a=user_input, b=choice).ratio(): choice
  for choice in choices
  }
  message += ""\nDid you mean: %s?"" % result[max(result)]
  return message","Given a list of choices and an invalid user input, display the closest items in the list that match the input.",1,0,0,1,2,1,0,0,1,2
"def _get_table_infos(
  self,
  trimmed=False):
  self.log.debug('starting the ``_get_table_infos`` method')
  sqlQuery = u % locals()
  tableInfo = readquery(
  log=self.log,
  sqlQuery=sqlQuery,
  dbConn=self.cataloguesDbConn,
  quiet=False
  )
  if trimmed:
  cleanTable = []
  for r in tableInfo:
  orow = collections.OrderedDict(sorted({}.items()))
  for c in self.basicColumns:
  if c in r:
  orow[c] = r[c]
  cleanTable.append(orow)
  tableInfo = cleanTable
  self.log.debug('completed the ``_get_table_infos`` method')
  return tableInfo",query the sherlock-catalogues database table metadata,0,0,1,1,2,0,0,1,1,2
"def get_yarn_applications(self, start_time, end_time, filter_str="""", limit=100,
  offset=0):
  params = {
  'from': start_time.isoformat(),
  'to': end_time.isoformat(),
  'filter': filter_str,
  'limit': limit,
  'offset': offset
  }
  return self._get(""yarnApplications"", ApiYarnApplicationResponse,
  params=params, api_version=6)",Returns a list of YARN applications that satisfy the filter @type start_time: datetime.datetime. Note that the datetime must either be time zone aware or specified in the server time zone. See the python datetime documentation for more details about python's time zone handling. @param start_time: Applications must have ended after this time @type end_time: datetime.datetime. Note that the datetime must either be time zone aware or specified in the server time zone. See the python datetime documentation for more details about python's time zone handling. @param filter_str: A filter to apply to the applications. For example: 'user = root and applicationDuration > 5s' @param limit: The maximum number of results to return @param offset: The offset into the return list @since: API v6,2,0,1,1,4,2,0,0,1,3
"def setUsers(self, *args, **kwargs):
  try:
  usrs = [ us for us in self.mambuusersclass(branchId=self['id'], *args, **kwargs) if us['userState'] == ""ACTIVE"" ]
  except AttributeError as ae:
  from .mambuuser import MambuUsers
  self.mambuusersclass = MambuUsers
  usrs = [ us for us in self.mambuusersclass(branchId=self['id'], *args, **kwargs) if us['userState'] == ""ACTIVE"" ]
  self['users'] = usrs
  return 1","Adds the active users for this branch to a 'users' field. Returns the number of requests done to Mambu. .. todo:: since pagination logic was added, is not always true that just 1 request was done. It may be more! But since request counter singleton holds true information about how many requests were done to Mambu, in fact this return value may be obsolete",2,0,0,0,2,1,0,0,1,2
"def create_submission(self, source_code, language_name=None, language_id=None,
  std_input="""", run=True, private=False):
  language_id = language_id or self._translate_language_name(language_name)
  result = self.client.service.createSubmission(self.user, self.password,
  source_code, language_id,
  std_input, run, private)
  result_dict = Ideone._transform_to_dict(result)
  Ideone._handle_error(result_dict)
  return result_dict","Create a submission and upload it to Ideone. Keyword Arguments ----------------- * source_code: a string of the programs source code * language_name: the human readable language string (e.g. 'python') * language_id: the ID of the programming language * std_input: the string to pass to the program on stdin * run: a boolean flag to signifying if Ideone should compile and run the program * private: a boolean flag signifying the code is private Returns ------- A dictionary with the keys error and link. The link is the unique id of the program. The URL of the submission is http://ideone.com/LINK. Examples -------- >>> ideone_object = Ideone('username', 'password') >>> ideone_object.create_submission('print(42)', language_name='python') {'error': 'OK', 'link' : 'LsSbo'}",1,0,0,1,2,1,0,0,2,3
"def get_hosting_devices_for_agent(self, context, host):
  agent_ids = self._dmplugin.get_cfg_agents(context, active=None,
  filters={'host': [host]})
  if agent_ids:
  return [self._dmplugin.get_device_info_for_agent(context, hd_db)
  for hd_db in self._dmplugin.get_hosting_devices_db(
  context, filters={'cfg_agent_id': [agent_ids[0].id]})]
  return []","Fetches routers that a Cisco cfg agent is managing. This function is supposed to be called when the agent has started, is ready to take on assignments and before any callbacks to fetch logical resources are issued. :param context: contains user information :param host: originator of callback :returns: dict of hosting devices managed by the cfg agent",0,0,1,0,1,1,0,1,1,3
"def check_role(*roles, **args_map):
  def f1(func, roles=roles):
  @wraps(func)
  def f2(*args, **kwargs):
  from uliweb import request, error
  arguments = {}
  for k, v in args_map.items():
  if v in kwargs:
  arguments[k] = kwargs[v]
  if not has_role(request.user, *roles, **arguments):
  error(_(""You have no roles to visit this page.""))
  return func(*args, **kwargs)
  return f2
  return f1","It's just like has_role, but it's a decorator. And it'll check request.user",1,0,0,0,1,1,0,0,1,2
"def credentials_from_clientsecrets_and_code(filename, scope, code,
  message=None,
  redirect_uri='postmessage',
  http=None,
  cache=None,
  device_uri=None):
  flow = flow_from_clientsecrets(filename, scope, message=message,
  cache=cache, redirect_uri=redirect_uri,
  device_uri=device_uri)
  credentials = flow.step2_exchange(code, http=http)
  return credentials","Returns OAuth2Credentials from a clientsecrets file and an auth code. Will create the right kind of Flow based on the contents of the clientsecrets file or will raise InvalidClientSecretsError for unknown types of Flows. Args: filename: string, File name of clientsecrets. scope: string or iterable of strings, scope(s) to request. code: string, An authorization code, most likely passed down from the client message: string, A friendly string to display to the user if the clientsecrets file is missing or invalid. If message is provided then sys.exit will be called in the case of an error. If message in not provided then clientsecrets.InvalidClientSecretsError will be raised. redirect_uri: string, this is generally set to 'postmessage' to match the redirect_uri that the client specified http: httplib2.Http, optional http instance to use to do the fetch cache: An optional cache service client that implements get() and set() methods. See clientsecrets.loadfile() for details. device_uri: string, OAuth 2.0 device authorization endpoint pkce: boolean, default: False, Generate and include a ""Proof Key for Code Exchange"" (PKCE) with your authorization and token requests. This adds security for installed applications that cannot protect a client_secret. See RFC 7636 for details. code_verifier: bytestring or None, default: None, parameter passed as part of the code exchange when pkce=True. If None, a code_verifier will automatically be generated as part of step1_get_authorize_url(). See RFC 7636 for details. Returns: An OAuth2Credentials object. Raises: FlowExchangeError: if the authorization code cannot be exchanged for an access token UnknownClientSecretsFlowError: if the file describes an unknown kind of Flow. clientsecrets.InvalidClientSecretsError: if the clientsecrets file is invalid.",1,0,0,0,1,1,0,0,1,2
"def authenticate(self, request, identification, password=None, check_password=True):
  User = get_user_model()
  try:
  django.core.validators.validate_email(identification)
  try: user = User.objects.get(email__iexact=identification)
  except User.DoesNotExist: return None
  except django.core.validators.ValidationError:
  try: user = User.objects.get(username__iexact=identification)
  except User.DoesNotExist: return None
  if check_password:
  if user.check_password(password):
  return user
  return None
  else: return user",Authenticates a user through the combination email/username with password. :param request: The authenticate() method of authentication backends requires request as the first positional argument from Django 2.1. :param identification: A string containing the username or e-mail of the user that is trying to authenticate. :password: Optional string containing the password for the user. :param check_password: Boolean that defines if the password should be checked for this user. Always keep this ``True``. This is only used by userena at activation when a user opens a page with a secret hash. :return: The signed in :class:`User`.,1,0,1,1,3,1,0,1,1,3
"def connect(db_url=None,
  pooling=hgvs.global_config.uta.pooling,
  application_name=None,
  mode=None,
  cache=None):
  _logger.debug('connecting to ' + str(db_url) + '...')
  if db_url is None:
  db_url = _get_ncbi_db_url()
  url = _parse_url(db_url)
  if url.scheme == 'postgresql':
  conn = NCBI_postgresql(
  url=url, pooling=pooling, application_name=application_name, mode=mode, cache=cache)
  else:
  raise RuntimeError(""{url.scheme} in {url} is not currently supported"".format(url=url))
  _logger.info('connected to ' + str(db_url) + '...')
  return conn","Connect to a uta/ncbi database instance. :param db_url: URL for database connection :type db_url: string :param pooling: whether to use connection pooling (postgresql only) :type pooling: bool :param application_name: log application name in connection (useful for debugging; PostgreSQL only) :type application_name: str When called with an explicit db_url argument, that db_url is used for connecting. When called without an explicit argument, the function default is determined by the environment variable UTA_DB_URL if it exists, or hgvs.datainterface.uta.public_db_url otherwise. >>> hdp = connect() >>> hdp.schema_version() '1.1' The format of the db_url is driver://user:pass@host/database (the same as that used by SQLAlchemy). Examples: A remote public postgresql database: postgresql://anonymous:anonymous@uta.biocommons.org/uta' A local postgresql database: postgresql://localhost/uta A local SQLite database: sqlite:////tmp/uta-0.0.6.db For postgresql db_urls, pooling=True causes connect to use a psycopg2.pool.ThreadedConnectionPool.",1,0,1,0,2,1,0,0,1,2
"def patch_namespaced_event(self, name, namespace, body, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.patch_namespaced_event_with_http_info(name, namespace, body, **kwargs)
  else:
  (data) = self.patch_namespaced_event_with_http_info(name, namespace, body, **kwargs)
  return data","partially update the specified Event This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.patch_namespaced_event(name, namespace, body, async_req=True) >>> result = thread.get() :param async_req bool :param str name: name of the Event (required) :param str namespace: object name and auth scope, such as for teams and projects (required) :param object body: (required) :param str pretty: If 'true', then the output is pretty printed. :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint. This field is required for apply requests (application/apply-patch) but optional for non-apply patch types (JsonPatch, MergePatch, StrategicMergePatch). :param bool force: Force is going to \""force\"" Apply requests. It means user will re-acquire conflicting fields owned by other people. Force flag must be unset for non-apply patch requests. :return: V1Event If the method is called asynchronously, returns the request thread.",0,0,0,1,1,1,0,0,1,2
"def main(mash_output, hash_cutoff, sample_id, assembly_file):
  input_f = open(mash_output, ""r"")
  master_dict = {}
  for line in input_f:
  tab_split = line.split(""\t"")
  current_seq = tab_split[1].strip()
  ref_accession = ""_"".join(tab_split[0].strip().split(""_"")[0:3])
  mash_dist = tab_split[2].strip()
  hashes_list = tab_split[-1].strip().split(""/"")
  perc_hashes = float(hashes_list[0]) / float(hashes_list[1])
  if ref_accession in master_dict.keys():
  current_seq += "", {}"".format(master_dict[ref_accession][-1])
  if perc_hashes > float(hash_cutoff):
  master_dict[ref_accession] = [
  round(1 - float(mash_dist), 2),
  round(perc_hashes, 2),
  current_seq
  ]
  send_to_output(master_dict, mash_output, sample_id, assembly_file)",Main function that allows to dump a mash dist txt file to a json file Parameters ---------- mash_output: str A string with the input file. hash_cutoff: str the percentage cutoff for the percentage of shared hashes between query and plasmid in database that is allowed for the plasmid to be reported to the results outputs sample_id: str The name of the sample.,0,0,0,0,0,1,0,0,1,2
"async def set_game_score(self, user_id: base.Integer, score: base.Integer,
  force: typing.Union[base.Boolean, None] = None,
  disable_edit_message: typing.Union[base.Boolean, None] = None,
  chat_id: typing.Union[base.Integer, None] = None,
  message_id: typing.Union[base.Integer, None] = None,
  inline_message_id: typing.Union[base.String,
  None] = None) -> types.Message or base.Boolean:
  payload = generate_payload(**locals())
  result = await self.request(api.Methods.SET_GAME_SCORE, payload)
  if isinstance(result, bool):
  return result
  return types.Message(**result)","Use this method to set the score of the specified user in a game. Source: https://core.telegram.org/bots/api#setgamescore :param user_id: User identifier :type user_id: :obj:`base.Integer` :param score: New score, must be non-negative :type score: :obj:`base.Integer` :param force: Pass True, if the high score is allowed to decrease This can be useful when fixing mistakes or banning cheaters :type force: :obj:`typing.Union[base.Boolean, None]` :param disable_edit_message: Pass True, if the game message should not be automatically edited to include the current scoreboard :type disable_edit_message: :obj:`typing.Union[base.Boolean, None]` :param chat_id: Required if inline_message_id is not specified. Unique identifier for the target chat :type chat_id: :obj:`typing.Union[base.Integer, None]` :param message_id: Required if inline_message_id is not specified. Identifier of the sent message :type message_id: :obj:`typing.Union[base.Integer, None]` :param inline_message_id: Required if chat_id and message_id are not specified. Identifier of the inline message :type inline_message_id: :obj:`typing.Union[base.String, None]` :return: On success, if the message was sent by the bot, returns the edited Message, otherwise returns True Returns an error, if the new score is not greater than the user's current score in the chat and force is False. :rtype: :obj:`typing.Union[types.Message, base.Boolean]`",1,0,0,2,3,1,0,0,2,3
"def read_priority_class(self, name, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.read_priority_class_with_http_info(name, **kwargs)
  else:
  (data) = self.read_priority_class_with_http_info(name, **kwargs)
  return data","read the specified PriorityClass This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.read_priority_class(name, async_req=True) >>> result = thread.get() :param async_req bool :param str name: name of the PriorityClass (required) :param str pretty: If 'true', then the output is pretty printed. :param bool exact: Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. Deprecated. Planned for removal in 1.18. :param bool export: Should this value be exported. Export strips fields that a user can not specify. Deprecated. Planned for removal in 1.18. :return: V1PriorityClass If the method is called asynchronously, returns the request thread.",1,0,0,2,3,1,0,0,1,2
"def requestedFormat(request,acceptedFormat):
  if 'format' in request.args:
  fieldFormat = request.args.get('format')
  if fieldFormat not in acceptedFormat:
  raise ValueError(""requested format not supported: ""+ fieldFormat)
  return fieldFormat
  else:
  return request.accept_mimetypes.best_match(acceptedFormat)","Return the response format requested by client Client could specify requested format using: (options are processed in this order) - `format` field in http request - `Accept` header in http request Example: chooseFormat(request, ['text/html','application/json']) Args: acceptedFormat: list containing all the accepted format Returns: string: the user requested mime-type (if supported) Raises: ValueError: if user request a mime-type not supported",1,0,0,1,2,1,0,0,1,2
"def host_install(self, user_name, host_names, ssh_port=None, password=None,
  private_key=None, passphrase=None, parallel_install_count=None,
  cm_repo_url=None, gpg_key_custom_url=None,
  java_install_strategy=None, unlimited_jce=None):
  host_install_args = {}
  if user_name:
  host_install_args['userName'] = user_name
  if host_names:
  host_install_args['hostNames'] = host_names
  if ssh_port:
  host_install_args['sshPort'] = ssh_port
  if password:
  host_install_args['password'] = password
  if private_key:
  host_install_args['privateKey'] = private_key
  if passphrase:
  host_install_args['passphrase'] = passphrase
  if parallel_install_count:
  host_install_args['parallelInstallCount'] = parallel_install_count
  if cm_repo_url:
  host_install_args['cmRepoUrl'] = cm_repo_url
  if gpg_key_custom_url:
  host_install_args['gpgKeyCustomUrl'] = gpg_key_custom_url
  if java_install_strategy is not None:
  host_install_args['javaInstallStrategy'] = java_install_strategy
  if unlimited_jce:
  host_install_args['unlimitedJCE'] = unlimited_jce
  return self._cmd('hostInstall', data=host_install_args)","Install Cloudera Manager Agent on a set of hosts. @param user_name: The username used to authenticate with the hosts. Root access to your hosts is required to install Cloudera packages. The installer will connect to your hosts via SSH and log in either directly as root or as another user with password-less sudo privileges to become root. @param host_names: List of names of hosts to configure for use with Cloudera Manager. A host may be specified by a hostname(FQDN) or an IP address. @param ssh_port: SSH port. If unset, defaults to 22. @param password: The password used to authenticate with the hosts. Specify either this or a private key. For password-less login, use an empty string as password. @param private_key: The private key to authenticate with the hosts. Specify either this or a password. @param passphrase: The passphrase associated with the private key used to authenticate with the hosts (optional). @param parallel_install_count: Number of simultaneous installations. Defaults to 10. Running a large number of installations at once can consume large amounts of network bandwidth and other system resources. @param cm_repo_url: The Cloudera Manager repository URL to use (optional). Example for SLES, Redhat or other RPM based distributions: http://archive-primary.cloudera.com/cm5/redhat/6/x86_64/cm/5/ Example for Ubuntu or other Debian based distributions: ""deb http://archive.cloudera.com/cm5/ubuntu/lucid/amd64/cm/ lucid-cm5 contrib"" @param gpg_key_custom_url: The Cloudera Manager public GPG key (optional). Example for SLES, Redhat or other RPM based distributions: http://archive-primary.cloudera.com/cm5/redhat/6/x86_64/cm/RPM-GPG-KEY-cloudera Example for Ubuntu or other Debian based distributions: http://archive.cloudera.com/debian/archive.key @param java_install_strategy: Added in v8: Strategy to use for JDK installation. Valid values are 1. AUTO (default): Cloudera Manager will install the JDK versions that are required when the ""AUTO"" option is selected. Cloudera Manager may overwrite any of the existing JDK installations. 2. NONE: Cloudera Manager will not install any JDK when ""NONE"" option is selected. It should be used if an existing JDK installation has to be used. @param unlimited_jce: Added in v8: Flag for unlimited strength JCE policy files installation If unset, defaults to false @return: Information about the submitted command. @since: API v6",1,0,0,2,3,1,0,0,1,2
"def get_valid_value(prompt, validator, default=None):
  ans = get_value(prompt, default)
  while not validator(ans):
  try:
  print validator.error_message
  except AttributeError:
  print 'Invalid value.'
  ans = get_value(prompt, default)
  return ans","Displays the provided prompt and gets input from the user. This behavior loops indefinitely until the provided validator returns True for the user input. If a default value is provided, it will be used only if the user hits Enter and does not provide a value. If the validator callable has an error_message attribute, it will be displayed for an invalid value, otherwise a generic message is used.",1,0,0,1,2,1,0,0,1,2
"def validate_pipeline(url, pipeline_id, auth, verify_ssl):
  validate_result = requests.get(url + '/' + pipeline_id + '/validate', headers=X_REQ_BY, auth=auth, verify=verify_ssl)
  validate_result.raise_for_status()
  previewer_id = validate_result.json()['previewerId']
  poll_validation_status(url, pipeline_id, previewer_id, auth, verify_ssl)
  preview_result = requests.get(url + '/' + pipeline_id + '/preview/' + validate_result.json()['previewerId'], headers=X_REQ_BY, auth=auth, verify=verify_ssl)
  logging.debug('result content: {}'.format(preview_result.content))
  return preview_result.json()","Validate a pipeline and show issues. Args: url (str): the host url in the form 'http://host:port/'. pipeline_id (str): the ID of of the exported pipeline. auth (tuple): a tuple of username, and password. verify_ssl (bool): whether to verify ssl certificates Returns: dict: the response json",2,0,0,1,3,2,0,0,1,3
"def hydrate(self, database, recursive=True):
  if isinstance(self, Document):
  self.reload(database)
  for field in self:
  obj = getattr(self, field)
  if isinstance(obj, Document):
  obj.reload(database)
  if recursive:
  obj.hydrate(database)
  return self","By default, recursively reloads all instances of Document in the model. Recursion can be turned off. :param database: the `Database` object source for rehydrating. :return: an updated instance of `Document` / self.",0,0,1,0,1,0,1,1,0,2
"def send_document(chat_id, document,
  reply_to_message_id=None, reply_markup=None,
  **kwargs):
  files = None
  if isinstance(document, InputFile):
  files = [document]
  document = None
  elif not isinstance(document, str):
  raise Exception('document must be instance of InputFile or str')
  params = dict(
  chat_id=chat_id,
  document=document
  )
  params.update(
  _clean_params(
  reply_to_message_id=reply_to_message_id,
  reply_markup=reply_markup
  )
  )
  return TelegramBotRPCRequest('sendDocument', params=params, files=files, on_result=Message.from_result, **kwargs)","Use this method to send general files. :param chat_id: Unique identifier for the message recipient  User or GroupChat id :param document: File to send. You can either pass a file_id as String to resend a file that is already on the Telegram servers, or upload a new file using multipart/form-data. :param reply_to_message_id: If the message is a reply, ID of the original message :param reply_markup: Additional interface options. A JSON-serialized object for a custom reply keyboard, instructions to hide keyboard or to force a reply from the user. :param \*\*kwargs: Args that get passed down to :class:`TelegramBotRPCRequest` :type chat_id: int :type document: InputFile or str :type reply_to_message_id: int :type reply_markup: ReplyKeyboardMarkup or ReplyKeyboardHide or ForceReply :returns: On success, the sent Message is returned. :rtype: TelegramBotRPCRequest",1,0,0,2,3,1,0,0,2,3
"def course_is_user_registered(self, course, username=None):
  if username is None:
  username = self.session_username()
  if self.has_staff_rights_on_course(course, username):
  return True
  return self._database.aggregations.find_one({""students"": username, ""courseid"": course.get_id()}) is not None","Checks if a user is registered :param course: a Course object :param username: The username of the user that we want to check. If None, uses self.session_username() :return: True if the user is registered, False else",1,0,1,1,3,1,0,1,1,3
"def username_matches_request_user(view_fn):
  @wraps(view_fn)
  def wrapper(request, username, *args, **kwargs):
  User = get_user_model()
  user = get_object_or_404(User, username=username)
  if user != request.user:
  return HttpResponseForbidden()
  else:
  return view_fn(request, user, *args, **kwargs)
  return wrapper","Checks if the username matches the request user, and if so replaces username with the actual user object. Returns 404 if the username does not exist, and 403 if it doesn't match.",1,0,1,1,3,1,0,1,1,3
"def get_njobs_in_queue(self, username=None):
  if username is None: username = getpass.getuser()
  njobs, process = self._get_njobs_in_queue(username=username)
  if process is not None and process.returncode != 0:
  err_msg = ('Error trying to get the number of jobs in the queue' +
  'The error response reads:\n {}'.format(process.stderr.read()))
  logger.critical(err_msg)
  if not isinstance(self, ShellAdapter):
  logger.info('The number of jobs currently in the queue is: {}'.format(njobs))
  return njobs","returns the number of jobs in the queue, probably using subprocess or shutil to call a command like 'qstat'. returns None when the number of jobs cannot be determined. Args: username: (str) the username of the jobs to count (default is to autodetect)",1,0,0,1,2,2,0,0,1,3
"def create(self, client=None, project=None, location=None):
  if self.user_project is not None:
  raise ValueError(""Cannot create bucket with 'user_project' set."")
  client = self._require_client(client)
  if project is None:
  project = client.project
  if project is None:
  raise ValueError(""Client project not set: pass an explicit project."")
  query_params = {""project"": project}
  properties = {key: self._properties[key] for key in self._changes}
  properties[""name""] = self.name
  if location is not None:
  properties[""location""] = location
  api_response = client._connection.api_request(
  method=""POST"",
  path=""/b"",
  query_params=query_params,
  data=properties,
  _target_object=self,
  )
  self._set_properties(api_response)","Creates current bucket. If the bucket already exists, will raise :class:`google.cloud.exceptions.Conflict`. This implements ""storage.buckets.insert"". If :attr:`user_project` is set, bills the API request to that project. :type client: :class:`~google.cloud.storage.client.Client` or ``NoneType`` :param client: Optional. The client to use. If not passed, falls back to the ``client`` stored on the current bucket. :type project: str :param project: Optional. The project under which the bucket is to be created. If not passed, uses the project set on the client. :raises ValueError: if :attr:`user_project` is set. :raises ValueError: if ``project`` is None and client's :attr:`project` is also None. :type location: str :param location: Optional. The location of the bucket. If not passed, the default location, US, will be used. See https://cloud.google.com/storage/docs/bucket-locations",1,0,0,1,2,1,0,0,1,2
"def __create_admin_entry(self, handleowner, permissions, index, handle, ttl=None):
  if handleowner is None:
  adminindex = '200'
  prefix = handle.split('/')[0]
  adminhandle = '0.NA/' + prefix
  else:
  adminindex, adminhandle = utilhandle.remove_index_from_handle(handleowner)
  data = {
  'value':{
  'index':adminindex,
  'handle':adminhandle,
  'permissions':permissions
  },
  'format':'admin'
  }
  entry = {'index':index, 'type':'HS_ADMIN', 'data':data}
  if ttl is not None:
  entry['ttl'] = ttl
  return entry","Create an entry of type ""HS_ADMIN"". :param username: The username, i.e. a handle with an index (index:prefix/suffix). The value referenced by the index contains authentcation information, e.g. a hidden entry containing a key. :param permissions: The permissions as a string of zeros and ones, e.g. '0111011101011'. If not all twelve bits are set, the remaining ones are set to zero. :param index: The integer to be used as index of this admin entry (not of the username!). Should be 1xx. :param ttl: Optional. If not set, the library's default is set. If there is no default, it is not set by this library, so Handle System sets it. :return: The entry as a dict.",1,1,0,1,3,1,0,0,1,2
"def function_info(function_index=1, function_name=None, line_number=None):
  frm = func_frame(function_index + 1, function_name)
  file_ = os.path.abspath(frm.f_code.co_filename)
  class_name = frm.f_locals.get('self', None)
  if class_name is not None:
  class_name = str(type(class_name)).split('.',1)[-1].split(""'"")[0]
  args, _, _, kwargs = inspect.getargvalues(frm)
  line_number = line_number or frm.f_lineno
  return {'class_name': class_name or '',
  'function_name': frm.f_code.co_name,
  'file': file_,
  'path': os.path.split(file_)[0],
  'basename': os.path.basename(file_).split('.')[0],
  'line_number': line_number or frm.f_lineno,
  'globals': frm.f_globals,
  'locals': frm.f_locals,
  'arguments': args,
  'kwargs': kwargs,
  'frame': frm}","This will return the class_name and function_name of the function traced back two functions. :param function_index: int of how many frames back the program should look (2 will give the parent of the caller) :param function_name: str of what function to look for (should not be used with function_index) :param line_number: int, some times the user may want to override this for testing purposes :return tuple: ('cls_name','func_name',line_number,globals())",0,0,0,1,1,0,0,0,1,1
"def __make_security_role_api_request(server_context, api, role, email=None, user_id=None, container_path=None):
  if email is None and user_id is None:
  raise ValueError(""Must supply either/both [email] or [user_id]"")
  url = server_context.build_url(security_controller, api, container_path)
  return server_context.make_request(url, {
  'roleClassName': role['uniqueName'],
  'principalId': user_id,
  'email': email
  })",Execute a request against the LabKey Security Controller Group Membership apis :param server_context: A LabKey server context. See utils.create_server_context. :param api: Action to execute :param user_id: user ids to apply action to :param role: (from get_roles) to remove user from :param container_path: Additional container context path :return: Request json object,2,0,0,1,3,1,0,0,1,2
"def funnel(self, steps, timeframe=None, timezone=None, max_age=None, all_keys=False):
  params = self.get_params(
  steps=steps,
  timeframe=timeframe,
  timezone=timezone,
  max_age=max_age,
  )
  return self.api.query(""funnel"", params, all_keys=all_keys)","Performs a Funnel query Returns an object containing the results for each step of the funnel. :param steps: array of dictionaries, one for each step. example: [{""event_collection"":""signup"",""actor_property"":""user.id""}, {""event_collection"":""purchase"",""actor_property:""user.id""}] :param timeframe: string or dict, the timeframe in which the events happened example: ""previous_7_days"" :param timezone: int, the timezone you'd like to use for the timeframe and interval in seconds :param max_age: an integer, greater than 30 seconds, the maximum 'staleness' you're willing to trade for increased query performance, in seconds :all_keys: set to true to return all keys on response (i.e. ""result"", ""actors"", ""steps"")",2,0,0,1,3,1,0,0,1,2
"def expand (self, user=False, vars=False, glob=False, resolve=False):
  from os import path
  from glob import glob
  text = text_type (self)
  if user:
  text = path.expanduser (text)
  if vars:
  text = path.expandvars (text)
  if glob:
  results = glob (text)
  if len (results) == 1:
  text = results[0]
  elif len (results) > 1:
  raise IOError ('glob of %r should\'ve returned 0 or 1 matches; got %d'
  % (text, len (results)))
  other = self.__class__ (text)
  if resolve:
  other = other.resolve ()
  return other","Return a new :class:`Path` with various expansions performed. All expansions are disabled by default but can be enabled by passing in true values in the keyword arguments. user : bool (default False) Expand ``~`` and ``~user`` home-directory constructs. If a username is unmatched or ``$HOME`` is unset, no change is made. Calls :func:`os.path.expanduser`. vars : bool (default False) Expand ``$var`` and ``${var}`` environment variable constructs. Unknown variables are not substituted. Calls :func:`os.path.expandvars`. glob : bool (default False) Evaluate the path as a :mod:`glob` expression and use the matched path. If the glob does not match anything, do not change anything. If the glob matches more than one path, raise an :exc:`IOError`. resolve : bool (default False) Call :meth:`resolve` on the return value before returning it.",1,0,0,1,2,1,0,0,1,2
"def pages(self):
  if self._owner_id is None:
  it = ProfileIterator.from_username(self._username, self.session)
  self._owner_id = it.owner_id
  return it
  return ProfileIterator(self._owner_id, self.session, self.rhx)",Obtain an iterator over Instagram post pages. Returns: PageIterator: an iterator over the instagram post pages. Raises: ValueError: when the requested user does not exist. RuntimeError: when the user is a private account and there is no logged user (or the logged user does not follow that account).,2,0,0,1,3,1,0,0,1,2
"def probe(self, key_id=None, ssh_user=None):
  ips = [ip for ip in self.info['public_ips'] if ':' not in ip]
  if not ips:
  raise Exception(""No public IPv4 address available to connect to"")
  payload = {
  'host': ips[0],
  'key': key_id,
  'ssh_user': ssh_user
  }
  data = json.dumps(payload)
  req = self.request(self.mist_client.uri + ""/clouds/"" + self.cloud.id +
  ""/machines/"" + self.id + ""/probe"", data=data)
  probe_info = req.post().json()
  self.probed = True
  return probe_info","If no parameter is provided, mist.io will try to probe the machine with the default :param key_id: Optional. Give if you explicitly want to probe with this key_id :param ssh_user: Optional. Give if you explicitly want a specific user :returns: A list of data received by the probing (e.g. uptime etc)",2,0,0,1,3,2,0,0,1,3
"def remove_users_from_user_group(self, id, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.remove_users_from_user_group_with_http_info(id, **kwargs)
  else:
  (data) = self.remove_users_from_user_group_with_http_info(id, **kwargs)
  return data","Remove multiple users from a specific user group # noqa: E501 # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.remove_users_from_user_group(id, async_req=True) >>> result = thread.get() :param async_req bool :param str id: (required) :param list[str] body: List of users that should be removed from user group :return: ResponseContainerUserGroup If the method is called asynchronously, returns the request thread.",2,1,0,1,4,2,0,0,1,3
"def _ActionDatabase(self, cmd, args = None, commit = True, error = True):
  goodlogging.Log.Info(""DB"", ""Database Command: {0} {1}"".format(cmd, args), verbosity=self.logVerbosity)
  with sqlite3.connect(self._dbPath) as db:
  try:
  if args is None:
  result = db.execute(cmd)
  else:
  result = db.execute(cmd, args)
  except sqlite3.OperationalError:
  if error is True:
  raise
  return None
  else:
  if commit is True:
  db.commit()
  return result.fetchall()","Do action on database. Parameters ---------- cmd : string SQL command. args : tuple [optional : default = None] Arguments to be passed along with the SQL command. e.g. cmd=""SELECT Value FROM Config WHERE Name=?"" args=(fieldName, ) commit : boolean [optional : default = True] If true commit database changes after command is executed. error : boolean [optional : default = True] If False then any sqlite3.OperationalError exceptions will cause this function to return None, otherwise the exception will be raised. Returns ---------- If a valid result is obtained from the database this will be returned. If an error occurs and the error argument is set to False then the return value will be None.",0,1,1,0,2,0,1,1,1,3
"def find_home_directory():
  if WINDOWS:
  directory = os.environ.get('APPDATA')
  if not directory:
  directory = os.path.expanduser(r'~\Application Data')
  else:
  import pwd
  entry = pwd.getpwuid(os.getuid())
  directory = entry.pw_dir
  return directory",Look up the home directory of the effective user id. :returns: The pathname of the home directory (a string). .. note:: On Windows this uses the ``%APPDATA%`` environment variable (if available) and otherwise falls back to ``~/Application Data``.,0,0,0,0,0,1,0,0,1,2
"def fetchallarrow(self, strings_as_dictionary=False, adaptive_integers=False):
  self._assert_valid_result_set()
  if _has_arrow_support():
  from turbodbc_arrow_support import make_arrow_result_set
  return make_arrow_result_set(
  self.impl.get_result_set(),
  strings_as_dictionary,
  adaptive_integers).fetch_all()
  else:
  raise Error(_NO_ARROW_SUPPORT_MSG)","Fetches all rows in the active result set generated with ``execute()`` or ``executemany()``. :param strings_as_dictionary: If true, fetch string columns as dictionary[string] instead of a plain string column. :param adaptive_integers: If true, instead of the integer type returned by the database (driver), this produce integer columns with the smallest possible integer type in which all values can be stored. Be aware that here the type depends on the resulting data. :return: ``pyarrow.Table``",1,0,1,1,3,1,0,1,1,3
"def send_hashtag(self, hashtag, user_ids, text='', thread_id=None):
  user_ids = _get_user_ids(self, user_ids)
  if not isinstance(text, str) and not isinstance(user_ids, (list, str)):
  self.logger.error('Text must be an string, user_ids must be an list or string')
  return False
  if self.reached_limit('messages'):
  self.logger.info(""Out of messages for today."")
  return False
  self.delay('message')
  if self.api.send_direct_item(
  'hashtag', user_ids, text=text, thread=thread_id, hashtag=hashtag
  ):
  self.total['messages'] += 1
  return True
  self.logger.info(""Message to {user_ids} wasn't sent"".format(user_ids=user_ids))
  return False",:param hashtag: hashtag :param self: bot :param text: text of message :param user_ids: list of user_ids for creating group or one user_id for send to one person :param thread_id: thread_id,1,0,0,2,3,1,0,0,1,2
"def get_float(prompt=None):
  while True:
  s = get_string(prompt)
  if s is None:
  return None
  if len(s) > 0 and re.search(r""^[+-]?\d*(?:\.\d*)?$"", s):
  try:
  return float(s)
  except ValueError:
  pass
  if prompt is None:
  print(""Retry: "", end="""")","Read a line of text from standard input and return the equivalent float as precisely as possible; if text does not represent a double, user is prompted to retry. If line can't be read, return None.",1,0,0,1,2,1,0,0,1,2
"def auth(self, request):
  request_token = super(ServiceTrello, self).auth(request)
  callback_url = self.callback_url(request)
  auth_url_str = '{auth_url}?oauth_token={token}'
  auth_url_str += '&scope={scope}&name={name}'
  auth_url_str += '&expiration={expiry}&oauth_callback={callback_url}'
  auth_url = auth_url_str.format(auth_url=self.AUTH_URL,
  token=request_token['oauth_token'],
  scope=self.scope,
  name=self.app_name,
  expiry=self.expiry,
  callback_url=callback_url)
  return auth_url",let's auth the user to the Service :param request: request object :return: callback url :rtype: string that contains the url to redirect after auth,2,0,0,2,4,2,0,0,1,3
"def connect(oauth_key, oauth_secret, username, password, oauth_redirect=_DEFAULT_REDIRECT, oauth_scopes=set(), useragent=_DEFAULT_USERAGENT, script_key=None):
 import praw
 oauth_token = get_oauth_token(oauth_key, oauth_secret, username, password, useragent=useragent, script_key=script_key)
 if oauth_token is None:
 log.debug(""Can't create PRAW instance without token"")
 return None
 r = praw.Reddit(user_agent=useragent, disable_update_check=True)
 r.set_oauth_app_info(oauth_key, oauth_secret, oauth_redirect)
 r.set_access_credentials(oauth_scopes, oauth_token)
 r.config.api_request_delay = 1
 return r","Creates a PRAW instance using OAuth with the given authentication information. Because the retrieved token is stored on the file system (script_key is used to distinguish between files), this function is safe to call across multiple instances or runs. The token is renewed after one hour. Note: Only script-based oauth is supported. :param oauth_key: Reddit oauth key :param oauth_secret: Reddit oauth secret :param username: Reddit username :param password: Reddit password :param oauth_redirect: Redirect used in registered Reddit app :param oauth_scopes: Set of scopes passed to PRAW :param useragent: Connection useragent (this should be changed, otherwise you'll be heavily rate limited) :param script_key: Key used to distinguish between local token files :return: A PRAW instance if a token could be retrieved, otherwise None.",2,0,0,0,2,2,0,0,1,3
"def extract_feature(self, extractor, force_extraction=False, verbose=0, add_args=None, custom_name=None):
  if self._prepopulated is False:
  raise errors.EmptyDatabase(self.dbpath)
  else:
  return extract_feature_base(self.dbpath, self.path_to_set, self._set_object, extractor, force_extraction,
  verbose, add_args, custom_name)","Extracts a feature and stores it in the database Parameters ---------- extractor : function, which takes the path of a data point and *args as parameters and returns a feature force_extraction : boolean, if True - will re-extract feature even if a feature with this name already exists in the database, otherwise, will only extract if the feature doesn't exist in the database. default value: False verbose : int, if bigger than 0, will print the current number of the file for which data is being extracted add_args : optional arguments for the extractor (list/dictionary/tuple/whatever). if None, the extractor should take only one input argument - the file path. default value: None custom_name : string, optional name for the feature (it will be stored in the database with the custom_name instead of extractor function name). if None, the extractor function name will be used. default value: None Returns ------- None",0,0,1,1,2,0,0,1,0,1
"def fields(self):
  result = self._get_key_values('fields')
  for key, value in result.items():
  if not isinstance(value, list):
  result[key] = [value]
  for key, value in result.items():
  schema = get_schema_from_type(key)
  for obj in value:
  if obj not in schema._declared_fields:
  raise InvalidField(""{} has no attribute {}"".format(schema.__name__, obj))
  return result","Return fields wanted by client. :return dict: a dict of sparse fieldsets information Return value will be a dict containing all fields by resource, for example:: { ""user"": ['name', 'email'], }",1,0,1,1,3,0,0,0,1,1
"def search_item_by_name_and_folder(self, name, folder_id, token=None):
  parameters = dict()
  parameters['name'] = name
  parameters['folderId'] = folder_id
  if token:
  parameters['token'] = token
  response = self.request('midas.item.searchbynameandfolder', parameters)
  return response['items']",Return all items with a given name and parent folder id. :param name: The name of the item to search by. :type name: string :param folder_id: The id of the parent folder to search by. :type folder_id: int | long :param token: (optional) A valid token for the user in question. :type token: None | string :returns: A list of all items with the given name and parent folder id. :rtype: list[dict],2,0,0,1,3,2,0,0,1,3
"def get_relationship(cls, request_args, id, related_collection_name, related_resource=None):
  try:
  included = request_args.get('include').split(',')
  except (SyntaxError, AttributeError):
  included = []
  try:
  offset = request_args.get('page[offset]', 0)
  limit = request_args.get('page[limit]', 20)
  this_resource = cls.nodes.get(id=id, active=True)
  if not related_resource:
  if request_args.get('include'):
  r = application_codes.error_response([application_codes.PARAMETER_NOT_SUPPORTED_VIOLATION])
  else:
  r = this_resource.relationship_collection_response(related_collection_name, offset, limit)
  else:
  r = this_resource.individual_relationship_response(related_collection_name, related_resource, included)
  except DoesNotExist:
  r = application_codes.error_response([application_codes.RESOURCE_NOT_FOUND])
  return r",Get a relationship :param request_args: :param id: The 'id' field of the node on the left side of the relationship in the database. The id field must \ be set in the model -- it is not the same as the node id :param related_collection_name: The name of the relationship :param related_resource: Deprecated for version 1.1.0 :return: A response according to the specification at http://jsonapi.org/format/#fetching-relationships,1,0,1,1,3,1,0,1,1,3
"async def get_token(cls, host, **params):
  params['grant_type'] = ""password""
  path = ""/oauth/v2/token""
  async with aiohttp.ClientSession() as sess:
  async with sess.post(host + path, data=params) as resp:
  data = await cls.handle_json_response(resp)
  return data.get(""access_token"")","POST /oauth/v2/token Get a new token :param host: host of the service :param params: will contain : params = {""grant_type"": ""password"", ""client_id"": ""a string"", ""client_secret"": ""a string"", ""username"": ""a login"", ""password"": ""a password""} :return: access token",2,0,0,2,4,2,0,0,1,3
"def make_octal_permissions_mode(user=(False, False, False), group=(False, False, False), other=(False, False, False)):
  mode = ''
  for name in (user, group, other):
  read, write, execute = name
  if execute and not all(i for i in (read, write)):
  code = 1
  elif write and not all(i for i in (read, execute)):
  code = 2
  elif all(i for i in (write, execute)) and not read:
  code = 3
  elif read and not all(i for i in (write, execute)):
  code = 4
  elif all(i for i in (read, execute)) and not write:
  code = 5
  elif all(i for i in (read, write)) and not execute:
  code = 6
  elif all(i for i in (read, write, execute)):
  code = 7
  else:
  code = 0
  mode += str(code)
  return int(mode)","Create a permissions bit in absolute notation (octal). The user, group and other parameters are tuples representing Read, Write and Execute values. All are set disallowed (set to false) by default and can be individually permitted. :param user: User permissions :param group: Group permissions :param other: Other permissions :return: Permissions bit",0,0,0,0,0,1,0,0,1,2
"def verify(password_hash, password):
  ensure(len(password_hash) == PWHASH_SIZE,
  ""The password hash must be exactly %s bytes long"" %
  nacl.bindings.crypto_pwhash_scryptsalsa208sha256_STRBYTES,
  raising=exc.ValueError)
  return nacl.bindings.crypto_pwhash_scryptsalsa208sha256_str_verify(
  password_hash, password
  )",Takes the output of scryptsalsa208sha256 and compares it against a user provided password to see if they are the same :param password_hash: bytes :param password: bytes :rtype: boolean .. versionadded:: 1.2,1,0,0,0,1,0,0,0,1,1
"def insert(key, value):
  value = pickle.dumps(value, protocol=constants.PICKLE_PROTOCOL)
  doc = {
  KEY_FIELD: key,
  VALUE_FIELD: Binary(value)
  }
  try:
  return collection.insert(doc)
  except pymongo.errors.DuplicateKeyError:
  return None","Store a value with a key. If the key is already present in the database, this does nothing.",0,1,0,0,1,0,1,1,0,2
"def add_userrnd_shot(project):
  rndseq = project.sequence_set.get(name=RNDSEQ_NAME)
  users = [u for u in project.users.all()]
  for user in users:
  shot, created = Shot.objects.get_or_create(name=user.username,
  project=project,
  sequence=rndseq,
  defaults={'description': 'rnd shot for user %s' % user.username})
  for t in shot.tasks.all():
  t.users.add(user)
  t.full_clean()
  t.save()",Add a rnd shot for every user in the project :param project: the project that needs its rnd shots updated :type project: :class:`muke.models.Project` :returns: None :rtype: None :raises: None,0,1,2,0,3,1,1,1,1,4
"def validate(opts):
  try:
  return _validate(opts)
  except ValidationException as e:
  print(""Command line arguments failed validation:"")
  print(e)
  sys.exit(0)
  except ValueError as e:
  print(""Incorrect type passed into anchorhub.validate_opts.validate()\n"")
  print(e)
  sys.exit(0)","Client facing validate function for command line arguments. Perform validation operations on opts, a namespace created from command line arguments. Returns True if all validation tests are successful. If an exception is raised by the validations, this gracefully exits the program and leaves a message to the user. Required attributes on opts: * input: String giving the path to input files * output: String giving the path to output destination * wrapper: String specifying the wrapper format * extensions: List of strings specifying the file extensions to look for * overwrite: Boolean specifying whether the original input files should be overridden :param opts: namespace containing necessary parameters :return: True, if all tests are successful",1,0,0,1,2,1,0,0,1,2
"def hurst_rs(data, nvals=None, fit=""RANSAC"", debug_plot=False,
  debug_data=False, plot_file=None, corrected=True, unbiased=True):
  data = np.asarray(data)
  total_N = len(data)
  if nvals is None:
  nvals = logmid_n(total_N, ratio=1/4.0, nsteps=15)
  rsvals = np.array([rs(data, n, unbiased=unbiased) for n in nvals])
  not_nan = np.logical_not(np.isnan(rsvals))
  rsvals = rsvals[not_nan]
  nvals = np.asarray(nvals)[not_nan]
  if len(rsvals) == 0:
  poly = [np.nan, np.nan]
  if debug_plot:
  warnings.warn(""Cannot display debug plot, all (R/S)_n are NaN"")
  else:
  xvals = np.log(nvals)
  yvals = np.log(rsvals)
  if corrected:
  yvals -= np.log([expected_rs(n) for n in nvals])
  poly = poly_fit(xvals, yvals, 1, fit=fit)
  if debug_plot:
  plot_reg(xvals, yvals, poly, ""log(n)"", ""log((R/S)_n)"",
  fname=plot_file)
  h = poly[0] + 0.5 if corrected else poly[0]
  if debug_data:
  return (h, (np.log(nvals), np.log(rsvals), poly))
  else:
  return h","Calculates the Hurst exponent by a standard rescaled range (R/S) approach. Explanation of Hurst exponent: The Hurst exponent is a measure for the ""long-term memory"" of a time series, meaning the long statistical dependencies in the data that do not originate from cycles. It originates from H.E. Hursts observations of the problem of long-term storage in water reservoirs. If x_i is the discharge of a river in year i and we observe this discharge for N years, we can calculate the storage capacity that would be required to keep the discharge steady at its mean value. To do so, we first substract the mean over all x_i from the individual x_i to obtain the departures x'_i from the mean for each year i. As the excess or deficit in discharge always carrys over from year i to year i+1, we need to examine the cumulative sum of x'_i, denoted by y_i. This cumulative sum represents the filling of our hypothetical storage. If the sum is above 0, we are storing excess discharge from the river, if it is below zero we have compensated a deficit in discharge by releasing water from the storage. The range (maximum - minimum) R of y_i therefore represents the total capacity required for the storage. Hurst showed that this value follows a steady trend for varying N if it is normalized by the standard deviation sigma over the x_i. Namely he obtained the following formula: R/sigma = (N/2)^K In this equation, K is called the Hurst exponent. Its value is 0.5 for white noise, but becomes greater for time series that exhibit some positive dependency on previous values. For negative dependencies it becomes less than 0.5. Explanation of the algorithm: The rescaled range (R/S) approach is directly derived from Hurst's definition. The time series of length N is split into non-overlapping subseries of length n. Then, R and S (S = sigma) are calculated for each subseries and the mean is taken over all subseries yielding (R/S)_n. This process is repeated for several lengths n. Finally, the exponent K is obtained by fitting a straight line to the plot of log((R/S)_n) vs log(n). There seems to be no consensus how to chose the subseries lenghts n. This function therefore leaves the choice to the user. The module provides some utility functions for ""typical"" values: * binary_n: N/2, N/4, N/8, ... * logarithmic_n: min_n, min_n * f, min_n * f^2, ... References: .. [h_1] H. E. Hurst, The problem of long-term storage in reservoirs, International Association of Scientific Hydrology. Bulletin, vol. 1, no. 3, pp. 1327, 1956. .. [h_2] H. E. Hurst, A suggested statistical model of some time series which occur in nature, Nature, vol. 180, p. 494, 1957. .. [h_3] R. Weron, Estimating long-range dependence: finite sample properties and confidence intervals, Physica A: Statistical Mechanics and its Applications, vol. 312, no. 1, pp. 285299, 2002. Reference Code: .. [h_a] ""hurst"" function in R-package ""pracma"", url: https://cran.r-project.org/web/packages/pracma/pracma.pdf Note: Pracma yields several estimates of the Hurst exponent, which are listed below. Unless otherwise stated they use the divisors of the length of the sequence as n. The length is reduced by at most 1% to find the value that has the most divisors. * The ""Simple R/S"" estimate is just log((R/S)_n) / log(n) for n = N. * The ""theoretical Hurst exponent"" is the value that would be expected of an uncorrected rescaled range approach for random noise of the size of the input data. * The ""empirical Hurst exponent"" is the uncorrected Hurst exponent obtained by the rescaled range approach. * The ""corrected empirical Hurst exponent"" is the Anis-Lloyd-Peters corrected Hurst exponent, but with sqrt(1/2 * pi * n) added to the (R/S)_n before the log. * The ""corrected R over S Hurst exponent"" uses the R-function ""lm"" instead of pracmas own ""polyfit"" and uses n = N/2, N/4, N/8, ... by successively halving the subsequences (which means that some subsequences may be one element longer than others). In contrast to its name it does not use the Anis-Lloyd-Peters correction factor. If you want to compare the output of pracma to the output of nolds, the ""empirical hurst exponent"" is the only measure that exactly corresponds to the Hurst measure implemented in nolds (by choosing corrected=False, fit=""poly"" and employing the same strategy for choosing n as the divisors of the (reduced) sequence length). .. [h_b] Rafael Weron, ""HURST: MATLAB function to compute the Hurst exponent using R/S Analysis"", url: https://ideas.repec.org/c/wuu/hscode/m11003.html Note: When the same values for nvals are used and fit is set to ""poly"", nolds yields exactly the same results as this implementation. .. [h_c] Bill Davidson, ""Hurst exponent"", url: http://www.mathworks.com/matlabcentral/fileexchange/9842-hurst-exponent .. [h_d] Tomaso Aste, ""Generalized Hurst exponent"", url: http://de.mathworks.com/matlabcentral/fileexchange/30076-generalized-hurst-exponent Args: data (array-like of float): time series Kwargs: nvals (iterable of int): sizes of subseries to use (default: logmid_n(total_N, ratio=1/4.0, nsteps=15) , that is 15 logarithmically spaced values in the medium 25% of the logarithmic range) Generally, the choice for n is a trade-off between the length and the number of the subsequences that are used for the calculation of the (R/S)_n. Very low values of n lead to high variance in the ``r`` and ``s`` while very high values may leave too few subsequences that the mean along them is still meaningful. Logarithmic spacing makes sense, because it translates to even spacing in the log-log-plot. fit (str): the fitting method to use for the line fit, either 'poly' for normal least squares polynomial fitting or 'RANSAC' for RANSAC-fitting which is more robust to outliers debug_plot (boolean): if True, a simple plot of the final line-fitting step will be shown debug_data (boolean): if True, debugging data will be returned alongside the result plot_file (str): if debug_plot is True and plot_file is not None, the plot will be saved under the given file name instead of directly showing it through ``plt.show()`` corrected (boolean): if True, the Anis-Lloyd-Peters correction factor will be applied to the output according to the expected value for the individual (R/S)_n (see [h_3]_) unbiased (boolean): if True, the standard deviation based on the unbiased variance (1/(N-1) instead of 1/N) will be used. This should be the default choice, since the true mean of the sequences is not known. This parameter should only be changed to recreate results of other implementations. Returns: float: estimated Hurst exponent K using a rescaled range approach (if K = 0.5 there are no long-range correlations in the data, if K < 0.5 there are negative long-range correlations, if K > 0.5 there are positive long-range correlations) (1d-vector, 1d-vector, list): only present if debug_data is True: debug data of the form ``(nvals, rsvals, poly)`` where ``nvals`` are the values used for log(n), ``rsvals`` are the corresponding log((R/S)_n) and ``poly`` are the line coefficients (``[slope, intercept]``)",1,0,0,1,2,1,0,0,1,2
"def get_single_int_autoincrement_colname(table_: Table) -> Optional[str]:
  n_autoinc = 0
  int_autoinc_names = []
  for col in table_.columns:
  if col.autoincrement:
  n_autoinc += 1
  if is_sqlatype_integer(col.type):
  int_autoinc_names.append(col.name)
  if n_autoinc > 1:
  log.warning(""Table {!r} has {} autoincrement columns"",
  table_.name, n_autoinc)
  if n_autoinc == 1 and len(int_autoinc_names) == 1:
  return int_autoinc_names[0]
  return None","If a table has a single integer ``AUTOINCREMENT`` column, this will return its name; otherwise, ``None``. - It's unlikely that a database has >1 ``AUTOINCREMENT`` field anyway, but we should check. - SQL Server's ``IDENTITY`` keyword is equivalent to MySQL's ``AUTOINCREMENT``. - Verify against SQL Server: .. code-block:: sql SELECT table_name, column_name FROM information_schema.columns WHERE COLUMNPROPERTY(OBJECT_ID(table_schema + '.' + table_name), column_name, 'IsIdentity') = 1 ORDER BY table_name; ... http://stackoverflow.com/questions/87747 - Also: .. code-block:: sql sp_columns 'tablename'; ... which is what SQLAlchemy does (``dialects/mssql/base.py``, in :func:`get_columns`).",0,0,1,0,1,0,0,0,0,0
"def _get_file(db, user_id, api_path, query_fields, decrypt_func):
  result = db.execute(
  _select_file(user_id, api_path, query_fields, limit=1),
  ).first()
  if result is None:
  raise NoSuchFile(api_path)
  if files.c.content in query_fields:
  return to_dict_with_content(query_fields, result, decrypt_func)
  else:
  return to_dict_no_content(query_fields, result)","Get file data for the given user_id, path, and query_fields. The query_fields parameter specifies which database fields should be included in the returned file data.",0,0,1,1,2,1,0,1,1,3
"def standard_suggestions(self):
  suggestions = BulletedList()
  suggestions.add(
  'Check that you have the latest version of InaSAFE installed '
  '- you may have encountered a bug that is fixed in a '
  'subsequent release.')
  suggestions.add(
  'Check the InaSAFE documentation to see if you are trying to '
  'do something unsupported.')
  suggestions.add(
  'Report the problem using the issue tracker at '
  'https://github.com/inasafe/inasafe/issues. Reporting an issue '
  'requires that you first create a free account at '
  'http://github.com. When you report the issue, '
  'please copy and paste the complete contents of this panel '
  'into the issue to ensure the best possible chance of getting '
  'your issue resolved.')
  suggestions.add(
  'Try contacting one of the InaSAFE development team by '
  'sending an email to info@inasafe.org. Please ensure that you '
  'copy and paste the complete contents of this panel into the '
  'email.')
  return suggestions",Standard generic suggestions. :return: List of standard suggestions for users who encounter errors. :rtype: BulletedList,1,0,0,1,2,1,0,0,1,2
"def prompt(message='', **kwargs):
  patch_stdout = kwargs.pop('patch_stdout', False)
  return_asyncio_coroutine = kwargs.pop('return_asyncio_coroutine', False)
  true_color = kwargs.pop('true_color', False)
  refresh_interval = kwargs.pop('refresh_interval', 0)
  eventloop = kwargs.pop('eventloop', None)
  application = create_prompt_application(message, **kwargs)
  return run_application(application,
  patch_stdout=patch_stdout,
  return_asyncio_coroutine=return_asyncio_coroutine,
  true_color=true_color,
  refresh_interval=refresh_interval,
  eventloop=eventloop)","Get input from the user and return it. This is a wrapper around a lot of ``prompt_toolkit`` functionality and can be a replacement for `raw_input`. (or GNU readline.) If you want to keep your history across several calls, create one :class:`~prompt_toolkit.history.History` instance and pass it every time. This function accepts many keyword arguments. Except for the following, they are a proxy to the arguments of :func:`.create_prompt_application`. :param patch_stdout: Replace ``sys.stdout`` by a proxy that ensures that print statements from other threads won't destroy the prompt. (They will be printed above the prompt instead.) :param return_asyncio_coroutine: When True, return a asyncio coroutine. (Python >3.3) :param true_color: When True, use 24bit colors instead of 256 colors. :param refresh_interval: (number; in seconds) When given, refresh the UI every so many seconds.",1,0,0,1,2,1,0,0,1,2
"def rowsBetween(start, end):
  if start <= Window._PRECEDING_THRESHOLD:
  start = Window.unboundedPreceding
  if end >= Window._FOLLOWING_THRESHOLD:
  end = Window.unboundedFollowing
  sc = SparkContext._active_spark_context
  jspec = sc._jvm.org.apache.spark.sql.expressions.Window.rowsBetween(start, end)
  return WindowSpec(jspec)","Creates a :class:`WindowSpec` with the frame boundaries defined, from `start` (inclusive) to `end` (inclusive). Both `start` and `end` are relative positions from the current row. For example, ""0"" means ""current row"", while ""-1"" means the row before the current row, and ""5"" means the fifth row after the current row. We recommend users use ``Window.unboundedPreceding``, ``Window.unboundedFollowing``, and ``Window.currentRow`` to specify special boundary values, rather than using integral values directly. A row based boundary is based on the position of the row within the partition. An offset indicates the number of rows above or below the current row, the frame for the current row starts or ends. For instance, given a row based sliding frame with a lower bound offset of -1 and a upper bound offset of +2. The frame for row with index 5 would range from index 4 to index 6. >>> from pyspark.sql import Window >>> from pyspark.sql import functions as func >>> from pyspark.sql import SQLContext >>> sc = SparkContext.getOrCreate() >>> sqlContext = SQLContext(sc) >>> tup = [(1, ""a""), (1, ""a""), (2, ""a""), (1, ""b""), (2, ""b""), (3, ""b"")] >>> df = sqlContext.createDataFrame(tup, [""id"", ""category""]) >>> window = Window.partitionBy(""category"").orderBy(""id"").rowsBetween(Window.currentRow, 1) >>> df.withColumn(""sum"", func.sum(""id"").over(window)).show() +---+--------+---+ | id|category|sum| +---+--------+---+ | 1| b| 3| | 2| b| 5| | 3| b| 3| | 1| a| 2| | 1| a| 3| | 2| a| 2| +---+--------+---+ :param start: boundary start, inclusive. The frame is unbounded if this is ``Window.unboundedPreceding``, or any value less than or equal to -9223372036854775808. :param end: boundary end, inclusive. The frame is unbounded if this is ``Window.unboundedFollowing``, or any value greater than or equal to 9223372036854775807.",1,0,0,0,1,1,0,0,1,2
"def enable_device(self):
  cmd_response = self.__send_command(const.CMD_ENABLEDEVICE)
  if cmd_response.get('status'):
  self.is_enabled = True
  return True
  else:
  raise ZKErrorResponse(""Can't enable device"")",re-enable the connected device and allow user activity in device again :return: bool,1,0,0,0,1,1,0,0,1,2
"def insert(self, packet, time=None, **kwargs):
  fields = {}
  pd = packet._defn
  for defn in pd.fields:
  val = getattr(packet.raw, defn.name)
  if pd.history and defn.name in pd.history:
  val = getattr(packet.history, defn.name)
  if val is not None and not (isinstance(val, float) and math.isnan(val)):
  fields[defn.name] = val
  if len(fields) == 0:
  log.error('No fields present to insert into Influx')
  return
  tags = kwargs.get('tags', {})
  if isinstance(time, dt.datetime):
  time = time.strftime(""%Y-%m-%dT%H:%M:%S"")
  data = {
  'measurement': pd.name,
  'tags': tags,
  'fields': fields
  }
  if time:
  data['time'] = time
  self._conn.write_points([data])",Insert a packet into the database Arguments packet The :class:`ait.core.tlm.Packet` instance to insert into the database time Optional parameter specifying the time value to use when inserting the record into the database. Default case does not provide a time value so Influx defaults to the current time when inserting the record. tags Optional kwargs argument for specifying a dictionary of tags to include when adding the values. Defaults to nothing.,0,1,0,0,1,1,1,0,1,3
"def create_collection(self, name, codec_options=None,
  read_preference=None, write_concern=None,
  read_concern=None, session=None, **kwargs):
  with self.__client._tmp_session(session) as s:
  if name in self.list_collection_names(
  filter={""name"": name}, session=s):
  raise CollectionInvalid(""collection %s already exists"" % name)
  return Collection(self, name, True, codec_options,
  read_preference, write_concern,
  read_concern, session=s, **kwargs)","Create a new :class:`~pymongo.collection.Collection` in this database. Normally collection creation is automatic. This method should only be used to specify options on creation. :class:`~pymongo.errors.CollectionInvalid` will be raised if the collection already exists. Options should be passed as keyword arguments to this method. Supported options vary with MongoDB release. Some examples include: - ""size"": desired initial size for the collection (in bytes). For capped collections this size is the max size of the collection. - ""capped"": if True, this is a capped collection - ""max"": maximum number of objects if capped (optional) See the MongoDB documentation for a full list of supported options by server version. :Parameters: - `name`: the name of the collection to create - `codec_options` (optional): An instance of :class:`~bson.codec_options.CodecOptions`. If ``None`` (the default) the :attr:`codec_options` of this :class:`Database` is used. - `read_preference` (optional): The read preference to use. If ``None`` (the default) the :attr:`read_preference` of this :class:`Database` is used. - `write_concern` (optional): An instance of :class:`~pymongo.write_concern.WriteConcern`. If ``None`` (the default) the :attr:`write_concern` of this :class:`Database` is used. - `read_concern` (optional): An instance of :class:`~pymongo.read_concern.ReadConcern`. If ``None`` (the default) the :attr:`read_concern` of this :class:`Database` is used. - `collation` (optional): An instance of :class:`~pymongo.collation.Collation`. - `session` (optional): a :class:`~pymongo.client_session.ClientSession`. - `**kwargs` (optional): additional keyword arguments will be passed as options for the create collection command .. versionchanged:: 3.6 Added ``session`` parameter. .. versionchanged:: 3.4 Added the collation option. .. versionchanged:: 3.0 Added the codec_options, read_preference, and write_concern options. .. versionchanged:: 2.2 Removed deprecated argument: options",1,0,1,1,3,1,0,0,1,2
"def sshclient_from_instance(instance, ssh_key_file,
  host_key_file='~/.ssh/known_hosts',
  user_name='root', ssh_pwd=None):
  s = FakeServer(instance, ssh_key_file)
  return SSHClient(s, host_key_file, user_name, ssh_pwd)","Create and return an SSHClient object given an instance object. :type instance: :class`boto.ec2.instance.Instance` object :param instance: The instance object. :type ssh_key_file: str :param ssh_key_file: A path to the private key file used to log into instance. :type host_key_file: str :param host_key_file: A path to the known_hosts file used by the SSH client. Defaults to ~/.ssh/known_hosts :type user_name: str :param user_name: The username to use when logging into the instance. Defaults to root. :type ssh_pwd: str :param ssh_pwd: The passphrase, if any, associated with private key.",0,0,0,1,1,1,0,0,0,1
"def callback_request(self, msg, reply_cb=None, inform_cb=None,
  user_data=None, timeout=None, use_mid=None):
  if timeout is None:
  timeout = self._request_timeout
  mid = self._get_mid_and_update_msg(msg, use_mid)
  if timeout is None:
  timeout_handle = None
  else:
  timeout_handle = self.ioloop.call_later(
  timeout, partial(self._handle_timeout, mid, self.ioloop.time()))
  self._push_async_request(
  mid, msg, reply_cb, inform_cb, user_data, timeout_handle)
  try:
  self.send_request(msg)
  except KatcpClientError, e:
  error_reply = Message.request(msg.name, ""fail"", str(e))
  error_reply.mid = mid
  self.handle_reply(error_reply)","Send a request messsage. Parameters ---------- msg : Message object The request message to send. reply_cb : function The reply callback with signature reply_cb(msg) or reply_cb(msg, \*user_data) inform_cb : function The inform callback with signature inform_cb(msg) or inform_cb(msg, \*user_data) user_data : tuple Optional user data to send to the reply and inform callbacks. timeout : float in seconds How long to wait for a reply. The default is the the timeout set when creating the AsyncClient. use_mid : boolean, optional Whether to use message IDs. Default is to use message IDs if the server supports them.",1,0,0,2,3,1,0,0,1,2
"async def finish_authentication(self, username, password):
  self.srp.step1(username, password)
  data = await self._send_plist(
  'step1', method='pin', user=username)
  resp = plistlib.loads(data)
  pub_key, key_proof = self.srp.step2(resp['pk'], resp['salt'])
  await self._send_plist(
  'step2',
  pk=binascii.unhexlify(pub_key),
  proof=binascii.unhexlify(key_proof))
  epk, tag = self.srp.step3()
  await self._send_plist('step3', epk=epk, authTag=tag)
  return True",Finish authentication process. A username (generated by new_credentials) and the PIN code shown on screen must be provided.,2,0,0,2,4,1,0,0,1,2
"def remove_config(chassis_id=None,
  community=None,
  contact=None,
  location=None,
  test=False,
  commit=True,
  **kwargs):
  dic = {
  'template_name': 'delete_snmp_config',
  'test': test,
  'commit': commit
  }
  if chassis_id:
  dic['chassis_id'] = chassis_id
  if community:
  dic['community'] = community
  if contact:
  dic['contact'] = contact
  if location:
  dic['location'] = location
  dic['inherit_napalm_device'] = napalm_device
  return __salt__['net.load_template'](**dic)","Removes a configuration element from the SNMP configuration. :param chassis_id: (optional) Chassis ID :param community: (optional) A dictionary having the following optional keys: - acl (if any policy / ACL need to be set) - mode: rw or ro. Default: ro :param contact: Contact details :param location: Location :param test: Dry run? If set as True, will apply the config, discard and return the changes. Default: False :param commit: Commit? (default: True) Sometimes it is not needed to commit the config immediately after loading the changes. E.g.: a state loads a couple of parts (add / remove / update) and would not be optimal to commit after each operation. Also, from the CLI when the user needs to apply the similar changes before committing, can specify commit=False and will not discard the config. :raise MergeConfigException: If there is an error on the configuration sent. :return: A dictionary having the following keys: - result (bool): if the config was applied successfully. It is `False` only in case of failure. In case there are no changes to be applied and successfully performs all operations it is still `True` and so will be the `already_configured` flag (example below) - comment (str): a message for the user - already_configured (bool): flag to check if there were no changes applied - diff (str): returns the config changes applied CLI Example: .. code-block:: bash salt '*' snmp.remove_config community='abcd'",1,0,0,2,3,1,0,0,1,2
"def key_wait():
  while 1:
  for event in get():
  if event.type == 'KEYDOWN':
  return event
  if event.type == 'QUIT':
  return KeyDown('F4', '', True, False, True, False, False)
  _time.sleep(.001)",Waits until the user presses a key. Then returns a :any:`KeyDown` event. Key events will repeat if held down. A click to close the window will be converted into an Alt+F4 KeyDown event. Returns: tdl.event.KeyDown: The pressed key.,1,0,0,1,2,1,0,0,1,2
"def user_input(
  field, default='', choices=None, password=False,
  empty_ok=False, accept=False):
  result = ''
  while not result:
  prompt = field
  if default:
  prompt += ' [{0}]'.format(default)
  prompt += ': '
  if accept and not (not default and not empty_ok):
  print(prompt)
  result = '{0}'.format(default)
  else:
  if password:
  result = getpass.getpass(prompt)
  else:
  result = input(prompt)
  result = result.strip()
  if not result:
  result = default
  if choices and result not in choices:
  print('Must be one of {0}'.format(choices))
  result = ''
  if empty_ok:
  break
  return result","Prompt user for input until a value is retrieved or default is accepted. Return the input. Arguments: *field* - Description of the input being prompted for. *default* - Default value for the input accepted with a Return-key. *password* - Whether the user input should not be echoed to screen. *empty_ok* - Whether it's okay to accept an empty input. *accept* - Whether to skip getting actual user input and just accept the default value, unless prevented by the combination of arguments *empty_ok* and *default*. That is, unless *default* is an empty string and *empty_ok* is False.",1,0,0,1,2,1,0,0,1,2
"def _sanitize_inputs(self, lattice_spacing, lattice_vectors,
  lattice_points, angles):
  if angles is not None and lattice_vectors is not None:
  raise ValueError('Overdefined system: angles and lattice_vectors '
  'provided. Only one of these should be passed.')
  self._validate_lattice_spacing(lattice_spacing)
  if angles is not None:
  self._validate_angles(angles)
  self.lattice_vectors = self._from_lattice_parameters(self.angles)
  else:
  self._validate_lattice_vectors(lattice_vectors)
  self.angles = self._from_lattice_vectors()
  self._validate_lattice_points(lattice_points)","Check for proper inputs and set instance attributes. validate_inputs takes the data passed to the constructor by the user and will ensure that the data is correctly formatted and will then set its instance attributes. validate_inputs checks that dimensionality is maintained, the unit cell is right handed, the area or volume of the unit cell is positive and non-zero for 2D and 3D respectively, lattice spacing are provided, basis vectors do not overlap when the unit cell is expanded. Exceptions Raised ----------------- TypeError : incorrect typing of the input parameters. ValueError : values are not within restrictions.",1,0,0,0,1,1,0,0,1,2
"def sqlite3(self, conn, sql, parameters=None, *args, **kwargs):
  if parameters is None:
  parameters = ()
  if isinstance(conn, (sqlite3api.Connection, sqlite3api.Cursor)):
  return self(conn.execute(sql, parameters))
  elif isinstance(conn, str):
  with sqlite3api.connect(conn, *args, **kwargs) as input_conn:
  return self(input_conn.execute(sql, parameters))
  else:
  raise ValueError('conn must be a must be a file path or sqlite3 Connection/Cursor')","Reads input by querying from a sqlite database. >>> seq.sqlite3('examples/users.db', 'select id, name from users where id = 1;').first() [(1, 'Tom')] :param conn: path or sqlite connection, cursor :param sql: SQL query string :param parameters: Parameters for sql query :return: Sequence wrapping SQL cursor",1,0,1,1,3,1,0,0,1,2
"def subcellular_location(self, location=None, entry_name=None, limit=None, as_df=False):
  q = self.session.query(models.SubcellularLocation)
  q = self.get_model_queries(q, ((location, models.SubcellularLocation.location),))
  q = self.get_many_to_many_queries(q, ((entry_name, models.SubcellularLocation.entries, models.Entry.name),))
  return self._limit_and_df(q, limit, as_df)","Method to query :class:`.models.SubcellularLocation` objects in database :param location: subcellular location(s) :type location: str or tuple(str) or None :param entry_name: name(s) in :class:`.models.Entry` :type entry_name: str or tuple(str) or None :param limit: - if `isinstance(limit,int)==True` -> limit - if `isinstance(limit,tuple)==True` -> format:= tuple(page_number, results_per_page) - if limit == None -> all results :type limit: int or tuple(int) or None :param bool as_df: if `True` results are returned as :class:`pandas.DataFrame` :return: - if `as_df == False` -> list(:class:`.models.SubcellularLocation`) - if `as_df == True` -> :class:`pandas.DataFrame` :rtype: list(:class:`.models.SubcellularLocation`) or :class:`pandas.DataFrame`",0,0,1,1,2,1,0,1,1,3
"def add_photo(self, collection_id, photo_id):
  url = ""/collections/%s/add"" % collection_id
  data = {
  ""collection_id"": collection_id,
  ""photo_id"": photo_id
  }
  result = self._post(url, data=data) or {}
  return CollectionModel.parse(result.get(""collection"")), PhotoModel.parse(result.get(""photo""))","Add a photo to one of the logged-in users collections. Requires the 'write_collections' scope. Note: If the photo is already in the collection, this acion has no effect. :param collection_id [string]: The collections ID. Required. :param photo_id [string]: The photos ID. Required. :return: [Tuple]: The Unsplash Collection and Photo",1,0,0,1,2,1,0,0,2,3
"def read_namespaced_resource_quota(self, name, namespace, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.read_namespaced_resource_quota_with_http_info(name, namespace, **kwargs)
  else:
  (data) = self.read_namespaced_resource_quota_with_http_info(name, namespace, **kwargs)
  return data","read the specified ResourceQuota This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.read_namespaced_resource_quota(name, namespace, async_req=True) >>> result = thread.get() :param async_req bool :param str name: name of the ResourceQuota (required) :param str namespace: object name and auth scope, such as for teams and projects (required) :param str pretty: If 'true', then the output is pretty printed. :param bool exact: Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. Deprecated. Planned for removal in 1.18. :param bool export: Should this value be exported. Export strips fields that a user can not specify. Deprecated. Planned for removal in 1.18. :return: V1ResourceQuota If the method is called asynchronously, returns the request thread.",2,0,0,1,3,1,0,0,1,2
"def get_blob_data(self, tag_target='asset', force=False):
  if hasattr(self, '_blob_data') and not force:
  return self._blob_data
  if six.PY2:
  self._blob_data = six.binary_type('')
  elif six.PY3:
  self._blob_data = six.binary_type('', encoding='ascii')
  asset_contents = self.contents.filter(tag=tag_target)
  for asset_content in asset_contents:
  blobs = asset_content.stream.get_blobs()
  for blob in blobs:
  self._blob_data += six.binary_type(blob.data)
  return self._blob_data","get asset version content using pg large object streams :param bool force: False by default, forces get content from database instead of using cached value :rtype: str :return: content in raw format",1,0,0,1,2,1,0,0,1,2
"def get_wikidata_qnum(wikiarticle, wikisite):
  resp = requests.get('https://www.wikidata.org/w/api.php', timeout=5, params={
  'action': 'wbgetentities',
  'titles': wikiarticle,
  'sites': wikisite,
  'props': '',
  'format': 'json'
  }).json()
  return list(resp['entities'])[0]","Retrieve the Query number for a wikidata database of metadata about a particular article >>> print(get_wikidata_qnum(wikiarticle=""Andromeda Galaxy"", wikisite=""enwiki"")) Q2469",2,0,0,1,3,2,0,0,1,3
"def _GetVSSStoreIdentifiers(self, scan_node):
  if not scan_node or not scan_node.path_spec:
  raise errors.SourceScannerError('Invalid scan node.')
  volume_system = vshadow_volume_system.VShadowVolumeSystem()
  volume_system.Open(scan_node.path_spec)
  volume_identifiers = self._source_scanner.GetVolumeIdentifiers(
  volume_system)
  if not volume_identifiers:
  return []
  if self._vss_stores:
  if self._vss_stores == 'all':
  vss_stores = range(1, volume_system.number_of_volumes + 1)
  else:
  vss_stores = self._vss_stores
  selected_volume_identifiers = self._NormalizedVolumeIdentifiers(
  volume_system, vss_stores, prefix='vss')
  if not set(selected_volume_identifiers).difference(volume_identifiers):
  return selected_volume_identifiers
  try:
  volume_identifiers = self._PromptUserForVSSStoreIdentifiers(
  volume_system, volume_identifiers)
  except KeyboardInterrupt:
  raise errors.UserAbort('File system scan aborted.')
  return self._NormalizedVolumeIdentifiers(
  volume_system, volume_identifiers, prefix='vss')",Determines the VSS store identifiers. Args: scan_node (dfvfs.SourceScanNode): scan node. Returns: list[str]: VSS store identifiers. Raises: SourceScannerError: if the format of or within the source is not supported or the scan node is invalid. UserAbort: if the user requested to abort.,0,0,0,1,1,1,0,0,1,2
"async def create(source_id: str, proof_request: str):
  constructor_params = (source_id,)
  c_source_id = c_char_p(source_id.encode('utf-8'))
  c_proof_request = c_char_p(json.dumps(proof_request).encode('utf-8'))
  c_params = (c_source_id, c_proof_request, )
  return await DisclosedProof._create(""vcx_disclosed_proof_create_with_request"",
  constructor_params,
  c_params)","Create a proof for fulfilling a corresponding proof request :param source_id: Tag associated by user of sdk :param proof_request: Proof Request data sent by requestor. Example: source_id = 'sourceId' request = { ""@topic"": { ""mid"": 9, ""tid"": 1 }, ""@type"": { ""name"": ""PROOF_REQUEST"", ""version"":""1.0"" }, ""msg_ref_id"": ""ymy5nth"", ""proof_request_data"": { ""name"": ""Account Certificate"", ""nonce"": ""838186471541979035208225"", ""requested_attributes"": { ""business_2"": { ""name"": ""business"" }, ""email_1"": { ""name"": ""email"" }, ""name_0"": { ""name"": ""name"" } }, ""requested_predicates"": {}, ""version"": ""0.1"" } } disclosed_proof = await DisclosedProof.create(source_id, request) :return: Disclosed Proof Object",1,0,0,1,2,1,0,0,2,3
"def download_storyitem(self, item: StoryItem, target: str) -> bool:
  date_local = item.date_local
  dirname = _PostPathFormatter(item).format(self.dirname_pattern, target=target)
  filename = dirname + '/' + self.format_filename(item, target=target)
  os.makedirs(os.path.dirname(filename), exist_ok=True)
  downloaded = False
  if not item.is_video or self.download_video_thumbnails is True:
  url = item.url
  downloaded = self.download_pic(filename=filename, url=url, mtime=date_local)
  if item.is_video and self.download_videos is True:
  downloaded |= self.download_pic(filename=filename, url=item.video_url, mtime=date_local)
  metadata_string = _ArbitraryItemFormatter(item).format(self.storyitem_metadata_txt_pattern).strip()
  if metadata_string:
  self.save_caption(filename=filename, mtime=item.date_local, caption=metadata_string)
  if self.save_metadata is not False:
  self.save_metadata_json(filename, item)
  self.context.log()
  return downloaded","Download one user story. :param item: Story item, as in story['items'] for story in :meth:`get_stories` :param target: Replacement for {target} in dirname_pattern and filename_pattern :return: True if something was downloaded, False otherwise, i.e. file was already there",2,0,0,0,2,1,0,0,1,2
"def update(self, key, data):
  if key:
  spec = {'_id': key}
  write = data.copy()
  response = self._collection.update(
  spec, {'$set': write}, multi=False, upsert=False, manipulate=True)
  if response.get('ok') != 1:
  raise SimplMongoError(""Error updating document '%s': %s"" %
  (spec.get('_id'), response.errmsg))
  LOG.debug(""DB UPDATE: %s.%s"", self.collection_name, response)
  return response.get('n')","Update document by key with partial data. Updates the document matching _id=<key> with 'data' Where 1st argument 'key' is <key> 'data' may contain dot notation fields in order to specify nested values in the documents, e.g. collection.update(<document_key>, {'my.nested.field': 'match!'}) The dictionary provided as 'data' will update only those corresponding fields and subfields in the database document, without clobbering other fields and subfields. :returns: count of records added/updated",0,1,1,0,2,1,1,1,1,4
"def get_entities(tweet):
  entity_key = ""entities"" if is_original_format(tweet) else ""twitter_entities""
  if get_tweet_type(tweet) == ""retweet"":
  retweet_entities = tweet.get(entity_key, [])
  all_entities = get_retweeted_tweet(tweet).get(entity_key,[]).copy()
  all_entities[""user_mentions""] = ([retweet_entities[""user_mentions""][0]] +
  all_entities[""user_mentions""])
  return all_entities
  else:
  return tweet.get(entity_key, [])","Helper function to simply grabbing the entities. \n Caveat: In the case of Retweets, a Retweet is stored as ""RT @someone: Some awesome status"". In the case where pre-appending the string ""RT @someone:"" causes the Tweet to exceed 140 characters, entites (hashtags, mentions, urls) beyond the 140 character mark are excluded from the Retweet's entities. This seems like counterintuitive behavior, so we ensure here that the entities of a Retweet are a superset of the entities of the Retweeted status. Args: tweet (Tweet or dict): Tweet in question Returns: dict: dictionary of potential entities. Example: >>> from tweet_parser.getter_methods.tweet_entities import get_entities >>> original = {""created_at"": ""Wed May 24 20:17:19 +0000 2017"", ... ""entities"": {""user_mentions"": [{ ... ""indices"": [14,26], #characters where the @ mention appears ... ""id_str"": ""2382763597"", #id of @ mentioned user as a string ... ""screen_name"": ""notFromShrek"", #screen_name of @ mentioned user ... ""name"": ""Fiona"", #display name of @ mentioned user ... ""id"": 2382763597 #id of @ mentioned user as an int ... }] ... } ... } >>> get_entities(original) {'user_mentions': [{'indices': [14, 26], 'id_str': '2382763597', 'screen_name': 'notFromShrek', 'name': 'Fiona', 'id': 2382763597}]}",1,0,0,1,2,1,0,0,1,2
"def create_lb_cookie_stickiness_policy(self, cookie_expiration_period,
  lb_name, policy_name):
  params = {
  'CookieExpirationPeriod' : cookie_expiration_period,
  'LoadBalancerName' : lb_name,
  'PolicyName' : policy_name,
  }
  return self.get_status('CreateLBCookieStickinessPolicy', params)","Generates a stickiness policy with sticky session lifetimes controlled by the lifetime of the browser (user-agent) or a specified expiration period. This policy can only be associated only with HTTP listeners. When a load balancer implements this policy, the load balancer uses a special cookie to track the backend server instance for each request. When the load balancer receives a request, it first checks to see if this cookie is present in the request. If so, the load balancer sends the request to the application server specified in the cookie. If not, the load balancer sends the request to a server that is chosen based on the existing load balancing algorithm. A cookie is inserted into the response for binding subsequent requests from the same user to that server. The validity of the cookie is based on the cookie expiration time, which is specified in the policy configuration.",1,0,0,1,2,1,0,0,1,2
"def draw_strokes(stroke_based_drawings):
  single_input = False
  if (not isinstance(stroke_based_drawings, _tc.SArray)
  and not isinstance(stroke_based_drawings, list)):
  raise _ToolkitError(""Input to draw_strokes must be of type ""
  + ""turicreate.SArray or list (for a single stroke-based drawing)"")
  if (isinstance(stroke_based_drawings, _tc.SArray)
  and stroke_based_drawings.dtype != list):
  raise _ToolkitError(""SArray input to draw_strokes must have dtype ""
  + ""list. Each element in the SArray should be a list of strokes, ""
  + ""where each stroke is a list of points, ""
  + ""and each point is represented as a dictionary ""
  + ""with two keys, \""x\"" and \""y\""."")
  if isinstance(stroke_based_drawings, list):
  single_input = True
  stroke_based_drawings = _tc.SArray([stroke_based_drawings])
  sf = _tc.SFrame({""drawings"": stroke_based_drawings})
  sf_with_drawings = _extensions._drawing_classifier_prepare_data(
  sf, ""drawings"")
  if single_input:
  return sf_with_drawings[""drawings""][0]
  return sf_with_drawings[""drawings""]","Visualizes drawings (ground truth or predictions) by returning images to represent the stroke-based data from the user. Parameters ---------- stroke_based_drawings: SArray or list An `SArray` of type `list`. Each element in the SArray should be a list of strokes, where each stroke is a list of points, and each point is represented as a dictionary with two keys, ""x"" and ""y"". A single stroke-based drawing is also supported, in which case, the type of the input would be list. Returns ------- drawings: SArray or _tc.Image Each stroke-based drawing is converted into a 28x28 grayscale drawing for the user to visualize what their strokes traced.",1,0,0,1,2,1,0,0,1,2
"def SensorShare(self, sensor_id, parameters):
  if not parameters['user']['id']:
  parameters['user'].pop('id')
  if not parameters['user']['username']:
  parameters['user'].pop('username')
  if self.__SenseApiCall__(""/sensors/{0}/users"".format(sensor_id), ""POST"", parameters = parameters):
  return True
  else:
  self.__error__ = ""api call unsuccessful""
  return False",Share a sensor with a user @param sensor_id (int) - Id of sensor to be shared @param parameters (dictionary) - Additional parameters for the call @return (bool) - Boolean indicating whether the ShareSensor call was successful,1,0,0,2,3,2,0,0,1,3
"async def get_states_list(self) -> typing.List[typing.Tuple[int]]:
  conn = await self.redis()
  result = []
  keys = await conn.execute('KEYS', 'fsm:*')
  for item in keys:
  *_, chat, user = item.decode('utf-8').split(':')
  result.append((chat, user))
  return result",Get list of all stored chat's and user's :return: list of tuples where first element is chat id and second is user id,1,0,0,1,2,1,0,0,1,2
"def _prompt_choice(var_name, options):
  choice_map = OrderedDict(
  ('{0}'.format(i), value) for i, value in enumerate(options, 1) if value[0] != 'test'
  )
  choices = choice_map.keys()
  default = '1'
  choice_lines = ['{0} - {1} - {2}'.format(c[0], c[1][0], c[1][1]) for c in choice_map.items()]
  prompt = '\n'.join((
  'Select {0}:'.format(var_name),
  '\n'.join(choice_lines),
  'Choose from {0}'.format(', '.join(choices))
  ))
  user_choice = click.prompt(
  prompt, type=click.Choice(choices), default=default
  )
  return choice_map[user_choice]","Prompt the user to choose between a list of options, index each one by adding an enumerator based on https://github.com/audreyr/cookiecutter/blob/master/cookiecutter/prompt.py#L51 :param var_name: The question to ask the user :type var_name: ``str`` :param options: A list of options :type options: ``list`` of ``tupple`` :rtype: ``tuple`` :returns: The selected user",1,0,0,1,2,1,0,0,1,2
"def download_dataset(self, dataset_key):
  owner_id, dataset_id = parse_dataset_key(dataset_key)
  try:
  return self._download_api.download_dataset(owner_id, dataset_id)
  except _swagger.rest.ApiException as e:
  raise RestApiError(cause=e)","Return a .zip containing all files within the dataset as uploaded. :param dataset_key : Dataset identifier, in the form of owner/id :type dataset_key: str :returns: .zip file contain files within dataset :rtype: file object :raises RestApiException: If a server error occurs Examples -------- >>> import datadotworld as dw >>> api_client = dw.api_client() >>> api_client.download_dataset( ... 'username/test-dataset') # doctest: +SKIP",2,0,0,1,3,1,0,0,1,2
"def get_group(path, follow_symlinks=True):
  func_name = '{0}.get_group'.format(__virtualname__)
  if __opts__.get('fun', '') == func_name:
  log.info('The function %s should not be used on Windows systems; '
  'see function docs for details. The value returned is the '
  'user (owner).', func_name)
  return get_user(path, follow_symlinks)","Return the group that owns a given file Under Windows, this will return the user (owner) of the file. While a file in Windows does have a 'primary group', this rarely used attribute generally has no bearing on permissions unless intentionally configured and is only used to support Unix compatibility features (e.g. Services For Unix, NFS services). Salt, therefore, remaps this function to provide functionality that somewhat resembles Unix behavior for API compatibility reasons. When managing Windows systems, this function is superfluous and will generate an info level log entry if used directly. If you do actually want to access the 'primary group' of a file, use `file.get_pgroup`. Args: path (str): The path to the file or directory follow_symlinks (bool): If the object specified by ``path`` is a symlink, get attributes of the linked file instead of the symlink itself. Default is True Returns: str: The name of the owner CLI Example: .. code-block:: bash salt '*' file.get_group c:\\temp\\test.txt",1,0,0,1,2,1,0,0,1,2
"def _get_LMv2_response(user_name, password, domain_name, server_challenge,
  client_challenge):
  nt_hash = comphash._ntowfv2(user_name, password, domain_name)
  challenge = server_challenge + client_challenge
  lm_hash = hmac.new(nt_hash, challenge, digestmod=hashlib.md5).digest()
  response = lm_hash + client_challenge
  return response",[MS-NLMP] v28.0 2016-07-14 2.2.2.4 LMv2_RESPONSE The LMv2_RESPONSE structure defines the NTLM v2 authentication LmChallengeResponse in the AUTHENTICATE_MESSAGE. This response is used only when NTLM v2 authentication is configured. :param user_name: The user name of the user we are trying to authenticate with :param password: The password of the user we are trying to authenticate with :param domain_name: The domain name of the user account we are authenticated with :param server_challenge: A random 8-byte response generated by the server in the CHALLENGE_MESSAGE :param client_challenge: A random 8-byte response generated by the client for the AUTHENTICATE_MESSAGE :return response: LmChallengeResponse to the server challenge,0,0,0,0,0,0,0,0,1,1
"def get_subnets(context, limit=None, page_reverse=False, sorts=['id'],
  marker=None, filters=None, fields=None):
  LOG.info(""get_subnets for tenant %s with filters %s fields %s"" %
  (context.tenant_id, filters, fields))
  filters = filters or {}
  subnets = db_api.subnet_find(context, limit=limit,
  page_reverse=page_reverse, sorts=sorts,
  marker_obj=marker, join_dns=True,
  join_routes=True, join_pool=True, **filters)
  for subnet in subnets:
  cache = subnet.get(""_allocation_pool_cache"")
  if not cache:
  db_api.subnet_update_set_alloc_pool_cache(
  context, subnet, subnet.allocation_pools)
  return v._make_subnets_list(subnets, fields=fields)",Retrieve a list of subnets. The contents of the list depends on the identity of the user making the request (as indicated by the context) as well as any filters. : param context: neutron api request context : param filters: a dictionary with keys that are valid keys for a subnet as listed in the RESOURCE_ATTRIBUTE_MAP object in neutron/api/v2/attributes.py. Values in this dictiontary are an iterable containing values that will be used for an exact match comparison for that value. Each result returned by this function will have matched one of the values for each key in filters. : param fields: a list of strings that are valid keys in a subnet dictionary as listed in the RESOURCE_ATTRIBUTE_MAP object in neutron/api/v2/attributes.py. Only these fields will be returned.,1,1,1,1,4,1,0,1,1,3
"def null(self, field, **validations):
  values = []
  for found in self._find_by_field(field):
  reality = found[""reality""]
  schema = {""type"": ""null""}
  skip = self._input_boolean(validations.pop(""skip"", False))
  if not skip:
  self._assert_schema(schema, reality)
  values.append(reality)
  return values","*Asserts the field as JSON null.* The field consists of parts separated by spaces, the parts being object property names or array indices starting from 0, and the root being the instance created by the last request (see `Output` for it). For asserting deeply nested properties or multiple objects at once, [http://goessner.net/articles/JsonPath|JSONPath] can be used with [https://github.com/h2non/jsonpath-ng#jsonpath-syntax|supported JSONPath expressions], the root being the response body. *Validations* The JSON Schema validation keywords [https://json-schema.org/understanding-json-schema/reference/generic.html|common for all types] can be used. Validations are optional but update the schema of the property (more accurate) if given. `Output Schema` can be used for the current schema of the field. The keyword will fail if any of the given validations fail. Given validations can be skipped altogether by adding ``skip=true``. When skipped, the schema is updated but the validations are not ran. Skip is intented mainly for debugging the updated schema before aborting. *Examples* | `PUT` | /users/1 | { ""deactivated_at"": null } | # https://jsonplaceholder.typicode.com/users/1 | | `Null` | response body deactivated_at | | | `Null` | $.deactivated_at | | # JSONPath alternative |",1,0,0,1,2,1,0,0,1,2
"def CreateUser(self, database_link, user, options=None):
  if options is None:
  options = {}
  database_id, path = self._GetDatabaseIdWithPathForUser(database_link, user)
  return self.Create(user,
  path,
  'users',
  database_id,
  None,
  options)",Creates a user. :param str database_link: The link to the database. :param dict user: The Azure Cosmos user to create. :param dict options: The request options for the request. :return: The created User. :rtype: dict,2,1,0,1,4,2,1,0,2,5
"def set_theme(request):
  next = request.POST.get('next', request.GET.get('next'))
  if not is_safe_url(url=next, host=request.get_host()):
  next = request.META.get('HTTP_REFERER')
  if not is_safe_url(url=next, host=request.get_host()):
  next = '/'
  response = http.HttpResponseRedirect(next)
  if request.method == 'POST':
  theme = request.POST.get('theme', None)
  if theme:
  if hasattr(request, 'session'):
  request.session['DJANGO_BOOTSTRAP_UI_THEME'] = theme
  else:
  response.set_cookie('DJANGO_BOOTSTRAP_UI_THEME', theme)
  return response","Redirect to a given url while setting the chosen theme in the session or cookie. The url and the theme identifier need to be specified in the request parameters. Since this view changes how the user will see the rest of the site, it must only be accessed as a POST request. If called as a GET request, it will redirect to the page in the request (the 'next' parameter) without changing any state.",1,0,0,1,2,1,0,0,1,2
"def get_upgrade_lock(dbname, connect_str, timeout=LOCK_TIMEOUT):
  engine = sqlalchemy.create_engine(connect_str)
  cursor = engine.execute(""SELECT GET_LOCK('upgrade_{}', {})"".format(dbname, timeout))
  lock = cursor.scalar()
  cursor.close()
  while not lock:
  logger.info('Cannot acquire {} upgrade lock. Sleeping {} seconds.'.format(dbname, timeout))
  time.sleep(timeout)
  cursor = engine.execute(""SELECT GET_LOCK('upgrade_{}', {})"".format(dbname, timeout))
  lock = cursor.scalar()
  cursor.close()
  logger.info('Acquired {} upgrade lock'.format(dbname))
  yield lock
  cursor = engine.execute(""SELECT RELEASE_LOCK('upgrade_{}')"".format(dbname))
  cursor.close()
  engine.dispose()
  logger.info('Released {} upgrade lock'.format(dbname))","Wait until you can get the lock, then yield it, and eventually release it. Inspired by: http://arr.gr/blog/2016/05/mysql-named-locks-in-python-context-managers/ :param dbname: database to upgrade :param connect_str: connection string to the database :param timeout: how long to wait between tries for the lock, default 5 seconds",1,0,1,1,3,0,0,1,0,1
"def delete_user(user, driver, user_delete=None):
  if connexion.request.is_json:
  user_delete = UserDelete.from_dict(connexion.request.get_json())
  response = errorIfUnauthorized(role='admin')
  if response:
  return response
  else:
  response = ApitaxResponse()
  driver: Driver = LoadedDrivers.getDriver(driver)
  if driver.deleteApitaxUser(User(username=user)):
  return Response(status=200, body=response.getResponseBody())
  return ErrorResponse(status=500, message='Failed to create user')",Delete a user Delete a user # noqa: E501 :param user: Delete user with this name :type user: str :param driver: The driver to use for the request. ie. github :type driver: str :param user_delete: The data needed to delete this user :type user_delete: dict | bytes :rtype: Response,2,0,0,2,4,1,1,0,1,3
"def _set_dag_run_state(dag_id, execution_date, state, session=None):
  DR = DagRun
  dr = session.query(DR).filter(
  DR.dag_id == dag_id,
  DR.execution_date == execution_date
  ).one()
  dr.state = state
  if state == State.RUNNING:
  dr.start_date = timezone.utcnow()
  dr.end_date = None
  else:
  dr.end_date = timezone.utcnow()
  session.merge(dr)",Helper method that set dag run state in the DB. :param dag_id: dag_id of target dag run :param execution_date: the execution date from which to start looking :param state: target state :param session: database session,0,1,1,0,2,0,1,1,1,3
"def add_pending(self, panel_obj, hgnc_gene, action, info=None):
  valid_actions = ['add', 'delete', 'edit']
  if action not in valid_actions:
  raise ValueError(""Invalid action {0}"".format(action))
  info = info or {}
  pending_action = {
  'hgnc_id': hgnc_gene['hgnc_id'],
  'action': action,
  'info': info,
  'symbol': hgnc_gene['hgnc_symbol'],
  }
  updated_panel = self.panel_collection.find_one_and_update(
  {'_id': panel_obj['_id']},
  {
  '$addToSet': {
  'pending': pending_action
  }
  },
  return_document=pymongo.ReturnDocument.AFTER
  )
  return updated_panel","Add a pending action to a gene panel Store the pending actions in panel.pending Args: panel_obj(dict): The panel that is about to be updated hgnc_gene(dict) action(str): choices=['add','delete','edit'] info(dict): additional gene info (disease_associated_transcripts, reduced_penetrance, mosaicism, database_entry_version , inheritance_models, comment) Returns: updated_panel(dict):",1,1,0,1,3,1,1,1,1,4
"def confirm_forgot_password(self, confirmation_code, password):
  params = {'ClientId': self.client_id,
  'Username': self.username,
  'ConfirmationCode': confirmation_code,
  'Password': password
  }
  self._add_secret_hash(params, 'SecretHash')
  response = self.client.confirm_forgot_password(**params)
  self._set_attributes(response, {'password': password})",Allows a user to enter a code provided when they reset their password to update their password. :param confirmation_code: The confirmation code sent by a user's request to retrieve a forgotten password :param password: New password,1,0,0,1,2,2,0,0,2,4
"def broadcastPush(self, pushMessage):
  desc = {
  ""name"": ""CodeSuccessReslut"",
  ""desc"": "" http "",
  ""fields"": [{
  ""name"": ""code"",
  ""type"": ""Integer"",
  ""desc"": "" 200 ""
  }, {
  ""name"": ""errorMessage"",
  ""type"": ""String"",
  ""desc"": """"
  }]
  }
  r = self.call_api(
  method=('API', 'POST', 'application/json'),
  action='/push.json',
  params=pushMessage)
  return Response(r, desc)",fromuserid  messagenullpush  @param pushMessage:json @return code: 200  @return errorMessage:,0,0,0,1,1,1,0,0,1,2
"def list_user_requests_view(request, targetUsername):
  if targetUsername == request.user.username:
  return list_my_requests_view(request)
  targetUser = get_object_or_404(User, username=targetUsername)
  targetProfile = get_object_or_404(UserProfile, user=targetUser)
  page_name = ""{0}'s Requests"".format(targetUsername)
  requests = Request.objects.filter(owner=targetProfile).exclude(
  ~Q(owner__user=request.user), private=True,
  )
  return render_to_response('list_requests.html', {
  'page_name': page_name,
  'requests': requests,
  'targetUsername': targetUsername,
  }, context_instance=RequestContext(request))",Show user his/her requests in list form.,1,0,1,1,3,2,0,1,1,4
"def connect_from_web(klass, url):
  if re.search(r'\s', url):
  return
  url = url.split(' ', 1)[0]
  for path in profiles():
  cfg = SafeConfigParser()
  cfg.read(path)
  for profile in cfg.sections():
  if not cfg.has_option(profile, 'weburl'):
  continue
  weburl = cfg.get(profile, 'weburl')
  if url.startswith(weburl):
  return klass(profile)","Find a connection that matches this kojiweb URL. Check all koji.conf.d files' kojiweb URLs and load the profile that matches the url we pass in here. For example, if a user pastes a kojiweb URL into chat, we want to discover the corresponding Koji instance hub automatically. See also from_web(). :param url: ``str``, for example ""http://cbs.centos.org/koji/buildinfo?buildID=21155"" :returns: A ""Connection"" instance",1,0,0,1,2,1,0,0,1,2
"def ParseConversationRow(self, parser_mediator, query, row, **unused_kwargs):
  query_hash = hash(query)
  event_data = TangoAndroidConversationEventData()
  event_data.conversation_identifier = self._GetRowValue(
  query_hash, row, 'conv_id')
  date_time = dfdatetime_semantic_time.NotSet()
  event = time_events.DateTimeValuesEvent(
  date_time, definitions.TIME_DESCRIPTION_NOT_A_TIME)
  parser_mediator.ProduceEventWithEventData(event, event_data)","Parses a conversation row from the database. Args: parser_mediator (ParserMediator): mediates interactions between parsers and other components, such as storage and dfvfs. query (str): query that created the row. row (sqlite3.Row): row resulting from query.",0,1,0,0,1,1,0,0,1,2
"def list_projects(self, tags=None, pattern=None, username=None, owner=None,
  namespace=None, fork=None, short=None, page=None,
  per_page=None):
  request_url = ""{}/api/0/projects"".format(self.instance)
  payload = {}
  if tags is not None:
  payload['tags'] = tags
  if pattern is not None:
  payload['pattern'] = pattern
  if username is not None:
  payload['username'] = username
  if owner is not None:
  payload['owner'] = owner
  if namespace is not None:
  payload['namespace'] = namespace
  if fork is not None:
  payload['fork'] = fork
  if short is not None:
  payload['short'] = short
  if page is not None:
  payload['page'] = str(page)
  if per_page is not None:
  payload['per_page'] = str(per_page)
  return_value = self._call_api(request_url, params=payload)
  return return_value['projects']",Lisk all projects on this Pagure instance. :param tags: filters the tags of the project :param pattern: filters the projects by the pattern string :param username: filters the username of the project administrators :param owner: filters the projects by ownership :param namespace: filters the projects by namespace :param fork: filters whether it is a fork (True) or not (False) :param short: whether to return the entrie JSON or just a sub-set :param page: specifies that pagination should be turned on and that this specific page should be displayed :param per_page: the number of projects to return per page. The maximum is 100 :return:,2,0,0,1,3,2,0,0,1,3
"def prepare_token_request(self, token_url, authorization_response=None,
  redirect_url=None, state=None, body='', **kwargs):
  if not is_secure_transport(token_url):
  raise InsecureTransportError()
  state = state or self.state
  if authorization_response:
  self.parse_request_uri_response(
  authorization_response, state=state)
  self.redirect_url = redirect_url or self.redirect_url
  body = self.prepare_request_body(body=body,
  redirect_uri=self.redirect_url, **kwargs)
  return token_url, FORM_ENC_HEADERS, body","Prepare a token creation request. Note that these requests usually require client authentication, either by including client_id or a set of provider specific authentication credentials. :param token_url: Provider token creation endpoint URL. :param authorization_response: The full redirection URL string, i.e. the location to which the user was redirected after successfull authorization. Used to mine credentials needed to obtain a token in this step, such as authorization code. :param redirect_url: The redirect_url supplied with the authorization request (if there was one). :param state: :param body: Existing request body (URL encoded string) to embed parameters into. This may contain extra paramters. Default ''. :param kwargs: Additional parameters to included in the request. :returns: The prepared request tuple with (url, headers, body).",0,0,0,1,1,1,0,0,1,2
"def send(self, user_id, stock_id, op_user_id=None, device_info=None,
  out_trade_no=None):
  if not out_trade_no:
  now = datetime.now()
  out_trade_no = '{0}{1}{2}'.format(
  self.mch_id,
  now.strftime('%Y%m%d%H%M%S'),
  random.randint(1000, 10000)
  )
  data = {
  'appid': self.appid,
  'coupon_stock_id': stock_id,
  'openid': user_id,
  'openid_count': 1,
  'partner_trade_no': out_trade_no,
  'op_user_id': op_user_id,
  'device_info': device_info,
  'version': '1.0',
  'type': 'XML',
  }
  return self._post('mmpaymkttransfers/send_coupon', data=data)", :param user_id:  openid :param stock_id:  ID :param op_user_id:  :param device_info:  :param out_trade_no:  :return: ,1,0,0,2,3,1,0,0,2,3
"def parse_profile_from_hcard(hcard: str, handle: str):
  from federation.entities.diaspora.entities import DiasporaProfile
  doc = html.fromstring(hcard)
  profile = DiasporaProfile(
  name=_get_element_text_or_none(doc, "".fn""),
  image_urls={
  ""small"": _get_element_attr_or_none(doc, "".entity_photo_small .photo"", ""src""),
  ""medium"": _get_element_attr_or_none(doc, "".entity_photo_medium .photo"", ""src""),
  ""large"": _get_element_attr_or_none(doc, "".entity_photo .photo"", ""src""),
  },
  public=True if _get_element_text_or_none(doc, "".searchable"") == ""true"" else False,
  id=handle,
  handle=handle,
  guid=_get_element_text_or_none(doc, "".uid""),
  public_key=_get_element_text_or_none(doc, "".key""),
  )
  return profile",Parse all the fields we can from a hCard document to get a Profile. :arg hcard: HTML hcard document (str) :arg handle: User handle in username@domain.tld format :returns: ``federation.entities.diaspora.entities.DiasporaProfile`` instance,0,0,0,1,1,1,0,0,1,2
"def GET(self):
  data = self.user_manager.session_lti_info()
  if data is None:
  raise web.notfound()
  try:
  course = self.course_factory.get_course(data[""task""][0])
  if data[""consumer_key""] not in course.lti_keys().keys():
  raise Exception()
  except:
  return self.template_helper.get_renderer().lti_bind(False, """", None, ""Invalid LTI data"")
  user_profile = self.database.users.find_one({""ltibindings."" + data[""task""][0] + ""."" + data[""consumer_key""]: data[""username""]})
  if user_profile:
  self.user_manager.connect_user(user_profile[""username""], user_profile[""realname""], user_profile[""email""], user_profile[""language""])
  if self.user_manager.session_logged_in():
  raise web.seeother(self.app.get_homepath() + ""/lti/task"")
  return self.template_helper.get_renderer().lti_login(False)","Checks if user is authenticated and calls POST_AUTH or performs login and calls GET_AUTH. Otherwise, returns the login template.",1,0,1,1,3,1,0,1,1,3
"def get_user_data_iobject(user=None,group=None,data_kind=DINGOS_USER_DATA_TYPE_NAME):
  logger.debug(""Get user settings called"")
  if not user.is_authenticated():
  user = None
  try:
  user_config = UserData.objects.get(user=user,group=group,data_kind=data_kind)
  return user_config.identifier.latest
  except:
  return None",Returns either stored settings of a given user or default settings. This behavior reflects the need for views to have some settings at hand when running. The settings are returned as dict object.,1,0,1,1,3,1,0,1,1,3
"def clear(self, models=(), commit=True):
  if not models:
  if os.path.exists(self.path):
  shutil.rmtree(self.path)
  else:
  database = self._database(writable=True)
  for model in models:
  database.delete_document(TERM_PREFIXES[DJANGO_CT] + get_model_ct(model))
  database.close()","Clear all instances of `models` from the database or all models, if not specified. Optional Arguments: `models` -- Models to clear from the database (default = []) If `models` is empty, an empty query is executed which matches all documents in the database. Afterwards, each match is deleted. Otherwise, for each model, a `delete_document` call is issued with the term `XCONTENTTYPE<app_name>.<model_name>`. This will delete all documents with the specified model type.",1,0,0,0,1,0,1,1,0,2
"def setPermissions(self, dbName, access) :
  import json
  if not self.URL :
  raise CreationError(""Please save user first"", None, None)
  rights = []
  if access :
  rights.append(""rw"")
  rights = ''.join(rights)
  if not self.connection.hasDatabase(dbName) :
  raise KeyError(""Unknown database: %s"" % dbName)
  url = ""%s/database/%s"" % (self.URL, dbName)
  r = self.connection.session.put(url, data = json.dumps({""grant"": rights}, default=str))
  if r.status_code < 200 or r.status_code > 202 :
  raise CreationError(""Unable to grant rights"", r.content)","Grant revoke rights on a database, 'access' is supposed to be boolean. ArangoDB grants/revokes both read and write rights at the same time",1,1,1,1,4,1,1,0,1,3
"def disease_comment(self, comment=None, entry_name=None, limit=None, as_df=False):
  q = self.session.query(models.DiseaseComment)
  q = self.get_model_queries(q, ((comment, models.DiseaseComment.comment),))
  q = self.get_one_to_many_queries(q, ((entry_name, models.Entry.name),))
  return self._limit_and_df(q, limit, as_df)","Method to query :class:`.models.DiseaseComment` objects in database :param comment: Comment(s) to disease :type comment: str or tuple(str) or None :param entry_name: name(s) in :class:`.models.Entry` :type entry_name: str or tuple(str) or None :param limit: - if `isinstance(limit,int)==True` -> limit - if `isinstance(limit,tuple)==True` -> format:= tuple(page_number, results_per_page) - if limit == None -> all results :type limit: int or tuple(int) or None :param bool as_df: if `True` results are returned as :class:`pandas.DataFrame` :return: - if `as_df == False` -> list(:class:`.models.DiseaseComment`) - if `as_df == True` -> :class:`pandas.DataFrame` :rtype: list(:class:`.models.DiseaseComment`) or :class:`pandas.DataFrame`",1,0,1,1,3,1,0,1,1,3
"def add_to_groups(self, groups=None, all_groups=False, group_type=None):
  if all_groups:
  if groups or group_type:
  raise ArgumentError(""When adding to all groups, do not specify specific groups or types"")
  glist = ""all""
  else:
  if not groups:
  groups = []
  if not group_type:
  group_type = GroupTypes.product
  elif group_type in GroupTypes.__members__:
  group_type = GroupTypes[group_type]
  if group_type not in GroupTypes:
  raise ArgumentError(""You must specify a GroupType value for argument group_type"")
  glist = {group_type.name: [group for group in groups]}
  return self.append(add=glist)","Add user to some (typically PLC) groups. Note that, if you add to no groups, the effect is simply to do an ""add to organization Everybody group"", so we let that be done. :param groups: list of group names the user should be added to :param all_groups: a boolean meaning add to all (don't specify groups or group_type in this case) :param group_type: the type of group (defaults to ""product"") :return: the User, so you can do User(...).add_to_groups(...).add_role(...)",1,1,0,0,2,1,0,0,1,2
"def setup_users_page(self, ):
  self.users_tablev.horizontalHeader().setResizeMode(QtGui.QHeaderView.ResizeToContents)
  log.debug(""Loading users for users page."")
  rootdata = treemodel.ListItemData(['Username', 'First', 'Last', 'Email'])
  rootitem = treemodel.TreeItem(rootdata)
  users = djadapter.users.all()
  for usr in users:
  usrdata = djitemdata.UserItemData(usr)
  treemodel.TreeItem(usrdata, rootitem)
  self.users_model = treemodel.TreeModel(rootitem)
  self.users_tablev.setModel(self.users_model)",Create and set the model on the users page :returns: None :rtype: None :raises: None,1,0,1,1,3,1,0,0,1,2
"def Open(self, filename):
  if not super(WinevtResourcesSqlite3DatabaseReader, self).Open(filename):
  return False
  version = self.GetMetadataAttribute('version')
  if not version or version != '20150315':
  raise RuntimeError('Unsupported version: {0:s}'.format(version))
  string_format = self.GetMetadataAttribute('string_format')
  if not string_format:
  string_format = 'wrc'
  if string_format not in ('pep3101', 'wrc'):
  raise RuntimeError('Unsupported string format: {0:s}'.format(
  string_format))
  self._string_format = string_format
  return True",Opens the database reader object. Args: filename (str): filename of the database. Returns: bool: True if successful. Raises: RuntimeError: if the version or string format of the database is not supported.,0,0,0,1,1,1,0,0,1,2
"def get_authenticated_user(self, callback):
  request_key = self.get_argument(""oauth_token"")
  oauth_verifier = self.get_argument(""oauth_verifier"", None)
  request_cookie = self.get_cookie(""_oauth_request_token"")
  if not request_cookie:
  log.warning(""Missing OAuth request token cookie"")
  callback(None)
  return
  self.clear_cookie(""_oauth_request_token"")
  cookie_key, cookie_secret = [base64.b64decode(i) for i in request_cookie.split(""|"")]
  if cookie_key != request_key:
  log.warning(""Request token does not match cookie"")
  callback(None)
  return
  token = dict(key=cookie_key, secret=cookie_secret)
  if oauth_verifier:
  token[""verifier""] = oauth_verifier
  http = httpclient.AsyncHTTPClient()
  http.fetch(self._oauth_access_token_url(token), self.async_callback(
  self._on_access_token, callback))","Gets the OAuth authorized user and access token on callback. This method should be called from the handler for your registered OAuth Callback URL to complete the registration process. We call callback with the authenticated user, which in addition to standard attributes like 'name' includes the 'access_key' attribute, which contains the OAuth access you can use to make authorized requests to this service on behalf of the user.",0,0,0,1,1,2,0,0,1,3
"def write_passes(self, outfile, rows):
  if self.rescale:
  rows = rescale_rows(rows, self.rescale)
  if self.bitdepth < 8:
  rows = pack_rows(rows, self.bitdepth)
  elif self.bitdepth == 16:
  rows = unpack_rows(rows)
  return self.write_packed(outfile, rows)","Write a PNG image to the output file. Most users are expected to find the :meth:`write` or :meth:`write_array` method more convenient. The rows should be given to this method in the order that they appear in the output file. For straightlaced images, this is the usual top to bottom ordering. For interlaced images the rows should have been interlaced before passing them to this function. `rows` should be an iterable that yields each row (each row being a sequence of values).",1,0,0,0,1,0,0,0,1,1
"def _save_table(
  self,
  raw=False,
  cls=None,
  force_insert=False,
  force_update=False,
  using=None,
  update_fields=None,
  ):
  updated = super(ModelMixin, self)._save_table(
  raw=raw,
  cls=cls,
  force_insert=force_insert,
  force_update=force_update,
  using=using,
  update_fields=update_fields,
  )
  self._linguist.decider.objects.save_translations([self])
  return updated",Overwrites model's ``_save_table`` method to save translations after instance has been saved (required to retrieve the object ID for ``Translation`` model). Preferred over overriding the object's ``save`` method to ensure that `pre_save` and ``post_save`` signals happen respectively before and after the translations have been saved to the database. Thus ``pre_save`` signals have access to the ``has_changed`` attribute on translated fields before the translations are saved and the attribute is reset. And `post_save`` signals always have access to the updated translations.,0,1,0,0,1,0,1,1,0,2
"def terminate(pid, sig, timeout):
  os.kill(pid, sig)
  start = time.time()
  while True:
  try:
  _, status = os.waitpid(pid, os.WNOHANG)
  except OSError as exc:
  if exc.errno != errno.ECHILD:
  raise
  else:
  if status:
  return True
  if not is_running(pid):
  return True
  if time.time()-start>=timeout:
  return False
  time.sleep(0.1)",Terminates process with PID `pid` and returns True if process finished during `timeout`. Current user must have permission to access process information.,1,0,0,1,2,1,0,0,1,2
"def userstream_user(self, delegate, stall_warnings=None,
  with_='followings', replies=None):
  params = {'stringify_friend_ids': 'true'}
  set_bool_param(params, 'stall_warnings', stall_warnings)
  set_str_param(params, 'with', with_)
  set_str_param(params, 'replies', replies)
  svc = TwitterStreamService(
  lambda: self._get_userstream('user.json', params),
  delegate)
  return svc","Streams messages for a single user. https://dev.twitter.com/docs/api/1.1/get/user The ``stringify_friend_ids`` parameter is always set to ``'true'`` for consistency with the use of string identifiers elsewhere. :param delegate: A delegate function that will be called for each message in the stream and will be passed the message dict as the only parameter. The message dicts passed to this function may represent any message type and the delegate is responsible for any dispatch that may be required. (:mod:`txtwitter.messagetools` may be helpful here.) :param bool stall_warnings: Specifies whether stall warnings should be delivered. :param str with_: If ``'followings'`` (the default), the stream will include messages from both the authenticated user and the authenticated user's followers. If ``'user'``, the stream will only include messages from (or mentioning) the autheticated user. All other values are invalid. (The underscore appended to the parameter name is to avoid conflicting with Python's ``with`` keyword.) :param str replies: If set to ``'all'``, replies to tweets will be included even if the authenticated user does not follow both parties. :returns: An unstarted :class:`TwitterStreamService`.",2,0,0,1,3,2,0,0,1,3
"async def oauth2_request(
  self,
  url: str,
  access_token: str = None,
  post_args: Dict[str, Any] = None,
  **args: Any
  ) -> Any:
  all_args = {}
  if access_token:
  all_args[""access_token""] = access_token
  all_args.update(args)
  if all_args:
  url += ""?"" + urllib.parse.urlencode(all_args)
  http = self.get_auth_http_client()
  if post_args is not None:
  response = await http.fetch(
  url, method=""POST"", body=urllib.parse.urlencode(post_args)
  )
  else:
  response = await http.fetch(url)
  return escape.json_decode(response.body)","Fetches the given URL auth an OAuth2 access token. If the request is a POST, ``post_args`` should be provided. Query string arguments should be given as keyword arguments. Example usage: ..testcode:: class MainHandler(tornado.web.RequestHandler, tornado.auth.FacebookGraphMixin): @tornado.web.authenticated async def get(self): new_entry = await self.oauth2_request( ""https://graph.facebook.com/me/feed"", post_args={""message"": ""I am posting from my Tornado application!""}, access_token=self.current_user[""access_token""]) if not new_entry: # Call failed; perhaps missing permission? await self.authorize_redirect() return self.finish(""Posted a message!"") .. testoutput:: :hide: .. versionadded:: 4.3 .. versionchanged::: 6.0 The ``callback`` argument was removed. Use the returned awaitable object instead.",2,0,0,2,4,2,0,0,1,3
"def transactions(self, account_id=None, reverse=True, limit=None):
  if not account_id:
  if len(self.accounts()) == 1:
  account_id = self.accounts()[0].id
  else:
  raise ValueError(""You need to pass account ID"")
  endpoint = '/transactions'
  response = self._get_response(
  method='get', endpoint=endpoint,
  params={
  'account_id': account_id,
  },
  )
  transactions = response.json()['transactions']
  if reverse:
  transactions.reverse()
  if limit:
  transactions = transactions[:limit]
  return [MonzoTransaction(data=t) for t in transactions]",Returns a list of transactions on the user's account. Official docs: https://monzo.com/docs/#list-transactions :param account_id: Monzo account ID :type account_id: str :param reverse: whether transactions should be in in descending order :type reverse: bool :param limit: how many transactions should be returned; None for all :type limit: int :returns: list of Monzo transactions :rtype: list of MonzoTransaction,2,0,0,1,3,2,0,0,1,3
"def create_dataset(self, owner_id, **kwargs):
  request = self.__build_dataset_obj(
  lambda: _swagger.DatasetCreateRequest(
  title=kwargs.get('title'),
  visibility=kwargs.get('visibility')),
  lambda name, url, expand_archive, description, labels:
  _swagger.FileCreateRequest(
  name=name,
  source=_swagger.FileSourceCreateRequest(
  url=url,
  expand_archive=expand_archive),
  description=description,
  labels=labels),
  kwargs)
  try:
  (_, _, headers) = self._datasets_api.create_dataset_with_http_info(
  owner_id, request, _return_http_data_only=False)
  if 'Location' in headers:
  return headers['Location']
  except _swagger.rest.ApiException as e:
  raise RestApiError(cause=e)","Create a new dataset :param owner_id: Username of the owner of the new dataset :type owner_id: str :param title: Dataset title (will be used to generate dataset id on creation) :type title: str :param description: Dataset description :type description: str, optional :param summary: Dataset summary markdown :type summary: str, optional :param tags: Dataset tags :type tags: list, optional :param license: Dataset license :type license: {'Public Domain', 'PDDL', 'CC-0', 'CC-BY', 'ODC-BY', 'CC-BY-SA', 'ODC-ODbL', 'CC BY-NC', 'CC BY-NC-SA', 'Other'} :param visibility: Dataset visibility :type visibility: {'OPEN', 'PRIVATE'} :param files: File name as dict, source URLs, description and labels() as properties :type files: dict, optional *Description and labels are optional* :returns: Newly created dataset key :rtype: str :raises RestApiException: If a server error occurs Examples -------- >>> import datadotworld as dw >>> api_client = dw.api_client() >>> url = 'http://www.acme.inc/example.csv' >>> api_client.create_dataset( ... 'username', title='Test dataset', visibility='PRIVATE', ... license='Public Domain', ... files={'dataset.csv':{'url': url}}) # doctest: +SKIP",1,0,0,2,3,1,0,0,1,2
"async def get(self, public_key):
 if settings.SIGNATURE_VERIFICATION:
 super().verify()
 logging.debug(""\n\n -- Input offers debugging"")
 message = json.loads(self.get_argument(""message""))
 cid = message.get(""cid"")
 coinid = message.get(""coinid"")
 if not cid:
 self.set_status(400)
 self.write({""error"":400, ""reason"":""Missed required fields.""})
 raise tornado.web.Finish
 account = await self.account.getaccountdata(public_key=public_key)
 if ""error"" in account.keys():
 self.set_status(account[""error""])
 self.write(account)
 raise tornado.web.Finish
 if coinid in settings.bridges.keys():
 self.account.blockchain.setendpoint(settings.bridges[coinid])
 offers = await self.account.blockchain.getcidoffers(cid=cid)
 logging.debug(""\n\n -- Offers"")
 logging.debug(offers)
 if isinstance(offers, dict):
 self.set_status(offers[""error""])
 self.write(offers)
 raise tornado.web.Finish
 for offer in offers:
 offer[""type""] = self.account.ident_offer[offer[""type""]]
 storage_offers = await self.account.getoffers(cid=cid, coinid=coinid)
 logging.debug(""\n\n -- Storage offers. "")
 logging.debug(storage_offers)
 self.write(json.dumps(offers + storage_offers))",Retrieves all users input and output offers Accepts: - public key,1,0,0,1,2,1,0,0,1,2
"def _format_option_value(optdict, value):
  if isinstance(value, (list, tuple)):
  value = "","".join(_format_option_value(optdict, item) for item in value)
  elif isinstance(value, dict):
  value = "","".join(""%s:%s"" % (k, v) for k, v in value.items())
  elif hasattr(value, ""match""):
  value = value.pattern
  elif optdict.get(""type"") == ""yn"":
  value = ""yes"" if value else ""no""
  elif isinstance(value, str) and value.isspace():
  value = ""'%s'"" % value
  return value",return the user input's value from a 'compiled' value,0,0,0,1,1,1,0,0,1,2
"def deleteResourceFile(self, pid, filename):
  url = ""{url_base}/resource/{pid}/files/{filename}"".format(url_base=self.url_base,
  pid=pid,
  filename=filename)
  r = self._request('DELETE', url)
  if r.status_code != 200:
  if r.status_code == 403:
  raise HydroShareNotAuthorized(('DELETE', url))
  elif r.status_code == 404:
  raise HydroShareNotFound((pid, filename))
  else:
  raise HydroShareHTTPException((url, 'DELETE', r.status_code))
  response = r.json()
  assert(response['resource_id'] == pid)
  return response['resource_id']","Delete a resource file :param pid: The HydroShare ID of the resource :param filename: String representing the name of the resource file to delete :return: Dictionary containing 'resource_id' the ID of the resource from which the file was deleted, and 'file_name' the filename of the file deleted. :raises: HydroShareNotAuthorized if user is not authorized to perform action. :raises: HydroShareNotFound if the resource or resource file was not found. :raises: HydroShareHTTPException if an unexpected HTTP response code is encountered.",2,0,0,2,4,2,0,0,1,3
"def ajax_editable_boolean_cell(item, attr, text='', override=None):
  if text:
  text = '&nbsp;(%s)' % unicode(text)
  if override is not None:
  a = [django_boolean_icon(override, text), text]
  else:
  value = getattr(item, attr)
  a = [
  '<input type=""checkbox""',
  value and ' checked=""checked""' or '',
  ' onclick=""return inplace_toggle_boolean(%d, \'%s\')"";' % (item.id, attr),
  ' />',
  text,
  ]
  a.insert(0, '<div id=""wrap_%s_%d"">' % ( attr, item.id ))
  a.append('</div>')
  return unicode(''.join(a))","Generate a html snippet for showing a boolean value on the admin page. Item is an object, attr is the attribute name we should display. Text is an optional explanatory text to be included in the output. This function will emit code to produce a checkbox input with its state corresponding to the item.attr attribute if no override value is passed. This input is wired to run a JS ajax updater to toggle the value. If override is passed in, ignores the attr attribute and returns a static image for the override boolean with no user interaction possible (useful for ""disabled and you can't change it"" situations).",1,0,1,1,3,1,0,0,1,2
"def db_read(self, db_number, start, size):
  logger.debug(""db_read, db_number:%s, start:%s, size:%s"" %
  (db_number, start, size))
  type_ = snap7.snap7types.wordlen_to_ctypes[snap7.snap7types.S7WLByte]
  data = (type_ * size)()
  result = (self.library.Cli_DBRead(
  self.pointer, db_number, start, size,
  byref(data)))
  check_error(result, context=""client"")
  return bytearray(data)",This is a lean function of Cli_ReadArea() to read PLC DB. :returns: user buffer.,1,0,0,1,2,1,0,0,1,2
"def _generate_event_doc(event):
  eventc = event.copy()
  eventc[""_id""] = '{}-{}'.format(
  event.get('tag', '').split('/')[2],
  event.get('tag', '').split('/')[3]
  )
  eventc[""timestamp""] = time.time()
  if eventc.get('data').get('return'):
  del eventc['data']['return']
  return eventc",Create a object that will be saved into the database based in options.,0,0,0,1,1,0,0,0,1,1
"def _hmac_sign_string(self, string_to_sign):
  new_hmac = hmac.new(
  self.connection._aws_secret_access_key.encode('utf-8'),
  digestmod=hashlib.sha256
  )
  new_hmac.update(string_to_sign.encode('utf-8'))
  digest = new_hmac.digest()
  return base64.b64encode(digest).decode('utf-8')","Route53 uses AWS an HMAC-based authentication scheme, involving the signing of a date string with the user's secret access key. More details on the specifics can be found in their documentation_. .. documentation:: http://docs.amazonwebservices.com/Route53/latest/DeveloperGuide/RESTAuthentication.html This method is used to sign said time string, for use in the request headers. :param str string_to_sign: The time string to sign. :rtype: str :returns: An HMAC signed string.",0,0,0,1,1,0,0,0,1,1
"def statuses_update(self, status, in_reply_to_status_id=None, lat=None,
  long=None, place_id=None, display_coordinates=None,
  trim_user=None, media_ids=None):
  params = {}
  set_str_param(params, 'status', status)
  set_str_param(params, 'in_reply_to_status_id', in_reply_to_status_id)
  set_float_param(params, 'lat', lat, min=-90, max=90)
  set_float_param(params, 'long', long, min=-180, max=180)
  set_str_param(params, 'place_id', place_id)
  set_bool_param(params, 'display_coordinates', display_coordinates)
  set_bool_param(params, 'trim_user', trim_user)
  set_list_param(params, 'media_ids', media_ids, max_len=4)
  return self._post_api('statuses/update.json', params)","Posts a tweet. https://dev.twitter.com/docs/api/1.1/post/statuses/update :param str status: (*required*) The text of your tweet, typically up to 140 characters. URL encode as necessary. t.co link wrapping may affect character counts. There are some special commands in this field to be aware of. For instance, preceding a message with ""D "" or ""M "" and following it with a screen name can create a direct message to that user if the relationship allows for it. :param str in_reply_to_status_id: The ID of an existing status that the update is in reply to. Note: This parameter will be ignored unless the author of the tweet this parameter references is mentioned within the status text. Therefore, you must include @username, where username is the author of the referenced tweet, within ``status``. :param float lat: The latitude of the location this tweet refers to. This parameter will be ignored unless it is inside the range -90.0 to +90.0 (North is positive) inclusive. It will also be ignored if there isn't a corresponding long parameter. :param float long: The longitude of the location this tweet refers to. The valid ranges for longitude is -180.0 to +180.0 (East is positive) inclusive. This parameter will be ignored if outside that range, if it is not a number, if geo_enabled is disabled, or if there not a corresponding lat parameter. :param str place_id: A place in the world. These IDs can be retrieved from GET geo/reverse_geocode. (TODO: Reference method when it exists.) :param bool display_coordinates: Whether or not to put a pin on the exact coordinates a tweet has been sent from. :param bool trim_user: When set to ``True``, the return value's user object includes only the status author's numerical ID. :param list media_ids: A list of images previously uploaded to Twitter (referenced by their ``media_id``) that are to be embedded in the tweet. Maximum of four images. :returns: A tweet dict containing the posted tweet.",2,0,0,1,3,2,0,0,1,3
"def getDefaultConfigObj(taskname,configObj,input_dict={},loadOnly=True):
  if configObj is None:
  configObj = teal.load(taskname)
  elif isinstance(configObj,str):
  if configObj.lower().strip() == 'defaults':
  configObj = teal.load(taskname,defaults=True)
  configObj.filename = taskname.lower()+'.cfg'
  else:
  configObj = teal.load(fileutil.osfn(configObj))
  if input_dict not in [None,{}]:
  validateUserPars(configObj,input_dict)
  cfgpars.mergeConfigObj(configObj, input_dict)
  if not loadOnly:
  configObj = teal.teal(configObj,loadOnly=False)
  return configObj",Return default configObj instance for task updated with user-specified values from input_dict. Parameters ---------- taskname : string Name of task to load into TEAL configObj : string The valid values for 'configObj' would be:: None - loads last saved user .cfg file 'defaults' - loads task default .cfg file name of .cfg file (string)- loads user-specified .cfg file input_dict : dict Set of parameters and values specified by user to be different from what gets loaded in from the .cfg file for the task loadOnly : bool Setting 'loadOnly' to False causes the TEAL GUI to start allowing the user to edit the values further and then run the task if desired.,1,0,0,1,2,1,0,0,1,2
"def list_subscriptions(self, user_token):
  response = _request('GET',
  url=self.url_v1('/user/subscriptions'),
  user_agent=self.user_agent,
  user_token=user_token,
  )
  _raise_for_status(response)
  return response.json()['topics']",Get the list of the topics which a user is subscribed to. :param str user_token: The token of the user. :return: The list of the topics. :rtype: list :raises `requests.exceptions.HTTPError`: If an HTTP error occurred.,2,0,0,1,3,2,0,0,1,3
"def errorhandler_callback(cls, exc):
  if exc.flash_message:
  flash(exc.flash_message, exc.flash_level)
  if exc.redirect is not MISSING:
  return redirect(url_for(exc.redirect, **exc.redirect_args))
  error_result = exc.error_page()
  if error_result is not None:
  return error_result, exc.status_code or 500","This function should be called in the global error handlers. This will allow for consolidating of cleanup tasks if the exception bubbles all the way to the top of the stack. For example, this method will automatically rollback the database session if the exception bubbles to the top. This is the method that :meth:`register_errorhandler` adds as an errorhandler. See the documentation there for more info. Args: exc (FleakerBaseException): The exception that was thrown that we are to handle.",0,0,0,1,1,1,0,0,1,2
"def email_confirm(request, confirmation_key,
  template_name='userena/email_confirm_fail.html',
  success_url=None, extra_context=None):
  user = UserenaSignup.objects.confirm_email(confirmation_key)
  if user:
  if userena_settings.USERENA_USE_MESSAGES:
  messages.success(request, _('Your email address has been changed.'),
  fail_silently=True)
  if success_url: redirect_to = success_url
  else: redirect_to = reverse('userena_email_confirm_complete',
  kwargs={'username': user.username})
  return redirect(redirect_to)
  else:
  if not extra_context: extra_context = dict()
  return ExtraContextTemplateView.as_view(template_name=template_name,
  extra_context=extra_context)(request)","Confirms an email address with a confirmation key. Confirms a new email address by running :func:`User.objects.confirm_email` method. If the method returns an :class:`User` the user will have his new e-mail address set and redirected to ``success_url``. If no ``User`` is returned the user will be represented with a fail message from ``template_name``. :param confirmation_key: String with a SHA1 representing the confirmation key used to verify a new email address. :param template_name: String containing the template name which should be rendered when confirmation fails. When confirmation is successful, no template is needed because the user will be redirected to ``success_url``. :param success_url: String containing the URL which is redirected to after a successful confirmation. Supplied argument must be able to be rendered by ``reverse`` function. :param extra_context: Dictionary of variables that are passed on to the template supplied by ``template_name``.",1,1,1,0,3,1,0,1,1,3
"def get_user_affinity(self, test):
  test.createOrReplaceTempView(self.f(""{prefix}df_test""))
  query = self.f(
  ""SELECT DISTINCT {col_user} FROM {prefix}df_test CLUSTER BY {col_user}""
  )
  df_test_users = self.spark.sql(query)
  df_test_users.write.mode(""overwrite"").saveAsTable(
  self.f(""{prefix}df_test_users"")
  )
  query = self.f(
  )
  return self.spark.sql(query)",Prepare test set for C++ SAR prediction code. Find all items the test users have seen in the past. Arguments: test (pySpark.DataFrame): input dataframe which contains test users.,0,1,1,0,2,1,0,1,1,3
"def login(self, username, password):
  finvars = self.clientvars
  if username and password:
  finvars['username'] = username
  finvars['password'] = password
  self.loggedin = True
  ret = self.send_command('login', ujson.dumps(finvars))
  if not isinstance(ret, str):
  if self.loggedin:
  self.loggedin = False
  raise UserLoginFailed(ret['msg'])
  else:
  raise GeneralLoginError(ret['msg'])","This handles login logic instead of stuffing all that in the __init__. :param username: The username to log in as or None :param password: The password for that user or None :return: Nothing :raises: :class:`Pymoe.errors.UserLoginFailed` - Didn't respond with Ok :raises: :class:`Pymoe.errors.GeneralLoginError` - For some reason, we were already logged in, tried to login again and it failed. This probably isn't bad.",1,0,0,1,2,2,0,0,1,3
"def _run_server():
  port = _get_server_port()
  SocketServer.TCPServer.allow_reuse_address = True
  server = SocketServer.TCPServer(
  ('', port),
  SimpleHTTPServer.SimpleHTTPRequestHandler
  )
  print('Your images are at http://127.0.0.1:%d/%s' % (
  port,
  INDEX_FILE_NAME
  ))
  try:
  server.serve_forever()
  except KeyboardInterrupt:
  print('User interrupted, stopping')
  except Exception as exptn:
  print(exptn)
  print('Unhandled exception in server, stopping')",Run the image server. This is blocking. Will handle user KeyboardInterrupt and other exceptions appropriately and return control once the server is stopped. @return {None},1,0,0,1,2,1,0,0,1,2
"def can_create_log_entry_with_record_types(self, log_entry_record_types):
  if self._catalog_session is not None:
  return self._catalog_session.can_create_catalog_with_record_types(catalog_record_types=log_entry_record_types)
  return True","Tests if this user can create a single ``LogEntry`` using the desired record types. While ``LoggingManager.getLogEntryRecordTypes()`` can be used to examine which records are supported, this method tests which record(s) are required for creating a specific ``LogEntry``. Providing an empty array tests if a ``LogEntry`` can be created with no records. arg: log_entry_record_types (osid.type.Type[]): array of log entry record types return: (boolean) - ``true`` if ``LogEntry`` creation using the specified record ``Types`` is supported, ``false`` otherwise raise: NullArgument - ``log_entry_record_types`` is ``null`` *compliance: mandatory -- This method must be implemented.*",1,0,0,1,2,1,0,0,1,2
"def send_output(self, value, stdout):
  writer = self.writer
  if value is not None:
  writer.write('{!r}\n'.format(value).encode('utf8'))
  if stdout:
  writer.write(stdout.encode('utf8'))
  yield from writer.drain()",Write the output or value of the expression back to user. >>> 5 5 >>> print('cash rules everything around me') cash rules everything around me,1,0,0,1,2,1,0,0,1,2
"def delete_threads(cls, session, thread_ids_or_threads, authcode=None):
  thread_ids = [thread.id if isinstance(thread, cls) else thread
  for thread in thread_ids_or_threads]
  if not authcode:
  authcode = helpers.get_authcode(html.fromstring(
  session.okc_get('messages').content
  ))
  data = {'access_token': authcode,
  'threadids': simplejson.dumps(thread_ids)}
  return session.okc_delete('apitun/messages/threads',
  params=data, data=data)",:param session: A logged in :class:`~okcupyd.session.Session`. :param thread_ids_or_threads: A list whose members are either :class:`~.MessageThread` instances or okc_ids of message threads. :param authcode: Authcode to use for this request. If none is provided A request to the logged in user's messages page will be made to retrieve one.,2,0,0,1,3,2,0,0,2,4
"def hash_folder(folder, regex='[!_]*'):
  file_hashes = {}
  for path in glob.glob(os.path.join(folder, regex)):
  if not os.path.isfile(path):
  continue
  with open(path, 'r') as fileP:
  md5_hash = hashlib.md5(fileP.read()).digest()
  file_name = os.path.basename(path)
  file_hashes[file_name] = urlsafe_b64encode(md5_hash)
  return file_hashes",Get the md5 sum of each file in the folder and return to the user :param folder: the folder to compute the sums over :param regex: an expression to limit the files we match :return: Note: by default we will hash every file in the folder Note: we will not match anything that starts with an underscore,0,0,0,1,1,1,0,0,1,2
"def lookup(self, key):
  try:
  result = self.lookup_in_database(key)
  except KeyError:
  pass
  else:
  return result
  while True:
  fut = self._lookup_cache[key]
  try:
  result = yield from fut
  except ValueError:
  continue
  else:
  return result","Look up the given `node` URL using the given `hash_` first in the database and then by waiting on the futures created with :meth:`create_query_future` for that node URL and hash. If the hash is not in the database, :meth:`lookup` iterates as long as there are pending futures for the given `hash_` and `node`. If there are no pending futures, :class:`KeyError` is raised. If a future raises a :class:`ValueError`, it is ignored. If the future returns a value, it is used as the result.",0,0,1,1,2,0,0,1,1,2
"def sync():
  if __grains__['os'] == 'Funtoo':
  cmd = 'eix-sync -q'
  else:
  cmd = 'eix-sync -q -C ""--ask"" -C ""n""'
  if 'makeconf.features_contains' in __salt__ and __salt__['makeconf.features_contains']('webrsync-gpg'):
  if salt.utils.path.which('emerge-delta-webrsync'):
  cmd += ' -W'
  else:
  cmd += ' -w'
  return __salt__['cmd.retcode'](cmd) == 0
  else:
  if __salt__['cmd.retcode'](cmd) == 0:
  return True
  if salt.utils.path.which('emerge-delta-webrsync'):
  cmd += ' -W'
  else:
  cmd += ' -w'
  return __salt__['cmd.retcode'](cmd) == 0",Sync portage/overlay trees and update the eix database CLI Example: .. code-block:: bash salt '*' eix.sync,0,1,1,1,3,1,0,0,1,2
"def format(element):
  result = []
  if type(element).__name__ in [""Subroutine"", ""Function""]:
  _format_executable(result, element)
  elif type(element).__name__ == ""CustomType"":
  _format_type(result, element)
  elif isinstance(element, ValueElement):
  _format_value_element(result, element)
  return '\n'.join(result)",Formats all of the docstrings in the specified element and its children into a user-friendly paragraph format for printing. :arg element: an instance of fortpy.element.CodeElement.,0,0,0,1,1,0,0,0,1,1
"def upsert_entity_relationships(self, queryset, entity_relationships):
  if entity_relationships:
  list(queryset.select_for_update().values_list(
  'id',
  flat=True
  ))
  return manager_utils.sync(
  queryset=queryset,
  model_objs=entity_relationships,
  unique_fields=['sub_entity_id', 'super_entity_id'],
  update_fields=[],
  return_upserts=True
  )",Upsert entity relationships to the database :param queryset: The base queryset to use :param entity_relationships: The entity relationships to ensure exist in the database,0,1,1,0,2,0,1,1,1,3
"def read_sql(
  sql,
  con,
  index_col=None,
  coerce_float=True,
  params=None,
  parse_dates=None,
  columns=None,
  chunksize=None,
 ):
  _, _, _, kwargs = inspect.getargvalues(inspect.currentframe())
  return DataFrame(query_compiler=BaseFactory.read_sql(**kwargs))","Read SQL query or database table into a DataFrame. Args: sql: string or SQLAlchemy Selectable (select or text object) SQL query to be executed or a table name. con: SQLAlchemy connectable (engine/connection) or database string URI or DBAPI2 connection (fallback mode) index_col: Column(s) to set as index(MultiIndex). coerce_float: Attempts to convert values of non-string, non-numeric objects (like decimal.Decimal) to floating point, useful for SQL result sets. params: List of parameters to pass to execute method. The syntax used to pass parameters is database driver dependent. Check your database driver documentation for which of the five syntax styles, described in PEP 249's paramstyle, is supported. parse_dates: - List of column names to parse as dates. - Dict of ``{column_name: format string}`` where format string is strftime compatible in case of parsing string times, or is one of (D, s, ns, ms, us) in case of parsing integer timestamps. - Dict of ``{column_name: arg dict}``, where the arg dict corresponds to the keyword arguments of :func:`pandas.to_datetime` Especially useful with databases without native Datetime support, such as SQLite. columns: List of column names to select from SQL table (only used when reading a table). chunksize: If specified, return an iterator where `chunksize` is the number of rows to include in each chunk. Returns: Modin Dataframe",0,0,1,1,2,0,0,1,1,2
"def modify_telnet_template(auth, url, telnet_template, template_name= None, template_id = None):
  if template_name is None:
  template_name = telnet_template['name']
  if template_id is None:
  telnet_templates = get_telnet_template(auth, url)
  template_id = None
  for template in telnet_templates:
  if template['name'] == template_name:
  template_id = template['id']
  f_url = url + ""/imcrs/plat/res/telnet/""+str(template_id)+""/update""
  response = requests.put(f_url, data = json.dumps(telnet_template), auth=auth, headers=HEADERS)
  try:
  return response.status_code
  except requests.exceptions.RequestException as error:
  return ""Error:\n"" + str(error) + "" modify_telnet_template: An Error has occured""","Function takes input of a dictionry containing the required key/value pair for the modification of a telnet template. :param auth: :param url: :param telnet_template: Human readable label which is the name of the specific telnet template :param template_id Internal IMC number which designates the specific telnet template :return: int value of HTTP response code 201 for proper creation or 404 for failed creation :rtype int Sample of proper KV pairs. Please see documentation for valid values for different fields. telnet_template = {""type"": ""0"", ""name"": ""User_with_Enable"", ""authType"": ""4"", ""userName"": ""newadmin"", ""userPassword"": ""newpassword"", ""superPassword"": ""newpassword"", ""authTypeStr"": ""Username + Password + Super/Manager Password"", ""timeout"": ""4"", ""retries"": ""1"", ""port"": ""23"", ""version"": ""1"", ""creator"": ""admin"", ""accessType"": ""1"", ""operatorGroupStr"": """"}",2,0,1,2,5,1,0,0,2,3
"async def iter_all(
  self,
  direction: msg.StreamDirection = msg.StreamDirection.Forward,
  from_position: Optional[Union[msg.Position, msg._PositionSentinel]] = None,
  batch_size: int = 100,
  resolve_links: bool = True,
  require_master: bool = False,
  correlation_id: Optional[uuid.UUID] = None,
  ):
  correlation_id = correlation_id
  cmd = convo.IterAllEvents(
  msg.Position.for_direction(direction, from_position),
  batch_size,
  resolve_links,
  require_master,
  direction,
  self.credential,
  correlation_id,
  )
  result = await self.dispatcher.start_conversation(cmd)
  iterator = await result
  async for event in iterator:
  yield event","Read through all the events in the database. Args: direction (optional): Controls whether to read forward or backward through the events. Defaults to StreamDirection.Forward from_position (optional): The position to start reading from. Defaults to photonpump.Beginning when direction is Forward, photonpump.End when direction is Backward. batch_size (optional): The maximum number of events to read at a time. resolve_links (optional): True if eventstore should automatically resolve Link Events, otherwise False. required_master (optional): True if this command must be sent direct to the master node, otherwise False. correlation_id (optional): A unique identifer for this command. Examples: Print every event from the database. >>> with async.connect() as conn: >>> async for event in conn.iter_all() >>> print(event) Print every event from the database in reverse order >>> with async.connect() as conn: >>> async for event in conn.iter_all(direction=StreamDirection.Backward): >>> print(event) Start reading from a known commit position >>> with async.connect() as conn: >>> async for event in conn.iter_all(from_position=Position(12345)) >>> print(event)",2,0,1,1,4,1,0,0,1,2
"def connect_and_open_channel(host='localhost',
  port=5672,
  username='guest', password='guest',
  virtual_host='/',
  on_connection_close=None, *,
  loop=None, **kwargs):
  connection = yield from connect(host, port, username, password, virtual_host, on_connection_close, loop=loop, **kwargs)
  channel = yield from connection.open_channel()
  return connection, channel","Connect to an AMQP server and open a channel on the connection. This function is a :ref:`coroutine <coroutine>`. Parameters of this function are the same as :func:`connect`. :return: a tuple of ``(connection, channel)``. Equivalent to:: connection = yield from connect(host, port, username, password, virtual_host, on_connection_close, loop=loop, **kwargs) channel = yield from connection.open_channel() return connection, channel",1,0,0,1,2,1,0,0,1,2
"def main():
  credentials = get_credentials()
  http = credentials.authorize(httplib2.Http())
  service = discovery.build('calendar', 'v3', http=http)
  now = datetime.datetime.utcnow().isoformat() + 'Z'
  print('Getting the upcoming 10 events')
  eventsResult = service.events().list(
  calendarId='primary', timeMin=now, maxResults=10, singleEvents=True,
  orderBy='startTime').execute()
  events = eventsResult.get('items', [])
  if not events:
  print('No upcoming events found.')
  for event in events:
  start = event['start'].get('dateTime', event['start'].get('date'))
  print(start, event['summary'])",Shows basic usage of the Google Calendar API. Creates a Google Calendar API service object and outputs a list of the next 10 events on the user's calendar.,2,0,0,1,3,2,0,0,1,3
"def start(self, show_exit_option=None):
  self.previous_active_menu = ConsoleMenu.currently_active_menu
  ConsoleMenu.currently_active_menu = None
  self.should_exit = False
  if show_exit_option is None:
  show_exit_option = self.show_exit_option
  if show_exit_option:
  self.add_exit()
  else:
  self.remove_exit()
  try:
  self._main_thread = threading.Thread(target=self._wrap_start, daemon=True)
  except TypeError:
  self._main_thread = threading.Thread(target=self._wrap_start)
  self._main_thread.daemon = True
  self._main_thread.start()","Start the menu in a new thread and allow the user to interact with it. The thread is a daemon, so :meth:`join()<consolemenu.ConsoleMenu.join>` should be called if there's a possibility that the main thread will exit before the menu is done Args: show_exit_option (bool): Specify whether the exit item should be shown, defaults to the value set in the constructor",1,0,0,0,1,1,0,0,1,2
"def get_agent_pool(self, pool_id, properties=None, action_filter=None):
  route_values = {}
  if pool_id is not None:
  route_values['poolId'] = self._serialize.url('pool_id', pool_id, 'int')
  query_parameters = {}
  if properties is not None:
  properties = "","".join(properties)
  query_parameters['properties'] = self._serialize.query('properties', properties, 'str')
  if action_filter is not None:
  query_parameters['actionFilter'] = self._serialize.query('action_filter', action_filter, 'str')
  response = self._send(http_method='GET',
  location_id='a8c47e17-4d56-4a56-92bb-de7ea7dc65be',
  version='5.1-preview.1',
  route_values=route_values,
  query_parameters=query_parameters)
  return self._deserialize('TaskAgentPool', response)",GetAgentPool. [Preview API] Get information about an agent pool. :param int pool_id: An agent pool ID :param [str] properties: Agent pool properties (comma-separated) :param str action_filter: Filter by whether the calling user has use or manage permissions :rtype: :class:`<TaskAgentPool> <azure.devops.v5_1.task-agent.models.TaskAgentPool>`,2,0,0,1,3,2,0,0,1,3
"def create(cls, monetary_account_paying_id, request_id,
  maximum_amount_per_month, custom_headers=None):
  if custom_headers is None:
  custom_headers = {}
  request_map = {
  cls.FIELD_MONETARY_ACCOUNT_PAYING_ID: monetary_account_paying_id,
  cls.FIELD_REQUEST_ID: request_id,
  cls.FIELD_MAXIMUM_AMOUNT_PER_MONTH: maximum_amount_per_month
  }
  request_map_string = converter.class_to_json(request_map)
  request_map_string = cls._remove_field_for_request(request_map_string)
  api_client = client.ApiClient(cls._get_api_context())
  request_bytes = request_map_string.encode()
  endpoint_url = cls._ENDPOINT_URL_CREATE.format(cls._determine_user_id())
  response_raw = api_client.post(endpoint_url, request_bytes,
  custom_headers)
  return BunqResponseInt.cast_from_bunq_response(
  cls._process_for_id(response_raw)
  )","Create a new SDD whitelist entry. :type user_id: int :param monetary_account_paying_id: ID of the monetary account of which you want to pay from. :type monetary_account_paying_id: int :param request_id: ID of the request for which you want to whitelist the originating SDD. :type request_id: int :param maximum_amount_per_month: The maximum amount of money that is allowed to be deducted based on the whitelist. :type maximum_amount_per_month: object_.Amount :type custom_headers: dict[str, str]|None :rtype: BunqResponseInt",1,0,0,2,3,1,0,0,2,3
"def upload_signing_cert(self, cert_body, user_name=None):
  params = {'CertificateBody' : cert_body}
  if user_name:
  params['UserName'] = user_name
  return self.get_response('UploadSigningCertificate', params,
  verb='POST')","Uploads an X.509 signing certificate and associates it with the specified user. If the user_name is not specified, it is determined implicitly based on the AWS Access Key ID used to sign the request. :type cert_body: string :param cert_body: The body of the signing certificate. :type user_name: string :param user_name: The username of the user",1,0,0,1,2,1,0,0,1,2
"def report(self, reason=None):
  url = self.reddit_session.config['report']
  data = {'id': self.fullname}
  if reason:
  data['reason'] = reason
  response = self.reddit_session.request_json(url, data=data)
  self.reddit_session.evict(
  [self.reddit_session.config['user'],
  urljoin(self.reddit_session.user._url, 'hidden')])
  return response",Report this object to the moderators. :param reason: The user-supplied reason for reporting a comment or submission. Default: None (blank reason) :returns: The json response from the server.,2,0,0,2,4,2,0,0,1,3
"def set_user(user, sender, instance, **kwargs):
  try:
  app_label, model_name = settings.AUTH_USER_MODEL.split('.')
  auth_user_model = apps.get_model(app_label, model_name)
  except ValueError:
  auth_user_model = apps.get_model('auth', 'user')
  if sender == LogAction and isinstance(user, auth_user_model) and instance.user is None:
  instance.user = user
  if hasattr(threadlocal, 'actionslog'):
  instance.remote_ip = threadlocal.actionslog['remote_ip']","Signal receiver with an extra, required 'user' kwarg. This method becomes a real (valid) signal receiver when it is curried with the user.",0,0,1,1,2,1,0,1,1,3
"def get_log_all(self, project, logstore, from_time, to_time, topic=None,
  query=None, reverse=False, offset=0):
  while True:
  response = self.get_log(project, logstore, from_time, to_time, topic=topic,
  query=query, reverse=reverse, offset=offset, size=100)
  yield response
  count = response.get_count()
  offset += count
  if count == 0 or is_stats_query(query):
  break","Get logs from log service. will retry when incomplete. Unsuccessful opertaion will cause an LogException. different with `get_log` with size=-1, It will try to iteratively fetch all data every 100 items and yield them, in CLI, it could apply jmes filter to each batch and make it possible to fetch larger volume of data. :type project: string :param project: project name :type logstore: string :param logstore: logstore name :type from_time: int/string :param from_time: the begin timestamp or format of time in readable time like ""%Y-%m-%d %H:%M:%S<time_zone>"" e.g. ""2018-01-02 12:12:10+8:00"", also support human readable string, e.g. ""1 hour ago"", ""now"", ""yesterday 0:0:0"", refer to https://aliyun-log-cli.readthedocs.io/en/latest/tutorials/tutorial_human_readable_datetime.html :type to_time: int/string :param to_time: the end timestamp or format of time in readable time like ""%Y-%m-%d %H:%M:%S<time_zone>"" e.g. ""2018-01-02 12:12:10+8:00"", also support human readable string, e.g. ""1 hour ago"", ""now"", ""yesterday 0:0:0"", refer to https://aliyun-log-cli.readthedocs.io/en/latest/tutorials/tutorial_human_readable_datetime.html :type topic: string :param topic: topic name of logs, could be None :type query: string :param query: user defined query, could be None :type reverse: bool :param reverse: if reverse is set to true, the query will return the latest logs first, default is false :type offset: int :param offset: offset to start, by default is 0 :return: GetLogsResponse iterator :raise: LogException",1,0,1,1,3,2,0,0,1,3
"def enroll(session, uidentity, organization,
  from_date=MIN_PERIOD_DATE, to_date=MAX_PERIOD_DATE):
  if not from_date:
  raise ValueError(""'from_date' cannot be None"")
  if not to_date:
  raise ValueError(""'to_date' cannot be None"")
  if from_date < MIN_PERIOD_DATE or from_date > MAX_PERIOD_DATE:
  raise ValueError(""'from_date' %s is out of bounds"" % str(from_date))
  if to_date < MIN_PERIOD_DATE or to_date > MAX_PERIOD_DATE:
  raise ValueError(""'to_date' %s is out of bounds"" % str(to_date))
  if from_date > to_date:
  raise ValueError(""'from_date' %s cannot be greater than %s""
  % (from_date, to_date))
  enrollment = Enrollment(uidentity=uidentity,
  organization=organization,
  start=from_date, end=to_date)
  uidentity.last_modified = datetime.datetime.utcnow()
  session.add(enrollment)
  return enrollment","Enroll a unique identity to an organization. The function adds a new relationship between the unique identity in `uidentity` and the given `organization` for the current session. The period of the enrollment can be given with the parameters `from_date` and `to_date`, where `from_date <= to_date`. Default values for these dates are `MIN_PERIOD_DATE` and `MAX_PERIOD_DATE`. These dates cannot be `None`. This function returns as result a new `Enrollment` object. :param session: database session :param uidentity: unique identity to enroll :param organization: organization where the unique identity is enrolled :param from_date: date when the enrollment starts :param to_date: date when the enrollment ends :return: a new enrollment :raises ValeError: when either `from_date` or `to_date` are `None`; when `from_date < MIN_PERIOD_DATE`; or `to_date > MAX_PERIOD_DATE` or `from_date > to_date`.",0,1,0,1,2,1,1,0,1,3
"def magic_timeit(setup, stmt, ncalls=None, repeat=3, force_ms=False):
  import timeit
  import math
  units = [""s"", ""ms"", 'us', ""ns""]
  scaling = [1, 1e3, 1e6, 1e9]
  timer = timeit.Timer(stmt, setup)
  if ncalls is None:
  number = 1
  for _ in range(1, 10):
  if timer.timeit(number) >= 0.1:
  break
  number *= 10
  else:
  number = ncalls
  best = min(timer.repeat(repeat, number)) / number
  if force_ms:
  order = 1
  else:
  if best > 0.0 and best < 1000.0:
  order = min(-int(math.floor(math.log10(best)) // 3), 3)
  elif best >= 1000.0:
  order = 0
  else:
  order = 3
  return {'loops': number,
  'repeat': repeat,
  'timing': best * scaling[order],
  'units': units[order]}","Time execution of a Python statement or expression Usage:\\ %timeit [-n<N> -r<R> [-t|-c]] statement Time execution of a Python statement or expression using the timeit module. Options: -n<N>: execute the given statement <N> times in a loop. If this value is not given, a fitting value is chosen. -r<R>: repeat the loop iteration <R> times and take the best result. Default: 3 -t: use time.time to measure the time, which is the default on Unix. This function measures wall time. -c: use time.clock to measure the time, which is the default on Windows and measures wall time. On Unix, resource.getrusage is used instead and returns the CPU user time. -p<P>: use a precision of <P> digits to display the timing result. Default: 3 Examples: In [1]: %timeit pass 10000000 loops, best of 3: 53.3 ns per loop In [2]: u = None In [3]: %timeit u is None 10000000 loops, best of 3: 184 ns per loop In [4]: %timeit -r 4 u == None 1000000 loops, best of 4: 242 ns per loop In [5]: import time In [6]: %timeit -n1 time.sleep(2) 1 loops, best of 3: 2 s per loop The times reported by %timeit will be slightly higher than those reported by the timeit.py script when variables are accessed. This is due to the fact that %timeit executes the statement in the namespace of the shell, compared with timeit.py, which uses a single setup statement to import function or create variables. Generally, the bias does not matter as long as results from timeit.py are not mixed with those from %timeit.",1,0,0,1,2,1,0,0,1,2
"def fromGeoCoordinateString(self, sr, strings,
  conversionType, conversionMode=None):
  url = self._url + ""/fromGeoCoordinateString""
  params = {
  ""f"" : ""json"",
  ""sr"" : sr,
  ""strings"" : strings,
  ""conversionType"" : conversionType
  }
  if not conversionMode is None:
  params['conversionMode'] = conversionMode
  return self._post(url=url, param_dict=params,
  securityHandler=self._securityHandler,
  proxy_url=self._proxy_url,
  proxy_port=self._proxy_port)","The fromGeoCoordinateString operation is performed on a geometry service resource. The operation converts an array of well-known strings into xy-coordinates based on the conversion type and spatial reference supplied by the user. An optional conversion mode parameter is available for some conversion types. Inputs: sr - The well-known ID of the spatial reference or a spatial reference json object. strings - An array of strings formatted as specified by conversionType. Syntax: [<string1>,...,<stringN>] Example: [""01N AA 66021 00000"",""11S NT 00000 62155"", ""31U BT 94071 65288""] conversionType - The conversion type of the input strings. Valid conversion types are: MGRS - Military Grid Reference System USNG - United States National Grid UTM - Universal Transverse Mercator GeoRef - World Geographic Reference System GARS - Global Area Reference System DMS - Degree Minute Second DDM - Degree Decimal Minute DD - Decimal Degree conversionMode - Conversion options for MGRS, UTM and GARS conversion types. Conversion options for MGRS and UTM conversion types. Valid conversion modes for MGRS are: mgrsDefault - Default. Uses the spheroid from the given spatial reference. mgrsNewStyle - Treats all spheroids as new, like WGS 1984. The 180 degree longitude falls into Zone 60. mgrsOldStyle - Treats all spheroids as old, like Bessel 1841. The 180 degree longitude falls into Zone 60. mgrsNewWith180InZone01 - Same as mgrsNewStyle except the 180 degree longitude falls into Zone 01. mgrsOldWith180InZone01 - Same as mgrsOldStyle except the 180 degree longitude falls into Zone 01. Valid conversion modes for UTM are: utmDefault - Default. No options. utmNorthSouth - Uses north/south latitude indicators instead of zone numbers. Non-standard. Default is recommended",1,0,0,1,2,2,0,0,1,3
"def read_csv(filename):
  field_names = ('latitude', 'longitude', 'name')
  data = utils.prepare_csv_read(filename, field_names, skipinitialspace=True)
  locations = {}
  args = []
  for index, row in enumerate(data, 1):
  name = '%02i:%s' % (index, row['name'])
  locations[name] = (row['latitude'], row['longitude'])
  args.append(name)
  return locations, args",Pull locations from a user's CSV file. Read gpsbabel_'s CSV output format .. _gpsbabel: http://www.gpsbabel.org/ Args: filename (str): CSV file to parse Returns: tuple of dict and list: List of locations as ``str`` objects,0,0,0,1,1,1,0,0,1,2
"def inserir(self, id_equipment, id_environment, is_router=0):
  equipment_environment_map = dict()
  equipment_environment_map['id_equipamento'] = id_equipment
  equipment_environment_map['id_ambiente'] = id_environment
  equipment_environment_map['is_router'] = is_router
  code, xml = self.submit(
  {'equipamento_ambiente': equipment_environment_map}, 'POST', 'equipamentoambiente/')
  return self.response(code, xml)",Inserts a new Related Equipment with Environment and returns its identifier :param id_equipment: Identifier of the Equipment. Integer value and greater than zero. :param id_environment: Identifier of the Environment. Integer value and greater than zero. :param is_router: Identifier of the Environment. Boolean value. :return: Dictionary with the following structure: :: {'equipamento_ambiente': {'id': < id_equipment_environment >}} :raise InvalidParameterError: The identifier of Equipment or Environment is null and invalid. :raise AmbienteNaoExisteError: Environment not registered. :raise EquipamentoNaoExisteError: Equipment not registered. :raise EquipamentoAmbienteError: Equipment is already associated with the Environment. :raise DataBaseError: Networkapi failed to access the database. :raise XMLError: Networkapi failed to generate the XML response.,1,1,0,1,3,2,0,0,1,3
"def get_executing_jobs(db):
  fields = 'id,pid,user_name,start_time'
  running = List()
  running._fields = fields.split(',')
  query = ( % fields)
  rows = db(query)
  for r in rows:
  if r.pid and psutil.pid_exists(r.pid):
  running.append(r)
  return running",":param db: a :class:`openquake.server.dbapi.Db` instance :returns: (id, pid, user_name, start_time) tuples",0,0,1,1,2,1,0,1,1,3
"def TextInfo(filename=None, editable=False, **kwargs):
  args = []
  if filename:
  args.append('--filename=%s' % filename)
  if editable:
  args.append('--editable')
  for generic_args in kwargs_helper(kwargs):
  args.append('--%s=%s' % generic_args)
  p = run_zenity('--text-info', *args)
  if p.wait() == 0:
  return p.stdout.read()","Show the text of a file to the user. This will raise a Zenity Text Information Dialog presenting the user with the contents of a file. It returns the contents of the text box. filename - The path to the file to show. editable - True if the text should be editable. kwargs - Optional command line parameters for Zenity such as height, width, etc.",1,0,0,1,2,1,0,0,1,2
"def iter_contributors(self, anon=False, number=-1, etag=None):
  url = self._build_url('contributors', base_url=self._api)
  params = {}
  if anon:
  params = {'anon': True}
  return self._iter(int(number), url, User, params, etag)","Iterate over the contributors to this repository. :param bool anon: (optional), True lists anonymous contributors as well :param int number: (optional), number of contributors to return. Default: -1 returns all contributors :param str etag: (optional), ETag from a previous request to the same endpoint :returns: generator of :class:`User <github3.users.User>`\ s",2,0,0,1,3,2,0,0,1,3
"def exec_command(cmd, in_data='', chdir=None, shell=None, emulate_tty=False):
  assert isinstance(cmd, mitogen.core.UnicodeType)
  return exec_args(
  args=[get_user_shell(), '-c', cmd],
  in_data=in_data,
  chdir=chdir,
  shell=shell,
  emulate_tty=emulate_tty,
  )","Run a command in a subprocess, emulating the argument handling behaviour of SSH. :param bytes cmd: String command line, passed to user's shell. :param bytes in_data: Optional standard input for the command. :return: (return code, stdout bytes, stderr bytes)",1,0,0,1,2,1,0,0,1,2
"async def add_user(self, username, password=None, display_name=None):
  if not display_name:
  display_name = username
  user_facade = client.UserManagerFacade.from_connection(
  self.connection())
  users = [client.AddUser(display_name=display_name,
  username=username,
  password=password)]
  results = await user_facade.AddUser(users)
  secret_key = results.results[0].secret_key
  return await self.get_user(username, secret_key=secret_key)",Add a user to this controller. :param str username: Username :param str password: Password :param str display_name: Display name :returns: A :class:`~juju.user.User` instance,1,1,1,2,5,2,0,0,1,3
"def ip_address_delete(session, ifname, ifaddr):
  def _remove_inet_addr(intf_inet, addr):
  addr_list = intf_inet.split(',')
  if addr not in addr_list:
  LOG.debug(
  'Interface ""%s"" does not have ""ifaddr"": %s',
  intf.ifname, addr)
  return intf_inet
  else:
  addr_list.remove(addr)
  return ','.join(addr_list)
  intf = ip_link_show(session, ifname=ifname)
  if not intf:
  LOG.debug('Interface ""%s"" does not exist', ifname)
  return None
  if ip.valid_ipv4(ifaddr):
  intf.inet = _remove_inet_addr(intf.inet, ifaddr)
  elif ip.valid_ipv6(ifaddr):
  intf.inet6 = _remove_inet_addr(intf.inet6, ifaddr)
  else:
  LOG.debug('Invalid IP address for ""ifaddr"": %s', ifaddr)
  return None
  return intf","Deletes an IP address from interface record identified with the given ""ifname"". The arguments are similar to ""ip address delete"" command of iproute2. :param session: Session instance connecting to database. :param ifname: Name of interface. :param ifaddr: IPv4 or IPv6 address. :return: Instance of record or ""None"" if failed.",1,1,1,0,3,1,0,0,1,2
"def confirm(text, default=False, abort=False, prompt_suffix=': ',
  show_default=True, err=False):
  prompt = _build_prompt(text, prompt_suffix, show_default,
  default and 'Y/n' or 'y/N')
  while 1:
  try:
  echo(prompt, nl=False, err=err)
  value = visible_prompt_func('').lower().strip()
  except (KeyboardInterrupt, EOFError):
  raise Abort()
  if value in ('y', 'yes'):
  rv = True
  elif value in ('n', 'no'):
  rv = False
  elif value == '':
  rv = default
  else:
  echo('Error: invalid input', err=err)
  continue
  break
  if abort and not rv:
  raise Abort()
  return rv","Prompts for confirmation (yes/no question). If the user aborts the input by sending a interrupt signal this function will catch it and raise a :exc:`Abort` exception. .. versionadded:: 4.0 Added the `err` parameter. :param text: the question to ask. :param default: the default for the prompt. :param abort: if this is set to `True` a negative answer aborts the exception by raising :exc:`Abort`. :param prompt_suffix: a suffix that should be added to the prompt. :param show_default: shows or hides the default value in the prompt. :param err: if set to true the file defaults to ``stderr`` instead of ``stdout``, the same as with echo.",1,0,0,1,2,1,0,0,1,2
"def add_primary_text(self, item_url, primary_text):
  c = self.conn.cursor()
  c.execute(""DELETE FROM primary_texts WHERE item_url=?"",
  (str(item_url),))
  self.conn.commit()
  c.execute(""INSERT INTO primary_texts VALUES (?, ?, ?)"",
  (str(item_url), primary_text, self.__now_iso_8601()))
  self.conn.commit()
  c.close()","Add the given primary text to the cache database, updating the existing record if the primary text is already present :type item_url: String or Item :param item_url: the URL of the corresponding item, or an Item object :type primary_text: String :param primary_text: the item's primary text",0,1,1,0,2,0,1,1,1,3
"def build_auth_uri(endpoint, client_id, redirect_uri=None, scope=None, state=None):
  params = {'response_type': 'code', 'client_id': client_id}
  if redirect_uri is not None:
  params['redirect_uri'] = redirect_uri
  if scope is not None:
  params['scope'] = ' '.join(scope)
  if state is not None:
  params['state'] = state
  return '%s?%s' % (endpoint, urlencode(params))",Helper method builds the uri that a user must be redirected to for authentication/authorization using the authorization_code grant type. endpoint - The authorization endpoint client_id - The client id redirect_uri - The redirect uri scope - A list of permissions to request state - An application state that will be sent back by the authorization server,1,0,0,1,2,1,0,0,1,2
"def init(calc_id='nojob', level=logging.INFO):
  if not logging.root.handlers:
  logging.basicConfig(level=level)
  if calc_id == 'job':
  calc_id = dbcmd('create_job', datastore.get_datadir())
  elif calc_id == 'nojob':
  calc_id = datastore.get_last_calc_id() + 1
  else:
  assert isinstance(calc_id, int), calc_id
  fmt = '[%(asctime)s
  for handler in logging.root.handlers:
  handler.setFormatter(logging.Formatter(fmt))
  return calc_id",1. initialize the root logger (if not already initialized) 2. set the format of the root handlers (if any) 3. return a new calculation ID candidate if calc_id is 'job' or 'nojob' (with 'nojob' the calculation ID is not stored in the database),0,1,1,0,2,0,0,0,0,0
"def sky(input=None,outExt=None,configObj=None, group=None, editpars=False, **inputDict):
  if input is not None:
  inputDict['input']=input
  inputDict['output']=None
  inputDict['updatewcs']=False
  inputDict['group']=group
  else:
  print(""Please supply an input image"", file=sys.stderr)
  raise ValueError
  configObj = util.getDefaultConfigObj(__taskname__,configObj,inputDict,loadOnly=(not editpars))
  if configObj is None:
  return
  if not editpars:
  run(configObj,outExt=outExt)","Perform sky subtraction on input list of images Parameters ---------- input : str or list of str a python list of image filenames, or just a single filename configObj : configObject an instance of configObject inputDict : dict, optional an optional list of parameters specified by the user outExt : str The extension of the output image. If the output already exists then the input image is overwritten Notes ----- These are parameters that the configObj should contain by default, they can be altered on the fly using the inputDict Parameters that should be in configobj: ========== =================================================================== Name Definition ========== =================================================================== skymethod 'Sky computation method' skysub 'Perform sky subtraction?' skywidth 'Bin width of histogram for sampling sky statistics (in sigma)' skystat 'Sky correction statistics parameter' skylower 'Lower limit of usable data for sky (always in electrons)' skyupper 'Upper limit of usable data for sky (always in electrons)' skyclip 'Number of clipping iterations' skylsigma 'Lower side clipping factor (in sigma)' skyusigma 'Upper side clipping factor (in sigma)' skymask_cat 'Catalog file listing image masks' use_static 'Use static mask for skymatch computations?' sky_bits 'Integer mask bit values considered good pixels in DQ array' skyfile 'Name of file with user-computed sky values' skyuser 'KEYWORD indicating a sky subtraction value if done by user' in_memory 'Optimize for speed or for memory use' ========== =================================================================== The output from sky subtraction is a copy of the original input file where all the science data extensions have been sky subtracted.",1,0,0,1,2,1,0,0,1,2
"def discharge_token(self, username):
  url = '{}discharge-token-for-user?username={}'.format(
  self.url, quote(username))
  logging.debug('Sending identity info to {}'.format(url))
  response = make_request(url, method='GET', timeout=self.timeout)
  try:
  macaroon = response['DischargeToken']
  json_macaroon = json.dumps(macaroon)
  except (KeyError, UnicodeDecodeError) as err:
  raise InvalidMacaroon(
  'Invalid macaroon from discharger: {}'.format(err.message))
  return base64.urlsafe_b64encode(""[{}]"".format(
  json_macaroon).encode('utf-8'))",Discharge token for a user. Raise a ServerError if an error occurs in the request process. @param username The logged in user. @return The resulting base64 encoded discharged token.,2,0,0,1,3,2,0,0,1,3
"def proxy_headers(self, proxy):
  headers = {}
  username, password = get_auth_from_url(proxy)
  if username and password:
  headers['Proxy-Authorization'] = _basic_auth_str(username,
  password)
  return headers","Returns a dictionary of the headers to add to any request sent through a proxy. This works with urllib3 magic to ensure that they are correctly sent to the proxy, rather than in a tunnelled request if CONNECT is being used. This should not be called from user code, and is only exposed for use when subclassing the :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`. :param proxies: The url of the proxy being used for this request. :param kwargs: Optional additional keyword arguments.",1,0,0,1,2,1,0,0,1,2
"def delete_query_index(self, design_document_id, index_type, index_name):
  if index_type == JSON_INDEX_TYPE:
  index = Index(self, design_document_id, index_name)
  elif index_type == TEXT_INDEX_TYPE:
  index = TextIndex(self, design_document_id, index_name)
  else:
  raise CloudantArgumentError(103, index_type)
  index.delete()","Deletes the query index identified by the design document id, index type and index name from the remote database. :param str design_document_id: The design document id that the index exists in. :param str index_type: The type of the index to be deleted. Must be either 'text' or 'json'. :param str index_name: The index name of the index to be deleted.",0,1,0,0,1,1,1,1,1,4
"def from_sqlite(cls, database_path, base_url, version='auto', client_id='ghost-admin'):
  import os
  import sqlite3
  fd = os.open(database_path, os.O_RDONLY)
  connection = sqlite3.connect('/dev/fd/%d' % fd)
  os.close(fd)
  try:
  row = connection.execute(
  'SELECT secret FROM clients WHERE slug = ?',
  (client_id,)
  ).fetchone()
  if row:
  return cls(
  base_url, version=version,
  client_id=client_id, client_secret=row[0]
  )
  else:
  raise GhostException(401, [{
  'errorType': 'InternalError',
  'message': 'No client_secret found for client_id: %s' % client_id
  }])
  finally:
  connection.close()","Initialize a new Ghost API client, reading the client ID and secret from the SQlite database. :param database_path: The path to the database file. :param base_url: The base url of the server :param version: The server version to use (default: `auto`) :param client_id: The client ID to look for in the database :return: A new Ghost API client instance",1,0,1,1,3,1,0,1,1,3
"def input_password(self, message=None):
  message = self.__prompt_formatter.format_prompt(message)
  try:
  if message:
  return getpass.getpass(message)
  else:
  return getpass.getpass()
  except BaseException:
  self.__screen.println('Warning: Unable to mask input; characters will be echoed to console')
  return self.input(message)","Prompt the user for a password. This is equivalent to the input() method, but does not echo inputted characters to the screen. :param message: the prompt message.",1,0,0,1,2,1,0,0,1,2
"def pack_req(cls, order_id, status_filter_list, code, start, end,
  trd_env, acc_id, trd_mkt, conn_id):
  from futuquant.common.pb.Trd_GetOrderList_pb2 import Request
  req = Request()
  req.c2s.header.trdEnv = TRD_ENV_MAP[trd_env]
  req.c2s.header.accID = acc_id
  req.c2s.header.trdMarket = TRD_MKT_MAP[trd_mkt]
  if code:
  req.c2s.filterConditions.codeList.append(code)
  if order_id:
  req.c2s.filterConditions.idList.append(int(order_id))
  if start:
  req.c2s.filterConditions.beginTime = start
  if end:
  req.c2s.filterConditions.endTime = end
  if len(status_filter_list):
  for order_status in status_filter_list:
  req.c2s.filterStatusList.append(ORDER_STATUS_MAP[order_status])
  return pack_pb_req(req, ProtoId.Trd_GetOrderList, conn_id)",Convert from user request for trading days to PLS request,1,0,0,1,2,1,0,0,1,2
"def membership_request_notifications(user):
  orgs = [o for o in user.organizations if o.is_admin(user)]
  notifications = []
  for org in orgs:
  for request in org.pending_requests:
  notifications.append((request.created, {
  'id': request.id,
  'organization': org.id,
  'user': {
  'id': request.user.id,
  'fullname': request.user.fullname,
  'avatar': str(request.user.avatar)
  }
  }))
  return notifications",Notify user about pending membership requests,1,0,1,1,3,1,0,0,1,2
"def parse_authorization_code_response(uri, state=None):
  if not is_secure_transport(uri):
  raise InsecureTransportError()
  query = urlparse.urlparse(uri).query
  params = dict(urlparse.parse_qsl(query))
  if not 'code' in params:
  raise MissingCodeError(""Missing code parameter in response."")
  if state and params.get('state', None) != state:
  raise MismatchingStateError()
  return params","Parse authorization grant response URI into a dict. If the resource owner grants the access request, the authorization server issues an authorization code and delivers it to the client by adding the following parameters to the query component of the redirection URI using the ``application/x-www-form-urlencoded`` format: **code** REQUIRED. The authorization code generated by the authorization server. The authorization code MUST expire shortly after it is issued to mitigate the risk of leaks. A maximum authorization code lifetime of 10 minutes is RECOMMENDED. The client MUST NOT use the authorization code more than once. If an authorization code is used more than once, the authorization server MUST deny the request and SHOULD revoke (when possible) all tokens previously issued based on that authorization code. The authorization code is bound to the client identifier and redirection URI. **state** REQUIRED if the ""state"" parameter was present in the client authorization request. The exact value received from the client. :param uri: The full redirect URL back to the client. :param state: The state parameter from the authorization request. For example, the authorization server redirects the user-agent by sending the following HTTP response: .. code-block:: http HTTP/1.1 302 Found Location: https://client.example.com/cb?code=SplxlOBeZQQYbYS6WxSbIA &state=xyz",1,0,0,1,2,1,0,0,1,2
"def get_security_file(filename, profile='default'):
  from IPython.core.profiledir import ProfileDir
  try:
  pd = ProfileDir.find_profile_dir_by_name(get_ipython_dir(), profile)
  except Exception:
  raise IOError(""Profile %r not found"")
  return filefind(filename, ['.', pd.security_dir])","Return the absolute path of a security file given by filename and profile This allows users and developers to find security files without knowledge of the IPython directory structure. The search path will be ['.', profile.security_dir] Parameters ---------- filename : str The file to be found. If it is passed as an absolute path, it will simply be returned. profile : str [default: 'default'] The name of the profile to search. Leaving this unspecified The file to be found. If it is passed as an absolute path, fname will simply be returned. Returns ------- Raises :exc:`IOError` if file not found or returns absolute path to file.",1,0,0,1,2,1,0,0,1,2
"def put_comments(self, resource, comment, timeout=None):
  params = {'apikey': self.api_key, 'resource': resource, 'comment': comment}
  try:
  response = requests.post(self.base + 'comments/put', params=params, proxies=self.proxies, timeout=timeout)
  except requests.RequestException as e:
  return dict(error=str(e))
  return _return_response_and_status_code(response)","Post a comment on a file or URL. The initial idea of VirusTotal Community was that users should be able to make comments on files and URLs, the comments may be malware analyses, false positive flags, disinfection instructions, etc. Imagine you have some automatic setup that can produce interesting results related to a given sample or URL that you submit to VirusTotal for antivirus characterization, you might want to give visibility to your setup by automatically reviewing samples and URLs with the output of your automation. :param resource: either a md5/sha1/sha256 hash of the file you want to review or the URL itself that you want to comment on. :param comment: the actual review, you can tag it using the ""#"" twitter-like syntax (e.g. #disinfection #zbot) and reference users using the ""@"" syntax (e.g. @VirusTotalTeam). :param timeout: The amount of time in seconds the request should wait before timing out. :return: If the comment was successfully posted the response code will be 1, 0 otherwise.",1,0,0,2,3,2,0,0,2,4
"def VFSOpen(pathspec,
  progress_callback = None
  ):
  if not VFS_HANDLERS:
  Init()
  fd = None
  vroot = _VFS_VIRTUALROOTS.get(pathspec.pathtype)
  if (not vroot or pathspec.is_virtualroot or
  pathspec.CollapsePath().startswith(vroot.CollapsePath())):
  working_pathspec = pathspec.Copy()
  else:
  working_pathspec = vroot.Copy()
  working_pathspec.last.nested_path = pathspec.Copy()
  while working_pathspec:
  component = working_pathspec.Pop()
  try:
  handler = VFS_HANDLERS[component.pathtype]
  except KeyError:
  raise UnsupportedHandlerError(component.pathtype)
  fd = handler.Open(
  fd=fd,
  component=component,
  handlers=dict(VFS_HANDLERS),
  pathspec=working_pathspec,
  progress_callback=progress_callback)
  if fd is None:
  raise ValueError(""VFSOpen cannot be called with empty PathSpec."")
  return fd","Expands pathspec to return an expanded Path. A pathspec is a specification of how to access the file by recursively opening each part of the path by different drivers. For example the following pathspec: pathtype: OS path: ""/dev/sda1"" nested_path { pathtype: TSK path: ""/home/image2.img"" nested_path { pathtype: TSK path: ""/home/a.txt"" } } Instructs the system to: 1) open /dev/sda1 using the OS driver. 2) Pass the obtained filelike object to the TSK driver to open ""/home/image2.img"". 3) The obtained filelike object should be passed to the TSK driver to open ""/home/a.txt"". The problem remains how to get to this expanded path specification. Since the server is not aware of all the files on the client, the server may request this: pathtype: OS path: ""/dev/sda1"" nested_path { pathtype: TSK path: ""/home/image2.img/home/a.txt"" } Or even this: pathtype: OS path: ""/dev/sda1/home/image2.img/home/a.txt"" This function converts the pathspec requested by the server into an expanded pathspec required to actually open the file. This is done by expanding each component of the pathspec in turn. Expanding the component is done by opening each leading directory in turn and checking if it is a directory of a file. If its a file, we examine the file headers to determine the next appropriate driver to use, and create a nested pathspec. Note that for some clients there might be a virtual root specified. This is a directory that gets prepended to all pathspecs of a given pathtype. For example if there is a virtual root defined as [""os:/virtualroot""], a path specification like pathtype: OS path: ""/home/user/*"" will get translated into pathtype: OS path: ""/virtualroot"" is_virtualroot: True nested_path { pathtype: OS path: ""/dev/sda1"" } Args: pathspec: A Path() protobuf to normalize. progress_callback: A callback to indicate that the open call is still working but needs more time. Returns: The open filelike object. This will contain the expanded Path() protobuf as the member fd.pathspec. Raises: IOError: if one of the path components can not be opened.",2,0,0,0,2,1,0,0,1,2
"def pots(self, refresh=False):
  if not refresh and self._cached_pots:
  return self._cached_pots
  endpoint = '/pots/listV1'
  response = self._get_response(
  method='get', endpoint=endpoint,
  )
  pots_json = response.json()['pots']
  pots = [MonzoPot(data=pot) for pot in pots_json]
  self._cached_pots = pots
  return pots",Returns a list of pots owned by the currently authorised user. Official docs: https://monzo.com/docs/#pots :param refresh: decides if the pots information should be refreshed. :type refresh: bool :returns: list of Monzo pots :rtype: list of MonzoPot,2,0,0,1,3,2,0,0,1,3
"def get_google_playlist(self, playlist):
 logger.info(""Loading playlist {0}"".format(playlist))
 for google_playlist in self.api.get_all_user_playlist_contents():
 if google_playlist['name'] == playlist or google_playlist['id'] == playlist:
 return google_playlist
 else:
 logger.warning(""Playlist {0} does not exist."".format(playlist))
 return {}","Get playlist information of a user-generated Google Music playlist. Parameters: playlist (str): Name or ID of Google Music playlist. Names are case-sensitive. Google allows multiple playlists with the same name. If multiple playlists have the same name, the first one encountered is used. Returns: dict: The playlist dict as returned by Mobileclient.get_all_user_playlist_contents.",2,0,0,1,3,2,0,0,1,3
"def get_project_tags(self, project, repository, tag_name):
  url = 'rest/api/1.0/projects/{project}/repos/{repository}/tags/{tag}'.format(project=project,
  repository=repository,
  tag=tag_name)
  return self.get(url)",Retrieve a tag in the specified repository. The authenticated user must have REPO_READ permission for the context repository to call this resource. Search uri is api/1.0/projects/{projectKey}/repos/{repositorySlug}/tags/{name:.*} :param project: :param repository: :param tag_name: OPTIONAL: :return:,1,0,0,1,2,2,0,0,1,3
"def inspect_distribution(self, image, auth_config=None):
  registry, _ = auth.resolve_repository_name(image)
  headers = {}
  if auth_config is None:
  header = auth.get_config_header(self, registry)
  if header:
  headers['X-Registry-Auth'] = header
  else:
  log.debug('Sending supplied auth config')
  headers['X-Registry-Auth'] = auth.encode_header(auth_config)
  url = self._url(""/distribution/{0}/json"", image)
  return self._result(
  self._get(url, headers=headers), True
  )",Get image digest and platform information by contacting the registry. Args: image (str): The image name to inspect auth_config (dict): Override the credentials that are found in the config for this request. ``auth_config`` should contain the ``username`` and ``password`` keys to be valid. Returns: (dict): A dict containing distribution data Raises: :py:class:`docker.errors.APIError` If the server returns an error.,2,0,0,1,3,2,0,0,1,3
"def selected_canvas_agglayer(self):
  if self.lstCanvasAggLayers.selectedItems():
  item = self.lstCanvasAggLayers.currentItem()
  else:
  return None
  try:
  layer_id = item.data(QtCore.Qt.UserRole)
  except (AttributeError, NameError):
  layer_id = None
  layer = QgsProject.instance().mapLayer(layer_id)
  return layer",Obtain the canvas aggregation layer selected by user. :returns: The currently selected map layer in the list. :rtype: QgsMapLayer,1,0,0,1,2,1,0,0,1,2
"def associate_ipv6(self, id_equip, id_ipv6):
  if not is_valid_int_param(id_equip):
  raise InvalidParameterError(
  u'The identifier of equipment is invalid or was not informed.')
  if not is_valid_int_param(id_ipv6):
  raise InvalidParameterError(
  u'The identifier of ip is invalid or was not informed.')
  url = 'ipv6/' + str(id_ipv6) + '/equipment/' + str(id_equip) + '/'
  code, xml = self.submit(None, 'PUT', url)
  return self.response(code, xml)",Associates an IPv6 to a equipament. :param id_equip: Identifier of the equipment. Integer value and greater than zero. :param id_ipv6: Identifier of the ip. Integer value and greater than zero. :return: Dictionary with the following structure: {'ip_equipamento': {'id': < id_ip_do_equipamento >}} :raise EquipamentoNaoExisteError: Equipment is not registered. :raise IpNaoExisteError: IP not registered. :raise IpError: IP is already associated with the equipment. :raise InvalidParameterError: Identifier of the equipment and/or IP is null or invalid. :raise DataBaseError: Networkapi failed to access the database. :raise XMLError: Networkapi failed to generate the XML response.,1,0,0,2,3,2,0,0,1,3
"def build(self):
  self.icon = 'icon_24px.png'
  config = self.config
  Logger.debug(
  ""ELiDEApp: starting with world {}, path {}"".format(
  config['LiSE']['world'],
  LiSE.__path__[-1]
  )
  )
  if config['ELiDE']['debugger'] == 'yes':
  import pdb
  pdb.set_trace()
  self.manager = ScreenManager(transition=NoTransition())
  if config['ELiDE']['inspector'] == 'yes':
  from kivy.core.window import Window
  from kivy.modules import inspector
  inspector.create_inspector(Window, self.manager)
  self._start_subprocess()
  self._add_screens()
  return self.manager","Make sure I can use the database, create the tables as needed, and return the root widget.",1,1,0,1,3,1,0,0,1,2
"def accession(self, accession=None, entry_name=None, limit=None, as_df=False):
  q = self.session.query(models.Accession)
  model_queries_config = (
  (accession, models.Accession.accession),
  )
  q = self.get_model_queries(q, model_queries_config)
  q = self.get_one_to_many_queries(q, ((entry_name, models.Entry.name),))
  return self._limit_and_df(q, limit, as_df)","Method to query :class:`.models.Accession` objects in database :param accession: UniProt Accession number(s) :type accession: str or tuple(str) or None :param entry_name: name(s) in :class:`.models.Entry` :type entry_name: str or tuple(str) or None :param limit: - if `isinstance(limit,int)==True` -> limit - if `isinstance(limit,tuple)==True` -> format:= tuple(page_number, results_per_page) - if limit == None -> all results :type limit: int or tuple(int) or None :param bool as_df: if `True` results are returned as :class:`pandas.DataFrame` :return: - if `as_df == False` -> list(:class:`.models.Accession`) - if `as_df == True` -> :class:`pandas.DataFrame` :rtype: list(:class:`.models.Accession`) or :class:`pandas.DataFrame`",1,0,1,1,3,1,0,1,1,3
"def upsert_smart_invite(self, smart_invite_id, recipient, event, callback_url=None, organizer=None):
  event['start'] = format_event_time(event['start'])
  event['end'] = format_event_time(event['end'])
  body = {
  'smart_invite_id': smart_invite_id,
  'event': event
  }
  if type(recipient) == dict:
  body['recipient'] = recipient
  elif type(recipient) == list:
  body['recipients'] = recipient
  if callback_url:
  body['callback_url'] = callback_url
  if organizer:
  body['organizer'] = organizer
  return self.request_handler.post('smart_invites', data=body, use_api_key=True).json()","Creates or updates smart invite. :param string smart_invite_id - A String uniquely identifying the event for your application (note: this is NOT an ID generated by Cronofy). :param string callback_url - The URL within your application you want Cronofy to send notifications to about user interactions with the Smart Invite. :param dict recipient - A Dict containing the intended recipient of the invite :email - A String for the email address you are going to send the Smart Invite to. :param dict event - A Dict describing the event with symbolized keys: :summary - A String to use as the summary, sometimes referred to as the name or title, of the event. :description - A String to use as the description, sometimes referred to as the notes or body, of the event. :start - The Time or Date the event starts. :end - The Time or Date the event ends. :url - The URL associated with the event. :location - A Dict describing the location of the event with keys (optional): :description - A String describing the location. :lat - A String of the location's latitude. :long - A String of the location's longitude. :reminders - An Array of Dicts describing the desired reminders for the event. Reminders should be specified in priority order as, for example, when the underlying provider only supports a single reminder then the first reminder will be used. :minutes - An Integer specifying the number of minutes before the start of the event that the reminder should occur. :transparency - The transparency state for the event (optional). Accepted values are ""transparent"" and ""opaque"". :color - The color of the event (optional). :param dict organizer - A Dict containing the organzier of the invite :name - A String for the name of the organizer.",1,0,0,2,3,1,0,0,1,2
"def add_user(self, user_id, custom_properties=None, headers=None, endpoint_url=None):
  endpoint_url = endpoint_url or self._endpoint_url
  url = endpoint_url + '/users'
  headers = headers or self._default_headers()
  payload = {""user_id"": user_id}
  if custom_properties is not None:
  payload[""user_properties""] = custom_properties
  response = requests.post(url, headers=headers, json=payload)
  return response",Creates a new identified user if he doesn't exist. :param str user_id: identified user's ID :param dict custom_properties: user properties :param dict headers: custom request headers (if isn't set default values are used) :param str endpoint_url: where to send the request (if isn't set default value is used) :return: Response,0,1,0,2,3,2,0,0,2,4
"def getRegisterUserInfo(self, svctype = ""Android NDrive App ver"", auth = 0):
  data = {'userid': self.user_id,
  'svctype': svctype,
  'auth': auth
  }
  s, metadata = self.GET('getRegisterUserInfo', data)
  if s is True:
  self.useridx = metadata['useridx']
  return True, metadata
  else:
  return False, metadata",Retrieve information about useridx :param svctype: Information about the platform you are using right now. :param auth: Authentication type :return: ``True`` when success or ``False`` when failed,1,0,0,1,2,2,0,0,1,3
"def profile_list(request, page=1, template_name='userena/profile_list.html',
  paginate_by=50, extra_context=None, **kwargs):
  warnings.warn(""views.profile_list is deprecated. Use ProfileListView instead"", DeprecationWarning, stacklevel=2)
  try:
  page = int(request.GET.get('page', None))
  except (TypeError, ValueError):
  page = page
  if userena_settings.USERENA_DISABLE_PROFILE_LIST \
  and not request.user.is_staff:
  raise Http404
  profile_model = get_profile_model()
  queryset = profile_model.objects.get_visible_profiles(request.user)
  if not extra_context: extra_context = dict()
  return ProfileListView.as_view(queryset=queryset,
  paginate_by=paginate_by,
  page=page,
  template_name=template_name,
  extra_context=extra_context,
  **kwargs)(request)",Returns a list of all profiles that are public. It's possible to disable this by changing ``USERENA_DISABLE_PROFILE_LIST`` to ``True`` in your settings. :param page: Integer of the active page used for pagination. Defaults to the first page. :param template_name: String defining the name of the template that is used to render the list of all users. Defaults to ``userena/list.html``. :param paginate_by: Integer defining the amount of displayed profiles per page. Defaults to 50 profiles per page. :param extra_context: Dictionary of variables that are passed on to the ``template_name`` template. **Context** ``profile_list`` A list of profiles. ``is_paginated`` A boolean representing whether the results are paginated. If the result is paginated. It will also contain the following variables. ``paginator`` An instance of ``django.core.paginator.Paginator``. ``page_obj`` An instance of ``django.core.paginator.Page``.,1,0,1,1,3,1,0,1,1,3
"def stdout_output(cls, cs, score_dict, verbose, limit):
  for ds, score_groups in six.iteritems(score_dict):
  for checker, rpair in six.iteritems(score_groups):
  groups, errors = rpair
  score_list, points, out_of = cs.standard_output(ds, limit,
  checker,
  groups)
  cs.standard_output_generation(groups, limit, points, out_of,
  check=checker)
  return groups","Calls output routine to display results in terminal, including scoring. Goes to verbose function if called by user. @param cs Compliance Checker Suite @param score_dict Dict with dataset name as key, list of results as value @param verbose Integer value for verbosity level @param limit The degree of strictness, 1 being the strictest, and going up from there.",1,0,0,1,2,1,0,0,1,2
"def get_unread(self, include_me=False, include_notifications=False, use_unread_count=False):
  raw_message_groups = self.wapi_functions.getUnreadMessages(include_me, include_notifications, use_unread_count)
  unread_messages = []
  for raw_message_group in raw_message_groups:
  chat = factory_chat(raw_message_group, self)
  messages = [factory_message(message, self) for message in raw_message_group['messages']]
  messages.sort(key=lambda message: message.timestamp)
  unread_messages.append(MessageGroup(chat, messages))
  return unread_messages",Fetches unread messages :param include_me: Include user's messages :type include_me: bool or None :param include_notifications: Include events happening on chat :type include_notifications: bool or None :param use_unread_count: If set uses chat's 'unreadCount' attribute to fetch last n messages from chat :type use_unread_count: bool :return: List of unread messages grouped by chats :rtype: list[MessageGroup],2,0,0,1,3,2,0,0,1,3
"def get_or_default_template_file_name(ctx, param, provided_value, include_build):
  search_paths = [
  ""template.yaml"",
  ""template.yml"",
  ]
  if include_build:
  search_paths.insert(0, os.path.join("".aws-sam"", ""build"", ""template.yaml""))
  if provided_value == _TEMPLATE_OPTION_DEFAULT_VALUE:
  provided_value = ""template.yml""
  for option in search_paths:
  if os.path.exists(option):
  provided_value = option
  break
  result = os.path.abspath(provided_value)
  LOG.debug(""Using SAM Template at %s"", result)
  return result",Default value for the template file name option is more complex than what Click can handle. This method either returns user provided file name or one of the two default options (template.yaml/template.yml) depending on the file that exists :param ctx: Click Context :param param: Param name :param provided_value: Value provided by Click. It could either be the default value or provided by user. :return: Actual value to be used in the CLI,1,0,0,1,2,1,0,0,1,2
"def setCentralWidget(self, widget, createsNew=True, autoCommit=True):
  if not isinstance(widget, XOrbRecordWidget):
  return False
  super(XOrbPopupButton, self).setCentralWidget(widget)
  widget.setAutoCommitOnSave(autoCommit)
  popup = self.popupWidget()
  popup.setAutoCloseOnAccept(False)
  if createsNew and widget.multipleCreateEnabled():
  btn = popup.addButton('Save && Create Another')
  btn.clicked.connect(widget.saveSilent)
  popup.accepted.connect(widget.save)
  widget.saved.connect(popup.close)
  widget.saved.connect(self.saved)
  if createsNew:
  popup.aboutToShow.connect(widget.reset)
  return True","Sets the central widget for this popup button. If createsNew is set to True, then the about to show signal from the popup will be linked to the widget's reset slot. If autoCommit is set to True, then the widget will commit it's information to the database. :param widget | <prjexui.widgets.xorbrecordwidget.XOrbRecordWidget> createsNew | <bool> autoCommit | <boo> :return <bool> | success",1,1,0,1,3,1,0,0,1,2
"def prompt_for_password(url, user=None, default_user=None):
  if user is None:
  default_user = default_user or getpass.getuser()
  while user is None:
  user = compat.console_input(
  ""Enter username for {} [{}]: "".format(url, default_user)
  )
  if user.strip() == """" and default_user:
  user = default_user
  if user:
  pw = getpass.getpass(
  ""Enter password for {}@{} (Ctrl+C to abort): "".format(user, url)
  )
  if pw or pw == """":
  return (user, pw)
  return None","Prompt for username and password. If a user name is passed, only prompt for a password. Args: url (str): hostname user (str, optional): Pass a valid name to skip prompting for a user name default_user (str, optional): Pass a valid name that is used as default when prompting for a user name Raises: KeyboardInterrupt if user hits Ctrl-C Returns: (username, password) or None",1,0,0,1,2,1,0,0,1,2
"def build(self, build_execution_configuration, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('callback'):
  return self.build_with_http_info(build_execution_configuration, **kwargs)
  else:
  (data) = self.build_with_http_info(build_execution_configuration, **kwargs)
  return data","Triggers the build execution for a given configuration. This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please define a `callback` function to be invoked when receiving the response. >>> def callback_function(response): >>> pprint(response) >>> >>> thread = api.build(build_execution_configuration, callback=callback_function) :param callback function: The callback function for asynchronous request. (optional) :param str build_execution_configuration: Build Execution Configuration. See org.jboss.pnc.spi.executor.BuildExecutionConfiguration. (required) :param str username_triggered: Username who triggered the build. If empty current user is used. :param str callback_url: Optional Callback URL :return: None If the method is called asynchronously, returns the request thread.",1,0,0,1,2,1,0,0,1,2
"def new_random_wallet(cls, user_entropy=None, network=BitcoinMainNet):
  seed = str(urandom(64))
  seed += str(int(time.time()*10**6))
  if user_entropy:
  user_entropy = str(user_entropy)
  seed += user_entropy
  return cls.from_master_secret(seed, network=network)","Generate a new wallet using a randomly generated 512 bit seed. Args: user_entropy: Optional user-supplied entropy which is combined combined with the random seed, to help counteract compromised PRNGs. You are encouraged to add an optional `user_entropy` string to protect against a compromised CSPRNG. This will be combined with the output from the CSPRNG. Note that if you do supply this value it only adds additional entropy and will not be sufficient to recover the random wallet. If you're even saving `user_entropy` at all, you're doing it wrong.",1,0,0,0,1,0,0,0,1,1
"def create(self, validated_data):
  email = validated_data.pop(""email"")
  password = validated_data.pop(""password"")
  user = get_user_model()(**validated_data)
  user.set_password(password)
  user.email = email
  email_query = models.EmailAddress.objects.filter(email=email)
  if email_query.exists():
  existing_email = email_query.get()
  existing_email.send_duplicate_notification()
  else:
  user.save()
  email_instance = models.EmailAddress.objects.create(
  email=email, user=user
  )
  email_instance.send_confirmation()
  signals.user_registered.send(sender=self.__class__, user=user)
  return user","Create a new user from the data passed to the serializer. If the provided email has not been verified yet, the user is created and a verification email is sent to the address. Otherwise we send a notification to the email address that someone attempted to register with an email that's already been verified. Args: validated_data (dict): The data passed to the serializer after it has been validated. Returns: A new user created from the provided data.",1,1,1,0,3,1,1,1,1,4
"def list_menu_multi(self, options, title=""Choose one or more values"", message=""Choose one or more values"",
  defaults: list=None, **kwargs):
  if defaults is None:
  defaults = []
  choices = []
  optionNum = 0
  for option in options:
  choices.append(str(optionNum))
  choices.append(option)
  if option in defaults:
  choices.append(""on"")
  else:
  choices.append(""off"")
  optionNum += 1
  return_code, output = self._run_kdialog(title, [""--separate-output"", ""--checklist"", message] + choices, kwargs)
  results = output.split()
  choices = [options[int(choice_index)] for choice_index in results]
  return DialogData(return_code, choices)","Show a multiple-selection list menu Usage: C{dialog.list_menu_multi(options, title=""Choose one or more values"", message=""Choose one or more values"", defaults=[], **kwargs)} @param options: list of options (strings) for the dialog @param title: window title for the dialog @param message: message displayed above the list @param defaults: list of default values to be selected @return: a tuple containing the exit code and user choice @rtype: C{DialogData(int, List[str])}",1,0,0,1,2,1,0,0,1,2
"def set(self,
  agent_id,
  name=None,
  description=None,
  redirect_domain=None,
  logo_media_id=None,
  report_location_flag=0,
  is_report_user=True,
  is_report_enter=True):
  agent_data = optionaldict()
  agent_data['agentid'] = agent_id
  agent_data['name'] = name
  agent_data['description'] = description
  agent_data['redirect_domain'] = redirect_domain
  agent_data['logo_mediaid'] = logo_media_id
  agent_data['report_location_flag'] = report_location_flag
  agent_data['isreportenter'] = 1 if is_report_enter else 0
  agent_data['isreportuser'] = 1 if is_report_user else 0
  return self._post(
  'agent/set',
  data=agent_data
  )", https://work.weixin.qq.com/api/doc#90000/90135/90228 :param agent_id: id :param name: 32utf8 :param description: 4120utf8 :param redirect_domain:  jssdk 85005 :param logo_media_id: mediaid  mediaid  :param report_location_flag:  01 :param is_report_enter: 01 :param is_report_user: 01 :return:  JSON ,1,0,0,2,3,1,0,0,1,2
"def search(self,
  start=1,
  num=10):
  url = self.location
  params = {
  ""f"" : ""json"",
  ""num"" : num,
  ""start"" : start
  }
  return self._get(url=url,
  param_dict=params,
  securityHandler=self._securityHandler,
  proxy_url=self._proxy_url,
  proxy_port=self._proxy_port)","Returns the items for the current location of the user's content Inputs: start - The number of the first entry in the result set response. The index number is 1-based. The default value of start is 1 (that is, the first search result). The start parameter, along with the num parameter, can be used to paginate the search results. num - The maximum number of results to be included in the result set response. The default value is 10, and the maximum allowed value is 100. The start parameter, along with the num parameter, can be used to paginate the search results. Output: returns a list of dictionary",1,0,0,1,2,2,0,0,1,3
"def read_namespaced_cron_job(self, name, namespace, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.read_namespaced_cron_job_with_http_info(name, namespace, **kwargs)
  else:
  (data) = self.read_namespaced_cron_job_with_http_info(name, namespace, **kwargs)
  return data","read the specified CronJob This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.read_namespaced_cron_job(name, namespace, async_req=True) >>> result = thread.get() :param async_req bool :param str name: name of the CronJob (required) :param str namespace: object name and auth scope, such as for teams and projects (required) :param str pretty: If 'true', then the output is pretty printed. :param bool exact: Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. Deprecated. Planned for removal in 1.18. :param bool export: Should this value be exported. Export strips fields that a user can not specify. Deprecated. Planned for removal in 1.18. :return: V2alpha1CronJob If the method is called asynchronously, returns the request thread.",2,0,0,1,3,1,0,0,1,2
"def get_game_high_scores(
  self,
  user_id: Union[int, str],
  chat_id: Union[int, str],
  message_id: int = None
  ):
  return pyrogram.GameHighScores._parse(
  self,
  self.send(
  functions.messages.GetGameHighScores(
  peer=self.resolve_peer(chat_id),
  id=message_id,
  user_id=self.resolve_peer(user_id)
  )
  )
  )","Use this method to get data for high score tables. Args: user_id (``int`` | ``str``): Unique identifier (int) or username (str) of the target chat. For your personal cloud (Saved Messages) you can simply use ""me"" or ""self"". For a contact that exists in your Telegram address book you can use his phone number (str). chat_id (``int`` | ``str``, *optional*): Unique identifier (int) or username (str) of the target chat. For your personal cloud (Saved Messages) you can simply use ""me"" or ""self"". For a contact that exists in your Telegram address book you can use his phone number (str). Required if inline_message_id is not specified. message_id (``int``, *optional*): Identifier of the sent message. Required if inline_message_id is not specified. Returns: On success, a :obj:`GameHighScores <pyrogram.GameHighScores>` object is returned. Raises: :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.",2,0,0,1,3,2,0,0,1,3
"async def get_game_high_scores(self, user_id: base.Integer,
  chat_id: typing.Union[base.Integer, None] = None,
  message_id: typing.Union[base.Integer, None] = None,
  inline_message_id: typing.Union[base.String,
  None] = None) -> typing.List[types.GameHighScore]:
  payload = generate_payload(**locals())
  result = await self.request(api.Methods.GET_GAME_HIGH_SCORES, payload)
  return [types.GameHighScore(**gamehighscore) for gamehighscore in result]","Use this method to get data for high score tables. This method will currently return scores for the target user, plus two of his closest neighbors on each side. Will also return the top three users if the user and his neighbors are not among them. Please note that this behavior is subject to change. Source: https://core.telegram.org/bots/api#getgamehighscores :param user_id: Target user id :type user_id: :obj:`base.Integer` :param chat_id: Required if inline_message_id is not specified. Unique identifier for the target chat :type chat_id: :obj:`typing.Union[base.Integer, None]` :param message_id: Required if inline_message_id is not specified. Identifier of the sent message :type message_id: :obj:`typing.Union[base.Integer, None]` :param inline_message_id: Required if chat_id and message_id are not specified. Identifier of the inline message :type inline_message_id: :obj:`typing.Union[base.String, None]` :return: Will return the score of the specified user and several of his neighbors in a game On success, returns an Array of GameHighScore objects. This method will currently return scores for the target user, plus two of his closest neighbors on each side. Will also return the top three users if the user and his neighbors are not among them. :rtype: :obj:`typing.List[types.GameHighScore]`",1,0,0,1,2,2,0,0,1,3
"def list_instances(i_info, param_str, numbered=False):
  print(param_str)
  for i in i_info:
  if numbered:
  print(""Instance {}
  print("" {6}Name: {1}{3:<22}{1}ID: {0}{4:<20}{1:<18}Status: {2}{5}{1}"".
  format(C_TI, C_NORM, C_STAT[i_info[i]['state']],
  i_info[i]['tag']['Name'], i_info[i]['id'],
  i_info[i]['state'], C_HEAD2))
  print("" AMI: {0}{2:<23}{1}AMI Name: {0}{3:.41}{1}"".
  format(C_TI, C_NORM, i_info[i]['ami'], i_info[i]['aminame']))
  list_tags(i_info[i]['tag'])
  debg.dprintx(""All Data"")
  debg.dprintx(i_info, True)","Display a list of all instances and their details. Iterates through all the instances in the dict, and displays information for each instance. Args: i_info (dict): information on instances and details. param_str (str): the title to display before the list. numbered (bool): optional - indicates wheter the list should be displayed with numbers before each instance. This is used when called from user_picklist.",1,0,0,1,2,1,0,0,1,2
"def get_starred_repos(self):
  starred_url = self.api_url + ""/starred""
  keep_finding = True
  current_page = 1
  repos_list = []
  while keep_finding:
  api_url = starred_url + ""?page="" + str(
  current_page)
  api_driver = GithubRawApi(
  api_url,
  True
  )
  for repo in api_driver:
  repo_username = repo[""owner""][""login""]
  repo_name = repo[""name""]
  repos_list.append(
  GithubUserRepository(repo_username, repo_name))
  if len(api_driver.api_content) < 1:
  keep_finding = False
  current_page += 1
  return repos_list",Gets repos starred by user :return: List of starred repositories,2,0,0,1,3,2,0,0,1,3
"def migrate_repo(self, auth, clone_addr,
  uid, repo_name, auth_username=None, auth_password=None,
  mirror=False, private=False, description=None):
  data = {
  ""clone_addr"": clone_addr,
  ""uid"": uid,
  ""repo_name"": repo_name,
  ""mirror"": mirror,
  ""private"": private,
  ""description"": description,
  }
  data = {k: v for (k, v) in data.items() if v is not None}
  url = ""/repos/migrate""
  response = self.post(url, auth=auth, data=data)
  return GogsRepo.from_json(response.json())",Migrate a repository from another Git hosting source for the authenticated user. :param auth.Authentication auth: authentication object :param str clone_addr: Remote Git address (HTTP/HTTPS URL or local path) :param int uid: user ID of repository owner :param str repo_name: Repository name :param bool mirror: Repository will be a mirror. Default is false :param bool private: Repository will be private. Default is false :param str description: Repository description :return: a representation of the migrated repository :rtype: GogsRepo :raises NetworkFailure: if there is an error communicating with the server :raises ApiFailure: if the request cannot be serviced,1,0,0,2,3,1,0,0,1,2
"def list(self, orgId=None, **request_parameters):
  check_type(orgId, basestring)
  params = dict_from_items_with_values(
  request_parameters,
  orgId=orgId,
  )
  items = self._session.get_items(API_ENDPOINT, params=params)
  for item in items:
  yield self._object_factory(OBJECT_TYPE, item)","List all licenses for a given organization. If no orgId is specified, the default is the organization of the authenticated user. Args: orgId(basestring): Specify the organization, by ID. **request_parameters: Additional request parameters (provides support for parameters that may be added in the future). Returns: GeneratorContainer: A GeneratorContainer which, when iterated, yields the licenses returned by the Webex Teams query. Raises: TypeError: If the parameter types are incorrect. ApiError: If the Webex Teams cloud returns an error.",2,0,0,1,3,2,0,0,1,3
"def patch_namespaced_ingress(self, name, namespace, body, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.patch_namespaced_ingress_with_http_info(name, namespace, body, **kwargs)
  else:
  (data) = self.patch_namespaced_ingress_with_http_info(name, namespace, body, **kwargs)
  return data","partially update the specified Ingress This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.patch_namespaced_ingress(name, namespace, body, async_req=True) >>> result = thread.get() :param async_req bool :param str name: name of the Ingress (required) :param str namespace: object name and auth scope, such as for teams and projects (required) :param object body: (required) :param str pretty: If 'true', then the output is pretty printed. :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint. This field is required for apply requests (application/apply-patch) but optional for non-apply patch types (JsonPatch, MergePatch, StrategicMergePatch). :param bool force: Force is going to \""force\"" Apply requests. It means user will re-acquire conflicting fields owned by other people. Force flag must be unset for non-apply patch requests. :return: NetworkingV1beta1Ingress If the method is called asynchronously, returns the request thread.",0,0,0,2,2,1,0,0,1,2
"def get_scores(self, beatmap_id, *, username=None, mode=OsuMode.osu, mods=None, limit=50):
  return self._make_req(endpoints.SCORES, dict(
  k=self.key,
  b=beatmap_id,
  u=username,
  type=_username_type(username),
  m=mode.value,
  mods=mods.value if mods else None,
  limit=limit), JsonList(BeatmapScore))","Get the top scores for a given beatmap. Parameters ---------- beatmap_id Individual Beatmap ID to lookup. username : str or int A `str` representing the user's username, or an `int` representing the user's id. If specified, restricts returned scores to the specified user. mode : :class:`osuapi.enums.OsuMode` The osu! game mode for which to look up. Defaults to osu!standard. mods : :class:`osuap:class:`osuapi.enums.OsuMod` If specified, restricts returned scores to the specified mods. limit Number of results to return. Defaults to 50, maximum 100.",2,0,0,1,3,1,0,0,1,2
"def collect(self, step, content):
  if step.startswith('user_message'):
  print(content)
  elif step.startswith('user_prompt'):
  self.collect_argument(step, content)
  elif step == 'record_asciinema':
  self.record_asciinema()
  elif step == ""record_environment"":
  self.record_environment()
  bot.debug(self.data)","given a name of a configuration key and the provided content, collect the required metadata from the user. Parameters ========== step: the key in the configuration. Can be one of: user_message_<name> runtime_arg_<name> record_asciinema record_environment user_prompt_<name> content: the default value or boolean to indicate doing the step.",1,0,0,1,2,1,0,0,1,2
"def get_access(self, id_access):
  if not is_valid_int_param(id_access):
  raise InvalidParameterError(u'Equipment Access ID is invalid.')
  url = 'equipamentoacesso/id/' + str(id_access) + '/'
  code, xml = self.submit(None, 'GET', url)
  return self.response(code, xml)","Get Equipment Access by id. :return: Dictionary with following: :: {'equipamento_acesso': {'id_equipamento': < id_equipamento >, 'fqdn': < fqdn >, 'user': < user >, 'pass': < pass >, 'id_tipo_acesso': < id_tipo_acesso >, 'enable_pass': < enable_pass >}}",2,0,0,1,3,2,0,0,1,3
"def verify(self, request, **kwargs):
  if isinstance(request, six.string_types):
  _dict = parse_qs(request)
  elif isinstance(request, dict):
  _dict = request
  else:
  raise ValueError(""Wrong type of input"")
  try:
  self._verify(_dict[""password""][0], _dict[""login""][0])
  timestamp = str(int(time.mktime(time.gmtime())))
  msg = ""::"".join([_dict[""login""][0], timestamp])
  info = self.symmetric.encrypt(msg.encode())
  self.active[info] = timestamp
  cookie = make_cookie(self.cookie_name, info, self.srv.seed)
  return_to = create_return_url(self.return_to, _dict[""query""][0],
  **{self.query_param: ""true""})
  resp = Redirect(return_to, headers=[cookie])
  except (ValueError, KeyError):
  resp = Unauthorized(""Unknown user or wrong password"")
  return resp",Verifies that the given username and password was correct :param request: Either the query part of a URL a urlencoded body of a HTTP message or a parse such. :param kwargs: Catch whatever else is sent. :return: redirect back to where ever the base applications wants the user after authentication.,1,0,0,2,3,1,0,0,1,2
"def create(self, chat_id, name, owner, user_list):
  return self._post(
  'chat/create',
  data={
  'chatid': chat_id,
  'name': name,
  'owner': owner,
  'userlist': user_list,
  }
  )","  https://qydev.weixin.qq.com/wiki/index.php?title= :param chat_id: id320-9a-zA-Z, 64bit [1, 2^63) [2^63, 2^64)id :param name:   :param owner: useriduserlist :param user_list: userid 31000 :return:  JSON ",1,0,0,1,2,1,0,0,2,3
"def update_bookmark(self, bookmark_id, favorite=None, archive=None, read_percent=None):
  rdb_url = self._generate_url('bookmarks/{0}'.format(bookmark_id))
  params = {}
  if favorite is not None:
  params['favorite'] = 1 if favorite == True else 0
  if archive is not None:
  params['archive'] = 1 if archive == True else 0
  if read_percent is not None:
  try:
  params['read_percent'] = float(read_percent)
  except ValueError:
  pass
  return self.post(rdb_url, params)","Updates given bookmark. The requested bookmark must belong to the current user. :param bookmark_id: ID of the bookmark to update. :param favorite (optional): Whether this article is favorited or not. :param archive (optional): Whether this article is archived or not. :param read_percent (optional): The read progress made in this article, where 1.0 means the bottom and 0.0 means the very top.",1,0,0,1,2,2,0,0,1,3
"def lcopt_bw2_setup(ecospold_path, overwrite=False, db_name=None):
  default_ei_name = ""Ecoinvent3_3_cutoff""
  if db_name is None:
  db_name = DEFAULT_PROJECT_STEM + default_ei_name
  if db_name in bw2.projects:
  if overwrite:
  bw2.projects.delete_project(name=db_name, delete_dir=True)
  else:
  print('Looks like bw2 is already set up - if you want to overwrite the existing version run lcopt.utils.lcopt_bw2_setup in a python shell using overwrite = True')
  return False
  bw2.projects.set_current(db_name)
  bw2.bw2setup()
  ei = bw2.SingleOutputEcospold2Importer(fix_mac_path_escapes(ecospold_path), default_ei_name)
  ei.apply_strategies()
  ei.statistics()
  ei.write_database()
  return True","Utility function to set up brightway2 to work correctly with lcopt. It requires the path to the ecospold files containing the Ecoinvent 3.3 cutoff database. If you don't have these files, log into `ecoinvent.org <http://www.ecoinvent.org/login-databases.html>`_ and go to the Files tab Download the file called ``ecoinvent 3.3_cutoff_ecoSpold02.7z`` Extract the file somewhere sensible on your machine, you might need to download `7-zip <http://www.7-zip.org/download.html>`_ to extract the files. Make a note of the path of the folder that contains the .ecospold files, its probably ``<path/extracted/to>/datasets/`` Use this path (as a string) as the first parameter in this function To overwrite an existing version, set overwrite=True",1,0,0,0,1,1,0,0,0,1
"def user_id(self, user):
  headers = {""Content-type"": ""application/x-www-form-urlencoded"",""Accept"": ""text/plain"",'Referer': 'http://'+self.domain+'/team_news.phtml',""User-Agent"": user_agent}
  req = self.session.get('http://'+self.domain+'/standings.phtml',headers=headers).content
  soup = BeautifulSoup(req)
  for i in soup.find('table',cellpadding=2).find_all('tr'):
  try:
  if (user == i.find_all('td')[2].text.encode('utf8')):
  return i.find('a')['href'].split('pid=')[1]
  except:
  continue
  return None",Get userid from a name @return: id,1,0,0,1,2,2,0,0,1,3
"def _OpenDatabaseWithWAL(
  self, parser_mediator, database_file_entry, database_file_object,
  filename):
  path_spec = database_file_entry.path_spec
  location = getattr(path_spec, 'location', None)
  if not path_spec or not location:
  return None, None
  location_wal = '{0:s}-wal'.format(location)
  file_system = database_file_entry.GetFileSystem()
  wal_path_spec = dfvfs_factory.Factory.NewPathSpec(
  file_system.type_indicator, parent=path_spec.parent,
  location=location_wal)
  wal_file_entry = file_system.GetFileEntryByPathSpec(wal_path_spec)
  if not wal_file_entry:
  return None, None
  wal_file_object = wal_file_entry.GetFileObject()
  if not wal_file_object:
  return None, None
  database_wal = SQLiteDatabase(
  filename, temporary_directory=parser_mediator.temporary_directory)
  try:
  database_wal.Open(database_file_object, wal_file_object=wal_file_object)
  except (IOError, ValueError, sqlite3.DatabaseError) as exception:
  parser_mediator.ProduceExtractionWarning((
  'unable to open SQLite database and WAL with error: '
  '{0!s}').format(exception))
  return None, None
  finally:
  wal_file_object.close()
  return database_wal, wal_file_entry",Opens a database with its Write-Ahead Log (WAL) committed. Args: parser_mediator (ParserMediator): parser mediator. database_file_entry (dfvfs.FileEntry): file entry of the database. database_file_object (dfvfs.FileIO): file-like object of the database. filename (str): name of the database file entry. Returns: tuple: contains: SQLiteDatabase: a database object with WAL file committed or None dfvfs.FileEntry: a file entry object of WAL file or None,1,0,1,0,2,1,0,0,1,2
"def extractArticleInfo(self, url, proxyUrl = None, headers = None, cookies = None):
  params = { ""url"": url }
  if proxyUrl:
  params[""proxyUrl""] = proxyUrl
  if headers:
  if isinstance(headers, dict):
  headers = json.dumps(headers)
  params[""headers""] = headers
  if cookies:
  if isinstance(cookies, dict):
  cookies = json.dumps(cookies)
  params[""cookies""] = cookies
  return self._er.jsonRequestAnalytics(""/api/v1/extractArticleInfo"", params)","extract all available information about an article available at url `url`. Returned information will include article title, body, authors, links in the articles, ... @param url: article url to extract article information from @param proxyUrl: proxy that should be used for downloading article information. format: {schema}://{username}:{pass}@{proxy url/ip} @param headers: dict with headers to set in the request (optional) @param cookies: dict with cookies to set in the request (optional) @returns: dict",2,0,0,1,3,2,0,0,1,3
"def add_group(group_name, system_group=False, gid=None):
  try:
  group_info = grp.getgrnam(group_name)
  log('group {0} already exists!'.format(group_name))
  if gid:
  group_info = grp.getgrgid(gid)
  log('group with gid {0} already exists!'.format(gid))
  except KeyError:
  log('creating group {0}'.format(group_name))
  add_new_group(group_name, system_group, gid)
  group_info = grp.getgrnam(group_name)
  return group_info","Add a group to the system Will log but otherwise succeed if the group already exists. :param str group_name: group to create :param bool system_group: Create system group :param int gid: GID for user being created :returns: The password database entry struct, as returned by `grp.getgrnam`",1,0,0,1,2,1,0,0,1,2
"def get(property_name):
  config = _read_config(_USER_CONFIG_FILE)
  section = _MAIN_SECTION_NAME
  try:
  property_value = config.get(section, property_name)
  except (NoOptionError, NoSectionError) as error:
  try:
  config = _read_config(_SYSTEM_CONFIG_FILE)
  property_value = config.get(section, property_name)
  except (NoOptionError, NoSectionError) as error:
  raise NoConfigOptionError(error)
  return property_value",Returns the value of the specified configuration property. Property values stored in the user configuration file take precedence over values stored in the system configuration file. :param property_name: The name of the property to retrieve. :return: The value of the property.,0,0,0,1,1,1,0,0,1,2
"def prepare_filenames(self, normalized_url, request):
  filenames = [normalized_url]
  if request.user.is_authenticated():
  filenames.insert(0, normalized_url + "".authenticated"")
  else:
  filenames.insert(0, normalized_url + "".anonymous"")
  return filenames","Prepare template filename list based on the user authenticated state If user is authenticated user, it use '_authenticated' as a suffix. Otherwise it use '_anonymous' as a suffix to produce the template filename list. The list include original filename at the end of the list. Args: normalized_url (str): A normalized url request (instance): An instance of HttpRequest Returns: list Examples: >>> from mock import MagicMock >>> request = MagicMock() >>> backend = AuthTemplateFilenameBackend() >>> request.user.is_authenticated.return_value = True >>> filenames = backend.prepare_filenames('foo/bar/hogehoge', ... request) >>> assert filenames == [ ... 'foo/bar/hogehoge_authenticated.html', ... 'foo/bar/hogehoge.html' ... ] >>> request.user.is_authenticated.return_value = False >>> filenames = backend.prepare_filenames('foo/bar/hogehoge', ... request) >>> assert filenames == [ ... 'foo/bar/hogehoge_anonymous.html', ... 'foo/bar/hogehoge.html' ... ] >>> request.user.is_authenticated.return_value = True >>> filenames = backend.prepare_filenames('', ... request) >>> assert filenames == [ ... 'index_authenticated.html', ... 'index.html' ... ] >>> request.user.is_authenticated.return_value = False >>> filenames = backend.prepare_filenames('', ... request) >>> assert filenames == [ ... 'index_anonymous.html', ... 'index.html' ... ]",1,0,0,1,2,1,0,0,1,2
"def delete_sessions(self, uid):
  title = '%s.delete_session' % self.__class__.__name__
  input_fields = {
  'uid': uid
  }
  for key, value in input_fields.items():
  object_title = '%s(%s=%s)' % (title, key, str(value))
  self.fields.validate(value, '.%s' % key, object_title)
  url = self.bucket_url + '/_user/%s/_session' % uid
  response = requests.delete(url)
  return response.status_code",a method to delete all session tokens associated with a user :param uid: string with id of user in bucket :return: integer with status code of delete operation,1,0,0,1,2,1,0,0,1,2
"def _get_captcha(reddit_session, captcha_id):
  url = urljoin(reddit_session.config['captcha'],
  captcha_id + '.png')
  sys.stdout.write('Captcha URL: {0}\nCaptcha: '.format(url))
  sys.stdout.flush()
  raw = sys.stdin.readline()
  if not raw:
  sys.stdin.close()
  return None
  return {'iden': captcha_id, 'captcha': raw.strip()}",Prompt user for captcha solution and return a prepared result.,1,0,0,1,2,1,0,0,1,2
"def blank_tiles(input_word):
  blanks = 0
  questions = 0
  input_letters = []
  for letter in input_word:
  if letter == ""_"":
  blanks += 1
  elif letter == ""?"":
  questions += 1
  else:
  input_letters.append(letter)
  return input_letters, blanks, questions","Searches a string for blank tile characters (""?"" and ""_""). Args: input_word: the user supplied string to search through Returns: a tuple of: input_word without blanks integer number of blanks (no points) integer number of questions (points)",1,0,0,1,2,1,0,0,1,2
"def is_user_changing_own_key(self, req, user):
  admin_detail = self.get_admin_detail(req)
  if not admin_detail:
  return False
  if '.admin' not in (g['name'] for g in admin_detail['groups']):
  if req.headers.get('x-auth-user-admin') == 'true' or \
  req.headers.get('x-auth-user-reseller-admin') == 'true':
  return False
  if '.reseller_admin' not in \
  (g['name'] for g in admin_detail['groups']) and \
  req.headers.get('x-auth-user-reseller-admin') == 'true':
  return False
  return req.headers.get('x-auth-admin-user') == user and \
  self.credentials_match(admin_detail,
  req.headers.get('x-auth-admin-key'))","Check if the user is changing his own key. :param req: The swob.Request to check. This contains x-auth-admin-user and x-auth-admin-key headers which are credentials of the user sending the request. :param user: User whose password is to be changed. :returns: True if user is changing his own key, False if not.",1,0,1,0,2,1,0,0,1,2
"def can_perform_awarding(self):
  if not self.user_ids:
  logger.debug(
  ' Badge %s: no users to check (empty user_ids property)',
  self.slug)
  return False
  if not self.badge:
  logger.debug(
  ' Badge %s: does not exist in the database (run badgify_sync badges)',
  self.slug)
  return False
  return True","Checks if we can perform awarding process (is ``user_ids`` property defined? Does Badge object exists? and so on). If we can perform db operations safely, returns ``True``. Otherwise, ``False``.",0,0,1,0,1,1,0,1,1,3
"def parse_config_file():
  for filename in ('.tagcube', os.path.expanduser('~/.tagcube')):
  filename = os.path.abspath(filename)
  if not os.path.exists(filename):
  msg = 'TagCube configuration file ""%s"" does not exist'
  cli_logger.debug(msg % filename)
  continue
  msg = 'Parsing tagcube configuration file ""%s""'
  cli_logger.debug(msg % filename)
  email, api_key = _parse_config_file_impl(filename)
  if email is not None and api_key is not None:
  msg = ('Found authentication credentials:\n'
  ' email: %s\n'
  ' api_key: %s')
  tokenized_api_key = '%s...%s' % (api_key[:3], api_key[-3:])
  args = (email, tokenized_api_key)
  cli_logger.debug(msg % args)
  return email, api_key
  else:
  msg = 'Configuration file does not contain credentials'
  cli_logger.debug(msg)
  else:
  return None, None","Find the .tagcube config file in the current directory, or in the user's home and parse it. The one in the current directory has precedence. :return: A tuple with: - email - api_token",2,0,0,1,3,1,0,0,1,2
"def open(cls, dbpath=Path(""~/.maas.db"").expanduser(),
  migrate_from=Path(""~/.maascli.db"").expanduser()):
  dbpath = Path(dbpath)
  migrate_from = Path(migrate_from)
  migrate = migrate_from.is_file() and not dbpath.exists()
  dbpath.touch(mode=0o600, exist_ok=True)
  migrate = migrate and not migrate_from.samefile(dbpath)
  database = sqlite3.connect(str(dbpath))
  try:
  store = cls(database)
  if migrate:
  schema_import(database, migrate_from)
  yield store
  else:
  yield store
  except:
  raise
  else:
  database.commit()
  finally:
  database.close()","Load a profiles database. Called without arguments this will open (and create) a database in the user's home directory. **Note** that this returns a context manager which will close the database on exit, saving if the exit is clean. :param dbpath: The path to the database file to create and open. :param migrate_from: Path to a database file to migrate from.",1,1,1,1,4,0,1,1,0,2
"def iter_dialogs(
  self,
  offset_date: int = 0,
  limit: int = 0
  ) -> Generator[""pyrogram.Dialog"", None, None]:
  current = 0
  total = limit or (1 << 31) - 1
  limit = min(100, total)
  pinned_dialogs = self.get_dialogs(
  pinned_only=True
  ).dialogs
  for dialog in pinned_dialogs:
  yield dialog
  current += 1
  if current >= total:
  return
  while True:
  dialogs = self.get_dialogs(
  offset_date=offset_date,
  limit=limit
  ).dialogs
  if not dialogs:
  return
  offset_date = dialogs[-1].top_message.date
  for dialog in dialogs:
  yield dialog
  current += 1
  if current >= total:
  return","Use this method to iterate through a user's dialogs sequentially. This convenience method does the same as repeatedly calling :meth:`get_dialogs` in a loop, thus saving you from the hassle of setting up boilerplate code. It is useful for getting the whole dialogs list with a single call. Args: offset_date (``int``): The offset date in Unix time taken from the top message of a :obj:`Dialog`. Defaults to 0 (most recent dialog). limit (``str``, *optional*): Limits the number of dialogs to be retrieved. By default, no limit is applied and all dialogs are returned. Returns: A generator yielding :obj:`Dialog <pyrogram.Dialog>` objects. Raises: :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.",1,0,0,1,2,2,0,0,1,3
"def get_albums(self, limit=None):
  url = (self._imgur._base_url + ""/3/account/{0}/albums/{1}"".format(self.name,
  '{}'))
  resp = self._imgur._send_request(url, limit=limit)
  return [Album(alb, self._imgur, False) for alb in resp]",Return a list of the user's albums. Secret and hidden albums are only returned if this is the logged-in user.,2,0,0,1,3,2,0,0,1,3
"def load(self, df, centerings):
  centering_variables = dict()
  if not df.empty and df.geometry.notna().any():
  for key, func in centerings.items():
  centering_variables[key] = func(df)
  return getattr(ccrs, self.__class__.__name__)(**{**centering_variables, **self.args})","A moderately mind-bendy meta-method which abstracts the internals of individual projections' load procedures. Parameters ---------- proj : geoplot.crs object instance A disguised reference to ``self``. df : GeoDataFrame The GeoDataFrame which has been passed as input to the plotter at the top level. This data is needed to calculate reasonable centering variables in cases in which the user does not already provide them; which is, incidentally, the reason behind all of this funny twice-instantiation loading in the first place. centerings: dct A dictionary containing names and centering methods. Certain projections have certain centering parameters whilst others lack them. For example, the geospatial projection contains both ``central_longitude`` and ``central_latitude`` instance parameter, which together control the center of the plot, while the North Pole Stereo projection has only a ``central_longitude`` instance parameter, implying that latitude is fixed (as indeed it is, as this projection is centered on the North Pole!). A top-level centerings method is provided in each of the ``geoplot`` top-level plot functions; each of the projection wrapper classes defined here in turn selects the functions from this list relevent to this particular instance and passes them to the ``_generic_load`` method here. We then in turn execute these functions to get defaults for our ``df`` and pass them off to our output ``cartopy.crs`` instance. Returns ------- crs : ``cartopy.crs`` object instance Returns a ``cartopy.crs`` object instance whose appropriate instance variables have been set to reasonable defaults wherever not already provided by the user.",0,0,0,1,1,0,0,0,1,1
"def account_statuses(self, id, only_media=False, pinned=False, exclude_replies=False, max_id=None, min_id=None, since_id=None, limit=None):
  id = self.__unpack_id(id)
  if max_id != None:
  max_id = self.__unpack_id(max_id)
  if min_id != None:
  min_id = self.__unpack_id(min_id)
  if since_id != None:
  since_id = self.__unpack_id(since_id)
  params = self.__generate_params(locals(), ['id'])
  if pinned == False:
  del params[""pinned""]
  if only_media == False:
  del params[""only_media""]
  if exclude_replies == False:
  del params[""exclude_replies""]
  url = '/api/v1/accounts/{0}/statuses'.format(str(id))
  return self.__api_request('GET', url, params)","Fetch statuses by user `id`. Same options as `timeline()`_ are permitted. Returned toots are from the perspective of the logged-in user, i.e. all statuses visible to the logged-in user (including DMs) are included. If `only_media` is set, return only statuses with media attachments. If `pinned` is set, return only statuses that have been pinned. Note that as of Mastodon 2.1.0, this only works properly for instance-local users. If `exclude_replies` is set, filter out all statuses that are replies. Does not require authentication. Returns a list of `toot dicts`_.",2,0,0,1,3,2,0,0,1,3
"def hash_model_values(model, clear=True, hash_field='values_hash', hash_fun=hash, ignore_pk=True, ignore_fields=[]):
  qs = getattr(model, 'objects', model)
  model = qs.model
  if ignore_pk:
  ignore_fields += [model._meta.pk.name]
  if not hasattr(model, hash_field):
  warnings.warn(""%r doesn't have a field named %s in which to store a hash value. Skipping."" % (model, hash_field))
  return
  for obj in qs:
  h = hash_fun(tuple([getattr(obj, k) for k in obj._meta.get_all_field_names() if k not in ignore_fields]))
  tracking_obj, created = ChangeLog.get_or_create(app=model._meta.app_label, model=model._meta.object_name, primary_key=obj.pk)
  tracking_obj.update(hash_value=h)","Hash values of DB table records to facilitate tracking changes to the DB table Intended for comparing records in one table to those in another (with potentially differing id/pk values) For example, changes to a table in a read-only MS SQL database can be quickly identified and mirrored to a writeable PostGRE DB where these hash values are stored along side the data.",0,0,1,0,1,0,1,1,0,2
"def create_token(self, auth, name, username=None):
  if username is None:
  username = self.authenticated_user(auth).username
  data = {""name"": name}
  response = self.post(""/users/{u}/tokens"".format(u=username), auth=auth, data=data)
  return Token.from_json(response.json())","Creates a new token with the specified name for the specified user. If no user is specified, uses user authenticated by ``auth``. :param auth.Authentication auth: authentication for user to retrieve. Must be a username-password authentication, due to a restriction of the Gogs API :param str name: name of new token :param str username: username of owner of new token :return: new token representation :rtype: Token :raises NetworkFailure: if there is an error communicating with the server :raises ApiFailure: if the request cannot be serviced",2,0,0,2,4,2,0,0,1,3
"def translate_dialog_to_lists(dialog_filename):
  dialog_file = open(dialog_filename, 'r')
  dialog_reader = unicodecsv.reader(dialog_file, delimiter='\t', quoting=csv.QUOTE_NONE)
  first_turn = True
  dialog = []
  same_user_utterances = []
  dialog.append(same_user_utterances)
  for dialog_line in dialog_reader:
  if first_turn:
  last_user = dialog_line[1]
  first_turn = False
  if last_user != dialog_line[1]:
  same_user_utterances = []
  dialog.append(same_user_utterances)
  same_user_utterances.append(dialog_line[3])
  last_user = dialog_line[1]
  dialog.append([dialog_end_symbol])
  return dialog",Translates the dialog to a list of lists of utterances. In the first list each item holds subsequent utterances from the same user. The second level list holds the individual utterances. :param dialog_filename: :return:,1,0,0,1,2,1,0,0,1,2
"def publish_report(report, args, old_commit, new_commit):
  output = """"
  if not args.quiet and not args.gist and not args.file:
  return report
  if args.gist:
  gist_url = post_gist(report, old_commit, new_commit)
  output += ""\nReport posted to GitHub Gist: {0}"".format(gist_url)
  if args.file is not None:
  with open(args.file, 'w') as f:
  f.write(report.encode('utf-8'))
  output += ""\nReport written to file: {0}"".format(args.file)
  return output",Publish the RST report based on the user request.,1,0,0,2,3,1,0,0,1,2
"def epost(database, ids: List[str], webenv=False, api_key=False, email=False, **kwargs) -> Optional[EpostResult]:
  url = BASE_URL + f'epost.fcgi'
  id = ','.join(ids)
  url_params = f'db={database}&id={id}'
  url_params = check_webenv(webenv, url_params)
  url_params = check_api_key(api_key, url_params)
  url_params = check_email(email, url_params)
  resp = entrez_try_put_multiple_times(url, url_params, num_tries=3)
  time.sleep(.5)
  return parse_epost(resp.text)",Post IDs using the Entrez ESearch API. Parameters ---------- database : str Entez database to search. ids : list List of IDs to submit to the server. webenv : str An Entrez WebEnv to post ids to. api_key : str A users API key which allows more requests per second email : str A users email which is required if not using API. Returns ------- requests.Response,0,0,0,1,1,2,0,0,1,3
"def get_account_user(self, account_id, user_id, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('asynchronous'):
  return self.get_account_user_with_http_info(account_id, user_id, **kwargs)
  else:
  (data) = self.get_account_user_with_http_info(account_id, user_id, **kwargs)
  return data","Details of the user. # noqa: E501 An endpoint for retrieving details of the user. **Example usage:** `curl https://api.us-east-1.mbedcloud.com/v3/accounts/{accountID}/users/{userID} -H 'Authorization: Bearer API_KEY'` # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass asynchronous=True >>> thread = api.get_account_user(account_id, user_id, asynchronous=True) >>> result = thread.get() :param asynchronous bool :param str account_id: Account ID. (required) :param str user_id: The ID of the user to be retrieved. (required) :return: UserInfoResp If the method is called asynchronously, returns the request thread.",2,0,0,1,3,2,0,0,1,3
"def from_utc_timestamp(timestamp, tz):
  warnings.warn(""Deprecated in 3.0. See SPARK-25496"", DeprecationWarning)
  sc = SparkContext._active_spark_context
  if isinstance(tz, Column):
  tz = _to_java_column(tz)
  return Column(sc._jvm.functions.from_utc_timestamp(_to_java_column(timestamp), tz))","This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in UTC, and renders that timestamp as a timestamp in the given time zone. However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not timezone-agnostic. So in Spark this function just shift the timestamp value from UTC timezone to the given timezone. This function may return confusing result if the input is a string with timezone, e.g. '2018-03-13T06:18:23+00:00'. The reason is that, Spark firstly cast the string to timestamp according to the timezone in the string, and finally display the result by converting the timestamp to string according to the session local timezone. :param timestamp: the column that contains timestamps :param tz: a string that has the ID of timezone, e.g. ""GMT"", ""America/Los_Angeles"", etc .. versionchanged:: 2.4 `tz` can take a :class:`Column` containing timezone ID strings. >>> df = spark.createDataFrame([('1997-02-28 10:30:00', 'JST')], ['ts', 'tz']) >>> df.select(from_utc_timestamp(df.ts, ""PST"").alias('local_time')).collect() [Row(local_time=datetime.datetime(1997, 2, 28, 2, 30))] >>> df.select(from_utc_timestamp(df.ts, df.tz).alias('local_time')).collect() [Row(local_time=datetime.datetime(1997, 2, 28, 19, 30))] .. note:: Deprecated in 3.0. See SPARK-25496",1,0,0,1,2,0,0,0,1,1
"def get_forced_variation(self, experiment, user_id):
  forced_variations = experiment.forcedVariations
  if forced_variations and user_id in forced_variations:
  variation_key = forced_variations.get(user_id)
  variation = self.config.get_variation_from_key(experiment.key, variation_key)
  if variation:
  self.logger.info('User ""%s"" is forced in variation ""%s"".' % (user_id, variation_key))
  return variation
  return None",Determine if a user is forced into a variation for the given experiment and return that variation. Args: experiment: Object representing the experiment for which user is to be bucketed. user_id: ID for the user. Returns: Variation in which the user with ID user_id is forced into. None if no variation.,0,0,1,1,2,1,0,0,1,2
"def prerank(rnk, gene_sets, outdir='GSEA_Prerank', pheno_pos='Pos', pheno_neg='Neg',
  min_size=15, max_size=500, permutation_num=1000, weighted_score_type=1,
  ascending=False, processes=1, figsize=(6.5,6), format='pdf',
  graph_num=20, no_plot=False, seed=None, verbose=False):
  pre = Prerank(rnk, gene_sets, outdir, pheno_pos, pheno_neg,
  min_size, max_size, permutation_num, weighted_score_type,
  ascending, processes, figsize, format, graph_num, no_plot, seed, verbose)
  pre.run()
  return pre","Run Gene Set Enrichment Analysis with pre-ranked correlation defined by user. :param rnk: pre-ranked correlation table or pandas DataFrame. Same input with ``GSEA`` .rnk file. :param gene_sets: Enrichr Library name or .gmt gene sets file or dict of gene sets. Same input with GSEA. :param outdir: results output directory. :param int permutation_num: Number of permutations for significance computation. Default: 1000. :param int min_size: Minimum allowed number of genes from gene set also the data set. Default: 15. :param int max_size: Maximum allowed number of genes from gene set also the data set. Defaults: 500. :param str weighted_score_type: Refer to :func:`algorithm.enrichment_score`. Default:1. :param bool ascending: Sorting order of rankings. Default: False. :param int processes: Number of Processes you are going to use. Default: 1. :param list figsize: Matplotlib figsize, accept a tuple or list, e.g. [width,height]. Default: [6.5,6]. :param str format: Matplotlib figure format. Default: 'pdf'. :param int graph_num: Plot graphs for top sets of each phenotype. :param bool no_plot: If equals to True, no figure will be drawn. Default: False. :param seed: Random seed. expect an integer. Default:None. :param bool verbose: Bool, increase output verbosity, print out progress of your job, Default: False. :return: Return a Prerank obj. All results store to a dictionary, obj.results, where contains:: | {es: enrichment score, | nes: normalized enrichment score, | p: P-value, | fdr: FDR, | size: gene set size, | matched_size: genes matched to the data, | genes: gene names from the data set | ledge_genes: leading edge genes}",0,0,0,1,1,1,0,0,1,2
"def get_activities(self, before=None, after=None, limit=None):
  if before:
  before = self._utc_datetime_to_epoch(before)
  if after:
  after = self._utc_datetime_to_epoch(after)
  params = dict(before=before, after=after)
  result_fetcher = functools.partial(self.protocol.get,
  '/athlete/activities',
  **params)
  return BatchedResultsIterator(entity=model.Activity,
  bind_client=self,
  result_fetcher=result_fetcher,
  limit=limit)",Get activities for authenticated user sorted by newest first. http://strava.github.io/api/v3/activities/ :param before: Result will start with activities whose start date is before specified date. (UTC) :type before: datetime.datetime or str or None :param after: Result will start with activities whose start date is after specified value. (UTC) :type after: datetime.datetime or str or None :param limit: How many maximum activities to return. :type limit: int or None :return: An iterator of :class:`stravalib.model.Activity` objects. :rtype: :class:`BatchedResultsIterator`,1,0,0,1,2,1,0,0,1,2
"def getScienceMetadataRDF(self, pid):
  url = ""{url_base}/scimeta/{pid}/"".format(url_base=self.url_base, pid=pid)
  r = self._request('GET', url)
  if r.status_code != 200:
  if r.status_code == 403:
  raise HydroShareNotAuthorized(('GET', url))
  elif r.status_code == 404:
  raise HydroShareNotFound((pid,))
  else:
  raise HydroShareHTTPException((url, 'GET', r.status_code))
  return str(r.content)","Get science metadata for a resource in XML+RDF format :param pid: The HydroShare ID of the resource :raises: HydroShareNotAuthorized if the user is not authorized to view the metadata. :raises: HydroShareNotFound if the resource was not found. :raises: HydroShareHTTPException to signal an HTTP error. :return: A string representing the XML+RDF serialization of science metadata. Example of data XML+RDF returned: <?xml version=""1.0""?> <!DOCTYPE rdf:RDF PUBLIC ""-//DUBLIN CORE//DCMES DTD 2002/07/31//EN"" ""http://dublincore.org/documents/2002/07/31/dcmes-xml/dcmes-xml-dtd.dtd""> <rdf:RDF xmlns:dc=""http://purl.org/dc/elements/1.1/"" xmlns:dcterms=""http://purl.org/dc/terms/"" xmlns:hsterms=""http://hydroshare.org/terms/"" xmlns:rdf=""http://www.w3.org/1999/02/22-rdf-syntax-ns#"" xmlns:rdfs1=""http://www.w3.org/2001/01/rdf-schema#""> <rdf:Description rdf:about=""http://www.hydroshare.org/resource/87ffb608900e407ab4b67d30c93b329e""> <dc:title>Great Salt Lake Level and Volume</dc:title> <dc:type rdf:resource=""http://www.hydroshare.org/terms/GenericResource""/> <dc:description> <rdf:Description> <dcterms:abstract>Time series of level, area and volume in the Great Salt Lake. Volume and area of the Great Salt Lake are derived from recorded levels</dcterms:abstract> </rdf:Description> </dc:description> <hsterms:awardInfo> <rdf:Description rdf:about=""http://www.nsf.gov""> <hsterms:fundingAgencyName>National Science Foundation</hsterms:fundingAgencyName> <hsterms:awardTitle>Model Execution Cyberinfrastructure </hsterms:awardTitle> <hsterms:awardNumber>NSF_9087658_2017</hsterms:awardNumber> </rdf:Description> </hsterms:awardInfo> <dc:creator> <rdf:Description> <hsterms:name>John Smith</hsterms:name> <hsterms:creatorOrder>1</hsterms:creatorOrder> <hsterms:organization>Utah State University</hsterms:organization> <hsterms:email>john.smith@gmail.com</hsterms:email> <hsterms:address>Engineering Building, USU, Logan, Utah</hsterms:address> <hsterms:phone rdf:resource=""tel:435-797-8967""/> </rdf:Description> </dc:creator> <dc:creator> <rdf:Description> <hsterms:name>Lisa Miller</hsterms:name> <hsterms:creatorOrder>2</hsterms:creatorOrder> </rdf:Description> </dc:creator> <dc:contributor> <rdf:Description> <hsterms:name>Jenny Parker</hsterms:name> <hsterms:organization>Univesity of Utah</hsterms:organization> <hsterms:email>jenny_parker@hotmail.com</hsterms:email> </rdf:Description> </dc:contributor> <dc:coverage> <dcterms:period> <rdf:value>start=2000-01-01T00:00:00; end=2010-12-12T00:00:00; scheme=W3C-DTF</rdf:value> </dcterms:period> </dc:coverage> <dc:date> <dcterms:created> <rdf:value>2017-01-03T17:06:18.932217+00:00</rdf:value> </dcterms:created> </dc:date> <dc:date> <dcterms:modified> <rdf:value>2017-01-03T17:35:34.067279+00:00</rdf:value> </dcterms:modified> </dc:date> <dc:format>image/tiff</dc:format> <dc:identifier> <rdf:Description> <hsterms:hydroShareIdentifier>http://www.hydroshare.org/resource/87ffb608900e407ab4b67d30c93b329e</hsterms:hydroShareIdentifier> </rdf:Description> </dc:identifier> <dc:language>eng</dc:language> <dc:rights> <rdf:Description> <hsterms:rightsStatement>This resource is shared under the Creative Commons Attribution CC BY.</hsterms:rightsStatement> <hsterms:URL rdf:resource=""http://creativecommons.org/licenses/by/4.0/""/> </rdf:Description> </dc:rights> <dc:subject>NSF</dc:subject> <dc:subject>Model</dc:subject> <dc:subject>Cyberinfrastructure</dc:subject> <hsterms:extendedMetadata> <rdf:Description> <hsterms:key>model</hsterms:key> <hsterms:value>ueb</hsterms:value> </rdf:Description> </hsterms:extendedMetadata> <hsterms:extendedMetadata> <rdf:Description> <hsterms:key>os</hsterms:key> <hsterms:value>windows</hsterms:value> </rdf:Description> </hsterms:extendedMetadata> </rdf:Description> <rdf:Description rdf:about=""http://www.hydroshare.org/terms/GenericResource""> <rdfs1:label>Generic</rdfs1:label> <rdfs1:isDefinedBy>http://www.hydroshare.org/terms</rdfs1:isDefinedBy> </rdf:Description> </rdf:RDF>",2,0,0,1,3,2,0,0,1,3
"def nvmlDeviceGetHandleByPciBusId(pciBusId):
  r
  c_busId = c_char_p(pciBusId)
  device = c_nvmlDevice_t()
  fn = _nvmlGetFunctionPointer(""nvmlDeviceGetHandleByPciBusId_v2"")
  ret = fn(c_busId, byref(device))
  _nvmlCheckReturn(ret)
  return bytes_to_str(device)","r"""""" /** * Acquire the handle for a particular device, based on its PCI bus id. * * For all products. * * This value corresponds to the nvmlPciInfo_t::busId returned by \ref nvmlDeviceGetPciInfo(). * * Starting from NVML 5, this API causes NVML to initialize the target GPU * NVML may initialize additional GPUs if: * - The target GPU is an SLI slave * * \note NVML 4.304 and older version of nvmlDeviceGetHandleByPciBusId""_v1"" returns NVML_ERROR_NOT_FOUND * instead of NVML_ERROR_NO_PERMISSION. * * @param pciBusId The PCI bus id of the target GPU * @param device Reference in which to return the device handle * * @return * - \ref NVML_SUCCESS if \a device has been set * - \ref NVML_ERROR_UNINITIALIZED if the library has not been successfully initialized * - \ref NVML_ERROR_INVALID_ARGUMENT if \a pciBusId is invalid or \a device is NULL * - \ref NVML_ERROR_NOT_FOUND if \a pciBusId does not match a valid device on the system * - \ref NVML_ERROR_INSUFFICIENT_POWER if the attached device has improperly attached external power cables * - \ref NVML_ERROR_NO_PERMISSION if the user doesn't have permission to talk to this device * - \ref NVML_ERROR_IRQ_ISSUE if NVIDIA kernel detected an interrupt issue with the attached GPUs * - \ref NVML_ERROR_GPU_IS_LOST if the target GPU has fallen off the bus or is otherwise inaccessible * - \ref NVML_ERROR_UNKNOWN on any unexpected error */ nvmlReturn_t DECLDIR nvmlDeviceGetHandleByPciBusId",1,0,0,0,1,1,0,0,1,2
"def prompt_for_value(self, ctx):
  default = self.get_default(ctx)
  if self.is_bool_flag:
  return confirm(self.prompt, default)
  return prompt(self.prompt, default=default, type=self.type,
  hide_input=self.hide_input, show_choices=self.show_choices,
  confirmation_prompt=self.confirmation_prompt,
  value_proc=lambda x: self.process_value(ctx, x))",This is an alternative flow that can be activated in the full value processing if a value does not exist. It will prompt the user until a valid value exists and then returns the processed value as result.,1,0,0,1,2,1,0,0,1,2
"def _parse_tuple(tup):
  row_loc, col_loc = slice(None), slice(None)
  if is_tuple(tup):
  row_loc = tup[0]
  if len(tup) == 2:
  col_loc = tup[1]
  if len(tup) > 2:
  raise IndexingError(""Too many indexers"")
  else:
  row_loc = tup
  ndim = _compute_ndim(row_loc, col_loc)
  row_scaler = is_scalar(row_loc)
  col_scaler = is_scalar(col_loc)
  row_loc = [row_loc] if row_scaler else row_loc
  col_loc = [col_loc] if col_scaler else col_loc
  return row_loc, col_loc, ndim, row_scaler, col_scaler","Unpack the user input for getitem and setitem and compute ndim loc[a] -> ([a], :), 1D loc[[a,b],] -> ([a,b], :), loc[a,b] -> ([a], [b]), 0D",1,0,0,0,1,0,0,0,1,1
"def create(cls, name, user=None, network_element=None, domain_name=None,
  zone=None, executable=None):
  ref_list = []
  if user:
  pass
  if network_element:
  ref_list.append(network_element.href)
  if domain_name:
  ref_list.append(domain_name.href)
  if zone:
  ref_list.append(zone.href)
  if executable:
  pass
  json = {'name': name,
  'ref': ref_list}
  return ElementCreator(cls, json)","Create a match expression :param str name: name of match expression :param str user: name of user or user group :param Element network_element: valid network element type, i.e. host, network, etc :param DomainName domain_name: domain name network element :param Zone zone: zone to use :param str executable: name of executable or group :raises ElementNotFound: specified object does not exist :return: instance with meta :rtype: MatchExpression",1,0,0,1,2,1,0,0,1,2
"def delete(
  self,
  endpoint,
  timeout=None,
  allow_redirects=None,
  validate=True,
  headers=None,
  ):
  endpoint = self._input_string(endpoint)
  request = deepcopy(self.request)
  request[""method""] = ""DELETE""
  if allow_redirects is not None:
  request[""allowRedirects""] = self._input_boolean(allow_redirects)
  if timeout is not None:
  request[""timeout""] = self._input_timeout(timeout)
  validate = self._input_boolean(validate)
  if headers:
  request[""headers""].update(self._input_object(headers))
  return self._request(endpoint, request, validate)[""response""]","*Sends a DELETE request to the endpoint.* The endpoint is joined with the URL given on library init (if any). If endpoint starts with ``http://`` or ``https://``, it is assumed an URL outside the tested API (which may affect logging). *Options* ``timeout``: A number of seconds to wait for the response before failing the keyword. ``allow_redirects``: If false, do not follow any redirects. ``validate``: If false, skips any request and response validations set by expectation keywords and a spec given on library init. ``headers``: Headers as a JSON object to add or override for the request. *Examples* | `DELETE` | /users/6 | | `DELETE` | http://localhost:8273/state | validate=false |",0,0,0,2,2,1,0,0,1,2
"def allowed(self, context):
  if not self._can_access(context['request']):
  return False
  for panel in self.get_panels():
  if panel.can_access(context):
  return True
  return False",Checks for role based access for this dashboard. Checks for access to any panels in the dashboard and of the dashboard itself. This method should be overridden to return the result of any policy checks required for the user to access this dashboard when more complex checks are required.,2,0,1,1,4,1,0,0,1,2
"def make_success_redirect(self):
  new_authorization_code = AuthorizationCode.objects.create(
  user=self.user,
  client=self.client,
  redirect_uri=(self.redirect_uri if self.request_redirect_uri else None)
  )
  new_authorization_code.scopes = self.valid_scope_objects
  new_authorization_code.save()
  response_params = {'code': new_authorization_code.value}
  if self.state is not None:
  response_params['state'] = self.state
  return HttpResponseRedirect(
  update_parameters(self.redirect_uri, response_params))",Return a Django ``HttpResponseRedirect`` describing the request success. The custom authorization endpoint should return the result of this method when the user grants the Client's authorization request. The request is assumed to have successfully been vetted by the :py:meth:`validate` method.,1,1,1,1,4,1,1,1,1,4
"def create(self, client=None):
  if self.notification_id is not None:
  raise ValueError(
  ""Notification already exists w/ id: {}"".format(self.notification_id)
  )
  client = self._require_client(client)
  query_params = {}
  if self.bucket.user_project is not None:
  query_params[""userProject""] = self.bucket.user_project
  path = ""/b/{}/notificationConfigs"".format(self.bucket.name)
  properties = self._properties.copy()
  properties[""topic""] = _TOPIC_REF_FMT.format(self.topic_project, self.topic_name)
  self._properties = client._connection.api_request(
  method=""POST"", path=path, query_params=query_params, data=properties
  )","API wrapper: create the notification. See: https://cloud.google.com/storage/docs/json_api/v1/notifications/insert If :attr:`user_project` is set on the bucket, bills the API request to that project. :type client: :class:`~google.cloud.storage.client.Client` :param client: (Optional) the client to use. If not passed, falls back to the ``client`` stored on the notification's bucket.",1,0,0,2,3,1,0,0,1,2
"def user_func(func, arg_types=None, return_type=None):
  class UserFunction(std_core.TypedFunction):
  name = func.__name__
  def __call__(self, *args, **kwargs):
  return func(*args, **kwargs)
  @classmethod
  def reflect_static_args(cls):
  return arg_types
  @classmethod
  def reflect_static_return(cls):
  return return_type
  return UserFunction()","Create an EFILTER-callable version of function 'func'. As a security precaution, EFILTER will not execute Python callables unless they implement the IApplicative protocol. There is a perfectly good implementation of this protocol in the standard library and user functions can inherit from it. This will declare a subclass of the standard library TypedFunction and return an instance of it that EFILTER will happily call. Arguments: func: A Python callable that will serve as the implementation. arg_types (optional): A tuple of argument types. If the function takes keyword arguments, they must still have a defined order. return_type (optional): The type the function returns. Returns: An instance of a custom subclass of efilter.stdlib.core.TypedFunction. Examples: def my_callback(tag): print(""I got %r"" % tag) api.apply(""if True then my_callback('Hello World!')"", vars={ ""my_callback"": api.user_func(my_callback) }) # This should print ""I got 'Hello World!'"".",0,0,0,0,0,1,0,0,1,2
"def get_user_roles(self, user, url_prefix, auth, session, send_opts):
  req = self.get_user_role_request(
  'GET', 'application/json', url_prefix, auth,
  user)
  prep = session.prepare_request(req)
  resp = session.send(prep, **send_opts)
  if resp.status_code == 200:
  return resp.json()
  msg = (
  'Failed getting roles for user: {}, got HTTP response: ({}) - {}'
  .format(user, resp.status_code, resp.text))
  raise HTTPError(msg, request = req, response = resp)",Get roles associated with the given user. Args: user (string): User name. url_prefix (string): Protocol + host such as https://api.theboss.io auth (string): Token to send in the request header. session (requests.Session): HTTP session to use for request. send_opts (dictionary): Additional arguments to pass to session.send(). Returns: (list): List of roles that user has. Raises: requests.HTTPError on failure.,2,0,0,1,3,2,0,0,1,3
"def get_results_as_xarray(self, parameter_space,
  result_parsing_function,
  output_labels, runs):
  np_array = np.array(
  self.get_space(
  self.db.get_complete_results(), {},
  collections.OrderedDict([(k, v) for k, v in
  parameter_space.items()]),
  runs, result_parsing_function))
  clean_parameter_space = collections.OrderedDict(
  [(k, v) for k, v in parameter_space.items()])
  clean_parameter_space['runs'] = range(runs)
  if isinstance(output_labels, list):
  clean_parameter_space['metrics'] = output_labels
  xr_array = xr.DataArray(np_array, coords=clean_parameter_space,
  dims=list(clean_parameter_space.keys()))
  return xr_array","Return the results relative to the desired parameter space in the form of an xarray data structure. Args: parameter_space (dict): The space of parameters to export. result_parsing_function (function): user-defined function, taking a result dictionary as argument, that can be used to parse the result files and return a list of values. output_labels (list): a list of labels to apply to the results dimensions, output by the result_parsing_function. runs (int): the number of runs to export for each parameter combination.",1,0,1,1,3,0,0,1,1,2
"def to_pickle(obj, path, compression='infer',
  protocol=pickle.HIGHEST_PROTOCOL):
  path = _stringify_path(path)
  f, fh = _get_handle(path, 'wb',
  compression=compression,
  is_text=False)
  if protocol < 0:
  protocol = pickle.HIGHEST_PROTOCOL
  try:
  f.write(pickle.dumps(obj, protocol=protocol))
  finally:
  f.close()
  for _f in fh:
  _f.close()","Pickle (serialize) object to file. Parameters ---------- obj : any object Any python object. path : str File path where the pickled object will be stored. compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer' A string representing the compression to use in the output file. By default, infers from the file extension in specified path. .. versionadded:: 0.20.0 protocol : int Int which indicates which protocol should be used by the pickler, default HIGHEST_PROTOCOL (see [1], paragraph 12.1.2). The possible values for this parameter depend on the version of Python. For Python 2.x, possible values are 0, 1, 2. For Python>=3.0, 3 is a valid value. For Python >= 3.4, 4 is a valid value. A negative value for the protocol parameter is equivalent to setting its value to HIGHEST_PROTOCOL. .. [1] https://docs.python.org/3/library/pickle.html .. versionadded:: 0.21.0 See Also -------- read_pickle : Load pickled pandas object (or any object) from file. DataFrame.to_hdf : Write DataFrame to an HDF5 file. DataFrame.to_sql : Write DataFrame to a SQL database. DataFrame.to_parquet : Write a DataFrame to the binary parquet format. Examples -------- >>> original_df = pd.DataFrame({""foo"": range(5), ""bar"": range(5, 10)}) >>> original_df foo bar 0 0 5 1 1 6 2 2 7 3 3 8 4 4 9 >>> pd.to_pickle(original_df, ""./dummy.pkl"") >>> unpickled_df = pd.read_pickle(""./dummy.pkl"") >>> unpickled_df foo bar 0 0 5 1 1 6 2 2 7 3 3 8 4 4 9 >>> import os >>> os.remove(""./dummy.pkl"")",1,1,1,0,3,1,0,0,1,2
"def convert(model, name=None, initial_types=None, doc_string='', target_opset=None,
  targeted_onnx=onnx.__version__, custom_conversion_functions=None, custom_shape_calculators=None):
  if initial_types is None:
  raise ValueError('Initial types are required. See usage of convert(...) in \
  onnxmltools.convert.libsvm.convert for details')
  if name is None:
  name = str(uuid4().hex)
  topology = parse_libsvm(model, initial_types, custom_conversion_functions,
  custom_shape_calculators)
  topology.compile()
  onnx_model = convert_topology(topology, name, doc_string, target_opset, targeted_onnx)
  return onnx_model",":param model: a libsvm model :param initial_types: a python list. Each element is a tuple of a variable name and a type defined in data_types.py :param name: The name of the graph (type: GraphProto) in the produced ONNX model (type: ModelProto) :param doc_string: A string attached onto the produced ONNX model :param target_opset: number, for example, 7 for ONNX 1.2, and 8 for ONNX 1.3. :param targeted_onnx: A string (for example, '1.1.2' and '1.2') used to specify the targeted ONNX version of the produced model. If ONNXMLTools cannot find a compatible ONNX python package, an error may be thrown. :param custom_conversion_functions: a dictionary for specifying the user customized conversion function :param custom_shape_calculators: a dictionary for specifying the user customized shape calculator :return: An ONNX model (type: ModelProto) which is equivalent to the input scikit-learn model",1,0,0,1,2,0,0,0,1,1
"def permission_check(data, command_permissions,
  command=None, permissions=None):
  if permissions:
  pass
  elif command:
  if hasattr(command, 'permissions'):
  permissions = command.permissions
  else:
  return True
  else:
  msg = ""{name} must be called with command or permissions argument""
  raise RuntimeError(msg.format(name=""_permission_check""))
  return any(data['sender']['id'] in command_permissions[permission]
  for permission in permissions
  if permission in command_permissions)","Check the permissions of the user requesting a command Parameters ---------- data : dict message data command_permissions : dict permissions of the command, contains all the roles as key and users with these permissions as values command : function the command that is run permissions : tuple or list a list of permissions for the command Returns ------- bool True if the user has the right permissions, False otherwise",1,0,0,1,2,1,0,0,1,2
"def get_assignments(
  self,
  gradebook_id='',
  simple=False,
  max_points=True,
  avg_stats=False,
  grading_stats=False
  ):
  params = dict(
  includeMaxPoints=json.dumps(max_points),
  includeAvgStats=json.dumps(avg_stats),
  includeGradingStats=json.dumps(grading_stats)
  )
  assignments = self.get(
  'assignments/{gradebookId}'.format(
  gradebookId=gradebook_id or self.gradebook_id
  ),
  params=params,
  )
  if simple:
  return [{'AssignmentName': x['name']}
  for x in assignments['data']]
  return assignments['data']","Get assignments for a gradebook. Return list of assignments for a given gradebook, specified by a py:attribute::gradebook_id. You can control if additional parameters are returned, but the response time with py:attribute::avg_stats and py:attribute::grading_stats enabled is significantly longer. Args: gradebook_id (str): unique identifier for gradebook, i.e. ``2314`` simple (bool): return just assignment names, default= ``False`` max_points (bool): Max points is a property of the grading scheme for the assignment rather than a property of the assignment itself, default= ``True`` avg_stats (bool): return average grade, default= ``False`` grading_stats (bool): return grading statistics, i.e. number of approved grades, unapproved grades, etc., default= ``False`` Raises: requests.RequestException: Exception connection error ValueError: Unable to decode response content Returns: list: list of assignment dictionaries An example return value is: .. code-block:: python [ { u'assignmentId': 2431240, u'categoryId': 1293820, u'description': u'', u'dueDate': 1372392000000, u'dueDateString': u'06-28-2013', u'gradebookId': 1293808, u'graderVisible': True, u'gradingSchemeId': 2431243, u'gradingSchemeType': u'NUMERIC', u'isComposite': False, u'isHomework': False, u'maxPointsTotal': 10.0, u'name': u'Homework 1', u'shortName': u'HW1', u'userDeleted': False, u'weight': 1.0 }, { u'assignmentId': 16708850, u'categoryId': 1293820, u'description': u'', u'dueDate': 1383541200000, u'dueDateString': u'11-04-2013', u'gradebookId': 1293808, u'graderVisible': False, u'gradingSchemeId': 16708851, u'gradingSchemeType': u'NUMERIC', u'isComposite': False, u'isHomework': False, u'maxPointsTotal': 100.0, u'name': u'midterm1', u'shortName': u'mid1', u'userDeleted': False, u'weight': 1.0 }, ]",1,0,0,1,2,1,0,0,1,2
"def bulk_insert(self, rows, return_model=False):
  if self.conflict_target or self.conflict_action:
  compiler = self._build_insert_compiler(rows)
  objs = compiler.execute_sql(return_id=True)
  if return_model:
  return [self.model(**dict(r, **k)) for r, k in zip(rows, objs)]
  else:
  return [dict(r, **k) for r, k in zip(rows, objs)]
  return super().bulk_create([self.model(**fields) for fields in rows])","Creates multiple new records in the database. This allows specifying custom conflict behavior using .on_conflict(). If no special behavior was specified, this uses the normal Django create(..) Arguments: rows: An array of dictionaries, where each dictionary describes the fields to insert. return_model (default: False): If model instances should be returned rather than just dicts. Returns: A list of either the dicts of the rows inserted, including the pk or the models of the rows inserted with defaults for any fields not specified",0,1,1,0,2,0,1,1,0,2
"def register_callback(self, callback_url, user_secret=None, **kwargs):
  if callback_url is None:
  raise ValueError('callback_url must be provided')
  headers = {}
  if 'headers' in kwargs:
  headers.update(kwargs.get('headers'))
  sdk_headers = get_sdk_headers('speech_to_text', 'V1',
  'register_callback')
  headers.update(sdk_headers)
  params = {'callback_url': callback_url, 'user_secret': user_secret}
  url = '/v1/register_callback'
  response = self.request(
  method='POST',
  url=url,
  headers=headers,
  params=params,
  accept_json=True)
  return response","Register a callback. Registers a callback URL with the service for use with subsequent asynchronous recognition requests. The service attempts to register, or white-list, the callback URL if it is not already registered by sending a `GET` request to the callback URL. The service passes a random alphanumeric challenge string via the `challenge_string` parameter of the request. The request includes an `Accept` header that specifies `text/plain` as the required response type. To be registered successfully, the callback URL must respond to the `GET` request from the service. The response must send status code 200 and must include the challenge string in its body. Set the `Content-Type` response header to `text/plain`. Upon receiving this response, the service responds to the original registration request with response code 201. The service sends only a single `GET` request to the callback URL. If the service does not receive a reply with a response code of 200 and a body that echoes the challenge string sent by the service within five seconds, it does not white-list the URL; it instead sends status code 400 in response to the **Register a callback** request. If the requested callback URL is already white-listed, the service responds to the initial registration request with response code 200. If you specify a user secret with the request, the service uses it as a key to calculate an HMAC-SHA1 signature of the challenge string in its response to the `POST` request. It sends this signature in the `X-Callback-Signature` header of its `GET` request to the URL during registration. It also uses the secret to calculate a signature over the payload of every callback notification that uses the URL. The signature provides authentication and data integrity for HTTP communications. After you successfully register a callback URL, you can use it with an indefinite number of recognition requests. You can register a maximum of 20 callback URLS in a one-hour span of time. **See also:** [Registering a callback URL](https://cloud.ibm.com/docs/services/speech-to-text/async.html#register). :param str callback_url: An HTTP or HTTPS URL to which callback notifications are to be sent. To be white-listed, the URL must successfully echo the challenge string during URL verification. During verification, the client can also check the signature that the service sends in the `X-Callback-Signature` header to verify the origin of the request. :param str user_secret: A user-specified string that the service uses to generate the HMAC-SHA1 signature that it sends via the `X-Callback-Signature` header. The service includes the header during URL verification and with every notification sent to the callback URL. It calculates the signature over the payload of the notification. If you omit the parameter, the service does not send the header. :param dict headers: A `dict` containing the request headers :return: A `DetailedResponse` containing the result, headers and HTTP status code. :rtype: DetailedResponse",2,0,0,2,4,2,0,0,2,4
"def get_config(self):
  if not self._config:
  namespace = {}
  if os.path.exists(self.config_path):
  execfile(self.config_path, namespace)
  self._config = namespace.get('config') or Configuration()
  return self._config",Load user configuration or return default when not found. :rtype: :class:`Configuration`,0,0,0,1,1,1,0,0,1,2
"def get_task_types(current):
  current.output['task_types'] = [{'name': bpmn_wf.name,
  'title': bpmn_wf.title}
  for bpmn_wf in BPMNWorkflow.objects.all()
  if current.has_permission(bpmn_wf.name)]","List task types for current user .. code-block:: python # request: { 'view': '_zops_get_task_types', } # response: { 'task_types': [ {'name': string, # wf name 'title': string, # title of workflow },] }",1,0,1,1,3,1,0,1,1,3
"def create_authentication_string(username, password):
  username_utf8 = username.encode('utf-8')
  userpw_utf8 = password.encode('utf-8')
  username_perc = quote(username_utf8)
  userpw_perc = quote(userpw_utf8)
  authinfostring = username_perc + ':' + userpw_perc
  authinfostring_base64 = base64.b64encode(authinfostring.encode('utf-8')).decode('utf-8')
  return authinfostring_base64",Creates an authentication string from the username and password. :username: Username. :password: Password. :return: The encoded string.,0,0,0,1,1,1,0,0,1,2
"def get_and_update_setting(self, name, default=None):
  setting = self._get_setting(name)
  if setting is None and default is not None:
  setting = default
  if setting is not None:
  updates = {name : setting}
  update_client_secrets(backend=self.client_name,
  updates=updates)
  return setting","Look for a setting in the environment (first priority) and then the settings file (second). If something is found, the settings file is updated. The order of operations works as follows: 1. The .sregistry settings file is used as a cache for the variable 2. the environment variable always takes priority to cache, and if found, will update the cache. 3. If the variable is not found and the cache is set, we are good 5. If the variable is not found and the cache isn't set, return default (default is None) So the user of the function can assume a return of None equates to not set anywhere, and take the appropriate action.",1,0,0,1,2,1,0,0,0,1
"def gravatar_url(cls, instance, default=""mm"", **kwargs):
  hash = hashlib.md5(instance.email.encode(""utf8"").lower()).hexdigest()
  if ""d"" not in kwargs:
  kwargs[""d""] = default
  params = ""&"".join(
  [
  six.moves.urllib.parse.urlencode({key: value})
  for key, value in kwargs.items()
  ]
  )
  return ""https://secure.gravatar.com/avatar/{}?{}"".format(hash, params)",returns user gravatar url :param instance: :param default: :param kwargs: :return:,1,0,1,1,3,1,0,0,1,2
"def yesno(self, prompt, error='Please type either y or n', intro=None,
  default=None):
  if default is None:
  prompt += ' (y/n):'
  else:
  if default is True:
  prompt += ' (Y/n):'
  default = 'y'
  if default is False:
  prompt += ' (y/N):'
  default = 'n'
  validator = lambda x: x in ['y', 'yes', 'n', 'no']
  val = self.rvpl(prompt, error=error, intro=intro, validator=validator,
  clean=lambda x: x.strip().lower(),
  strict=default is None, default=default)
  return val in ['y', 'yes']","Ask user for yes or no answer The prompt will include a typical '(y/n):' at the end. Depending on whether ``default`` was specified, this may also be '(Y/n):' or '(y/N):'. The ``default`` argument can be ``True`` or ``False``, with meaning of 'yes' and 'no' respectively. Default is ``None`` which means no default. When default value is specified, malformed or empty response will cause the ``default`` value to be returned. Optional ``intro`` text can be specified which will be shown above the prompt.",1,0,0,1,2,1,0,0,1,2
"def by_user_names(cls, user_names, db_session=None):
  user_names = [(name or """").lower() for name in user_names]
  db_session = get_db_session(db_session)
  query = db_session.query(cls.model)
  query = query.filter(sa.func.lower(cls.model.user_name).in_(user_names))
  return query",fetch user objects by user names :param user_names: :param db_session: :return:,0,0,1,0,1,1,0,1,1,3
"def get_authorize_url(self, redirect_uri, **kw):
  redirect = redirect_uri if redirect_uri else self._redirect_uri
  if not redirect:
  raise APIError('21305', 'Parameter absent: redirect_uri', 'OAuth2 request')
  response_type = kw.pop('response_type', 'code')
  return 'https://api.weibo.com/oauth2/authorize?%s' % \
  _encode_params(client_id=self._client_id,
  response_type=response_type,
  redirect_uri=redirect, **kw)",return the authorization url that the user should be redirected to.,1,0,0,1,2,2,0,0,1,3
"def delete_ace(self, domain=None, user=None, sid=None):
  if sid is None:
  if domain is None:
  domain = self.cifs_server.domain
  sid = UnityAclUser.get_sid(self._cli, user=user, domain=domain)
  if isinstance(sid, six.string_types):
  sid = [sid]
  ace_list = [self._make_remove_ace_entry(s) for s in sid]
  resp = self.action(""setACEs"", cifsShareACEs=ace_list)
  resp.raise_if_err()
  return resp",delete ACE for the share delete ACE for the share. User could either supply the domain and username or the sid of the user. :param domain: domain of the user :param user: username :param sid: sid of the user or sid list of the user :return: REST API response,2,0,0,1,3,1,0,0,1,2
"def add_device_notification(self, data_name, attr, callback, user_handle=None):
  if self._port is not None:
  notification_handle, user_handle = adsSyncAddDeviceNotificationReqEx(
  self._port, self._adr, data_name, attr, callback, user_handle
  )
  return notification_handle, user_handle
  return None","Add a device notification. :param str data_name: PLC storage address :param pyads.structs.NotificationAttrib attr: object that contains all the attributes for the definition of a notification :param callback: callback function that gets executed on in the event of a notification :rtype: (int, int) :returns: notification handle, user handle Save the notification handle and the user handle on creating a notification if you want to be able to remove the notification later in your code. **Usage**: >>> import pyads >>> from ctypes import size_of >>> >>> # Connect to the local TwinCAT PLC >>> plc = pyads.Connection('127.0.0.1.1.1', 851) >>> >>> # Create callback function that prints the value >>> def mycallback(adr, notification, user): >>> contents = notification.contents >>> value = next( >>> map(int, >>> bytearray(contents.data)[0:contents.cbSampleSize]) >>> ) >>> print(value) >>> >>> with plc: >>> # Add notification with default settings >>> attr = pyads.NotificationAttrib(size_of(pyads.PLCTYPE_INT)) >>> >>> hnotification, huser = plc.add_device_notification( >>> adr, attr, mycallback) >>> >>> # Remove notification >>> plc.del_device_notification(hnotification, huser)",2,0,0,1,3,1,0,0,1,2
"def run_commands(*commands, **kwargs):
  encoding = kwargs.pop('encoding', 'json')
  send_enable = kwargs.pop('send_enable', True)
  output = call('run_commands',
  commands,
  encoding=encoding,
  send_enable=send_enable,
  **kwargs)
  if encoding == 'text':
  ret = []
  for res in output:
  ret.append(res['output'])
  return ret
  return output","Sends the commands over the transport to the device. This function sends the commands to the device using the nodes transport. This is a lower layer function that shouldn't normally need to be used, preferring instead to use ``config()`` or ``enable()``. transport: ``https`` Specifies the type of connection transport to use. Valid values for the connection are ``socket``, ``http_local``, ``http``, and ``https``. .. note:: This argument does not need to be specified when running in a :mod:`pyeapi <salt.proxy.arista_pyeapi>` Proxy Minion. host: ``localhost`` The IP address or DNS host name of the connection device. .. note:: This argument does not need to be specified when running in a :mod:`pyeapi <salt.proxy.arista_pyeapi>` Proxy Minion. username: ``admin`` The username to pass to the device to authenticate the eAPI connection. .. note:: This argument does not need to be specified when running in a :mod:`pyeapi <salt.proxy.arista_pyeapi>` Proxy Minion. password The password to pass to the device to authenticate the eAPI connection. .. note:: This argument does not need to be specified when running in a :mod:`pyeapi <salt.proxy.arista_pyeapi>` Proxy Minion. port The TCP port of the endpoint for the eAPI connection. If this keyword is not specified, the default value is automatically determined by the transport type (``80`` for ``http``, or ``443`` for ``https``). .. note:: This argument does not need to be specified when running in a :mod:`pyeapi <salt.proxy.arista_pyeapi>` Proxy Minion. enablepwd The enable mode password if required by the destination node. .. note:: This argument does not need to be specified when running in a :mod:`pyeapi <salt.proxy.arista_pyeapi>` Proxy Minion. CLI Example: .. code-block:: bash salt '*' pyeapi.run_commands 'show version' salt '*' pyeapi.run_commands 'show version' encoding=text salt '*' pyeapi.run_commands 'show version' encoding=text host=cr1.thn.lon username=example password=weak Output example: .. code-block:: text veos1: |_ ---------- architecture: i386 bootupTimestamp: 1527541728.53 hardwareRevision: internalBuildId: 63d2e89a-220d-4b8a-a9b3-0524fa8f9c5f internalVersion: 4.18.1F-4591672.4181F isIntlVersion: False memFree: 501468 memTotal: 1893316 modelName: vEOS serialNumber: systemMacAddress: 52:54:00:3f:e6:d0 version: 4.18.1F",1,0,0,1,2,1,0,0,1,2
"def inserir(self, id_permission, read, write, id_group):
  perms_map = dict()
  perms_map['id_permission'] = id_permission
  perms_map['read'] = read
  perms_map['write'] = write
  perms_map['id_group'] = id_group
  code, xml = self.submit(
  {'administrative_permission': perms_map}, 'POST', 'aperms/')
  return self.response(code, xml)","Inserts a new Administrative Permission and returns its identifier. :param id_permission: Identifier of the Permission. Integer value and greater than zero. :param read: Read. 0 or 1 :param write: Write. 0 or 1 :param id_group: Identifier of the Group of User. Integer value and greater than zero. :return: Dictionary with the following structure: :: {'perm': {'id': < id_perm >}} :raise InvalidParameterError: The identifier of Administrative Permission, identifier of Group of User, read or write is null and invalid. :raise ValorIndicacaoPermissaoInvalidoError: The value of read or write is null and invalid. :raise PermissaoAdministrativaDuplicadaError: Function already registered for the user group. :raise GrupoUsuarioNaoExisteError: Group of User not registered. :raise DataBaseError: Networkapi failed to access the database. :raise XMLError: Networkapi failed to generate the XML response.",1,1,0,1,3,1,0,0,2,3
"def parse_second_row(row, url):
  tags = row.findall('./td')
  category, subcategory, quality, language = Parser.parse_torrent_properties(tags[0])
  user_info = tags[1].find('./a')
  user = user_info.text_content()
  user_url = url.combine(user_info.get('href'))
  torrent_link = Parser.parse_torrent_link(tags[2])
  size = tags[3].text
  comments = tags[4].text
  times_completed = tags[5].text
  seeders = tags[6].text
  leechers = tags[7].text
  return [category, subcategory, quality, language, user, user_url, torrent_link,
  size, comments, times_completed, seeders, leechers]","Static method that parses a given table row element by using helper methods `Parser.parse_category_subcategory_and_or_quality`, `Parser.parse_torrent_link` and scrapping torrent's category, subcategory, quality, language, user, user url, torrent link, size, comments, times completed, seeders and leechers. Used specifically with a torrent's second table row. :param lxml.HtmlElement row: row to parse :param urls.Url url_instance: Url used to combine base url's with scrapped links from tr :return: scrapped category, subcategory, quality, language, user, user url, torrent link, size, comments, times completed, seeders and leechers :rtype: list",0,0,0,1,1,1,0,0,1,2
"def requestAvatarId(self, credentials):
  username, domain = credentials.username.split(""@"")
  key = self.users.key(domain, username)
  if key is None:
  return defer.fail(UnauthorizedLogin())
  def _cbPasswordChecked(passwordIsCorrect):
  if passwordIsCorrect:
  return username + '@' + domain
  else:
  raise UnauthorizedLogin()
  return defer.maybeDeferred(credentials.checkPassword,
  key).addCallback(_cbPasswordChecked)","Return the ID associated with these credentials. @param credentials: something which implements one of the interfaces in self.credentialInterfaces. @return: a Deferred which will fire a string which identifies an avatar, an empty tuple to specify an authenticated anonymous user (provided as checkers.ANONYMOUS) or fire a Failure(UnauthorizedLogin). @see: L{twisted.cred.credentials}",0,0,1,1,2,2,0,0,1,3
"def update(self, teamId, name=None, **request_parameters):
  check_type(teamId, basestring, may_be_none=False)
  check_type(name, basestring)
  put_data = dict_from_items_with_values(
  request_parameters,
  name=name,
  )
  json_data = self._session.put(API_ENDPOINT + '/' + teamId,
  json=put_data)
  return self._object_factory(OBJECT_TYPE, json_data)","Update details for a team, by ID. Args: teamId(basestring): The team ID. name(basestring): A user-friendly name for the team. **request_parameters: Additional request parameters (provides support for parameters that may be added in the future). Returns: Team: A Team object with the updated Webex Teams team details. Raises: TypeError: If the parameter types are incorrect. ApiError: If the Webex Teams cloud returns an error.",1,0,0,2,3,1,0,0,2,3
"def post_signup(self, user, login_user=None, send_email=None):
  self.signup_signal.send(self, user=user)
  if (login_user is None and self.options[""login_user_on_signup""]) or login_user:
  self._login(user, user.signup_provider)
  to_email = getattr(user, self.options[""email_column""], None)
  if to_email and ((send_email is None and self.options[""send_welcome_email""]) or send_email):
  template = ""users/welcome.txt"" if self.options[""send_welcome_email""] == True else self.options[""send_welcome_email""]
  current_app.features.emails.send(to_email, template, user=user)","Executes post signup actions: sending the signal, logging in the user and sending the welcome email",1,0,0,1,2,1,0,0,1,2
"def get_visible_profiles(self, user=None):
  profiles = self.all()
  filter_kwargs = {'user__is_active': True}
  profiles = profiles.filter(**filter_kwargs)
  if user and isinstance(user, AnonymousUser):
  profiles = profiles.exclude(Q(privacy='closed') | Q(privacy='registered'))
  else: profiles = profiles.exclude(Q(privacy='closed'))
  return profiles","Returns all the visible profiles available to this user. For now keeps it simple by just applying the cases when a user is not active, a user has it's profile closed to everyone or a user only allows registered users to view their profile. :param user: A Django :class:`User` instance. :return: All profiles that are visible to this user.",1,0,1,1,3,1,0,1,1,3
"def validate_userid_signature(user: User) -> Optional[Address]:
  match = USERID_RE.match(user.user_id)
  if not match:
  return None
  encoded_address = match.group(1)
  address: Address = to_canonical_address(encoded_address)
  try:
  displayname = user.get_display_name()
  recovered = recover(
  data=user.user_id.encode(),
  signature=decode_hex(displayname),
  )
  if not (address and recovered and recovered == address):
  return None
  except (
  DecodeError,
  TypeError,
  InvalidSignature,
  MatrixRequestError,
  json.decoder.JSONDecodeError,
  ):
  return None
  return address","Validate a userId format and signature on displayName, and return its address",0,0,0,1,1,1,0,0,1,2
"def read_html(io, match='.+', flavor=None, header=None, index_col=None,
  skiprows=None, attrs=None, parse_dates=False,
  tupleize_cols=None, thousands=',', encoding=None,
  decimal='.', converters=None, na_values=None,
  keep_default_na=True, displayed_only=True):
  r
  _importers()
  if isinstance(skiprows, numbers.Integral) and skiprows < 0:
  raise ValueError('cannot skip rows starting from the end of the '
  'data (you passed a negative value)')
  _validate_header_arg(header)
  return _parse(flavor=flavor, io=io, match=match, header=header,
  index_col=index_col, skiprows=skiprows,
  parse_dates=parse_dates, tupleize_cols=tupleize_cols,
  thousands=thousands, attrs=attrs, encoding=encoding,
  decimal=decimal, converters=converters, na_values=na_values,
  keep_default_na=keep_default_na,
  displayed_only=displayed_only)","r""""""Read HTML tables into a ``list`` of ``DataFrame`` objects. Parameters ---------- io : str or file-like A URL, a file-like object, or a raw string containing HTML. Note that lxml only accepts the http, ftp and file url protocols. If you have a URL that starts with ``'https'`` you might try removing the ``'s'``. match : str or compiled regular expression, optional The set of tables containing text matching this regex or string will be returned. Unless the HTML is extremely simple you will probably need to pass a non-empty string here. Defaults to '.+' (match any non-empty string). The default value will return all tables contained on a page. This value is converted to a regular expression so that there is consistent behavior between Beautiful Soup and lxml. flavor : str or None, container of strings The parsing engine to use. 'bs4' and 'html5lib' are synonymous with each other, they are both there for backwards compatibility. The default of ``None`` tries to use ``lxml`` to parse and if that fails it falls back on ``bs4`` + ``html5lib``. header : int or list-like or None, optional The row (or list of rows for a :class:`~pandas.MultiIndex`) to use to make the columns headers. index_col : int or list-like or None, optional The column (or list of columns) to use to create the index. skiprows : int or list-like or slice or None, optional 0-based. Number of rows to skip after parsing the column integer. If a sequence of integers or a slice is given, will skip the rows indexed by that sequence. Note that a single element sequence means 'skip the nth row' whereas an integer means 'skip n rows'. attrs : dict or None, optional This is a dictionary of attributes that you can pass to use to identify the table in the HTML. These are not checked for validity before being passed to lxml or Beautiful Soup. However, these attributes must be valid HTML table attributes to work correctly. For example, :: attrs = {'id': 'table'} is a valid attribute dictionary because the 'id' HTML tag attribute is a valid HTML attribute for *any* HTML tag as per `this document <http://www.w3.org/TR/html-markup/global-attributes.html>`__. :: attrs = {'asdf': 'table'} is *not* a valid attribute dictionary because 'asdf' is not a valid HTML attribute even if it is a valid XML attribute. Valid HTML 4.01 table attributes can be found `here <http://www.w3.org/TR/REC-html40/struct/tables.html#h-11.2>`__. A working draft of the HTML 5 spec can be found `here <http://www.w3.org/TR/html-markup/table.html>`__. It contains the latest information on table attributes for the modern web. parse_dates : bool, optional See :func:`~read_csv` for more details. tupleize_cols : bool, optional If ``False`` try to parse multiple header rows into a :class:`~pandas.MultiIndex`, otherwise return raw tuples. Defaults to ``False``. .. deprecated:: 0.21.0 This argument will be removed and will always convert to MultiIndex thousands : str, optional Separator to use to parse thousands. Defaults to ``','``. encoding : str or None, optional The encoding used to decode the web page. Defaults to ``None``.``None`` preserves the previous encoding behavior, which depends on the underlying parser library (e.g., the parser library will try to use the encoding provided by the document). decimal : str, default '.' Character to recognize as decimal point (e.g. use ',' for European data). .. versionadded:: 0.19.0 converters : dict, default None Dict of functions for converting values in certain columns. Keys can either be integers or column labels, values are functions that take one input argument, the cell (not column) content, and return the transformed content. .. versionadded:: 0.19.0 na_values : iterable, default None Custom NA values .. versionadded:: 0.19.0 keep_default_na : bool, default True If na_values are specified and keep_default_na is False the default NaN values are overridden, otherwise they're appended to .. versionadded:: 0.19.0 displayed_only : bool, default True Whether elements with ""display: none"" should be parsed .. versionadded:: 0.23.0 Returns ------- dfs : list of DataFrames See Also -------- read_csv Notes ----- Before using this function you should read the :ref:`gotchas about the HTML parsing libraries <io.html.gotchas>`. Expect to do some cleanup after you call this function. For example, you might need to manually assign column names if the column names are converted to NaN when you pass the `header=0` argument. We try to assume as little as possible about the structure of the table and push the idiosyncrasies of the HTML contained in the table to the user. This function searches for ``<table>`` elements and only for ``<tr>`` and ``<th>`` rows and ``<td>`` elements within each ``<tr>`` or ``<th>`` element in the table. ``<td>`` stands for ""table data"". This function attempts to properly handle ``colspan`` and ``rowspan`` attributes. If the function has a ``<thead>`` argument, it is used to construct the header, otherwise the function attempts to find the header within the body (by putting rows with only ``<th>`` elements into the header). .. versionadded:: 0.21.0 Similar to :func:`~read_csv` the `header` argument is applied **after** `skiprows` is applied. This function will *always* return a list of :class:`DataFrame` *or* it will fail, e.g., it will *not* return an empty list. Examples -------- See the :ref:`read_html documentation in the IO section of the docs <io.read_html>` for some examples of reading in HTML tables.",1,0,0,1,2,1,0,0,1,2
"async def facebook_request(
  self,
  path: str,
  access_token: str = None,
  post_args: Dict[str, Any] = None,
  **args: Any
  ) -> Any:
  url = self._FACEBOOK_BASE_URL + path
  return await self.oauth2_request(
  url, access_token=access_token, post_args=post_args, **args
  )","Fetches the given relative API path, e.g., ""/btaylor/picture"" If the request is a POST, ``post_args`` should be provided. Query string arguments should be given as keyword arguments. An introduction to the Facebook Graph API can be found at http://developers.facebook.com/docs/api Many methods require an OAuth access token which you can obtain through `~OAuth2Mixin.authorize_redirect` and `get_authenticated_user`. The user returned through that process includes an ``access_token`` attribute that can be used to make authenticated requests via this method. Example usage: .. testcode:: class MainHandler(tornado.web.RequestHandler, tornado.auth.FacebookGraphMixin): @tornado.web.authenticated async def get(self): new_entry = await self.facebook_request( ""/me/feed"", post_args={""message"": ""I am posting from my Tornado application!""}, access_token=self.current_user[""access_token""]) if not new_entry: # Call failed; perhaps missing permission? yield self.authorize_redirect() return self.finish(""Posted a message!"") .. testoutput:: :hide: The given path is relative to ``self._FACEBOOK_BASE_URL``, by default ""https://graph.facebook.com"". This method is a wrapper around `OAuth2Mixin.oauth2_request`; the only difference is that this method takes a relative path, while ``oauth2_request`` takes a complete url. .. versionchanged:: 3.1 Added the ability to override ``self._FACEBOOK_BASE_URL``. .. versionchanged:: 6.0 The ``callback`` argument was removed. Use the returned awaitable object instead.",1,0,0,1,2,2,0,0,1,3
"def create(self, friendly_name, activity_sid=values.unset,
  attributes=values.unset):
  data = values.of({
  'FriendlyName': friendly_name,
  'ActivitySid': activity_sid,
  'Attributes': attributes,
  })
  payload = self._version.create(
  'POST',
  self._uri,
  data=data,
  )
  return WorkerInstance(self._version, payload, workspace_sid=self._solution['workspace_sid'], )",Create a new WorkerInstance :param unicode friendly_name: String representing user-friendly name for the Worker. :param unicode activity_sid: A valid Activity describing the worker's initial state. :param unicode attributes: JSON object describing this worker. :returns: Newly created WorkerInstance :rtype: twilio.rest.taskrouter.v1.workspace.worker.WorkerInstance,1,0,0,1,2,1,0,0,2,3
"def authenticate(self):
  basic_auth = request.authorization
  is_valid = False
  user = None
  if basic_auth:
  is_valid, user = self.check_basic_auth(
  basic_auth.username, basic_auth.password
  )
  else:
  token = request.headers.get('Authorization', None)
  param_token = request.args.get('access_token')
  if token or param_token:
  if token:
  token = token[6:]
  else:
  token = param_token
  log.debug('Received token: %s', token)
  is_valid, user = self.check_token_auth(token)
  return (is_valid, user)","Authenticate user by any means and return either true or false. Args: Returns: tuple (is_valid, username): True is valid user, False if not",2,0,1,1,4,2,0,0,1,3
"def search(session, query):
  flat_query = """".join(query.split())
  artists = session.query(Artist).filter(
  or_(Artist.name.ilike(f""%%{query}%%""),
  Artist.name.ilike(f""%%{flat_query}%%""))
  ).all()
  albums = session.query(Album).filter(
  Album.title.ilike(f""%%{query}%%"")).all()
  tracks = session.query(Track).filter(
  Track.title.ilike(f""%%{query}%%"")).all()
  return dict(artists=artists,
  albums=albums,
  tracks=tracks)","Naive search of the database for `query`. :return: A dict with keys 'artists', 'albums', and 'tracks'. Each containing a list of the respective ORM type.",1,0,1,1,3,1,0,1,1,3
"def get_query_cache_key(compiler):
  sql, params = compiler.as_sql()
  check_parameter_types(params)
  cache_key = '%s:%s:%s' % (compiler.using, sql,
  [text_type(p) for p in params])
  return sha1(cache_key.encode('utf-8')).hexdigest()",Generates a cache key from a SQLCompiler. This cache key is specific to the SQL query and its context (which database is used). The same query in the same context (= the same database) must generate the same cache key. :arg compiler: A SQLCompiler that will generate the SQL query :type compiler: django.db.models.sql.compiler.SQLCompiler :return: A cache key :rtype: int,1,0,0,0,1,0,0,0,1,1
"def get_dep_statuses(self, ti, session, dep_context=None):
  from airflow.ti_deps.dep_context import DepContext
  if dep_context is None:
  dep_context = DepContext()
  if self.IGNOREABLE and dep_context.ignore_all_deps:
  yield self._passing_status(
  reason=""Context specified all dependencies should be ignored."")
  return
  if self.IS_TASK_DEP and dep_context.ignore_task_deps:
  yield self._passing_status(
  reason=""Context specified all task dependencies should be ignored."")
  return
  for dep_status in self._get_dep_statuses(ti, session, dep_context):
  yield dep_status",Wrapper around the private _get_dep_statuses method that contains some global checks for all dependencies. :param ti: the task instance to get the dependency status for :type ti: airflow.models.TaskInstance :param session: database session :type session: sqlalchemy.orm.session.Session :param dep_context: the context for which this dependency should be evaluated for :type dep_context: DepContext,1,0,0,1,2,1,0,0,1,2
"def get_group_member_profile(self, group_id, user_id, timeout=None):
  response = self._get(
  '/v2/bot/group/{group_id}/member/{user_id}'.format(group_id=group_id, user_id=user_id),
  timeout=timeout
  )
  return Profile.new_from_json_dict(response.json)","Call get group member profile API. https://devdocs.line.me/en/#get-group-room-member-profile Gets the user profile of a member of a group that the bot is in. This can be the user ID of a user who has not added the bot as a friend or has blocked the bot. :param str group_id: Group ID :param str user_id: User ID :param timeout: (optional) How long to wait for the server to send data before giving up, as a float, or a (connect timeout, read timeout) float tuple. Default is self.http_client.timeout :type timeout: float | tuple(float, float) :rtype: :py:class:`linebot.models.responses.Profile` :return: Profile instance",2,0,0,1,3,2,0,0,1,3
"def user_information(self, access_token):
  payload = {'access_token': access_token}
  url = ""https://graph.facebook.com/v2.5/me""
  if self.config[""fields""]:
  payload[""fields""] = "","".join(self.config[""fields""])
  resp = requests.get(url, params=payload)
  data = json.loads(resp.text)
  try:
  picture_url = data[""picture""][""data""][""url""]
  data[""picture""] = picture_url
  except KeyError as e:
  pass
  return data","Will retrieve the user information data for the authenticated user. :type access_token: str :rtype: dict[str, str] :param access_token: The access token to be used to retrieve the data. :return: Dictionary with attribute name as key and attribute value as value.",2,0,0,1,3,2,0,0,1,3
"def DomainsGet(self, parameters = None, domain_id = -1):
  url = ''
  if parameters is None and domain_id <> -1:
  url = '/domains/{0}.json'.format(domain_id)
  else:
  url = '/domains.json'
  if self.__SenseApiCall__(url, 'GET', parameters = parameters):
  return True
  else:
  self.__error__ = ""api call unsuccessful""
  return False",This method returns the domains of the current user. The list also contains the domains to which the users has not yet been accepted. @param parameters (dictonary) - Dictionary containing the parameters of the request. @return (bool) - Boolean indicating whether DomainsGet was successful.,2,0,0,1,3,2,0,0,1,3
"def cmd(str, print_ret=False, usr_pwd=None, run=True):
 if usr_pwd:
 str = 'echo {} | sudo -u {} {} '.format(usr_pwd[1], usr_pwd[0], str)
 print(' [>] {}'.format(str))
 if run:
 err, ret = commands.getstatusoutput(str)
 else:
 err = None
 ret = None
 if err:
 print(' [x] {}'.format(ret))
 raise Exception(ret)
 if ret and print_ret:
 lines = ret.split('\n')
 for line in lines:
 print(' [<] {}'.format(line))
 return ret","Executes a command and throws an exception on error. in: str - command print_ret - print command return usr_pwd - execute command as another user (user_name, password) run - really execute command? out: returns the command output",1,0,0,1,2,1,0,0,1,2
"def external_session_url(self, email, chrome, url, integrator_id, client_id):
  body = {
  ""Email"": email,
  ""Chrome"": chrome,
  ""Url"": url,
  ""IntegratorID"": integrator_id,
  ""ClientID"": client_id}
  response = self._put('/externalsession.json', json.dumps(body))
  return json_to_py(response)","Get a URL which initiates a new external session for the user with the given email. Full details: http://www.campaignmonitor.com/api/account/#single_sign_on :param email: String The representing the email address of the Campaign Monitor user for whom the login session should be created. :param chrome: String representing which 'chrome' to display - Must be either ""all"", ""tabs"", or ""none"". :param url: String representing the URL to display once logged in. e.g. ""/subscribers/"" :param integrator_id: String representing the Integrator ID. You need to contact Campaign Monitor support to get an Integrator ID. :param client_id: String representing the Client ID of the client which should be active once logged in to the Campaign Monitor account. :returns Object containing a single field SessionUrl which represents the URL to initiate the external Campaign Monitor session.",1,0,0,1,2,1,0,0,1,2
"def get_queue_metadata(self, queue_name, timeout=None):
  _validate_not_none('queue_name', queue_name)
  request = HTTPRequest()
  request.method = 'GET'
  request.host = self._get_host()
  request.path = _get_path(queue_name)
  request.query = [
  ('comp', 'metadata'),
  ('timeout', _int_to_str(timeout)),
  ]
  response = self._perform_request(request)
  return _parse_metadata_and_message_count(response)","Retrieves user-defined metadata and queue properties on the specified queue. Metadata is associated with the queue as name-value pairs. :param str queue_name: The name of an existing queue. :param int timeout: The server timeout, expressed in seconds. :return: A dictionary representing the queue metadata with an approximate_message_count int property on the dict estimating the number of messages in the queue. :rtype: a dict mapping str to str",1,0,0,1,2,2,0,0,1,3
"def host_get(host=None, name=None, hostids=None, **kwargs):
  conn_args = _login(**kwargs)
  ret = {}
  try:
  if conn_args:
  method = 'host.get'
  params = {""output"": ""extend"", ""filter"": {}}
  if not name and not hostids and not host:
  return False
  if name:
  params['filter'].setdefault('name', name)
  if hostids:
  params.setdefault('hostids', hostids)
  if host:
  params['filter'].setdefault('host', host)
  params = _params_extend(params, **kwargs)
  ret = _query(method, params, conn_args['url'], conn_args['auth'])
  return ret['result'] if ret['result'] else False
  else:
  raise KeyError
  except KeyError:
  return ret",".. versionadded:: 2016.3.0 Retrieve hosts according to the given parameters .. note:: This function accepts all optional host.get parameters: keyword argument names differ depending on your zabbix version, see here__. .. __: https://www.zabbix.com/documentation/2.4/manual/api/reference/host/get :param host: technical name of the host :param name: visible name of the host :param hostids: ids of the hosts :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring) :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring) :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring) :return: Array with convenient hosts details, False if no host found or on failure. CLI Example: .. code-block:: bash salt '*' zabbix.host_get 'Zabbix server'",1,0,0,1,2,1,0,0,1,2
"def report(self, account_id, status_ids = None, comment = None, forward = False):
  account_id = self.__unpack_id(account_id)
  if not status_ids is None:
  if not isinstance(status_ids, list):
  status_ids = [status_ids]
  status_ids = list(map(lambda x: self.__unpack_id(x), status_ids))
  params_initial = locals()
  if forward == False:
  del params_initial['forward']
  params = self.__generate_params(params_initial)
  return self.__api_request('POST', '/api/v1/reports/', params)","Report statuses to the instances administrators. Accepts a list of toot IDs associated with the report, and a comment. Set forward to True to forward a report of a remote user to that users instance as well as sending it to the instance local administrators. Returns a `report dict`_.",2,0,0,1,3,1,0,0,1,2
"def determine_result(self, returncode, returnsignal, output, isTimeout):
  if not output:
  return 'ERROR - no output'
  last = output[-1]
  if isTimeout:
  return 'TIMEOUT'
  if returncode != 0:
  return 'ERROR - {0}'.format( last )
  if 'result:' in last:
  res = last.split(':', maxsplit=1)[1].strip()
  return self.RESMAP.get( res, result.RESULT_UNKNOWN );
  else:
  return 'UNKNOWN ERROR'","Parse the output of the tool and extract the verification result. This method always needs to be overridden. If the tool gave a result, this method needs to return one of the benchexec.result.RESULT_* strings. Otherwise an arbitrary string can be returned that will be shown to the user and should give some indication of the failure reason (e.g., ""CRASH"", ""OUT_OF_MEMORY"", etc.).",0,0,0,1,1,1,0,0,1,2
"def _register_oauth_session_user(self, user):
  oauth_session = get_oauth_session()
  if oauth_session:
  if ""provider"" in oauth_session and ""user_id"" in oauth_session:
  user.add_federated_login(provider=oauth_session.get(""provider""),
  federated_id=oauth_session.get(""user_id""))
  delete_oauth_session()",Add the :param user: :return:,1,0,0,1,2,1,0,0,0,1
"def validate_creds(creds):
  try:
  auth_type, auth_rest = creds.split(':', 1)
  except ValueError:
  raise ValueError(""Missing ':' in %s"" % creds)
  authtypes = sys.modules[__name__]
  auth_encoder = getattr(authtypes, auth_type.title(), None)
  if auth_encoder is None:
  raise ValueError('Invalid auth_type: %s' % auth_type)
  auth_encoder = auth_encoder()
  parsed_creds = dict(type=auth_type, salt=None, hash=None)
  parsed_creds.update(auth_encoder.validate(auth_rest))
  return auth_encoder, parsed_creds",Parse and validate user credentials whether format is right :param creds: User credentials :returns: Auth_type class instance and parsed user credentials in dict :raises ValueError: If credential format is wrong (eg: bad auth_type),1,0,0,0,1,1,0,0,1,2
"def conn(host=None, user=None, password=None, init_fun=None, reset=False):
  if not hasattr(conn, 'connection') or reset:
  host = host if host is not None else config['database.host']
  user = user if user is not None else config['database.user']
  password = password if password is not None else config['database.password']
  if user is None:
  user = input(""Please enter DataJoint username: "")
  if password is None:
  password = getpass(prompt=""Please enter DataJoint password: "")
  init_fun = init_fun if init_fun is not None else config['connection.init_function']
  conn.connection = Connection(host, user, password, init_fun)
  return conn.connection","Returns a persistent connection object to be shared by multiple modules. If the connection is not yet established or reset=True, a new connection is set up. If connection information is not provided, it is taken from config which takes the information from dj_local_conf.json. If the password is not specified in that file datajoint prompts for the password. :param host: hostname :param user: mysql user :param password: mysql password :param init_fun: initialization function :param reset: whether the connection should be reset or not",1,0,1,0,2,1,0,0,1,2
"def editAccountInfo(self, short_name=None, author_name=None, author_url=None):
  return self.make_method(""editAccountInfo"", {
  ""access_token"": self.access_token,
  ""short_name"": short_name,
  ""author_name"": author_name,
  ""author_url"": author_url
  })","Use this method to update information about a Telegraph account. :param short_name: Optional. New account name. :type short_name: str :param author_name: Optional. New default author name used when creating new articles. :type author_name: str :param author_url: Optional. New default profile link, opened when users click on the author's name below the title. Can be any link, not necessarily to a Telegram profile or channel. :type author_url: str :returns: Account object with the default fields.",1,0,0,2,3,2,0,0,1,3
"def get_groups_for_user(self, user_name, marker=None, max_items=None):
  params = {'UserName' : user_name}
  if marker:
  params['Marker'] = marker
  if max_items:
  params['MaxItems'] = max_items
  return self.get_response('ListGroupsForUser', params,
  list_marker='Groups')",List the groups that a specified user belongs to. :type user_name: string :param user_name: The name of the user to list groups for. :type marker: string :param marker: Use this only when paginating results and only in follow-up request after you've received a response where the results are truncated. Set this to the value of the Marker element in the response you just received. :type max_items: int :param max_items: Use this only when paginating results to indicate the maximum number of groups you want in the response.,2,0,0,1,3,2,0,0,1,3
"async def ask_async(self,
  patch_stdout: bool = False,
  kbi_msg: str = DEFAULT_KBI_MESSAGE) -> Any:
  if self.should_skip_question:
  return self.default
  try:
  sys.stdout.flush()
  return await self.unsafe_ask_async(patch_stdout)
  except KeyboardInterrupt:
  print(""\n{}\n"".format(kbi_msg))
  return None",Ask the question using asyncio and return user response.,1,0,0,1,2,1,0,0,1,2
"def get_subreddit_gallery(self, subreddit, sort='time', window='top',
  limit=None):
  url = (self._base_url + ""/3/gallery/r/{0}/{1}/{2}/{3}"".format(
  subreddit, sort, window, '{}'))
  resp = self._send_request(url, limit=limit)
  return [_get_album_or_image(thing, self) for thing in resp]","Return a list of gallery albums/images submitted to a subreddit. A subreddit is a subsection of the website www.reddit.com, where users can, among other things, post images. :param subreddit: A valid subreddit name. :param sort: time | top - defaults to top. :param window: Change the date range of the request if the section is ""top"", day | week | month | year | all, defaults to day. :param limit: The number of items to return.",2,0,0,1,3,2,0,0,1,3
"def ip_address_add(session, ifname, ifaddr):
  def _append_inet_addr(intf_inet, addr):
  addr_list = intf_inet.split(',')
  if addr in addr_list:
  LOG.debug(
  'Interface ""%s"" has already ""ifaddr"": %s',
  intf.ifname, addr)
  return intf_inet
  else:
  addr_list.append(addr)
  return ','.join(addr_list)
  intf = ip_link_show(session, ifname=ifname)
  if not intf:
  LOG.debug('Interface ""%s"" does not exist', ifname)
  return None
  if ip.valid_ipv4(ifaddr):
  intf.inet = _append_inet_addr(intf.inet, ifaddr)
  elif ip.valid_ipv6(ifaddr):
  intf.inet6 = _append_inet_addr(intf.inet6, ifaddr)
  else:
  LOG.debug('Invalid IP address for ""ifaddr"": %s', ifaddr)
  return None
  return intf","Adds an IP address to interface record identified with the given ""ifname"". The arguments are similar to ""ip address add"" command of iproute2. :param session: Session instance connecting to database. :param ifname: Name of interface. :param ifaddr: IPv4 or IPv6 address. :return: Instance of record or ""None"" if failed.",1,0,0,1,2,1,1,0,0,2
"def eligible_for_direct_audit_enrollment(self, request, enterprise_customer, resource_id, course_key=None):
  course_identifier = course_key if course_key else resource_id
  return request.GET.get('audit') and \
  request.path == self.COURSE_ENROLLMENT_VIEW_URL.format(enterprise_customer.uuid, course_identifier) and \
  enterprise_customer.catalog_contains_course(resource_id) and \
  EnrollmentApiClient().has_course_mode(resource_id, 'audit')",Return whether a request is eligible for direct audit enrollment for a particular enterprise customer. 'resource_id' can be either course_run_id or program_uuid. We check for the following criteria: - The `audit` query parameter. - The user's being routed to the course enrollment landing page. - The customer's catalog contains the course in question. - The audit track is an available mode for the course.,2,0,0,0,2,1,0,0,1,2
"def toCrash(self, getMemoryDump = False):
  crash = Marshaller.loads(str(self.data))
  if not isinstance(crash, Crash):
  raise TypeError(
  ""Expected Crash instance, got %s instead"" % type(crash))
  crash._rowid = self.id
  if not crash.memoryMap:
  memory = getattr(self, ""memory"", [])
  if memory:
  crash.memoryMap = [dto.toMBI(getMemoryDump) for dto in memory]
  return crash",Returns a L{Crash} object using the data retrieved from the database. @type getMemoryDump: bool @param getMemoryDump: If C{True} retrieve the memory dump. Defaults to C{False} since this may be a costly operation. @rtype: L{Crash} @return: Crash object.,0,0,1,1,2,1,0,0,0,1
"def deprecated(reason, replacement, gone_in, issue=None):
  message = ""DEPRECATION: "" + reason
  if replacement is not None:
  message += "" A possible replacement is {}."".format(replacement)
  if issue is not None:
  url = ""https://github.com/pypa/pip/issues/"" + str(issue)
  message += "" You can find discussion regarding this at {}."".format(url)
  if gone_in is not None and parse(current_version) >= parse(gone_in):
  raise PipDeprecationWarning(message)
  warnings.warn(message, category=PipDeprecationWarning, stacklevel=2)","Helper to deprecate existing functionality. reason: Textual reason shown to the user about why this functionality has been deprecated. replacement: Textual suggestion shown to the user about what alternative functionality they can use. gone_in: The version of pip does this functionality should get removed in. Raises errors if pip's current version is greater than or equal to this. issue: Issue number on the tracker that would serve as a useful place for users to find related discussion and provide feedback. Always pass replacement, gone_in and issue as keyword arguments for clarity at the call site.",1,0,0,1,2,1,0,0,1,2
"def db_list(**connection_args):
  dbc = _connect(**connection_args)
  if dbc is None:
  return []
  cur = dbc.cursor()
  qry = 'SHOW DATABASES'
  try:
  _execute(cur, qry)
  except MySQLdb.OperationalError as exc:
  err = 'MySQL Error {0}: {1}'.format(*exc.args)
  __context__['mysql.error'] = err
  log.error(err)
  return []
  ret = []
  results = cur.fetchall()
  for dbs in results:
  ret.append(dbs[0])
  log.debug(ret)
  return ret",Return a list of databases of a MySQL server using the output from the ``SHOW DATABASES`` query. CLI Example: .. code-block:: bash salt '*' mysql.db_list,1,0,1,1,3,1,0,1,1,3
"def createprojectuser(self, user_id, name, **kwargs):
  data = {'name': name}
  if kwargs:
  data.update(kwargs)
  request = requests.post(
  '{0}/user/{1}'.format(self.projects_url, user_id), headers=self.headers,
  data=data, verify=self.verify_ssl, auth=self.auth, timeout=self.timeout)
  if request.status_code == 201:
  return True
  else:
  return False",Creates a new project owned by the specified user. Available only for admins. :param user_id: user_id of owner :param name: new project name :param description: short project description :param default_branch: 'master' by default :param issues_enabled: :param merge_requests_enabled: :param wiki_enabled: :param snippets_enabled: :param public: if true same as setting visibility_level = 20 :param visibility_level: :param import_url: :param sudo: :return:,1,0,0,1,2,1,0,0,2,3
"def input(self, prompt=None, default=None, validators=None, enable_quit=False, quit_string='q',
  quit_message='(enter q to Quit)'):
  prompt = self.__prompt_formatter.format_prompt(prompt=prompt, default=default, enable_quit=enable_quit,
  quit_string=quit_string, quit_message=quit_message)
  input_string = self.__screen.input(prompt=prompt)
  if enable_quit and quit_string == input_string:
  raise UserQuit
  if default is not None and input_string.strip() == '':
  input_string = default
  validation_result = self.validate_input(input_string, validators)
  return InputResult(input_string=input_string, validation_result=validation_result)",Prompt the user for input. :param prompt: the message to prompt the user. :param default: the default value to suggest as an answer. :param validators: list of validators to perform input validation. :param enable_quit: specifies whether the user can cancel out of the input prompt. :param quit_string: the string whcih the user must input in order to quit. :param quit_message: the message to explain how to quit. :return: an InputResult tuple.,1,0,0,1,2,1,0,0,1,2
"def load(cls, campaign_dir, ns_path=None, runner_type='Auto',
  optimized=True, check_repo=True):
  if ns_path is not None:
  ns_path = os.path.abspath(ns_path)
  campaign_dir = os.path.abspath(campaign_dir)
  db = DatabaseManager.load(campaign_dir)
  script = db.get_script()
  runner = None
  if ns_path is not None:
  runner = CampaignManager.create_runner(ns_path, script,
  runner_type, optimized)
  return cls(db, runner, check_repo)","Load an existing simulation campaign. Note that specifying an ns-3 installation is not compulsory when using this method: existing results will be available, but in order to run additional simulations it will be necessary to specify a SimulationRunner object, and assign it to the CampaignManager. Args: campaign_dir (str): path to the directory in which to save the simulation campaign database. ns_path (str): path to the ns-3 installation to employ in this campaign. runner_type (str): implementation of the SimulationRunner to use. Value can be: SimulationRunner (for running sequential simulations locally), ParallelRunner (for running parallel simulations locally), GridRunner (for running simulations using a DRMAA-compatible parallel task scheduler). optimized (bool): whether to configure the runner to employ an optimized ns-3 build.",0,0,1,0,1,1,0,1,0,2
"def command(self, verb, args=None):
  if self.__generating:
  raise NNTPSyncError(""Command issued while a generator is active"")
  cmd = verb
  if args:
  cmd += "" "" + args
  cmd += ""\r\n""
  self.socket.sendall(cmd)
  try:
  code, message = self.status()
  except NNTPTemporaryError as e:
  if e.code() != 480:
  raise e
  code, message = self.command(""AUTHINFO USER"", self.username)
  if code == 381:
  code, message = self.command(""AUTHINFO PASS"", self.password)
  if code != 281:
  raise NNTPReplyError(code, message)
  code, message = self.command(verb, args)
  return code, message",Call a command on the server. If the user has not authenticated then authentication will be done as part of calling the command on the server. For commands that don't return a status message the status message will default to an empty string. Args: verb: The verb of the command to call. args: The arguments of the command as a string (default None). Returns: A tuple of status code (as an integer) and status message. Note: You can run raw commands by supplying the full command (including args) in the verb. Note: Although it is possible you shouldn't issue more than one command at a time by adding newlines to the verb as it will most likely lead to undesirable results.,2,0,0,2,4,1,0,0,1,2
"def sendfrom(self, user_id, dest_address, amount, minconf=1):
  amount = Decimal(amount).quantize(self.quantum, rounding=ROUND_HALF_EVEN)
  txhash = self.rpc.call(""sendfrom"",
  user_id, dest_address, float(str(amount)), minconf
  )
  self.logger.debug(""Send %s %s from %s to %s"" % (str(amount), self.coin,
  str(user_id), dest_address))
  self.logger.debug(""Transaction hash: %s"" % txhash)
  return txhash",Send coins from user's account. Args: user_id (str): this user's unique identifier dest_address (str): address which is to receive coins amount (str or Decimal): amount to send (eight decimal points) minconf (int): ensure the account has a valid balance using this many confirmations (default=1) Returns: str: transaction ID,1,0,0,1,2,1,0,0,1,2
"def is_writeable(path, check_parent=False):
  if os.access(path, os.F_OK) and os.access(path, os.W_OK):
  return True
  if os.access(path, os.F_OK) and not os.access(path, os.W_OK):
  return False
  if check_parent is False:
  return False
  parent_dir = os.path.dirname(path)
  if not os.access(parent_dir, os.F_OK):
  return False
  return os.access(parent_dir, os.W_OK)","Check if a given path is writeable by the current user. :param path: The path to check :param check_parent: If the path to check does not exist, check for the ability to write to the parent directory instead :returns: True or False",1,0,0,0,1,1,0,0,0,1
"def update(self, update_dict=None, raw=False, **kwargs):
  if update_dict is None:
  update_dict = kwargs
  if raw:
  self._collection.update_one({ID_KEY: self[ID_KEY]}, update_dict)
  new_data = self._collection.find_one({ID_KEY: self[ID_KEY]})
  dict.clear(self)
  dict.update(self, new_data)
  else:
  for key, value in update_dict.items():
  self._check_type(key, value)
  dict.update(self, update_dict)
  self._collection.update_one({ID_KEY: self[ID_KEY]}, {SET: update_dict})","Applies updates both to the database object and to the database via the mongo update method with the $set argument. Use the `raw` keyword to perform an arbitrary mongo update query. WARNING: Raw updates do not perform type checking. WARNING: While the update operation itself is atomic, it is not atomic with loads and modifications to the object. You must provide your own synchronization if you have multiple threads or processes possibly modifying the same database object. While this is safer from a concurrency perspective than the access pattern load -> modify -> save as it only updates keys specified in the update_dict, it will still overwrite updates to those same keys that were made while the object was held in memory. @param update_dict: dictionary of updates to apply @param raw: if set to True, uses the contents of update_dict directly to perform the update rather than wrapping them in $set. @param **kwargs: used as update_dict if no update_dict is None",1,1,1,1,4,1,1,1,0,3
"def update_layer(LayerId=None, Name=None, Shortname=None, Attributes=None, CloudWatchLogsConfiguration=None, CustomInstanceProfileArn=None, CustomJson=None, CustomSecurityGroupIds=None, Packages=None, VolumeConfigurations=None, EnableAutoHealing=None, AutoAssignElasticIps=None, AutoAssignPublicIps=None, CustomRecipes=None, InstallUpdatesOnBoot=None, UseEbsOptimizedInstances=None, LifecycleEventConfiguration=None):
  pass","Updates a specified layer. See also: AWS API Documentation :example: response = client.update_layer( LayerId='string', Name='string', Shortname='string', Attributes={ 'string': 'string' }, CloudWatchLogsConfiguration={ 'Enabled': True|False, 'LogStreams': [ { 'LogGroupName': 'string', 'DatetimeFormat': 'string', 'TimeZone': 'LOCAL'|'UTC', 'File': 'string', 'FileFingerprintLines': 'string', 'MultiLineStartPattern': 'string', 'InitialPosition': 'start_of_file'|'end_of_file', 'Encoding': 'ascii'|'big5'|'big5hkscs'|'cp037'|'cp424'|'cp437'|'cp500'|'cp720'|'cp737'|'cp775'|'cp850'|'cp852'|'cp855'|'cp856'|'cp857'|'cp858'|'cp860'|'cp861'|'cp862'|'cp863'|'cp864'|'cp865'|'cp866'|'cp869'|'cp874'|'cp875'|'cp932'|'cp949'|'cp950'|'cp1006'|'cp1026'|'cp1140'|'cp1250'|'cp1251'|'cp1252'|'cp1253'|'cp1254'|'cp1255'|'cp1256'|'cp1257'|'cp1258'|'euc_jp'|'euc_jis_2004'|'euc_jisx0213'|'euc_kr'|'gb2312'|'gbk'|'gb18030'|'hz'|'iso2022_jp'|'iso2022_jp_1'|'iso2022_jp_2'|'iso2022_jp_2004'|'iso2022_jp_3'|'iso2022_jp_ext'|'iso2022_kr'|'latin_1'|'iso8859_2'|'iso8859_3'|'iso8859_4'|'iso8859_5'|'iso8859_6'|'iso8859_7'|'iso8859_8'|'iso8859_9'|'iso8859_10'|'iso8859_13'|'iso8859_14'|'iso8859_15'|'iso8859_16'|'johab'|'koi8_r'|'koi8_u'|'mac_cyrillic'|'mac_greek'|'mac_iceland'|'mac_latin2'|'mac_roman'|'mac_turkish'|'ptcp154'|'shift_jis'|'shift_jis_2004'|'shift_jisx0213'|'utf_32'|'utf_32_be'|'utf_32_le'|'utf_16'|'utf_16_be'|'utf_16_le'|'utf_7'|'utf_8'|'utf_8_sig', 'BufferDuration': 123, 'BatchCount': 123, 'BatchSize': 123 }, ] }, CustomInstanceProfileArn='string', CustomJson='string', CustomSecurityGroupIds=[ 'string', ], Packages=[ 'string', ], VolumeConfigurations=[ { 'MountPoint': 'string', 'RaidLevel': 123, 'NumberOfDisks': 123, 'Size': 123, 'VolumeType': 'string', 'Iops': 123 }, ], EnableAutoHealing=True|False, AutoAssignElasticIps=True|False, AutoAssignPublicIps=True|False, CustomRecipes={ 'Setup': [ 'string', ], 'Configure': [ 'string', ], 'Deploy': [ 'string', ], 'Undeploy': [ 'string', ], 'Shutdown': [ 'string', ] }, InstallUpdatesOnBoot=True|False, UseEbsOptimizedInstances=True|False, LifecycleEventConfiguration={ 'Shutdown': { 'ExecutionTimeout': 123, 'DelayUntilElbConnectionsDrained': True|False } } ) :type LayerId: string :param LayerId: [REQUIRED] The layer ID. :type Name: string :param Name: The layer name, which is used by the console. :type Shortname: string :param Shortname: For custom layers only, use this parameter to specify the layer's short name, which is used internally by AWS OpsWorks Stacks and by Chef. The short name is also used as the name for the directory where your app files are installed. It can have a maximum of 200 characters and must be in the following format: /A[a-z0-9-_.]+Z/. The built-in layers' short names are defined by AWS OpsWorks Stacks. For more information, see the Layer Reference :type Attributes: dict :param Attributes: One or more user-defined key/value pairs to be added to the stack attributes. (string) -- (string) -- :type CloudWatchLogsConfiguration: dict :param CloudWatchLogsConfiguration: Specifies CloudWatch Logs configuration options for the layer. For more information, see CloudWatchLogsLogStream . Enabled (boolean) --Whether CloudWatch Logs is enabled for a layer. LogStreams (list) --A list of configuration options for CloudWatch Logs. (dict) --Describes the Amazon CloudWatch logs configuration for a layer. For detailed information about members of this data type, see the CloudWatch Logs Agent Reference . LogGroupName (string) --Specifies the destination log group. A log group is created automatically if it doesn't already exist. Log group names can be between 1 and 512 characters long. Allowed characters include a-z, A-Z, 0-9, '_' (underscore), '-' (hyphen), '/' (forward slash), and '.' (period). DatetimeFormat (string) --Specifies how the time stamp is extracted from logs. For more information, see the CloudWatch Logs Agent Reference . TimeZone (string) --Specifies the time zone of log event time stamps. File (string) --Specifies log files that you want to push to CloudWatch Logs. File can point to a specific file or multiple files (by using wild card characters such as /var/log/system.log* ). Only the latest file is pushed to CloudWatch Logs, based on file modification time. We recommend that you use wild card characters to specify a series of files of the same type, such as access_log.2014-06-01-01 , access_log.2014-06-01-02 , and so on by using a pattern like access_log.* . Don't use a wildcard to match multiple file types, such as access_log_80 and access_log_443 . To specify multiple, different file types, add another log stream entry to the configuration file, so that each log file type is stored in a different log group. Zipped files are not supported. FileFingerprintLines (string) --Specifies the range of lines for identifying a file. The valid values are one number, or two dash-delimited numbers, such as '1', '2-5'. The default value is '1', meaning the first line is used to calculate the fingerprint. Fingerprint lines are not sent to CloudWatch Logs unless all specified lines are available. MultiLineStartPattern (string) --Specifies the pattern for identifying the start of a log message. InitialPosition (string) --Specifies where to start to read data (start_of_file or end_of_file). The default is start_of_file. This setting is only used if there is no state persisted for that log stream. Encoding (string) --Specifies the encoding of the log file so that the file can be read correctly. The default is utf_8 . Encodings supported by Python codecs.decode() can be used here. BufferDuration (integer) --Specifies the time duration for the batching of log events. The minimum value is 5000ms and default value is 5000ms. BatchCount (integer) --Specifies the max number of log events in a batch, up to 10000. The default value is 1000. BatchSize (integer) --Specifies the maximum size of log events in a batch, in bytes, up to 1048576 bytes. The default value is 32768 bytes. This size is calculated as the sum of all event messages in UTF-8, plus 26 bytes for each log event. :type CustomInstanceProfileArn: string :param CustomInstanceProfileArn: The ARN of an IAM profile to be used for all of the layer's EC2 instances. For more information about IAM ARNs, see Using Identifiers . :type CustomJson: string :param CustomJson: A JSON-formatted string containing custom stack configuration and deployment attributes to be installed on the layer's instances. For more information, see Using Custom JSON . :type CustomSecurityGroupIds: list :param CustomSecurityGroupIds: An array containing the layer's custom security group IDs. (string) -- :type Packages: list :param Packages: An array of Package objects that describe the layer's packages. (string) -- :type VolumeConfigurations: list :param VolumeConfigurations: A VolumeConfigurations object that describes the layer's Amazon EBS volumes. (dict) --Describes an Amazon EBS volume configuration. MountPoint (string) -- [REQUIRED]The volume mount point. For example '/dev/sdh'. RaidLevel (integer) --The volume RAID level . NumberOfDisks (integer) -- [REQUIRED]The number of disks in the volume. Size (integer) -- [REQUIRED]The volume size. VolumeType (string) --The volume type: standard - Magnetic io1 - Provisioned IOPS (SSD) gp2 - General Purpose (SSD) Iops (integer) --For PIOPS volumes, the IOPS per disk. :type EnableAutoHealing: boolean :param EnableAutoHealing: Whether to disable auto healing for the layer. :type AutoAssignElasticIps: boolean :param AutoAssignElasticIps: Whether to automatically assign an Elastic IP address to the layer's instances. For more information, see How to Edit a Layer . :type AutoAssignPublicIps: boolean :param AutoAssignPublicIps: For stacks that are running in a VPC, whether to automatically assign a public IP address to the layer's instances. For more information, see How to Edit a Layer . :type CustomRecipes: dict :param CustomRecipes: A LayerCustomRecipes object that specifies the layer's custom recipes. Setup (list) --An array of custom recipe names to be run following a setup event. (string) -- Configure (list) --An array of custom recipe names to be run following a configure event. (string) -- Deploy (list) --An array of custom recipe names to be run following a deploy event. (string) -- Undeploy (list) --An array of custom recipe names to be run following a undeploy event. (string) -- Shutdown (list) --An array of custom recipe names to be run following a shutdown event. (string) -- :type InstallUpdatesOnBoot: boolean :param InstallUpdatesOnBoot: Whether to install operating system and package updates when the instance boots. The default value is true . To control when updates are installed, set this value to false . You must then update your instances manually by using CreateDeployment to run the update_dependencies stack command or manually running yum (Amazon Linux) or apt-get (Ubuntu) on the instances. Note We strongly recommend using the default value of true , to ensure that your instances have the latest security updates. :type UseEbsOptimizedInstances: boolean :param UseEbsOptimizedInstances: Whether to use Amazon EBS-optimized instances. :type LifecycleEventConfiguration: dict :param LifecycleEventConfiguration: Shutdown (dict) --A ShutdownEventConfiguration object that specifies the Shutdown event configuration. ExecutionTimeout (integer) --The time, in seconds, that AWS OpsWorks Stacks will wait after triggering a Shutdown event before shutting down an instance. DelayUntilElbConnectionsDrained (boolean) --Whether to enable Elastic Load Balancing connection draining. For more information, see Connection Draining",1,0,0,1,2,1,0,0,2,3
"def installation_report(self, req, dist, what=""Installed""):
  msg = ""\n%(what)s %(eggloc)s%(extras)s""
  if self.multi_version and not self.no_report:
  msg += '\n' + self.__mv_warning
  if self.install_dir not in map(normalize_path, sys.path):
  msg += '\n' + self.__id_warning
  eggloc = dist.location
  name = dist.project_name
  version = dist.version
  extras = ''
  return msg % locals()",Helpful installation message for display to package users,1,0,0,1,2,1,0,0,1,2
"def user(self, username=None, pk=None, **kwargs):
  _users = self.users(username=username, pk=pk, **kwargs)
  if len(_users) == 0:
  raise NotFoundError(""No user criteria matches"")
  if len(_users) != 1:
  raise MultipleFoundError(""Multiple users fit criteria"")
  return _users[0]",User of KE-chain. Provides single user of :class:`User` of KE-chain. You can filter on username or id or an advanced filter. :param username: (optional) username to filter :type username: basestring or None :param pk: (optional) id of the user to filter :type pk: basestring or None :param kwargs: Additional filtering keyword=value arguments :type kwargs: dict or None :return: List of :class:`User` :raises NotFoundError: when a user could not be found :raises MultipleFoundError: when more than a single user can be found,1,0,1,1,3,2,0,0,1,3
"def add_artifact(
  self,
  filename,
  name=None,
  metadata=None,
  content_type=None,
  ):
  filename = os.path.abspath(filename)
  name = os.path.basename(filename) if name is None else name
  self._emit_artifact_added(name, filename, metadata, content_type)","Add a file as an artifact. In Sacred terminology an artifact is a file produced by the experiment run. In case of a MongoObserver that means storing the file in the database. See also :py:meth:`sacred.Experiment.add_artifact`. Parameters ---------- filename : str name of the file to be stored as artifact name : str, optional optionally set the name of the artifact. Defaults to the filename. metadata: dict optionally attach metadata to the artifact. This only has an effect when using the MongoObserver. content_type: str, optional optionally attach a content-type to the artifact. This only has an effect when using the MongoObserver.",1,1,0,0,2,1,0,0,1,2
"def update_guest_additions(self, source, arguments, flags):
  if not isinstance(source, basestring):
  raise TypeError(""source can only be an instance of type basestring"")
  if not isinstance(arguments, list):
  raise TypeError(""arguments can only be an instance of type list"")
  for a in arguments[:10]:
  if not isinstance(a, basestring):
  raise TypeError(
  ""array can only contain objects of type basestring"")
  if not isinstance(flags, list):
  raise TypeError(""flags can only be an instance of type list"")
  for a in flags[:10]:
  if not isinstance(a, AdditionsUpdateFlag):
  raise TypeError(
  ""array can only contain objects of type AdditionsUpdateFlag"")
  progress = self._call(""updateGuestAdditions"",
  in_p=[source, arguments, flags])
  progress = IProgress(progress)
  return progress","Automatically updates already installed Guest Additions in a VM. At the moment only Windows guests are supported. Because the VirtualBox Guest Additions drivers are not WHQL-certified yet there might be warning dialogs during the actual Guest Additions update. These need to be confirmed manually in order to continue the installation process. This applies to Windows 2000 and Windows XP guests and therefore these guests can't be updated in a fully automated fashion without user interaction. However, to start a Guest Additions update for the mentioned Windows versions anyway, the flag AdditionsUpdateFlag_WaitForUpdateStartOnly can be specified. See :py:class:`AdditionsUpdateFlag` for more information. in source of type str Path to the Guest Additions .ISO file to use for the update. in arguments of type str Optional command line arguments to use for the Guest Additions installer. Useful for retrofitting features which weren't installed before in the guest. in flags of type :class:`AdditionsUpdateFlag` :py:class:`AdditionsUpdateFlag` flags. return progress of type :class:`IProgress` Progress object to track the operation completion. raises :class:`VBoxErrorNotSupported` Guest OS is not supported for automated Guest Additions updates or the already installed Guest Additions are not ready yet. raises :class:`VBoxErrorIprtError` Error while updating.",0,0,0,1,1,1,0,0,1,2
"def stage_import_from_url(self, url, token=None, username=None, password=None, insecure=False):
  schema = ImportSchema()
  resp = self.service.post(self.base,
  params={'url': url, 'token': token, 'username': username, 'password': password, 'insecure': insecure})
  return self.service.decode(schema, resp)",Stage an import from a URL to another CDRouter system. :param url: URL to import as string. :param token: (optional) API token to use as string (may be required if importing from a CDRouter 10+ system). :param username: (optional) API username to use as string (may be required if importing from a CDRouter 10+ system). :param password: (optional) API password to use as string (may be required if importing from a CDRouter 10+ system). :param insecure: (optional) Allow insecure HTTPS connections if bool `True`. :return: :class:`imports.Import <imports.Import>` object,1,0,0,1,2,2,0,0,2,4
"def schema_list(dbname, user=None,
  db_user=None, db_password=None,
  db_host=None, db_port=None):
  ret = {}
  query = (''.join([
  'SELECT '
  'pg_namespace.nspname as ""name"",'
  'pg_namespace.nspacl as ""acl"", '
  'pg_roles.rolname as ""owner"" '
  'FROM pg_namespace '
  'LEFT JOIN pg_roles ON pg_roles.oid = pg_namespace.nspowner '
  ]))
  rows = psql_query(query, runas=user,
  host=db_host,
  user=db_user,
  port=db_port,
  maintenance_db=dbname,
  password=db_password)
  for row in rows:
  retrow = {}
  for key in ('owner', 'acl'):
  retrow[key] = row[key]
  ret[row['name']] = retrow
  return ret",Return a dict with information about schemas in a Postgres database. CLI Example: .. code-block:: bash salt '*' postgres.schema_list dbname dbname Database name we query on user The system user the operation should be performed on behalf of db_user database username if different from config or default db_password user password if any password for a specified user db_host Database host if different from config or default db_port Database port if different from config or default,1,0,1,1,3,1,0,1,1,3
"def insert_query(connection, publicId, aead, keyhandle, aeadobj):
  keyhandle = key_handle_to_int(keyhandle)
  if not keyhandle == aead.key_handle:
  print(""WARNING: keyhandle does not match aead.key_handle"")
  return None
  try:
  sql = aeadobj.insert().values(public_id=publicId, keyhandle=aead.key_handle, nonce=aead.nonce, aead=aead.data)
  result = connection.execute(sql)
  return result
  except sqlalchemy.exc.IntegrityError:
  pass
  return None",this functions read the response fields and creates sql query. then inserts everything inside the database,0,1,0,0,1,0,1,1,0,2
"def parse_with_retrieved(self, retrieved):
  success = False
  node_list = []
  try:
  out_folder = retrieved['retrieved']
  except KeyError:
  self.logger.error(""No retrieved folder found"")
  return success, node_list
  list_of_files = out_folder.get_folder_list()
  output_files = [self._calc._OUTPUT_FILE_NAME]
  if set(output_files) <= set(list_of_files):
  pass
  else:
  self.logger.error(
  ""Not all expected output files {} were found"".format(
  output_files))
  return success, node_list
  success = True
  return success, node_list","Parse output data folder, store results in database. :param retrieved: a dictionary of retrieved nodes, where the key is the link name :returns: a tuple with two values ``(bool, node_list)``, where: * ``bool``: variable to tell if the parsing succeeded * ``node_list``: list of new nodes to be stored in the db (as a list of tuples ``(link_name, node)``)",1,0,0,1,2,1,0,0,1,2
"def set_game_score(token, user_id, score, force=None, disable_edit_message=None, chat_id=None, message_id=None,
  inline_message_id=None):
  method_url = r'setGameScore'
  payload = {'user_id': user_id, 'score': score}
  if force:
  payload['force'] = force
  if chat_id:
  payload['chat_id'] = chat_id
  if message_id:
  payload['message_id'] = message_id
  if inline_message_id:
  payload['inline_message_id'] = inline_message_id
  if disable_edit_message:
  payload['disable_edit_message'] = disable_edit_message
  return _make_request(token, method_url, params=payload)","Use this method to set the score of the specified user in a game. On success, if the message was sent by the bot, returns the edited Message, otherwise returns True. Returns an error, if the new score is not greater than the user's current score in the chat. :param token: Bot's token (you don't need to fill this) :param user_id: User identifier :param score: New score, must be non-negative :param force: (Optional) Pass True, if the high score is allowed to decrease. This can be useful when fixing mistakes or banning cheaters :param disable_edit_message: (Optional) Pass True, if the game message should not be automatically edited to include the current scoreboard :param chat_id: (Optional, required if inline_message_id is not specified) Unique identifier for the target chat (or username of the target channel in the format @channelusername) :param message_id: (Optional, required if inline_message_id is not specified) Unique identifier of the sent message :param inline_message_id: (Optional, required if chat_id and message_id are not specified) Identifier of the inline message :return:",1,0,0,2,3,2,0,0,2,4
"def generate_binding_credentials(self, binding):
  uri = self.clusters.get(binding.instance.get_cluster(), None)
  if not uri:
  raise ErrClusterConfig(binding.instance.get_cluster())
  creds = {""username"" : self.generate_binding_username(binding),
  ""password"" : pwgen(32, symbols=False),
  ""database"" : binding.instance.get_dbname()}
  uri = uri % (
  creds[""username""],
  creds[""password""],
  creds[""database""])
  creds[""uri""] = uri
  return creds",Generate binding credentials This function will permit to define the configuration to connect to the instance. Those credentials will be stored on a secret and exposed to a a Pod. We should at least returns the 'username' and 'password'. Args: binding (AtlasServiceBinding.Binding): A binding Returns: dict: All credentials and secrets. Raises: ErrClusterConfig: Connection string to the cluster is not available.,0,0,0,1,1,1,0,0,0,1
"async def unban_chat_member(self, chat_id: typing.Union[base.Integer, base.String],
  user_id: base.Integer) -> base.Boolean:
  payload = generate_payload(**locals())
  result = await self.request(api.Methods.UNBAN_CHAT_MEMBER, payload)
  return result","Use this method to unban a previously kicked user in a supergroup or channel. ` The user will not return to the group or channel automatically, but will be able to join via link, etc. The bot must be an administrator for this to work. Source: https://core.telegram.org/bots/api#unbanchatmember :param chat_id: Unique identifier for the target group or username of the target supergroup or channel :type chat_id: :obj:`typing.Union[base.Integer, base.String]` :param user_id: Unique identifier of the target user :type user_id: :obj:`base.Integer` :return: Returns True on success :rtype: :obj:`base.Boolean`",2,0,0,2,4,2,0,0,1,3
"def nearest(self, coordinates, num_results=1, objects=False):
  if objects:
  return self._nearest_obj(coordinates, num_results, objects)
  p_mins, p_maxs = self.get_coordinate_pointers(coordinates)
  p_num_results = ctypes.pointer(ctypes.c_uint64(num_results))
  it = ctypes.pointer(ctypes.c_int64())
  core.rt.Index_NearestNeighbors_id(self.handle,
  p_mins,
  p_maxs,
  self.properties.dimension,
  ctypes.byref(it),
  p_num_results)
  return self._get_ids(it, p_num_results.contents.value)","Returns the ``k``-nearest objects to the given coordinates. :param coordinates: sequence or array This may be an object that satisfies the numpy array protocol, providing the index's dimension * 2 coordinate pairs representing the `mink` and `maxk` coordinates in each dimension defining the bounds of the query window. :param num_results: integer The number of results to return nearest to the given coordinates. If two index entries are equidistant, *both* are returned. This property means that :attr:`num_results` may return more items than specified :param objects: True / False / 'raw' If True, the nearest method will return index objects that were pickled when they were stored with each index entry, as well as the id and bounds of the index entries. If 'raw', it will return the object as entered into the database without the :class:`rtree.index.Item` wrapper. Example of finding the three items nearest to this one:: >>> from rtree import index >>> idx = index.Index() >>> idx.insert(4321, (34.37, 26.73, 49.37, 41.73), obj=42) >>> hits = idx.nearest((0, 0, 10, 10), 3, objects=True)",1,0,1,1,3,1,0,0,1,2
"def smart_open_write(path=None, mode='wb', encoding=None):
  if path is not None:
  fh = io.open(path, mode=mode, encoding=encoding)
  else:
  fh = io.open(sys.stdout.fileno(), mode=mode, encoding=encoding)
  try:
  yield fh
  finally:
  if fh.fileno() != sys.stdout.fileno():
  fh.close()","Open a file for writing or return ``stdout``. Adapted from StackOverflow user ""Wolph"" (http://stackoverflow.com/a/17603000).",0,0,0,1,1,1,0,0,1,2
"def language_id(self):
  lang_id = self.kwargs['lang_id']
  if lang_id not in {l[0] for l in rosetta_settings.ROSETTA_LANGUAGES}:
  raise Http404
  if not can_translate_language(self.request.user, lang_id):
  raise Http404
  return lang_id","Determine/return the language id from the url kwargs, after validating that: 1. the language is in rosetta_settings.ROSETTA_LANGUAGES, and 2. the current user is permitted to translate that language (If either of the above fail, throw a 404.)",1,0,0,1,2,1,0,0,1,2
"def parse_scope(self, tup_tree):
  self.check_node(tup_tree, 'SCOPE', (),
  ('CLASS', 'ASSOCIATION', 'REFERENCE', 'PROPERTY',
  'METHOD', 'PARAMETER', 'INDICATION'), ())
  scopes = NocaseDict()
  for k, v in attrs(tup_tree).items():
  v_ = self.unpack_boolean(v)
  if v_ is None:
  raise CIMXMLParseError(
  _format(""Element {0!A} has an invalid value {1!A} for its ""
  ""boolean attribute {2!A}"", name(tup_tree), v, k),
  conn_id=self.conn_id)
  scopes[k] = v_
  return scopes","Parse a SCOPE element and return a dictionary with an item for each specified scope attribute. The keys of the dictionary items are the scope names in upper case; the values are the Python boolean values True or False. Unspecified scope attributes are not represented in the returned dictionary; the user is expected to assume their default value of False. The returned dictionary does not preserve order of the scope attributes. :: <!ELEMENT SCOPE EMPTY> <!ATTLIST SCOPE CLASS (true | false) ""false"" ASSOCIATION (true | false) ""false"" REFERENCE (true | false) ""false"" PROPERTY (true | false) ""false"" METHOD (true | false) ""false"" PARAMETER (true | false) ""false"" INDICATION (true | false) ""false""",0,0,0,1,1,1,0,0,1,2
"def session(self):
  return self.session_class(
  client_key=self.client_key,
  client_secret=self.client_secret,
  signature_method=self.signature_method,
  signature_type=self.signature_type,
  rsa_key=self.rsa_key,
  client_class=self.client_class,
  force_include_body=self.force_include_body,
  blueprint=self,
  base_url=self.base_url,
  **self.kwargs
  )",This is a session between the consumer (your website) and the provider (e.g. Twitter). It is *not* a session between a user of your website and your website. :return:,0,0,0,1,1,1,0,0,1,2
"def open(filename, flag='c', protocol=None, writeback=False,
  maxsize=DEFAULT_MAXSIZE, timeout=DEFAULT_TIMEOUT):
  import dbm
  dict = dbm.open(filename, flag)
  if maxsize is None and timeout is None:
  return Shelf(dict, protocol, writeback)
  elif maxsize is None:
  return TimeoutShelf(dict, protocol, writeback, timeout=timeout)
  elif timeout is None:
  return LRUShelf(dict, protocol, writeback, maxsize=maxsize)
  return LRUTimeoutShelf(dict, protocol, writeback, timeout=timeout,
  maxsize=maxsize)","Open a database file as a persistent dictionary. The persistent dictionary file is opened using :func:`dbm.open`, so performance will depend on which :mod:`dbm` modules are installed. :func:`open` chooses to open a :class:`Shelf <shelve.Shelf>`, :class:`LRUShelf`, :class:`TimeoutShelf`, or :class:`LRUTimeoutShelf` depending on the values of keyword arguments *maxsize* and *timeout*. A :data:`None` value for *maxsize* and *timeout* will disable the LRU cache management and automatic data timeout features respectively. :param filename: The base filename for the underlying database that is passed to :func:`dbm.open`. :param flag: The flag to pass to :func:`dbm.open`. :param protocol: The pickle protocol to pass to :func:`pickle.dump`. :param writeback: Whether or not to write back all accessed entries on :meth:`Shelf.sync <shelve.Shelf.sync>` and :meth:`Shelf.close <shelve.Shelf.close>` :type writeback: bool :param maxsize: The maximum size the container is allowed to grow to. ``0`` means that no size limit is enforced. :data:`None` means that LRU cache management is disabled. :type maxsize: integer or :data:`None` :param timeout: The default timeout value for data (in seconds). ``0`` means that the data never expires. :data:`None` means that automatic timeout features will be disabled. :type timeout: integer or :data:`None` :return: A shelf :rtype: :class:`~shelve.Shelf`, :class:`LRUShelf`, :class:`TimeoutShelf`, or :class:`LRUTimeoutShelf`",1,1,1,0,3,1,0,0,0,1
"def _ask(question, default=None, data_type='str', show_hint=False):
  data = default
  if data_type == 'bool':
  data = None
  default_string = ""Y"" if default else ""N""
  while data not in ('Y', 'J', 'N', '1', '0'):
  data = input(""%s? [%s]: "" % (question, default_string)).upper()
  if data == '':
  return default
  return data in ('Y', 'J', '1')
  elif data_type in ('str', 'unicode'):
  if show_hint:
  msg = ""%s? [%s] (%s): "" % (question, default, data_type)
  else:
  msg = question
  data = input(msg)
  if len(data) == 0:
  data = default
  elif data_type == 'int':
  if show_hint:
  msg = ""%s? [%s] (%s): "" % (question, default, data_type)
  else:
  msg = question
  data = input(msg)
  if len(data) == 0:
  data = int(default)
  else:
  data = int(data)
  return data",Interactively ask the user for data,1,0,0,1,2,1,0,0,1,2
"def is_following(user, obj, flag=''):
  check(obj)
  qs = apps.get_model('actstream', 'follow').objects.filter(
  user=user, object_id=obj.pk,
  content_type=ContentType.objects.get_for_model(obj)
  )
  if flag:
  qs = qs.filter(flag=flag)
  return qs.exists()","Checks if a ""follow"" relationship exists. Returns True if exists, False otherwise. Pass a string value to ``flag`` to determine which type of ""follow"" relationship you want to check. Example:: is_following(request.user, group) is_following(request.user, group, flag='liking')",1,0,1,0,2,1,0,1,1,3
"def reload(self):
  old_doc = self.mongokat_collection.find_one({""_id"": self['_id']}, read_use=""primary"")
  if not old_doc:
  raise OperationFailure('Can not reload an unsaved document.'
  ' %s is not found in the database. Maybe _id was a string and not ObjectId?' % self['_id'])
  else:
  for k in list(self.keys()):
  del self[k]
  self.update(dotdict(old_doc))
  self._initialized_with_doc = False","allow to refresh the document, so after using update(), it could reload its value from the database. Be carreful : reload() will erase all unsaved values. If no _id is set in the document, a KeyError is raised.",1,0,1,0,2,1,1,1,0,3
"def _is_duplicate_file(db_session, filename, hashes):
  file_ = (
  db_session.query(File)
  .filter(
  (File.filename == filename)
  | (File.blake2_256_digest == hashes[""blake2_256""])
  )
  .first()
  )
  if file_ is not None:
  return (
  file_.filename == filename
  and file_.sha256_digest == hashes[""sha256""]
  and file_.md5_digest == hashes[""md5""]
  and file_.blake2_256_digest == hashes[""blake2_256""]
  )
  return None","Check to see if file already exists, and if it's content matches. A file is considered to exist if its filename *or* blake2 digest are present in a file row in the database. Returns: - True: This file is a duplicate and all further processing should halt. - False: This file exists, but it is not a duplicate. - None: This file does not exist.",0,0,1,1,2,0,0,1,1,2
"def checkout(branch, quiet=False, as_path=False):
  try:
  if as_path:
  branch = '-- %s' % branch
  run('checkout %s %s' % (quiet and '-q' or '', branch))
  return True
  except GitError as e:
  if 'need to resolve your current index' in e.output:
  raise
  return False","Check out that branch Defaults to a quiet checkout, giving no stdout if stdout it wanted, call with quiet = False Defaults to checking out branches If as_path is true, then treat ""branch"" like a file, i.e. $ git checkout -- branch All errors will pass silently, just returning False except any messages about ""you need to resolve your current index"" These indicate that the repository is not in a normal state and action by a user is usually needed to resolve that So the exception is allowed to rise probably stopping the script",1,0,0,1,2,1,0,0,1,2
"def hist(hist_function, *, options={}, **interact_params):
  params = {
  'marks': [{
  'sample': _array_or_placeholder(hist_function),
  'bins': _get_option('bins'),
  'normalized': _get_option('normalized'),
  'scales': (
  lambda opts: {'sample': opts['x_sc'], 'count': opts['y_sc']}
  ),
  }],
  }
  fig = options.get('_fig', False) or _create_fig(options=options)
  [hist] = _create_marks(
  fig=fig, marks=[bq.Hist], options=options, params=params
  )
  _add_marks(fig, [hist])
  def wrapped(**interact_params):
  hist.sample = util.maybe_call(hist_function, interact_params)
  controls = widgets.interactive(wrapped, **interact_params)
  return widgets.VBox([controls, fig])","Generates an interactive histogram that allows users to change the parameters of the input hist_function. Args: hist_function (Array | (*args -> Array int | Array float)): Function that takes in parameters to interact with and returns an array of numbers. These numbers will be plotted in the resulting histogram. Kwargs: {options} interact_params (dict): Keyword arguments in the same format as `ipywidgets.interact`. One argument is required for each argument of `hist_function`. Returns: VBox with two children: the interactive controls and the figure. >>> def gen_random(n_points): ... return np.random.normal(size=n_points) >>> hist(gen_random, n_points=(0, 1000, 10)) VBox(...)",1,0,0,1,2,1,0,0,1,2
"def addgroupmember(self, group_id, user_id, access_level):
  if not isinstance(access_level, int):
  if access_level.lower() == 'owner':
  access_level = 50
  elif access_level.lower() == 'master':
  access_level = 40
  elif access_level.lower() == 'developer':
  access_level = 30
  elif access_level.lower() == 'reporter':
  access_level = 20
  elif access_level.lower() == 'guest':
  access_level = 10
  else:
  return False
  data = {'id': group_id, 'user_id': user_id, 'access_level': access_level}
  request = requests.post(
  '{0}/{1}/members'.format(self.groups_url, group_id),
  headers=self.headers, data=data, verify=self.verify_ssl, auth=self.auth, timeout=self.timeout)
  return request.status_code == 201","Adds a project member to a project :param user_id: user id :param access_level: access level, see gitlab help to know more :return: True if success",1,1,0,2,4,1,0,0,2,3
"def get_name(self, tag):
  name = super(functionTagProcessor, self).get_name(tag)
  if self.include_function_signatures:
  func_args = tag.findChild('arglist')
  if func_args and len(func_args.contents):
  name += func_args.contents[0]
  ret_type = tag.findChild('type')
  if ret_type and len(ret_type.contents):
  name += ' -> ' + ret_type.contents[0]
  return name","Override. Extract a representative ""name"" from a function tag. get_name's output can be controlled through keyword arguments that are provided when initializing a functionTagProcessor. For instance, function arguments and return types can be included by passing include_function_signatures=True to __init__(). Args: tag: A BeautifulSoup Tag for a function. Returns: A string that would be appropriate to use as an entry name for a function in a Zeal database.",0,0,0,1,1,1,0,0,1,2
"def resolve(self, authorization: http.Header):
  from django_apistar.authentication.models import Token
  if authorization is None:
  return None
  scheme, token = authorization.split()
  if scheme.lower() != 'bearer':
  return None
  try:
  user = Token.objects.get(key=token).user
  except Token.DoesNotExist:
  return None
  return user","Determine the user associated with a request, using Token Authentication.",1,0,1,1,3,1,0,1,1,3
"def store_work_results(self, results, collection, md5):
  results['md5'] = md5
  results['__time_stamp'] = datetime.datetime.utcnow()
  if 'mod_time' not in results:
  results['mod_time'] = results['__time_stamp']
  try:
  self.database[collection].update({'md5':md5}, self.clean_for_storage(results), True)
  except pymongo.errors.OperationFailure:
  print 'Could not update exising object in capped collection, punting...'
  print 'collection: %s md5:%s' % (collection, md5)",Store the output results of the worker. Args: results: a dictionary. collection: the database collection to store the results in. md5: the md5 of sample data to be updated.,0,1,1,0,2,0,1,1,0,2
"def set_password(self,
  password,
  user='',
  shutit_pexpect_child=None,
  note=None):
 shutit_global.shutit_global_object.yield_to_draw()
 shutit_pexpect_child = shutit_pexpect_child or self.get_current_shutit_pexpect_session().pexpect_child
 shutit_pexpect_session = self.get_shutit_pexpect_session_from_child(shutit_pexpect_child)
 return shutit_pexpect_session.set_password(password,user=user,note=note)","Sets the password for the current user or passed-in user. As a side effect, installs the ""password"" package. @param user: username to set the password for. Defaults to '' (i.e. current user) @param password: password to set for the user @param shutit_pexpect_child: See send() @param note: See send()",1,0,0,1,2,1,0,0,1,2
"def from_user_input(cls, address_book, user_input,
  supported_private_objects, version, localize_dates):
  contact = cls(address_book, None, supported_private_objects, version,
  localize_dates)
  contact._process_user_input(user_input)
  return contact",Use this if you want to create a new contact from user input.,0,1,1,0,2,1,0,0,1,2
"async def save(self):
  if hasattr(self, ""before_save""):
  self.before_save()
  query = r.table(self.table_name)
  if self._state.get(""id""):
  query = query \
  .get(self._state.get(""id"")) \
  .update(self.__db_repr, return_changes=True)
  else:
  query = query \
  .insert(self.__db_repr, return_changes=True)
  resp = await query.run(await conn.get())
  try:
  changes = resp[""changes""]
  if len(changes) > 0:
  self.wrap(resp[""changes""][0][""new_val""])
  except KeyError:
  raise UnexpectedDbResponse()
  if resp[""skipped""] > 0:
  raise UnexpectedDbResponse(
  ""Model with id `%s` not found in the database."" %
  self._state.get(""id""))
  return self","Persists the model to the database. If the model holds no primary key, a new one will automatically created by RethinkDB. Otherwise it will overwrite the current model persisted to the database.",1,1,1,0,3,1,1,1,1,4
"def parse_input_args(input_args):
  input_args = input_args if input_args else {}
  for task, args in input_args.items():
  if not isinstance(task, EOTask):
  raise ValueError('Invalid input argument {}, should be an instance of EOTask'.format(task))
  if not isinstance(args, (tuple, dict)):
  raise ValueError('Execution input arguments of each task should be a dictionary or a tuple, for task '
  '{} got arguments of type {}'.format(task.__class__.__name__, type(args)))
  return input_args",Parses EOWorkflow input arguments provided by user and raises an error if something is wrong. This is done automatically in the process of workflow execution,1,0,0,0,1,1,0,0,1,2
"def reencrypt_all_users(engine,
  old_crypto_factory,
  new_crypto_factory,
  logger):
  logger.info(""Beginning re-encryption for all users."")
  for user_id in all_user_ids(engine):
  reencrypt_single_user(
  engine,
  user_id,
  old_crypto=old_crypto_factory(user_id),
  new_crypto=new_crypto_factory(user_id),
  logger=logger,
  )
  logger.info(""Finished re-encryption for all users."")","Re-encrypt data for all users. This function is idempotent, meaning that it should be possible to apply the same re-encryption process multiple times without having any effect on the database. Idempotency is achieved by first attempting to decrypt with the old crypto and falling back to the new crypto on failure. An important consequence of this strategy is that **decrypting** a database is not supported with this function, because ``NoEncryption.decrypt`` always succeeds. To decrypt an already-encrypted database, use ``unencrypt_all_users`` instead. It is, however, possible to perform an initial encryption of a database by passing a function returning a ``NoEncryption`` as ``old_crypto_factory``. Parameters ---------- engine : SQLAlchemy.engine Engine encapsulating database connections. old_crypto_factory : function[str -> Any] A function from user_id to an object providing the interface required by PostgresContentsManager.crypto. Results of this will be used for decryption of existing database content. new_crypto_factory : function[str -> Any] A function from user_id to an object providing the interface required by PostgresContentsManager.crypto. Results of this will be used for re-encryption of database content. This **must not** return instances of ``NoEncryption``. Use ``unencrypt_all_users`` if you want to unencrypt a database. logger : logging.Logger, optional A logger to user during re-encryption. See Also -------- reencrypt_user unencrypt_all_users",1,1,1,0,3,1,0,0,0,1
"def create_continuous_query(database, name, query, resample_time=None, coverage_period=None, **client_args):
  client = _client(**client_args)
  full_query = 'CREATE CONTINUOUS QUERY {name} ON {database}'
  if resample_time:
  full_query += ' RESAMPLE EVERY {resample_time}'
  if coverage_period:
  full_query += ' FOR {coverage_period}'
  full_query += ' BEGIN {query} END'
  query = full_query.format(
  name=name,
  database=database,
  query=query,
  resample_time=resample_time,
  coverage_period=coverage_period
  )
  client.query(query)
  return True","Create a continuous query. database Name of the database for which the continuous query will be created on. name Name of the continuous query to create. query The continuous query string. resample_time : None Duration between continuous query resampling. coverage_period : None Duration specifying time period per sample. CLI Example: .. code-block:: bash salt '*' influxdb.create_continuous_query mydb cq_month 'SELECT mean(*) INTO mydb.a_month.:MEASUREMENT FROM mydb.a_week./.*/ GROUP BY time(5m), *'",1,1,0,0,2,1,1,1,1,4
"def direct_messages_new(self, text, user_id=None, screen_name=None):
  params = {}
  set_str_param(params, 'text', text)
  set_str_param(params, 'user_id', user_id)
  set_str_param(params, 'screen_name', screen_name)
  return self._post_api('direct_messages/new.json', params)",Sends a new direct message to the given user from the authenticating user. https://dev.twitter.com/docs/api/1.1/post/direct_messages/new :param str text: (*required*) The text of your direct message. :param str user_id: The ID of the user who should receive the direct message. Required if ``screen_name`` isn't given. :param str screen_name: The screen name of the user who should receive the direct message. Required if ``user_id`` isn't given. :returns: A direct message dict containing the sent direct message.,1,0,0,2,3,2,0,0,1,3
"def update(cls, **kwargs):
  q = cls._get_instance(**{'id': kwargs['id']})
  if q:
  for k, v in kwargs.items():
  setattr(q, k, v)
  _action_and_commit(q, session.add)
  else:
  cls.get_or_create(**kwargs)","If a record matching the instance id already exists in the database, update it. If a record matching the instance id does not already exist, create a new record.",0,1,1,0,2,1,1,1,0,3
"def set_monitor(self, monitor):
  if type(monitor) != bool:
  raise InvalidInput(""Monitor value must be bool"")
  self._roast['record'] = bool2int(monitor)
  self._q.put(self._config)
  if self._roast['record']:
  self._roast_start = now_time(str=True)
  self._roast['start_time'] = self._roast_start
  else:
  self._roast_end = now_time(str=True)
  self._roast['end_time'] = self._roast_end
  self._roast['date'] = now_date(str=True)
  et = load_time(self._roast['end_time'])
  st = load_time(self._roast['start_time'])
  self._roast['duration'] = timedelta2period(et - st)
  return self.get_roast_properties()",Set the monitor config. This module assumes that users will connect to the roaster and get reading information _before_ they want to begin collecting roast details. This method is critical to enabling the collection of roast information and ensuring it gets saved in memory. :param monitor: Value to set the monitor :type monitor: bool :returns: None :raises: InvalidInput,1,0,0,1,2,1,0,0,0,1
"def permission_check(apikey, endpoint):
  try:
  ak = APIKeys.objects.get(apikey=apikey)
  apitree = cPickle.loads(ak.apitree.encode(""ascii""))
  if apitree.match(endpoint):
  return ak.user if ak.user else AnonymousUser(), ak.seckey
  except APIKeys.DoesNotExist:
  pass
  return None, None","return (user, seckey) if url end point is in allowed entry point list",1,0,1,1,3,1,0,1,1,3
"def can_create_gradebook_column_with_record_types(self, gradebook_column_record_types):
  if self._catalog_session is not None:
  return self._catalog_session.can_create_catalog_with_record_types(catalog_record_types=gradebook_column_record_types)
  return True","Tests if this user can create a single ``GradebookColumn`` using the desired record types. While ``GradingManager.getGradebookColumnRecordTypes()`` can be used to examine which records are supported, this method tests which record(s) are required for creating a specific ``GradebookColumn``. Providing an empty array tests if a ``GradebookColumn`` can be created with no records. arg: gradebook_column_record_types (osid.type.Type[]): array of gradebook column record types return: (boolean) - ``true`` if ``GradebookColumn`` creation using the specified record ``Types`` is supported, ``false`` otherwise raise: NullArgument - ``gradebook_column_record_types`` is ``null`` *compliance: mandatory -- This method must be implemented.*",1,0,0,1,2,1,0,0,1,2
"def get_users(self, full_name=None, email=None, username=None):
  data = {}
  if full_name:
  data['full_name_contains'] = full_name
  if email:
  data['email'] = email
  if username:
  data['username'] = username
  return self._get_collection('/users', data)","Send GET request to /users for users with optional full_name, email, and/or username filtering. :param full_name: str name of the user we are searching for :param email: str: optional email to filter by :param username: str: optional username to filter by :return: requests.Response containing the successful result",2,0,0,1,3,2,0,0,1,3
"def process_block(cls, bigchain, new_height, txns):
  initiated_elections = cls._get_initiated_elections(new_height, txns)
  if initiated_elections:
  bigchain.store_elections(initiated_elections)
  elections = cls._get_votes(txns)
  validator_update = None
  for election_id, votes in elections.items():
  election = bigchain.get_transaction(election_id)
  if election is None:
  continue
  if not election.has_concluded(bigchain, votes):
  continue
  validator_update = election.on_approval(bigchain, new_height)
  election.store(bigchain, new_height, is_concluded=True)
  return [validator_update] if validator_update else []","Looks for election and vote transactions inside the block, records and processes elections. Every election is recorded in the database. Every vote has a chance to conclude the corresponding election. When an election is concluded, the corresponding database record is marked as such. Elections and votes are processed in the order in which they appear in the block. Elections are concluded in the order of appearance of their first votes in the block. For every election concluded in the block, calls its `on_approval` method. The returned value of the last `on_approval`, if any, is a validator set update to be applied in one of the following blocks. `on_approval` methods are implemented by elections of particular type. The method may contain side effects but should be idempotent. To account for other concluded elections, if it requires so, the method should rely on the database state.",0,1,1,0,2,1,1,0,0,2
"def _add_matched_objects_to_database(
  self,
  matchedObjects):
  self.log.info(
  'starting the ``_add_matched_objects_to_database`` method')
  print ""Adding the matched sources to the `pyephem_positions` database table""
  allMatches = []
  for m in matchedObjects:
  allMatches += m
  dbSettings = self.settings[""database settings""][""atlasMovers""]
  insert_list_of_dictionaries_into_database_tables(
  dbConn=self.atlasMoversDBConn,
  log=self.log,
  dictList=allMatches,
  dbTableName=""pyephem_positions"",
  uniqueKeyList=[""expname"", ""object_name""],
  dateModified=True,
  batchSize=10000,
  replace=True,
  dbSettings=dbSettings
  )
  self.log.info(
  'completed the ``_add_matched_objects_to_database`` method')
  return None",*add mathced objects to database* **Key Arguments:** - ``matchedObjects`` -- these objects matched in the neighbourhood of the ATLAS exposures (list of dictionaries),1,1,0,0,2,0,1,1,0,2
"def render_latex(input_text, dpath=None, fname=None, preamb_extra=None,
  verbose=1, **kwargs):
  import utool as ut
  import vtool as vt
  input_text_ = '\pagenumbering{gobble}\n' + input_text
  img_fname = ut.ensure_ext(fname, ['.jpg'] + list(ut.IMG_EXTENSIONS))
  img_fpath = join(dpath, img_fname)
  pdf_fpath = ut.compile_latex_text(
  input_text_, fname=fname, dpath=dpath, preamb_extra=preamb_extra,
  verbose=verbose, move=False)
  ext = splitext(img_fname)[1]
  fpath_in = ut.convert_pdf_to_image(pdf_fpath, ext=ext, verbose=verbose)
  vt.clipwhite_ondisk(fpath_in, fpath_out=img_fpath, verbose=verbose > 1)
  return img_fpath","Renders latex text into a jpeg. Whitespace that would have appeared in the PDF is removed, so the jpeg is cropped only the the relevant part. This is ideal for figures that only take a single page. Args: input_text (?): dpath (str): directory path(default = None) fname (str): file name(default = None) preamb_extra (None): (default = None) verbose (int): verbosity flag(default = 1) Returns: str: jpg_fpath - file path string CommandLine: python -m utool.util_latex render_latex '$O(n^2)$' --fpath=~/slides/tmp.jpg Script: >>> # SCRIPT >>> from utool.util_latex import * # NOQA >>> from os.path import split, expanduser >>> import utool as ut >>> input_text = ' '.join(ut.get_varargs()[1:]) >>> dpath, fname = split(ut.argval('--fpath', '')) >>> dpath = expanduser(ut.argval('--dpath', dpath)) >>> fname = ut.argval('--fname', fname) >>> kwargs = ut.dict_subset(ut.argparse_funckw(ut.convert_pdf_to_image), ['dpi', 'quality']) >>> jpg_fpath = render_latex(input_text, dpath, fname, **kwargs) >>> if ut.argflag('--diskshow'): >>> ut.startfile(jpg_fpath)",1,0,0,1,2,1,0,0,1,2
"def add_listener_destinations(self, server_id, listener_urls, owned=True):
  if isinstance(listener_urls, list):
  dest_insts = []
  for listener_url in listener_urls:
  new_dest_insts = self.add_listener_destinations(
  server_id, listener_url)
  dest_insts.extend(new_dest_insts)
  return dest_insts
  listener_url = listener_urls
  dest_inst = self._create_destination(server_id, listener_url, owned)
  return [dest_inst]","Register WBEM listeners to be the target of indications sent by a WBEM server. This function automatically creates a listener destination instance (of CIM class ""CIM_ListenerDestinationCIMXML"") for each specified listener URL in the Interop namespace of the specified WBEM server. The form of the `Name` property of the created destination instance is: ``""pywbemdestination:"" {ownership} "":"" {subscription_manager_id} "":"" {guid}`` where ``{ownership}`` is ``""owned""`` or ``""permanent""`` dependent on the `owned` argument; ``{subscription_manager_id}`` is the subscription manager ID; and ``{guid}`` is a globally unique identifier. Owned listener destinations are added or updated conditionally: If the listener destination instance to be added is already registered with this subscription manager and has the same property values, it is not created or modified. If it has the same path but different property values, it is modified to get the desired property values. If an instance with this path does not exist yet (the normal case), it is created. Permanent listener destinations are created unconditionally, and it is up to the user to ensure that such an instance does not exist yet. Parameters: server_id (:term:`string`): The server ID of the WBEM server, returned by :meth:`~pywbem.WBEMSubscriptionManager.add_server`. listener_urls (:term:`string` or list of :term:`string`): The URL or URLs of the WBEM listeners to be registered. The WBEM listener may be a :class:`~pywbem.WBEMListener` object or any external WBEM listener. Each listener URL string must have the format: ``[{scheme}://]{host}:{port}`` The following URL schemes are supported: * ``https``: Causes HTTPS to be used. * ``http``: Causes HTTP to be used. This is the default The host can be specified in any of the usual formats: * a short or fully qualified DNS hostname * a literal (= dotted) IPv4 address * a literal IPv6 address, formatted as defined in :term:`RFC3986` with the extensions for zone identifiers as defined in :term:`RFC6874`, supporting ``-`` (minus) for the delimiter before the zone ID string, as an additional choice to ``%25``. Note that the port is required in listener URLs. See :class:`~pywbem.WBEMConnection` for examples of valid URLs, with the caveat that the port in server URLs is optional. owned (:class:`py:bool`): Defines the ownership type of the created listener destination instances: If `True`, they will be owned. Otherwise, they will be permanent. See :ref:`WBEMSubscriptionManager` for details about these ownership types. Returns: :class:`py:list` of :class:`~pywbem.CIMInstance`: The created listener destination instances for the defined listener URLs. Raises: Exceptions raised by :class:`~pywbem.WBEMConnection`.",1,0,0,2,3,1,0,0,0,1
"def read_namespaced_service_account(self, name, namespace, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.read_namespaced_service_account_with_http_info(name, namespace, **kwargs)
  else:
  (data) = self.read_namespaced_service_account_with_http_info(name, namespace, **kwargs)
  return data","read the specified ServiceAccount This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.read_namespaced_service_account(name, namespace, async_req=True) >>> result = thread.get() :param async_req bool :param str name: name of the ServiceAccount (required) :param str namespace: object name and auth scope, such as for teams and projects (required) :param str pretty: If 'true', then the output is pretty printed. :param bool exact: Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. Deprecated. Planned for removal in 1.18. :param bool export: Should this value be exported. Export strips fields that a user can not specify. Deprecated. Planned for removal in 1.18. :return: V1ServiceAccount If the method is called asynchronously, returns the request thread.",2,0,0,1,3,1,0,0,1,2
"def node_status_changed_handler(**kwargs):
  obj = kwargs['instance']
  obj.old_status = kwargs['old_status'].name
  obj.new_status = kwargs['new_status'].name
  queryset = exclude_owner_of_node(obj)
  create_notifications.delay(**{
  ""users"": queryset,
  ""notification_model"": Notification,
  ""notification_type"": ""node_status_changed"",
  ""related_object"": obj
  })
  if obj.user is not None:
  create_notifications.delay(**{
  ""users"": [obj.user],
  ""notification_model"": Notification,
  ""notification_type"": ""node_own_status_changed"",
  ""related_object"": obj
  })",send notification when the status of a node changes according to users's settings,0,0,1,0,1,1,0,1,1,3
"def unlinkUser(self, delete=False):
  userid = self.getUsername()
  user = self.getUser()
  if user:
  logger.debug(""Unlinking User '{}' from Contact '{}'"".format(
  userid, self.Title()))
  if not self._unlinkUser():
  return False
  if delete:
  logger.debug(""Removing Plone User '{}'"".format(userid))
  api.user.delete(username=userid)
  return True
  return False","Unlink the user to the Contact :returns: True if OK, False if no User was unlinked :rtype: bool",1,0,1,1,3,1,0,0,1,2
"def query_input(question, default=None, color=default_color):
  if default is None or default == '':
  prompt = ' '
  elif type(default) == str:
  prompt = flo(' [{default}] ')
  else:
  raise ValueError(""invalid default answer: '%s'"" % default)
  while True:
  sys.stdout.write(color(question + prompt))
  choice = raw_input()
  if default is not None and choice == '':
  return default
  if choice != '':
  return choice","Ask a question for input via raw_input() and return their answer. ""question"" is a string that is presented to the user. ""default"" is the presumed answer if the user just hits <Enter>. The ""answer"" return value is a str.",1,0,0,1,2,1,0,0,1,2
"def __getDataFromURL(url):
  code = 0
  while code != 200:
  req = Request(url)
  try:
  response = urlopen(req)
  code = response.code
  sleep(0.01)
  except HTTPError as error:
  code = error.code
  if code == 404:
  break
  except URLError as error:
  sleep(3)
  if code == 404:
  raise Exception(""User was not found"")
  return response.read().decode('utf-8')",Read HTML data from an user GitHub profile. :param url: URL of the webpage to download. :type url: str. :return: webpage donwloaded. :rtype: str.,1,0,0,1,2,1,0,0,1,2
"def datetime_entry(self, prompt, message=None, formats=['%x %X'], show_example=False,
  rofi_args=None, **kwargs):
  def datetime_validator(text):
  for format in formats:
  try:
  dt = datetime.strptime(text, format)
  except ValueError:
  continue
  else:
  return (dt, None)
  return (None, 'Please enter a valid date and time.')
  if show_example:
  message = message or """"
  message += ""Current date and time in the correct format: "" + datetime.now().strftime(formats[0])
  return self.generic_entry(prompt, datetime_validator, message, rofi_args, **kwargs)","Prompt the user to enter a date and time. Parameters ---------- prompt: string Prompt to display to the user. message: string, optional Message to display under the entry line. formats: list of strings, optional The formats that the user can enter the date and time in. These should be format strings as accepted by the datetime.datetime.strptime() function from the standard library. They are tried in order, and the first that returns a datetime object without error is selected. Note that the '%x %X' in the default list is the current locale's date and time representation. show_example: Boolean If True, the current date and time in the first format given is appended to the message. Returns ------- datetime.datetime, or None if the dialog is cancelled.",1,0,0,1,2,1,0,0,1,2
"def _load_txt(file, devices, channels, header, **kwargs):
  kwargs_txt = _filter_keywords(numpy.loadtxt, kwargs)
  out_dict = {}
  for dev_nbr, device in enumerate(devices):
  out_dict[device] = {}
  columns = []
  for chn in channels[dev_nbr]:
  columns.append(header[device][""column labels""][chn])
  out_dict[device][""CH"" + str(chn)] = numpy.loadtxt(fname=file, usecols=header[device][""column labels""][chn],
  **kwargs_txt)
  return out_dict","Function used for reading .txt files generated by OpenSignals. ---------- Parameters ---------- file : file, str, or pathlib.Path File, filename, or generator to read. If the filename extension is ``.gz`` or ``.bz2``, the file is first decompressed. Note that generators should return byte strings for Python 3k. devices : list [""mac_address_1"" <str>, ""mac_address_2"" <str>...] List of devices selected by the user. channels : list [[mac_address_1_channel_1 <int>, mac_address_1_channel_2 <int>...], [mac_address_2_channel_1 <int>...]...] From which channels will the data be loaded. header : dict File header with relevant metadata for identifying which columns may be read. **kwargs : list of variable keyword arguments. The valid keywords are those used by numpy.loadtxt function. Returns ------- out_dict : dict Data read from the text file.",0,0,0,1,1,1,0,0,1,2
"def refresh(self, force=False, soon=86400):
  if hasattr(self.provider_class, 'refresh_credentials'):
  if force or self.expire_soon(soon):
  logging.info('PROVIDER NAME: {0}'.format(self.provider_name))
  return self.provider_class(
  self, None, self.provider_name).refresh_credentials(self)",Refreshes the credentials only if the **provider** supports it and if it will expire in less than one day. It does nothing in other cases. .. note:: The credentials will be refreshed only if it gives sense i.e. only |oauth2|_ has the notion of credentials *refreshment/extension*. And there are also differences across providers e.g. Google supports refreshment only if there is a ``refresh_token`` in the credentials and that in turn is present only if the ``access_type`` parameter was set to ``offline`` in the **user authorization request**. :param bool force: If ``True`` the credentials will be refreshed even if they won't expire soon. :param int soon: Number of seconds specifying what means *soon*.,1,0,0,0,1,1,0,0,0,1
"def get_all_user_groups(self, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.get_all_user_groups_with_http_info(**kwargs)
  else:
  (data) = self.get_all_user_groups_with_http_info(**kwargs)
  return data","Get all user groups for a customer # noqa: E501 # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.get_all_user_groups(async_req=True) >>> result = thread.get() :param async_req bool :param int offset: :param int limit: :return: ResponseContainerPagedUserGroup If the method is called asynchronously, returns the request thread.",2,0,0,1,3,2,0,0,1,3
"def process_ndex_network(network_id, username=None, password=None,
  require_grounding=True):
  nd = ndex2.client.Ndex2(username=username, password=password)
  res = nd.get_network_as_cx_stream(network_id)
  if res.status_code != 200:
  logger.error('Problem downloading network: status code %s' %
  res.status_code)
  logger.error('Response: %s' % res.text)
  return None
  json_list = res.json()
  summary = nd.get_network_summary(network_id)
  return process_cx(json_list, summary=summary,
  require_grounding=require_grounding)",Process an NDEx network into Statements. Parameters ---------- network_id : str NDEx network ID. username : str NDEx username. password : str NDEx password. require_grounding: bool Whether network nodes lacking grounding information should be included among the extracted Statements (default is True). Returns ------- NdexCxProcessor Processor containing Statements. Returns None if there if the HTTP status code indicates an unsuccessful request.,2,0,0,1,3,2,0,0,1,3
"def login(self, username, password):
  state_snapshot = self._state.copy()
  try:
  self._ajax_api.User_Login(name=username, password=password)
  self._android_api.login(account=username, password=password)
  self._manga_api.cr_login(account=username, password=password)
  except Exception as err:
  self._state = state_snapshot
  raise err
  self._state['username'] = username
  self._state['password'] = password
  return self.logged_in","Login with the given username/email and password Calling this method is not required if credentials were provided in the constructor, but it could be used to switch users or something maybe @return bool",1,0,0,2,3,1,0,0,1,2
"def read_node(self, name, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.read_node_with_http_info(name, **kwargs)
  else:
  (data) = self.read_node_with_http_info(name, **kwargs)
  return data","read_node # noqa: E501 read the specified Node # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.read_node(name, async_req=True) >>> result = thread.get() :param async_req bool :param str name: name of the Node (required) :param str pretty: If 'true', then the output is pretty printed. :param bool exact: Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. :param bool export: Should this value be exported. Export strips fields that a user can not specify. :return: V1Node If the method is called asynchronously, returns the request thread.",2,0,0,1,3,2,0,0,1,3
"def download_session_video(self, scenario_name, timeout=5):
  if (self.driver_wrapper.get_driver_platform().lower() != 'linux' or
  not self.driver_wrapper.config.getboolean_optional('Capabilities', 'enableVideo')):
  return
  path_file = os.path.join(self.videos_directory, '%s.%s' % (scenario_name, MP4_EXTENSION))
  if self.driver_wrapper.server_type == 'selenoid':
  filename = '%s.%s' % (self.session_id, MP4_EXTENSION)
  else:
  filename = self.session_id
  video_url = '{}/video/{}'.format(self.server_url, filename)
  if self.browser_remote:
  self.__download_file(video_url, path_file, timeout)
  self.__remove_file(video_url)","download the execution video file if the scenario fails or the video is enabled, renaming the file to scenario name and removing the video file in the server. GGR request: http://<username>:<password>@<ggr_host>:<ggr_port>/video/<session_id> selenoid request: http://<username>:<password>@<ggr_host>:<ggr_port>/video/<session_id>.mp4 :param scenario_name: scenario name :param timeout: threshold until the video file is downloaded",1,0,0,1,2,1,0,0,1,2
"def _post_message(channel,
  message,
  username,
  as_user,
  api_key=None):
  parameters = dict()
  parameters['channel'] = channel
  parameters['username'] = username
  parameters['as_user'] = as_user
  parameters['text'] = '```' + message + '```'
  result = salt.utils.slack.query(function='message',
  api_key=api_key,
  method='POST',
  header_dict={'Content-Type': 'application/x-www-form-urlencoded'},
  data=_urlencode(parameters))
  log.debug('Slack message post result: %s', result)
  if result:
  return True
  else:
  return False","Send a message to a Slack room. :param channel: The room name. :param message: The message to send to the Slack room. :param username: Specify who the message is from. :param as_user: Sets the profile picture which have been added through Slack itself. :param api_key: The Slack api key, if not specified in the configuration. :param api_version: The Slack api version, if not specified in the configuration. :return: Boolean if message was sent successfully.",1,0,0,1,2,1,0,0,2,3
"def get_commit_info(self, project, repository, commit, path=None):
  url = 'rest/api/1.0/projects/{project}/repos/{repository}/commits/{commitId}'.format(project=project,
  repository=repository,
  commitId=commit)
  params = {}
  if path:
  params['path'] = path
  return self.get(url, params=params)","Retrieve a single commit identified by its ID>. In general, that ID is a SHA1. From 2.11, ref names like ""refs/heads/master"" are no longer accepted by this resource. The authenticated user must have REPO_READ permission for the specified repository to call this resource. :param project: :param repository: :param commit: the commit ID to retrieve :param path :OPTIONAL an optional path to filter the commit by. If supplied the details returned may not be for the specified commit. Instead, starting from the specified commit, they will be the details for the first commit affecting the specified path. :return:",2,0,0,1,3,2,0,0,1,3
"def delete_quick(self, get_count=False):
  query = 'DELETE FROM ' + self.full_table_name + self.where_clause
  self.connection.query(query)
  count = self.connection.query(""SELECT ROW_COUNT()"").fetchone()[0] if get_count else None
  self._log(query[:255])
  return count","Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail.",0,1,1,0,2,0,0,1,1,2
"def citingArticles(self, uid, count=100, offset=1, editions=None,
  timeSpan=None, retrieveParameters=None):
  return self._search.service.citingArticles(
  databaseId='WOS',
  uid=uid,
  editions=editions,
  timeSpan=timeSpan,
  queryLanguage='en',
  retrieveParameters=(retrieveParameters or
  self.make_retrieveParameters(offset, count))
  )","The citingArticles operation finds citing articles for the article specified by unique identifier. You may specify only one identifier per request. Web of Science Core Collection (WOS) is the only valid database for this operation. :uid: A unique item identifier. It cannot be None or empty string. :count: Number of records to display in the result. Cannot be less than 0 and cannot be greater than 100. If count is 0 then only the summary information will be returned. :offset: First record in results to return. Must be greater than zero :editions: List of editions to be searched. If None, user permissions will be substituted. Fields: collection - Name of the collection edition - Name of the edition :timeSpan: This element defines specifies a range of publication dates. If timeSpan is null, then the maximum time span will be inferred from the editions data. Fields: begin - Beginning date for this search. Format: YYYY-MM-DD end - Ending date for this search. Format: YYYY-MM-DD :retrieveParameters: Retrieve parameters. If omitted the result of make_retrieveParameters(offset, count, 'RS', 'D') is used.",2,0,0,1,3,2,0,0,1,3
"def wait_for(self, event, *, check=None, timeout=None):
  future = self.loop.create_future()
  if check is None:
  def _check(*args):
  return True
  check = _check
  ev = event.lower()
  try:
  listeners = self._listeners[ev]
  except KeyError:
  listeners = []
  self._listeners[ev] = listeners
  listeners.append((future, check))
  return asyncio.wait_for(future, timeout, loop=self.loop)","|coro| Waits for a WebSocket event to be dispatched. This could be used to wait for a user to reply to a message, or to react to a message, or to edit a message in a self-contained way. The ``timeout`` parameter is passed onto :func:`asyncio.wait_for`. By default, it does not timeout. Note that this does propagate the :exc:`asyncio.TimeoutError` for you in case of timeout and is provided for ease of use. In case the event returns multiple arguments, a :class:`tuple` containing those arguments is returned instead. Please check the :ref:`documentation <discord-api-events>` for a list of events and their parameters. This function returns the **first event that meets the requirements**. Examples --------- Waiting for a user reply: :: @client.event async def on_message(message): if message.content.startswith('$greet'): channel = message.channel await channel.send('Say hello!') def check(m): return m.content == 'hello' and m.channel == channel msg = await client.wait_for('message', check=check) await channel.send('Hello {.author}!'.format(msg)) Waiting for a thumbs up reaction from the message author: :: @client.event async def on_message(message): if message.content.startswith('$thumb'): channel = message.channel await channel.send('Send me that \N{THUMBS UP SIGN} reaction, mate') def check(reaction, user): return user == message.author and str(reaction.emoji) == '\N{THUMBS UP SIGN}' try: reaction, user = await client.wait_for('reaction_add', timeout=60.0, check=check) except asyncio.TimeoutError: await channel.send('\N{THUMBS DOWN SIGN}') else: await channel.send('\N{THUMBS UP SIGN}') Parameters ------------ event: :class:`str` The event name, similar to the :ref:`event reference <discord-api-events>`, but without the ``on_`` prefix, to wait for. check: Optional[predicate] A predicate to check what to wait for. The arguments must meet the parameters of the event being waited for. timeout: Optional[:class:`float`] The number of seconds to wait before timing out and raising :exc:`asyncio.TimeoutError`. Raises ------- asyncio.TimeoutError If a timeout is provided and it was reached. Returns -------- Any Returns no arguments, a single argument, or a :class:`tuple` of multiple arguments that mirrors the parameters passed in the :ref:`event reference <discord-api-events>`.",1,0,0,1,2,1,0,0,1,2
"def is_admin():
  if os.name == 'nt':
  import ctypes
  import traceback
  try:
  return ctypes.windll.shell32.IsUserAnAdmin()
  except:
  traceback.print_exc()
  return False
  else:
  return os.getuid() == 0","https://stackoverflow.com/a/19719292 @return: True if the current user is an 'Admin' whatever that means (root on Unix), otherwise False. Warning: The inner function fails unless you have Windows XP SP2 or higher. The failure causes a traceback to be printed and this function to return False.",1,0,0,1,2,1,0,0,1,2
"def set_message_last_post(cr, uid, pool, models):
  if type(models) is not list:
  models = [models]
  for model in models:
  model_pool = pool[model]
  cr.execute(
  ""UPDATE {table} ""
  ""SET message_last_post=(SELECT max(mm.date) ""
  ""FROM mail_message mm ""
  ""WHERE mm.model=%s ""
  ""AND mm.date IS NOT NULL ""
  ""AND mm.res_id={table}.id)"".format(
  table=model_pool._table), (model,)
  )","Given a list of models, set their 'message_last_post' fields to an estimated last post datetime. To be called in post-migration scripts :param cr: database cursor :param uid: user id, assumed to be openerp.SUPERUSER_ID :param pool: orm pool, assumed to be openerp.pooler.get_pool(cr.dbname) :param models: a list of model names for which 'message_last_post' needs \ to be filled :return:",1,1,1,0,3,1,1,1,0,3
"async def unpack_message(wallet_handle: int,
  jwe: bytes) -> bytes:
  logger = logging.getLogger(__name__)
  logger.debug(""unpack_message: >>> wallet_handle: %r, jwe: %r"",
  wallet_handle,
  jwe)
  def transform_cb(arr_ptr: POINTER(c_uint8), arr_len: c_uint32):
  return bytes(arr_ptr[:arr_len]),
  if not hasattr(unpack_message, ""cb""):
  logger.debug(""unpack_message: Creating callback"")
  unpack_message.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, POINTER(c_uint8), c_uint32), transform_cb)
  c_wallet_handle = c_int32(wallet_handle)
  c_jwe_len = c_uint32(len(jwe))
  res = await do_call('indy_unpack_message',
  c_wallet_handle,
  jwe,
  c_jwe_len,
  unpack_message.cb)
  logger.debug(""unpack_message: <<< res: %r"", res)
  return res","Unpacks a JWE-like formatted message outputted by pack_message (Experimental) #Params command_handle: command handle to map callback to user context. wallet_handle: wallet handler (created by open_wallet) message: the output of a pack message #Returns -> See HIPE 0028 for details (Authcrypt mode) { ""message"": <decrypted message>, ""recipient_verkey"": <recipient verkey used to decrypt>, ""sender_verkey"": <sender verkey used to encrypt> } (Anoncrypt mode) { ""message"": <decrypted message>, ""recipient_verkey"": <recipient verkey used to decrypt>, }",1,0,0,2,3,1,0,0,1,2
"def _getPublicSignupInfo(siteStore):
  for tr in siteStore.query(_SignupTracker):
  si = tr.signupItem
  p = getattr(si, 'prompt', None)
  u = getattr(si, 'prefixURL', None)
  if p is not None and u is not None:
  yield (p, u'/'+u)","Get information about public web-based signup mechanisms. @param siteStore: a store with some signups installed on it (as indicated by _SignupTracker instances). @return: a generator which yields 2-tuples of (prompt, url) where 'prompt' is unicode briefly describing the signup mechanism (e.g. ""Sign Up""), and 'url' is a (unicode) local URL linking to a page where an anonymous user can access it.",0,0,1,1,2,1,0,0,1,2
"def get_comments_data(self, slug):
  all_the_data = []
  for item in self.chan.findall(""item""):
  if not item.find('{wp}post_name').text == slug:
  continue
  item_dict = self.item_dict(item)
  if not item_dict or not item_dict.get('title'):
  continue
  slug = item_dict.get('{wp}post_name') or re.sub(item_dict['title'],' ','-')
  for comment in item.findall(""{wp}comment""):
  comment = self.translate_wp_comment(comment)
  comment['slug'] = slug
  all_the_data.append(comment)
  return all_the_data","Returns a flat list of all comments in XML dump. Formatted as the JSON output from Wordpress API. Keys: ('content', 'slug', 'date', 'status', 'author', 'ID', 'parent') date format: '%Y-%m-%dT%H:%M:%S' author: {'username': 'Name', 'URL': ''}",1,0,0,1,2,1,0,0,1,2
"def batch_add_ips(ips):
  ips_created = 0
  if len(ips) > 0:
  for ip in ips:
  (s0, s1, s2, s3) = ip.split('.')
  (ip_db, is_ip_created) = IP.objects.get_or_create(seg_0=s0, seg_1=s1, seg_2=s2, seg_3=s3, )
  if is_ip_created:
  ips_created += 1
  return ips_created",Adds the given list of IPs to the database if the IP is not already there. :param ips: list of IPs :return: number of created IPs :type ips: list :rtype: int,1,1,1,0,3,1,1,1,1,4
"def shadowGet(self, srcCallback, srcTimeout):
  with self._dataStructureLock:
  self._shadowSubscribeCallbackTable[""get""] = srcCallback
  self._shadowSubscribeStatusTable[""get""] += 1
  currentToken = self._tokenHandler.getNextToken()
  self._tokenPool[currentToken] = Timer(srcTimeout, self._timerHandler, [""get"", currentToken])
  self._basicJSONParserHandler.setString(""{}"")
  self._basicJSONParserHandler.validateJSON()
  self._basicJSONParserHandler.setAttributeValue(""clientToken"", currentToken)
  currentPayload = self._basicJSONParserHandler.regenerateString()
  if not self._isPersistentSubscribe or not self._isGetSubscribed:
  self._shadowManagerHandler.basicShadowSubscribe(self._shadowName, ""get"", self.generalCallback)
  self._isGetSubscribed = True
  self._logger.info(""Subscribed to get accepted/rejected topics for deviceShadow: "" + self._shadowName)
  self._shadowManagerHandler.basicShadowPublish(self._shadowName, ""get"", currentPayload)
  self._tokenPool[currentToken].start()
  return currentToken","**Description** Retrieve the device shadow JSON document from AWS IoT by publishing an empty JSON document to the corresponding shadow topics. Shadow response topics will be subscribed to receive responses from AWS IoT regarding the result of the get operation. Retrieved shadow JSON document will be available in the registered callback. If no response is received within the provided timeout, a timeout notification will be passed into the registered callback. **Syntax** .. code:: python # Retrieve the shadow JSON document from AWS IoT, with a timeout set to 5 seconds BotShadow.shadowGet(customCallback, 5) **Parameters** *srcCallback* - Function to be called when the response for this shadow request comes back. Should be in form :code:`customCallback(payload, responseStatus, token)`, where :code:`payload` is the JSON document returned, :code:`responseStatus` indicates whether the request has been accepted, rejected or is a delta message, :code:`token` is the token used for tracing in this request. *srcTimeout* - Timeout to determine whether the request is invalid. When a request gets timeout, a timeout notification will be generated and put into the registered callback to notify users. **Returns** The token used for tracing in this shadow request.",2,0,0,2,4,1,0,0,1,2
"def facebook_request(self, path, callback, access_token=None,
  post_args=None, **args):
  url = ""https://graph.facebook.com"" + path
  all_args = {}
  if access_token:
  all_args[""access_token""] = access_token
  all_args.update(args)
  all_args.update(post_args or {})
  if all_args: url += ""?"" + urllib.urlencode(all_args)
  callback = self.async_callback(self._on_facebook_request, callback)
  http = httpclient.AsyncHTTPClient()
  if post_args is not None:
  http.fetch(url, method=""POST"", body=urllib.urlencode(post_args),
  callback=callback)
  else:
  http.fetch(url, callback=callback)","Fetches the given relative API path, e.g., ""/btaylor/picture"" If the request is a POST, post_args should be provided. Query string arguments should be given as keyword arguments. An introduction to the Facebook Graph API can be found at http://developers.facebook.com/docs/api Many methods require an OAuth access token which you can obtain through authorize_redirect() and get_authenticated_user(). The user returned through that process includes an 'access_token' attribute that can be used to make authenticated requests via this method. Example usage:: class MainHandler(tornado.web.RequestHandler, tornado.auth.FacebookGraphMixin): @tornado.web.authenticated @tornado.web.asynchronous def get(self): self.facebook_request( ""/me/feed"", post_args={""message"": ""I am posting from my Tornado application!""}, access_token=self.current_user[""access_token""], callback=self.async_callback(self._on_post)) def _on_post(self, new_entry): if not new_entry: # Call failed; perhaps missing permission? self.authorize_redirect() return self.finish(""Posted a message!"")",2,0,0,2,4,2,0,0,1,3
"def get_account(self, username):
  try:
  account = self.model.objects.get(
  **self._filter_user_by(username)
  )
  except self.model.DoesNotExist:
  return None
  return account",return user by username.,0,0,1,1,2,1,0,1,1,3
"def create_user(self, email, first_name, last_name, password, role=""user"", metadata={}):
  data = {
  'firstName': first_name,
  'lastName': last_name,
  'email': email,
  'metadata': metadata,
  'role': role.upper() if role else role,
  'newPassword': password,
  }
  response = self.post('createUser', data)
  return self._handle_empty(email, response)","Create a new user :type email: str :param email: User's email :type first_name: str :param first_name: User's first name :type last_name: str :param last_name: User's last name :type password: str :param password: User's password :type role: str :param role: User's default role, one of ""admin"" or ""user"" :type metadata: dict :param metadata: User metadata :rtype: dict :return: an empty dictionary",1,0,0,2,3,1,0,0,2,3
"def encode_access_token(identity, secret, algorithm, expires_delta, fresh,
  user_claims, csrf, identity_claim_key, user_claims_key,
  json_encoder=None):
  if isinstance(fresh, datetime.timedelta):
  now = datetime.datetime.utcnow()
  fresh = timegm((now + fresh).utctimetuple())
  token_data = {
  identity_claim_key: identity,
  'fresh': fresh,
  'type': 'access',
  }
  if user_claims:
  token_data[user_claims_key] = user_claims
  if csrf:
  token_data['csrf'] = _create_csrf_token()
  return _encode_jwt(token_data, expires_delta, secret, algorithm,
  json_encoder=json_encoder)","Creates a new encoded (utf-8) access token. :param identity: Identifier for who this token is for (ex, username). This data must be json serializable :param secret: Secret key to encode the JWT with :param algorithm: Which algorithm to encode this JWT with :param expires_delta: How far in the future this token should expire (set to False to disable expiration) :type expires_delta: datetime.timedelta or False :param fresh: If this should be a 'fresh' token or not. If a datetime.timedelta is given this will indicate how long this token will remain fresh. :param user_claims: Custom claims to include in this token. This data must be json serializable :param csrf: Whether to include a csrf double submit claim in this token (boolean) :param identity_claim_key: Which key should be used to store the identity :param user_claims_key: Which key should be used to store the user claims :return: Encoded access token",0,0,0,1,1,1,0,0,1,2
"def make_error_redirect(self, authorization_error=None):
  if not self.redirect_uri:
  return HttpResponseRedirect(self.missing_redirect_uri)
  authorization_error = (authorization_error or
  AccessDenied('user denied the request'))
  response_params = get_error_details(authorization_error)
  if self.state is not None:
  response_params['state'] = self.state
  return HttpResponseRedirect(
  update_parameters(self.redirect_uri, response_params))","Return a Django ``HttpResponseRedirect`` describing the request failure. If the :py:meth:`validate` method raises an error, the authorization endpoint should return the result of calling this method like so: >>> auth_code_generator = ( >>> AuthorizationCodeGenerator('/oauth2/missing_redirect_uri/')) >>> try: >>> auth_code_generator.validate(request) >>> except AuthorizationError as authorization_error: >>> return auth_code_generator.make_error_redirect(authorization_error) If there is no known Client ``redirect_uri`` (because it is malformed, or the Client is invalid, or if the supplied ``redirect_uri`` does not match the regsitered value, or some other request failure) then the response will redirect to the ``missing_redirect_uri`` passed to the :py:meth:`__init__` method. Also used to signify user denial; call this method without passing in the optional ``authorization_error`` argument to return a generic :py:class:`AccessDenied` message. >>> if not user_accepted_request: >>> return auth_code_generator.make_error_redirect()",0,0,0,1,1,1,0,0,1,2
"def basicauth(self, realm = b'all', nofail = False):
  ""Try to get the basic authorize info, return (username, password) if succeeded, return 401 otherwise""
  if b'authorization' in self.headerdict:
  auth = self.headerdict[b'authorization']
  auth_pair = auth.split(b' ', 1)
  if len(auth_pair) < 2:
  raise HttpInputException('Authorization header is malformed')
  if auth_pair[0].lower() == b'basic':
  try:
  userpass = base64.b64decode(auth_pair[1])
  except Exception:
  raise HttpInputException('Invalid base-64 string')
  userpass_pair = userpass.split(b':', 1)
  if len(userpass_pair) != 2:
  raise HttpInputException('Authorization header is malformed')
  return userpass_pair
  if nofail:
  return (None, None)
  else:
  self.basicauthfail(realm)","Try to get the basic authorize info, return (username, password) if succeeded, return 401 otherwise",1,0,0,1,2,2,0,0,1,3
"def read_queue(self):
  queue = AmazonSQS().create_queue(settings.RESTCLIENTS_AMAZON_QUEUE)
  queue.set_message_class(RawMessage)
  message = queue.read()
  if message is None:
  return
  body = message.get_body()
  queue.delete_message(message)
  return body","This is responsible for reading events off the queue, and sending notifications to users, via email or SMS. The following are necessary for AWS access: RESTCLIENTS_AMAZON_AWS_ACCESS_KEY RESTCLIENTS_AMAZON_AWS_SECRET_KEY RESTCLIENTS_AMAZON_QUEUE - the AWS Queue name you want to read events from RESTCLIENTS_AMAZON_SQS_DAO_CLASS",1,0,0,0,1,1,0,0,1,2
"def account_info(remote, resp):
  info = get_user_info(remote)
  return {
  'user': {
  'email': info['email'],
  'profile': {
  'username': info['username'],
  'full_name': info['name']
  },
  },
  'external_id': get_user_id(remote, info['preferred_username']),
  'external_method': GLOBUS_EXTERNAL_METHOD
  }","Retrieve remote account information used to find local user. It returns a dictionary with the following structure: .. code-block:: python { 'user': { 'email': '...', 'profile': { 'username': '...', 'full_name': '...', } }, 'external_id': 'globus-unique-identifier', 'external_method': 'globus', } Information inside the user dictionary are available for other modules. For example, they are used from the module invenio-userprofiles to fill the user profile. :param remote: The remote application. :param resp: The response. :returns: A dictionary with the user information.",2,0,0,1,3,1,0,0,1,2
"def select_one_user(users):
  if len(users) == 1:
  select_i = 0
  else:
  table = PrettyTable(['Sequence', 'Name'])
  for i, user in enumerate(users, 1):
  table.add_row([i, user['nickname']])
  click.echo(table)
  select_i = click.prompt('Select one user', type=int, default=1)
  while select_i < 1 or select_i > len(users):
  select_i = click.prompt('Error Select! Select Again', type=int)
  user_id = users[select_i-1]['userId']
  user_name = users[select_i-1]['nickname']
  user = User(user_id, user_name)
  return user",Display the users returned by search api. :params users: API['result']['userprofiles'] :return: a User object.,1,0,0,1,2,1,0,0,1,2
"def _detect_sudo(self, _execnet=None):
  exc = _execnet or execnet
  gw = exc.makegateway(
  self._make_connection_string(self.hostname, use_sudo=False)
  )
  channel = gw.remote_exec(
  'import getpass; channel.send(getpass.getuser())'
  )
  result = channel.receive()
  gw.exit()
  if result == 'root':
  return False
  self.logger.debug('connection detected need for sudo')
  return True",``sudo`` detection has to create a different connection to the remote host so that we can reliably ensure that ``getuser()`` will return the right information. After getting the user info it closes the connection and returns a boolean,1,0,0,0,1,1,0,0,1,2
"def send_image_message(self, user_id, media_id):
  return self.request.post(
  url='https://api.weixin.qq.com/cgi-bin/message/custom/send',
  data={
  'touser': user_id,
  'msgtype': 'image',
  'image': {
  'media_id': media_id,
  },
  }
  )","  http://mp.weixin.qq.com/wiki/7/12a5a320ae96fecdf0e15cb06123de9f.html :param user_id:  ID,   WechatMessage  source :param media_id: ID  :func:`upload_media`   :return:  JSON ",2,0,0,1,3,1,0,0,2,3
"def _set_get_nameserver_detail(self, v, load=False):
  if hasattr(v, ""_utype""):
  v = v._utype(v)
  try:
  t = YANGDynClass(v,base=get_nameserver_detail.get_nameserver_detail, is_leaf=True, yang_name=""get-nameserver-detail"", rest_name=""get-nameserver-detail"", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=False, extensions={u'tailf-common': {u'info': u'display detail device information.', u'hidden': u'rpccmd', u'actionpoint': u'show_ns_detail'}}, namespace='urn:brocade.com:mgmt:brocade-nameserver', defining_module='brocade-nameserver', yang_type='rpc', is_config=True)
  except (TypeError, ValueError):
  raise ValueError({
  'error-string': ,
  'defined-type': ""rpc"",
  'generated-type': ,
  })
  self.__get_nameserver_detail = t
  if hasattr(self, '_set'):
  self._set()","Setter method for get_nameserver_detail, mapped from YANG variable /brocade_nameserver_rpc/get_nameserver_detail (rpc) If this variable is read-only (config: false) in the source YANG file, then _set_get_nameserver_detail is considered as a private method. Backends looking to populate this variable should do so via calling thisObj._set_get_nameserver_detail() directly. YANG Description: A function to display the detailed information of the devices stored in the Name Server database.",1,0,0,0,1,1,0,0,1,2
"def get_all_entity_type_saved_searches(self, entitytype, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.get_all_entity_type_saved_searches_with_http_info(entitytype, **kwargs)
  else:
  (data) = self.get_all_entity_type_saved_searches_with_http_info(entitytype, **kwargs)
  return data","Get all saved searches for a specific entity type for a user # noqa: E501 # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.get_all_entity_type_saved_searches(entitytype, async_req=True) >>> result = thread.get() :param async_req bool :param str entitytype: (required) :param int offset: :param int limit: :return: ResponseContainerPagedSavedSearch If the method is called asynchronously, returns the request thread.",1,0,1,1,3,2,0,0,1,3
"def interact(self, escape_character = b'\x1d', input_filter = None, output_filter = None):
  if PY3: self.stdout.write(_cast_unicode(self.buffer, self.encoding))
  else: self.stdout.write(self.buffer)
  self.stdout.flush()
  self.buffer = self._empty_buffer
  mode = tty.tcgetattr(self.STDIN_FILENO)
  tty.setraw(self.STDIN_FILENO)
  try:
  self.__interact_copy(escape_character, input_filter, output_filter)
  finally:
  tty.tcsetattr(self.STDIN_FILENO, tty.TCSAFLUSH, mode)","This gives control of the child process to the interactive user (the human at the keyboard). Keystrokes are sent to the child process, and the stdout and stderr output of the child process is printed. This simply echos the child stdout and child stderr to the real stdout and it echos the real stdin to the child stdin. When the user types the escape_character this method will stop. The default for escape_character is ^]. This should not be confused with ASCII 27 -- the ESC character. ASCII 29 was chosen for historical merit because this is the character used by 'telnet' as the escape character. The escape_character will not be sent to the child process. You may pass in optional input and output filter functions. These functions should take a string and return a string. The output_filter will be passed all the output from the child process. The input_filter will be passed all the keyboard input from the user. The input_filter is run BEFORE the check for the escape_character. Note that if you change the window size of the parent the SIGWINCH signal will not be passed through to the child. If you want the child window size to change when the parent's window size changes then do something like the following example:: import pexpect, struct, fcntl, termios, signal, sys def sigwinch_passthrough (sig, data): s = struct.pack(""HHHH"", 0, 0, 0, 0) a = struct.unpack('hhhh', fcntl.ioctl(sys.stdout.fileno(), termios.TIOCGWINSZ , s)) global p p.setwinsize(a[0],a[1]) p = pexpect.spawn('/bin/bash') # Note this is global and used in sigwinch_passthrough. signal.signal(signal.SIGWINCH, sigwinch_passthrough) p.interact()",1,0,0,1,2,1,0,0,1,2
"def getRolesForUser(self, username, filter=None, maxCount=None):
  uURL = self._url + ""/roles/getRolesForUser""
  params = {
  ""f"" : ""json"",
  ""username"" : username
  }
  if filter is not None:
  params['filter'] = filter
  if maxCount is not None:
  params['maxCount'] = maxCount
  return self._post(url=uURL, param_dict=params,
  securityHandler=self._securityHandler,
  proxy_url=self._proxy_url,
  proxy_port=self._proxy_port)",This operation returns a list of role names that have been assigned to a particular user account. Inputs: username - name of the user for whom the returned roles filter - filter to be applied to the resultant role set. maxCount - maximum number of results to return for this query,2,0,0,1,3,2,0,0,1,3
"def get_remote_user(request):
  if 'HTTP_AUTHORIZATION' not in request.environ:
  return
  authorization = request.environ['HTTP_AUTHORIZATION']
  try:
  authmeth, auth = authorization.split(' ', 1)
  except ValueError:
  return
  if authmeth.lower() != 'basic':
  return
  try:
  auth = base64.b64decode(auth.strip().encode('latin1')).decode('latin1')
  except (binascii.Error, TypeError):
  return
  try:
  login, password = auth.split(':', 1)
  except ValueError:
  return
  return login, password",Parse basic HTTP_AUTHORIZATION and return user name,1,0,0,1,2,1,0,0,1,2
"def _get_storage_model():
  storage_model_settings = getattr(django.conf.settings,
  'GOOGLE_OAUTH2_STORAGE_MODEL', None)
  if storage_model_settings is not None:
  return (storage_model_settings['model'],
  storage_model_settings['user_property'],
  storage_model_settings['credentials_property'])
  else:
  return None, None, None","This configures whether the credentials will be stored in the session or the Django ORM based on the settings. By default, the credentials will be stored in the session, unless `GOOGLE_OAUTH2_STORAGE_MODEL` is found in the settings. Usually, the ORM storage is used to integrate credentials into an existing Django user system. Returns: A tuple containing three strings, or None. If ``GOOGLE_OAUTH2_STORAGE_MODEL`` is configured, the tuple will contain the fully qualifed path of the `django.db.model`, the name of the ``django.contrib.auth.models.User`` field on the model, and the name of the :class:`oauth2client.contrib.django_util.models.CredentialsField` field on the model. If Django ORM storage is not configured, this function returns None.",0,0,1,1,2,1,0,0,0,1
"def user_lookup(self, cloudflare_email=None, unique_id=None):
  if not cloudflare_email and not unique_id:
  raise KeyError(
  'Either cloudflare_email or unique_id must be present')
  params = {'act': 'user_lookup'}
  if cloudflare_email:
  params['cloudflare_email'] = cloudflare_email
  else:
  params['unique_id'] = unique_id
  return self._request(params)",Lookup user data based on either his cloudflare_email or his unique_id. :param cloudflare_email: email associated with user :type cloudflare_email: str :param unique_id: unique id associated with user :type unique_id: str :returns: :rtype: dict,2,0,0,1,3,2,0,0,1,3
"def hostgroup_delete(hostgroupids, **kwargs):
  conn_args = _login(**kwargs)
  ret = {}
  try:
  if conn_args:
  method = 'hostgroup.delete'
  if not isinstance(hostgroupids, list):
  params = [hostgroupids]
  else:
  params = hostgroupids
  ret = _query(method, params, conn_args['url'], conn_args['auth'])
  return ret['result']['groupids']
  else:
  raise KeyError
  except KeyError:
  return ret","Delete the host group. .. versionadded:: 2016.3.0 :param hostgroupids: IDs of the host groups to delete :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring) :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring) :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring) :return: ID of the deleted host groups, False on failure. CLI Example: .. code-block:: bash salt '*' zabbix.hostgroup_delete 23",1,0,0,1,2,2,0,0,1,3
"def _get_xephem_orbital_elements(
  self):
  self.log.info('starting the ``_get_xephem_orbital_elements`` method')
  print ""Getting the XEphem orbital element strings from the database""
  sqlQuery = u % locals()
  rows = readquery(
  log=self.log,
  sqlQuery=sqlQuery,
  dbConn=self.atlasMoversDBConn,
  quiet=False
  )
  xephemOE = list(rows)
  self.log.info('completed the ``_get_xephem_orbital_elements`` method')
  return xephemOE",*get xephem orbital elements* **Key Arguments:** - ``xephemOE`` -- a list of xephem database format strings for use with pyephem,0,0,1,1,2,0,0,1,1,2
"def get_current_course_run(course, users_active_course_runs):
  current_course_run = None
  filtered_course_runs = []
  all_course_runs = course['course_runs']
  if users_active_course_runs:
  current_course_run = get_closest_course_run(users_active_course_runs)
  else:
  for course_run in all_course_runs:
  if is_course_run_enrollable(course_run) and is_course_run_upgradeable(course_run):
  filtered_course_runs.append(course_run)
  if not filtered_course_runs:
  filtered_course_runs = all_course_runs
  if filtered_course_runs:
  current_course_run = get_closest_course_run(filtered_course_runs)
  return current_course_run","Return the current course run on the following conditions. - If user has active course runs (already enrolled) then return course run with closest start date Otherwise it will check the following logic: - Course run is enrollable (see is_course_run_enrollable) - Course run has a verified seat and the upgrade deadline has not expired. - Course run start date is closer to now than any other enrollable/upgradeable course runs. - If no enrollable/upgradeable course runs, return course run with most recent start date.",1,0,1,1,3,1,0,0,1,2
"def _generate_response(self, response: dict, request: dict) -> dict:
  response_template = deepcopy(self.response_template)
  response_template['sessionAttributes']['sessionId'] = request['session']['sessionId']
  for key, value in response_template.items():
  if key not in response.keys():
  response[key] = value
  return response",Populates generated response with additional data conforming Alexa response specification. Args: response: Raw user input extracted from Alexa request. request: Alexa request. Returns: response: Response conforming Alexa response specification.,0,0,0,1,1,1,0,0,1,2
"def create_token(self, data, options=None):
  if not options:
  options = {}
  options.update({'admin': self.admin, 'debug': self.debug})
  claims = self._create_options_claims(options)
  claims['v'] = self.TOKEN_VERSION
  claims['iat'] = int(time.mktime(time.gmtime()))
  claims['d'] = data
  return self._encode_token(self.secret, claims)","Generates a secure authentication token. Our token format follows the JSON Web Token (JWT) standard: header.claims.signature Where: 1) 'header' is a stringified, base64-encoded JSON object containing version and algorithm information. 2) 'claims' is a stringified, base64-encoded JSON object containing a set of claims: Library-generated claims: 'iat' -> The issued at time in seconds since the epoch as a number 'd' -> The arbitrary JSON object supplied by the user. User-supplied claims (these are all optional): 'exp' (optional) -> The expiration time of this token, as a number of seconds since the epoch. 'nbf' (optional) -> The 'not before' time before which the token should be rejected (seconds since the epoch) 'admin' (optional) -> If set to true, this client will bypass all security rules (use this to authenticate servers) 'debug' (optional) -> 'set to true to make this client receive debug information about security rule execution. 'simulate' (optional, internal-only for now) -> Set to true to neuter all API operations (listens / puts will run security rules but not actually write or return data). 3) A signature that proves the validity of this token (see: http://tools.ietf.org/html/draft-ietf-jose-json-web-signature-07) For base64-encoding we use URL-safe base64 encoding. This ensures that the entire token is URL-safe and could, for instance, be placed as a query argument without any encoding (and this is what the JWT spec requires). Args: data - a json serializable object of data to be included in the token options - An optional dictionary of additional claims for the token. Possible keys include: a) 'expires' -- A timestamp (as a number of seconds since the epoch) denoting a time after which this token should no longer be valid. b) 'notBefore' -- A timestamp (as a number of seconds since the epoch) denoting a time before which this token should be rejected by the server. c) 'admin' -- Set to true to bypass all security rules (use this for your trusted servers). d) 'debug' -- Set to true to enable debug mode (so you can see the results of Rules API operations) e) 'simulate' -- (internal-only for now) Set to true to neuter all API operations (listens / puts will run security rules but not actually write or return data) Returns: A signed Firebase Authentication Token Raises: ValueError: if an invalid key is specified in options",0,0,0,1,1,1,0,0,1,2
"def getFieldsForActiveJobsOfType(self, jobType, fields=[]):
  dbFields = [self._jobs.pubToDBNameDict[x] for x in fields]
  dbFieldsStr = ','.join(['job_id'] + dbFields)
  with ConnectionFactory.get() as conn:
  query = \
  'SELECT DISTINCT %s ' \
  'FROM %s j ' \
  'LEFT JOIN %s m USING(job_id) '\
  'WHERE j.status != %%s ' \
  'AND _eng_job_type = %%s' % (dbFieldsStr, self.jobsTableName,
  self.modelsTableName)
  conn.cursor.execute(query, [self.STATUS_COMPLETED, jobType])
  return conn.cursor.fetchall()","Helper function for querying the models table including relevant job info where the job type matches the specified jobType. Only records for which there is a matching jobId in both tables is returned, and only the requested fields are returned in each result, assuming that there is not a conflict. This function is useful, for example, in querying a cluster for a list of actively running production models (according to the state of the client jobs database). jobType must be one of the JOB_TYPE_XXXX enumerations. Parameters: ---------------------------------------------------------------- jobType: jobType enum fields: list of fields to return Returns: List of tuples containing the jobId and requested field values",0,0,1,0,1,0,1,1,0,2
"def edit( key = '', value = '', parent = None ):
  dlg = XKeyValueDialog(parent)
  dlg.setKey(key)
  dlg.setValue(value)
  if ( dlg.exec_() ):
  return (True, dlg.key(), dlg.value())
  return (False, '', '')","Prompts the user to edit the inputed key/value pairing. :param key | <str> value | <str> parent | <QWidget> :return (<bool> accepted, <str> key, <str> value)",1,0,0,1,2,1,0,0,1,2
"def ReplaceUser(self, user_link, user, options=None):
  if options is None:
  options = {}
  CosmosClient.__ValidateResource(user)
  path = base.GetPathFromLink(user_link)
  user_id = base.GetResourceIdOrFullNameFromLink(user_link)
  return self.Replace(user,
  path,
  'users',
  user_id,
  None,
  options)",Replaces a user and return it. :param str user_link: The link to the user entity. :param dict user: :param dict options: The request options for the request. :return: The new User. :rtype: dict,1,1,0,2,4,2,0,0,2,4
"def init_sqlite_db(path, initTime=False):
  sqlite_base_url = 'sqlite:///'
  sqlalchemy_url = sqlite_base_url + path
  init_time = init_db(sqlalchemy_url)
  if initTime:
  print('TIME: {0} seconds'.format(init_time))
  return sqlalchemy_url","Initialize SQLite Database Args: path(str): Path to database (Ex. '/home/username/my_sqlite.db'). initTime(Optional[bool]): If True, it will print the amount of time to generate database. Example:: from gsshapy.lib.db_tools import init_sqlite_db, create_session sqlite_db_path = '/home/username/my_sqlite.db' init_postgresql_db(path=sqlite_db_path) sqlalchemy_url = init_sqlite_db(path=sqlite_db_path) db_work_sessionmaker = get_sessionmaker(sqlalchemy_url) db_work_session = db_work_sessionmaker() ##DO WORK db_work_session.close()",1,1,0,0,2,1,0,0,0,1
"def add_vo_information_about_user(self, name_id):
  ava = {}
  try:
  (ava, _) = self.users.get_identity(name_id)
  except KeyError:
  pass
  if self.vorg:
  if self.vorg.do_aggregation(name_id):
  ava = self.users.get_identity(name_id)[0]
  return ava",Add information to the knowledge I have about the user. This is for Virtual organizations. :param name_id: The subject identifier :return: A possibly extended knowledge.,1,0,1,0,2,1,0,1,1,3
"def next_sequence_id(session, sequence_ids, parent_vid, table_class, force_query = False):
  from sqlalchemy import text
  seq_col = table_class.sequence_id.property.columns[0].name
  try:
  parent_col = table_class._parent_col
  except AttributeError:
  parent_col = table_class.d_vid.property.columns[0].name
  assert bool(parent_vid)
  key = (parent_vid, table_class.__name__)
  number = sequence_ids.get(key, None)
  if (not number and session) or force_query:
  sql = text(""SELECT max({seq_col})+1 FROM {table} WHERE {parent_col} = '{vid}'""
  .format(table=table_class.__tablename__, parent_col=parent_col,
  seq_col=seq_col, vid=parent_vid))
  max_id, = session.execute(sql).fetchone()
  if not max_id:
  max_id = 1
  sequence_ids[key] = int(max_id)
  elif not session:
  sequence_ids[key] = 1
  else:
  sequence_ids[key] += 1
  return sequence_ids[key]","Return the next sequence id for a object, identified by the vid of the parent object, and the database prefix for the child object. On the first call, will load the max sequence number from the database, but subsequence calls will run in process, so this isn't suitable for multi-process operation -- all of the tables in a dataset should be created by one process The child table must have a sequence_id value. :param session: Database session or connection ( must have an execute() method ) :param sequence_ids: A dict for caching sequence ids :param parent_vid: The VID of the parent object, which sets the namespace for the sequence :param table_class: Table class of the child object, the one getting a number :return:",0,0,1,0,1,0,1,1,0,2
"def send_mass_text(self, group_or_users, content,
  is_to_all=False, preview=False,
  send_ignore_reprint=0, client_msg_id=None):
  return self._send_mass_message(
  group_or_users,
  'text',
  {
  'text': {
  'content': content
  }
  },
  is_to_all,
  preview,
  send_ignore_reprint,
  client_msg_id,
  )",  https://mp.weixin.qq.com/wiki?id=mp1481187827_i0l21 :param group_or_users: / OpenID   is_to_all  True   None  :param content:  :param is_to_all: truefalsetrue false group_id :type is_to_all: bool :param preview:  group_or_users openid :type preview: bool :param send_ignore_reprint:    send_ignore_reprint 1   send_ignore_reprint 0  send_ignore_reprint 0 :type send_ignore_reprint: int :param client_msg_id:  msgid 64  :type client_msg_id: str :return:  JSON ,1,0,0,2,3,1,0,0,1,2
"def regex(pattern, prompt=None, empty=False, flags=0):
  s = _prompt_input(prompt)
  if empty and not s:
  return None
  else:
  m = re.match(pattern, s, flags=flags)
  if m:
  return m
  else:
  return regex(pattern, prompt=prompt, empty=empty, flags=flags)","Prompt a string that matches a regular expression. Parameters ---------- pattern : str A regular expression that must be matched. prompt : str, optional Use an alternative prompt. empty : bool, optional Allow an empty response. flags : int, optional Flags that will be passed to ``re.match``. Returns ------- Match or None A match object if the user entered a matching string. None if the user pressed only Enter and ``empty`` was True. See Also -------- re.match",1,0,0,1,2,1,0,0,1,2
"def send_mail(form, from_name):
  form[""from""] = ""{} <mailgun@{}>"".format(from_name, pulsarpy.MAIL_DOMAIN),
  if not pulsarpy.MAIL_SERVER_URL:
  raise Exception(""MAILGUN_DOMAIN environment variable not set."")
  if not pulsarpy.MAIL_AUTH[1]:
  raise Exception(""MAILGUN_API_KEY environment varible not set."")
  res = requests.post(pulsarpy.MAIL_SERVER_URL, data=form, auth=pulsarpy.MAIL_AUTH)
  res.raise_for_status()
  return res","Sends a mail using the configured mail server for Pulsar. See mailgun documentation at https://documentation.mailgun.com/en/latest/user_manual.html#sending-via-api for specifics. Args: form: `dict`. The mail form fields, i.e. 'to', 'from', ... Returns: `requests.models.Response` instance. Raises: `requests.exceptions.HTTPError`: The status code is not ok. `Exception`: The environment variable MAILGUN_DOMAIN or MAILGUN_API_KEY isn't set. Example:: payload = { ""from""=""{} <mailgun@{}>"".format(from_name, pulsarpy.MAIL_DOMAIN), ""subject"": ""mailgun test"", ""text"": ""howdy there"", ""to"": ""nathankw@stanford.edu"", } send_mail(payload)",2,0,0,2,4,1,0,0,1,2
"def refresh(self, executor, callbacks, completer_options=None):
  if completer_options is None:
  completer_options = {}
  if self.is_refreshing():
  self._restart_refresh.set()
  return [(None, None, None, 'Auto-completion refresh restarted.')]
  else:
  self._completer_thread = threading.Thread(
  target=self._bg_refresh,
  args=(executor, callbacks, completer_options),
  name='completion_refresh')
  self._completer_thread.setDaemon(True)
  self._completer_thread.start()
  return [(None, None, None,
  'Auto-completion refresh started in the background.')]","Creates a SQLCompleter object and populates it with the relevant completion suggestions in a background thread. executor - SQLExecute object, used to extract the credentials to connect to the database. callbacks - A function or a list of functions to call after the thread has completed the refresh. The newly created completion object will be passed in as an argument to each callback. completer_options - dict of options to pass to SQLCompleter.",1,0,0,1,2,1,0,0,1,2
"def wsgi_middleware(self, app, cors=False):
  _app = StaticServerMiddleware(app, '/' + self.prefix, self.path,
  cors=self.cors)
  def app(environ, start_response):
  if not hasattr(self, 'host_url'):
  self.host_url = (environ['wsgi.url_scheme'] + '://' +
  environ['HTTP_HOST'] + '/')
  return _app(environ, start_response)
  return app","WSGI middlewares that wraps the given ``app`` and serves actual image files. :: fs_store = HttpExposedFileSystemStore('userimages', 'images/') app = fs_store.wsgi_middleware(app) :param app: the wsgi app to wrap :type app: :class:`~typing.Callable`\ [[], :class:`~typing.Iterable`\ [:class:`bytes`]] :returns: the another wsgi app that wraps ``app`` :rtype: :class:`StaticServerMiddleware`",1,0,0,1,2,1,0,0,1,2
"def handle_memory(self, obj):
  if obj.subject is not None:
  with self.con as db:
  SchemaBase.note(
  db,
  obj.subject,
  obj.state,
  obj.object,
  text=obj.text,
  html=obj.html,
  )
  return obj",Handle a memory event. This function accesses the internal database. It writes a record containing state information and an optional note. :param obj: A :py:class:`~turberfield.dialogue.model.Model.Memory` object. :return: The supplied object.,0,1,0,0,1,0,0,0,0,0
"def export_content_groups(self, group_id, export_type, skip_notifications=None):
  path = {}
  data = {}
  params = {}
  path[""group_id""] = group_id
  self._validate_enum(export_type, [""common_cartridge"", ""qti"", ""zip""])
  data[""export_type""] = export_type
  if skip_notifications is not None:
  data[""skip_notifications""] = skip_notifications
  self.logger.debug(""POST /api/v1/groups/{group_id}/content_exports with query params: {params} and form data: {data}"".format(params=params, data=data, **path))
  return self.generic_request(""POST"", ""/api/v1/groups/{group_id}/content_exports"".format(**path), data=data, params=params, single_item=True)","Export content. Begin a content export job for a course, group, or user. You can use the {api:ProgressController#show Progress API} to track the progress of the export. The migration's progress is linked to with the _progress_url_ value. When the export completes, use the {api:ContentExportsApiController#show Show content export} endpoint to retrieve a download URL for the exported content.",2,0,0,2,4,2,0,0,1,3
"def compact_bucket(db, buck_key, limit):
  records = db.lrange(str(buck_key), 0, -1)
  loader = limits.BucketLoader(limit.bucket_class, db, limit,
  str(buck_key), records, stop_summarize=True)
  buck_record = msgpack.dumps(dict(bucket=loader.bucket.dehydrate(),
  uuid=str(uuid.uuid4())))
  result = db.linsert(str(buck_key), 'after', loader.last_summarize_rec,
  buck_record)
  if result < 0:
  LOG.warning(""Bucket compaction on %s failed; will retry"" % buck_key)
  return
  db.ltrim(str(buck_key), loader.last_summarize_idx + 1, -1)","Perform the compaction operation. This reads in the bucket information from the database, builds a compacted bucket record, inserts that record in the appropriate place in the database, then removes outdated updates. :param db: A database handle for the Redis database. :param buck_key: A turnstile.limits.BucketKey instance containing the bucket key. :param limit: The turnstile.limits.Limit object corresponding to the bucket.",0,0,1,0,1,0,1,1,0,2
"def update_user(self, email, first_name, last_name, password, metadata={}):
  data = {
  'email': email,
  'firstName': first_name,
  'lastName': last_name,
  'newPassword': password,
  'metadata': metadata,
  }
  response = self.post('updateUser', data)
  return self._handle_empty(email, response)",Update an existing user :type email: str :param email: User's email :type first_name: str :param first_name: User's first name :type last_name: str :param last_name: User's last name :type password: str :param password: User's password :type metadata: dict :param metadata: User metadata :rtype: dict :return: a dictionary containing user information,2,0,0,2,4,1,0,0,2,3
"def do_POST(self, ):
  log.debug('POST')
  self._set_headers()
  ruri = constants.REDIRECT_URI.replace('http://', 'https://')
  self.server.set_token(ruri + self.path.replace('?', '","Handle POST requests When the user is redirected, this handler will respond with a website which will send a post request with the url fragment as parameters. This will get the parameters and store the original redirection url and fragments in :data:`LoginServer.tokenurl`. :returns: None :rtype: None :raises: None",1,0,0,1,2,1,0,0,1,2
"def _login(self, username, password):
  login_url = 'https://logentries.com/login/'
  login_page_response = self.session.get(url=login_url, headers=self.default_headers)
  if not login_page_response.ok:
  raise ServerException(login_page_response.text)
  login_headers = {
  'Referer': login_url,
  'X-Requested-With': 'XMLHttpRequest',
  }
  login_headers.update(self.default_headers)
  login_response = self.session.post(
  'https://logentries.com/login/ajax/',
  headers=login_headers,
  data=self._get_login_payload(
  username,
  password),
  )
  if not login_response.ok:
  raise ServerException(login_response.text)
  app_response = self.session.get('https://logentries.com/app/', headers=self.default_headers)
  return app_response.url.split('/')[-1]",._login() makes three requests: * One to the /login/ page to get a CSRF cookie * One to /login/ajax/ to get a logged-in session cookie * One to /app/ to get the beginning of the account id :param username: A valid username (email) :type username: str :param password: A valid password :type password: str :return: The account's url id :rtype: str,2,0,0,1,3,2,0,0,2,4
"def set_permissions(obj_name,
  principal,
  permissions,
  access_mode='grant',
  applies_to=None,
  obj_type='file',
  reset_perms=False,
  protected=None):
  if applies_to is None:
  if 'registry' in obj_type.lower():
  applies_to = 'this_key_subkeys'
  elif obj_type.lower() == 'file':
  applies_to = 'this_folder_subfolders_files'
  if reset_perms:
  obj_dacl = dacl(obj_type=obj_type)
  else:
  obj_dacl = dacl(obj_name, obj_type)
  obj_dacl.rm_ace(principal, access_mode)
  obj_dacl.add_ace(principal, access_mode, permissions, applies_to)
  obj_dacl.order_acl()
  obj_dacl.save(obj_name, protected)
  return True","Set the permissions of an object. This can be a file, folder, registry key, printer, service, etc... Args: obj_name (str): The object for which to set permissions. This can be the path to a file or folder, a registry key, printer, etc. For more information about how to format the name see: https://msdn.microsoft.com/en-us/library/windows/desktop/aa379593(v=vs.85).aspx principal (str): The name of the user or group for which to set permissions. Can also pass a SID. permissions (str, list): The type of permissions to grant/deny the user. Can be one of the basic permissions, or a list of advanced permissions. access_mode (Optional[str]): Whether to grant or deny user the access. Valid options are: - grant (default): Grants the user access - deny: Denies the user access applies_to (Optional[str]): The objects to which these permissions will apply. Not all these options apply to all object types. Defaults to 'this_folder_subfolders_files' obj_type (Optional[str]): The type of object for which to set permissions. Default is 'file' reset_perms (Optional[bool]): True will overwrite the permissions on the specified object. False will append the permissions. Default is False protected (Optional[bool]): True will disable inheritance for the object. False will enable inheritance. None will make no change. Default is None. Returns: bool: True if successful, raises an error otherwise Usage: .. code-block:: python salt.utils.win_dacl.set_permissions( 'C:\\Temp', 'jsnuffy', 'full_control', 'grant')",1,0,0,0,1,1,1,0,0,2
"def get_user_by_token(self, token):
  if not self.jwt_loader_implementation:
  return self.default_token_user_loader(token)
  try:
  implementation = import_string(self.jwt_loader_implementation)
  except ImportError:
  msg = 'Failed to import custom JWT user loader implementation. '
  msg += 'Check that configured module exists [{}]'
  raise x.ConfigurationException(
  msg.format(self.jwt_loader_implementation)
  )
  return implementation(token)","Get user by token Using for logging in. Check to see if a custom token user loader was registered and uses that. Otherwise falls back to default loader implementation. You should be fine with default implementation as long as your token has user_id claim in it. :param token: str, user token :return: boiler.user.models.User",1,0,1,1,3,1,0,0,1,2
"def mktz(zone=None):
  if zone is None:
  zone = tzlocal.get_localzone().zone
  zone = six.u(zone)
  tz = dateutil.tz.gettz(zone)
  if not tz:
  raise TimezoneError('Timezone ""%s"" can not be read' % (zone))
  if not hasattr(tz, 'zone'):
  tz.zone = zone
  for p in dateutil.tz.TZPATHS:
  if zone.startswith(p):
  tz.zone = zone[len(p) + 1:]
  break
  return tz","Return a new timezone (tzinfo object) based on the zone using the python-dateutil package. The concise name 'mktz' is for convenient when using it on the console. Parameters ---------- zone : `String` The zone for the timezone. This defaults to local, returning: tzlocal.get_localzone() Returns ------- An instance of a timezone which implements the tzinfo interface. Raises - - - - - - TimezoneError : Raised if a user inputs a bad timezone name.",1,0,0,1,2,1,0,0,1,2
"def read_pickle(path, compression='infer'):
  path = _stringify_path(path)
  f, fh = _get_handle(path, 'rb', compression=compression, is_text=False)
  try:
  with warnings.catch_warnings(record=True):
  warnings.simplefilter(""ignore"", Warning)
  return pickle.load(f)
  except Exception:
  try:
  return pc.load(f, encoding=None)
  except Exception:
  return pc.load(f, encoding='latin1')
  finally:
  f.close()
  for _f in fh:
  _f.close()","Load pickled pandas object (or any object) from file. .. warning:: Loading pickled data received from untrusted sources can be unsafe. See `here <https://docs.python.org/3/library/pickle.html>`__. Parameters ---------- path : str File path where the pickled object will be loaded. compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer' For on-the-fly decompression of on-disk data. If 'infer', then use gzip, bz2, xz or zip if path ends in '.gz', '.bz2', '.xz', or '.zip' respectively, and no decompression otherwise. Set to None for no decompression. .. versionadded:: 0.20.0 Returns ------- unpickled : same type as object stored in file See Also -------- DataFrame.to_pickle : Pickle (serialize) DataFrame object to file. Series.to_pickle : Pickle (serialize) Series object to file. read_hdf : Read HDF5 file into a DataFrame. read_sql : Read SQL query or database table into a DataFrame. read_parquet : Load a parquet object, returning a DataFrame. Examples -------- >>> original_df = pd.DataFrame({""foo"": range(5), ""bar"": range(5, 10)}) >>> original_df foo bar 0 0 5 1 1 6 2 2 7 3 3 8 4 4 9 >>> pd.to_pickle(original_df, ""./dummy.pkl"") >>> unpickled_df = pd.read_pickle(""./dummy.pkl"") >>> unpickled_df foo bar 0 0 5 1 1 6 2 2 7 3 3 8 4 4 9 >>> import os >>> os.remove(""./dummy.pkl"")",1,0,0,1,2,1,0,0,1,2
"def accept(self, request, uuid=None):
  invitation = self.get_object()
  if invitation.state != models.Invitation.State.PENDING:
  raise ValidationError(_('Only pending invitation can be accepted.'))
  elif invitation.civil_number and invitation.civil_number != request.user.civil_number:
  raise ValidationError(_('User has an invalid civil number.'))
  if invitation.project:
  if invitation.project.has_user(request.user):
  raise ValidationError(_('User already has role within this project.'))
  elif invitation.customer.has_user(request.user):
  raise ValidationError(_('User already has role within this customer.'))
  if settings.WALDUR_CORE['VALIDATE_INVITATION_EMAIL'] and invitation.email != request.user.email:
  raise ValidationError(_('Invitation and user emails mismatch.'))
  replace_email = bool(request.data.get('replace_email'))
  invitation.accept(request.user, replace_email=replace_email)
  return Response({'detail': _('Invitation has been successfully accepted.')},
  status=status.HTTP_200_OK)",Accept invitation for current user. To replace user's email with email from invitation - add parameter 'replace_email' to request POST body.,1,0,1,1,3,1,0,1,1,3
"def query_stock(self, stock_id, op_user_id=None, device_info=None):
  data = {
  'appid': self.appid,
  'coupon_stock_id': stock_id,
  'op_user_id': op_user_id,
  'device_info': device_info,
  'version': '1.0',
  'type': 'XML',
  }
  return self._post('mmpaymkttransfers/query_coupon_stock', data=data)", :param stock_id:  ID :param op_user_id:  :param device_info:  :return: ,2,0,0,1,3,2,0,0,1,3
"def _set_show_zoning_enabled_configuration(self, v, load=False):
  if hasattr(v, ""_utype""):
  v = v._utype(v)
  try:
  t = YANGDynClass(v,base=show_zoning_enabled_configuration.show_zoning_enabled_configuration, is_leaf=True, yang_name=""show-zoning-enabled-configuration"", rest_name=""show-zoning-enabled-configuration"", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=False, extensions={u'tailf-common': {u'info': u'Display the Zoning Enabled-Configuration', u'hidden': u'rpccmd', u'actionpoint': u'show_zoning_configuration_db'}}, namespace='urn:brocade.com:mgmt:brocade-zone', defining_module='brocade-zone', yang_type='rpc', is_config=True)
  except (TypeError, ValueError):
  raise ValueError({
  'error-string': ,
  'defined-type': ""rpc"",
  'generated-type': ,
  })
  self.__show_zoning_enabled_configuration = t
  if hasattr(self, '_set'):
  self._set()","Setter method for show_zoning_enabled_configuration, mapped from YANG variable /brocade_zone_rpc/show_zoning_enabled_configuration (rpc) If this variable is read-only (config: false) in the source YANG file, then _set_show_zoning_enabled_configuration is considered as a private method. Backends looking to populate this variable should do so via calling thisObj._set_show_zoning_enabled_configuration() directly. YANG Description: This will display the Zoning Enabled-Configuration database.",1,0,0,0,1,1,0,0,1,2
"def _get_params(self, validator_parameter, name_prefix):
  params_validator = self.request.get(validator_parameter)
  user_params = {}
  for key in self.request.arguments():
  if key.startswith(name_prefix):
  values = self.request.get_all(key)
  adjusted_key = key[len(name_prefix):]
  if len(values) == 1:
  user_params[adjusted_key] = values[0]
  else:
  user_params[adjusted_key] = values
  if params_validator:
  resolved_validator = util.for_name(params_validator)
  resolved_validator(user_params)
  return user_params",Retrieves additional user-supplied params for the job and validates them. Args: validator_parameter: name of the request parameter which supplies validator for this parameter set. name_prefix: common prefix for all parameter names in the request. Raises: Any exception raised by the 'params_validator' request parameter if the params fail to validate. Returns: The user parameters.,1,0,0,0,1,1,0,0,1,2
"def handle_bodhi(msg):
  if 'bodhi.update.comment' in msg.topic:
  username = msg.msg['comment']['author']
  elif 'bodhi.buildroot_override' in msg.topic:
  username = msg.msg['override']['submitter']
  else:
  username = msg.msg.get('update', {}).get('submitter')
  return username","Given a bodhi message, return the FAS username.",1,0,0,1,2,1,0,0,1,2
"def delete_all(self, user=None):
  qs = self.active()
  if user:
  qs = qs.filter(recipient=user)
  soft_delete = getattr(settings, 'NOTIFY_SOFT_DELETE', True)
  if soft_delete:
  qs.update(deleted=True)
  else:
  qs.delete()",Method to soft-delete all notifications of a User (if supplied) :param user: Notification recipient. :return: Updates QuerySet as soft-deleted.,1,1,1,0,3,1,1,1,1,4
"def json_parse(self, content):
  try:
  data = json.loads(content)
  except ValueError, e:
  return {'meta': { 'status': 500, 'msg': 'Server Error'}, 'response': {""error"": ""Malformed JSON or HTML was returned.""}}
  if 'error' in data:
  return {'meta': { 'status': 400, 'msg': 'Bad Request'}, 'response': {""error"": data['error']}}
  elif 'result' in data:
  return data['result']
  else:
  return {}",Wraps and abstracts content validation and JSON parsing to make sure the user gets the correct response. :param content: The content returned from the web request to be parsed as json :returns: a dict of the json response,0,0,0,1,1,1,0,0,1,2
"def trap(func):
  @wraps(func)
  def wrapper(*args, **kwargs):
  sys.stdall = CarbonCopy()
  my_stdout, sys.stdout = sys.stdout, CarbonCopy(cc=sys.stdall)
  my_stderr, sys.stderr = sys.stderr, CarbonCopy(cc=sys.stdall)
  try:
  return func(*args, **kwargs)
  finally:
  sys.stdout = my_stdout
  sys.stderr = my_stderr
  del sys.stdall
  return wrapper","Replace sys.std(out|err) with a wrapper during execution, restored after. In addition, a new combined-streams output (another wrapper) will appear at ``sys.stdall``. This stream will resemble what a user sees at a terminal, i.e. both out/err streams intermingled.",0,0,0,1,1,1,0,0,1,2
"def _run_and_return(self, cmd, sudo=None):
  sudo = self._get_sudo(sudo)
  result = self._run_command(cmd,
  sudo=sudo,
  quiet=True,
  return_result=True)
  if len(result) == 0:
  return
  elif not self.quiet:
  bot.println(result['message'])
  return result['return_code']","Run a command, show the message to the user if quiet isn't set, and return the return code. This is a wrapper for the OCI client to run a command and easily return the return code value (what the user is ultimately interested in). Parameters ========== cmd: the command (list) to run. sudo: whether to add sudo or not.",1,0,0,1,2,1,0,0,1,2
"def get_user_roles(self, user=None):
  if user is None:
  user = g.user
  if user.is_anonymous:
  public_role = appbuilder.config.get('AUTH_ROLE_PUBLIC')
  return [appbuilder.security_manager.find_role(public_role)] \
  if public_role else []
  return user.roles",Get all the roles associated with the user. :param user: the ab_user in FAB model. :return: a list of roles associated with the user.,1,0,1,1,3,1,0,0,1,2
"def create_headers(requester: str, *, accept: str = accept_format(),
  oauth_token: Optional[str] = None,
  jwt: Optional[str] = None) -> Dict[str, str]:
  if oauth_token is not None and jwt is not None:
  raise ValueError(""Cannot pass both oauth_token and jwt."")
  headers = {""user-agent"": requester, ""accept"": accept}
  if oauth_token is not None:
  headers[""authorization""] = f""token {oauth_token}""
  elif jwt is not None:
  headers[""authorization""] = f""bearer {jwt}""
  return headers","Create a dict representing GitHub-specific header fields. The user agent is set according to who the requester is. GitHub asks it be either a username or project name. The 'accept' argument corresponds to the 'accept' field and defaults to the default result of accept_format(). You should only need to change this value if you are using a different version of the API -- e.g. one that is under development -- or if you are looking for a different format return type, e.g. wanting the rendered HTML of a Markdown file. The 'oauth_token' allows making an authenticated request using a personal access token. This can be important if you need the expanded rate limit provided by an authenticated request. The 'jwt' allows authenticating as a GitHub App by passing in the bearer token. You can only supply only one of oauth_token or jwt, not both. For consistency, all keys in the returned dict will be lowercased.",2,0,0,0,2,1,0,0,1,2
"def log_update(sender, instance, **kwargs):
  if instance.pk is not None:
  try:
  old = sender.objects.get(pk=instance.pk)
  except sender.DoesNotExist:
  pass
  else:
  new = instance
  changes = model_instance_diff(old, new)
  if changes:
  log_entry = LogEntry.objects.log_create(
  instance,
  action=LogEntry.Action.UPDATE,
  changes=json.dumps(changes),
  )","Signal receiver that creates a log entry when a model instance is changed and saved to the database. Direct use is discouraged, connect your model through :py:func:`auditlog.registry.register` instead.",0,1,1,0,2,1,1,1,0,3
"def status(self):
  task_id = self.request.id
  try:
  return UserTaskStatus.objects.get(task_id=task_id)
  except UserTaskStatus.DoesNotExist:
  arguments_dict = self.arguments_as_dict(*self.request.args, **self.request.kwargs)
  name = self.generate_name(arguments_dict)
  task_class = '.'.join([self.__class__.__module__, self.__class__.__name__])
  total_steps = self.calculate_total_steps(arguments_dict)
  user_id = arguments_dict['user_id']
  return UserTaskStatus.objects.get_or_create(
  task_id=task_id, defaults={'user_id': user_id, 'name': name, 'task_class': task_class,
  'total_steps': total_steps})[0]",Get the :py:class:`~user_tasks.models.UserTaskStatus` model instance for this UserTaskMixin.,1,0,1,1,3,1,1,1,1,4
"def sync_projects(self):
  p = self.keystone_event._service.projects.list()
  for proj in p:
  if proj.name in not_create_project_name:
  continue
  LOG.info(""Syncing project %s"" % proj.name)
  self.project_create_func(proj.id, proj=proj)",Sync projects. This function will retrieve project from keystone and populate them dfa database and dcnm,0,0,1,1,2,1,0,0,1,2
"def _restore_output(self, statement: Statement, saved_state: utils.RedirectionSavedState) -> None:
  if saved_state.redirecting:
  if statement.output and not statement.output_to:
  self.stdout.seek(0)
  write_to_paste_buffer(self.stdout.read())
  try:
  self.stdout.close()
  except BrokenPipeError:
  pass
  self.stdout = saved_state.saved_self_stdout
  sys.stdout = saved_state.saved_sys_stdout
  if self.cur_pipe_proc_reader is not None:
  self.cur_pipe_proc_reader.wait()
  self.cur_pipe_proc_reader = saved_state.saved_pipe_proc_reader",Handles restoring state after output redirection as well as the actual pipe operation if present. :param statement: Statement object which contains the parsed input from the user :param saved_state: contains information needed to restore state data,0,0,0,0,0,1,0,0,0,1
"def _access_policy_pyxb_to_model(sci_model, sysmeta_pyxb):
  _delete_existing_access_policy(sysmeta_pyxb)
  allow_rights_holder = d1_common.types.dataoneTypes.AccessRule()
  permission = d1_common.types.dataoneTypes.Permission(
  d1_gmn.app.auth.CHANGEPERMISSION_STR
  )
  allow_rights_holder.permission.append(permission)
  allow_rights_holder.subject.append(
  d1_common.xml.get_req_val(sysmeta_pyxb.rightsHolder)
  )
  top_level = _get_highest_level_action_for_rule(allow_rights_holder)
  _insert_permission_rows(sci_model, allow_rights_holder, top_level)
  if _has_access_policy_pyxb(sysmeta_pyxb):
  for allow_rule in sysmeta_pyxb.accessPolicy.allow:
  top_level = _get_highest_level_action_for_rule(allow_rule)
  _insert_permission_rows(sci_model, allow_rule, top_level)","Create or update the database representation of the sysmeta_pyxb access policy. If called without an access policy, any existing permissions on the object are removed and the access policy for the rights holder is recreated. Preconditions: - Each subject has been verified to a valid DataONE account. - Subject has changePermission for object. Postconditions: - The Permission and related tables contain the new access policy. Notes: - There can be multiple rules in a policy and each rule can contain multiple subjects. So there are two ways that the same subject can be specified multiple times in a policy. If this happens, multiple, conflicting action levels may be provided for the subject. This is handled by checking for an existing row for the subject for this object and updating it if it contains a lower action level. The end result is that there is one row for each subject, for each object and this row contains the highest action level.",0,1,1,0,2,1,1,0,0,2
"def insert_permission(
  self,
  file_id,
  value,
  perm_type,
  role,
  notify=True,
  email_message=None,
  with_link=False
  ):
  url = '{0}/{1}/permissions'.format(DRIVE_FILES_API_V2_URL, file_id)
  payload = {
  'value': value,
  'type': perm_type,
  'role': role,
  'withLink': with_link
  }
  params = {
  'sendNotificationEmails': notify,
  'emailMessage': email_message
  }
  self.request(
  'post',
  url,
  json=payload,
  params=params
  )","Creates a new permission for a file. :param file_id: a spreadsheet ID (aka file ID.) :type file_id: str :param value: user or group e-mail address, domain name or None for 'default' type. :type value: str, None :param perm_type: (optional) The account type. Allowed values are: ``user``, ``group``, ``domain``, ``anyone`` :type perm_type: str :param role: (optional) The primary role for this user. Allowed values are: ``owner``, ``writer``, ``reader`` :type str: :param notify: (optional) Whether to send an email to the target user/domain. :type notify: str :param email_message: (optional) An email message to be sent if notify=True. :type email_message: str :param with_link: (optional) Whether the link is required for this permission to be active. :type with_link: bool Examples:: # Give write permissions to otto@example.com gc.insert_permission( '0BmgG6nO_6dprnRRUWl1UFE', 'otto@example.org', perm_type='user', role='writer' ) # Make the spreadsheet publicly readable gc.insert_permission( '0BmgG6nO_6dprnRRUWl1UFE', None, perm_type='anyone', role='reader' )",1,0,0,1,2,1,0,0,2,3
"def _attach_to_model(self, model):
  super(RelatedFieldMixin, self)._attach_to_model(model)
  if model.abstract:
  return
  self.related_name = self._get_related_name()
  self.related_to = self._get_related_model_name()
  if not hasattr(self.database, '_relations'):
  self.database._relations = {}
  self.database._relations.setdefault(self.related_to, [])
  self._assert_relation_does_not_exists()
  relation = (self._model._name, self.name, self.related_name)
  self.database._relations[self.related_to].append(relation)","When we have a model, save the relation in the database, to later create RelatedCollection objects in the related model",0,1,1,0,2,0,0,1,0,1
"def create_folder_cmd_line(query, default_name=None, default_path=None):
  default = None
  if default_name and default_path:
  default = os.path.join(default_path, default_name)
  user_input = input(query + ' [default {}]: '.format(default))
  if len(user_input) == 0:
  user_input = default
  if not user_input:
  return None
  if not os.path.isdir(user_input):
  try:
  os.makedirs(user_input)
  except OSError:
  return None
  return user_input",Queries the user for a path to be created :param str query: Query that asks the user for a specific folder path to be created :param str default_name: Default name of the folder to be created :param str default_path: Path in which the folder is created if the user doesn't specify a path :return: Input path from the user or `default_path` if nothing is specified or None if directory could ne be created :rtype: str,1,0,0,1,2,1,0,0,1,2
"def save_instance(self, instance, using_transactions=True, dry_run=False):
  self.before_save_instance(instance, using_transactions, dry_run)
  if not using_transactions and dry_run:
  pass
  else:
  instance.save()
  self.after_save_instance(instance, using_transactions, dry_run)","Takes care of saving the object to the database. Keep in mind that this is done by calling ``instance.save()``, so objects are not created in bulk!",0,1,0,0,1,0,1,0,0,1
"def add_table(self, name, columns, type_map=None, if_not_exists=False):
  if type_map is None and isinstance(columns, dict): types = columns
  if type_map is None: types = {}
  if if_not_exists: query = 'CREATE TABLE IF NOT EXISTS ""%s"" (%s);'
  else: query = 'CREATE table ""%s"" (%s);'
  cols = ','.join(['""' + c + '""' + ' ' + types.get(c, 'text') for c in columns])
  self.own_cursor.execute(query % (self.main_table, cols))","Add add a new table to the database. For instance you could do this: self.add_table('data', {'id':'integer', 'source':'text', 'pubmed':'integer'})",0,1,0,0,1,0,1,1,0,2
"def interactive(self, bConfirmQuit = True, bShowBanner = True):
  print('')
  print(""-"" * 79)
  print(""Interactive debugging session started."")
  print(""Use the \""help\"" command to list all available commands."")
  print(""Use the \""quit\"" command to close this session."")
  print(""-"" * 79)
  if self.lastEvent is None:
  print('')
  console = ConsoleDebugger()
  console.confirm_quit = bConfirmQuit
  console.load_history()
  try:
  console.start_using_debugger(self)
  console.loop()
  finally:
  console.stop_using_debugger()
  console.save_history()
  print('')
  print(""-"" * 79)
  print(""Interactive debugging session closed."")
  print(""-"" * 79)
  print('')","Start an interactive debugging session. @type bConfirmQuit: bool @param bConfirmQuit: Set to C{True} to ask the user for confirmation before closing the session, C{False} otherwise. @type bShowBanner: bool @param bShowBanner: Set to C{True} to show a banner before entering the session and after leaving it, C{False} otherwise. @warn: This will temporarily disable the user-defined event handler! This method returns when the user closes the session.",1,0,0,1,2,1,0,0,1,2
"def alias_name(self,
  alias_name=None,
  is_previous_name=None,
  hgnc_symbol=None,
  hgnc_identifier=None,
  limit=None,
  as_df=False):
  q = self.session.query(models.AliasName)
  model_queries_config = (
  (alias_name, models.AliasName.alias_name),
  (is_previous_name, models.AliasName.is_previous_name),
  )
  q = self.get_model_queries(q, model_queries_config)
  one_to_many_queries_config = (
  (hgnc_symbol, models.HGNC.symbol),
  (hgnc_identifier, models.HGNC.identifier)
  )
  q = self.get_one_to_many_queries(q, one_to_many_queries_config)
  return self._limit_and_df(q, limit, as_df)","Method to query :class:`.models.AliasName` objects in database :param alias_name: alias name(s) :type alias_name: str or tuple(str) or None :param is_previous_name: flag for 'is previous' :type is_previous_name: bool or tuple(bool) or None :param hgnc_symbol: HGNC symbol(s) :type hgnc_symbol: str or tuple(str) or None :param hgnc_identifier: identifiers(s) in :class:`.models.HGNC` :type hgnc_identifier: int or tuple(int) or None :param limit: - if `isinstance(limit,int)==True` -> limit - if `isinstance(limit,tuple)==True` -> format:= tuple(page_number, results_per_page) - if limit == None -> all results :type limit: int or tuple(int) or None :param bool as_df: if `True` results are returned as :class:`pandas.DataFrame` :return: - if `as_df == False` -> list(:class:`.models.AliasSymbol`) - if `as_df == True` -> :class:`pandas.DataFrame` :rtype: list(:class:`.models.AliasSymbol`) or :class:`pandas.DataFrame`",0,0,1,1,2,1,0,1,1,3
"def inform_if_paths_invalid(egrc_path, examples_dir, custom_dir, debug=True):
  if (not debug):
  return
  if (egrc_path):
  _inform_if_path_does_not_exist(egrc_path)
  if (examples_dir):
  _inform_if_path_does_not_exist(examples_dir)
  if (custom_dir):
  _inform_if_path_does_not_exist(custom_dir)","If egrc_path, examples_dir, or custom_dir is truthy and debug is True, informs the user that a path is not set. This should be used to verify input arguments from the command line.",1,0,0,0,1,1,0,0,0,1
"def sim(sense1: ""wn.Synset"", sense2: ""wn.Synset"", option: str = ""path"") -> float:
  option = option.lower()
  if option.lower() in [""path"", ""path_similarity"",
  ""wup"", ""wupa"", ""wu-palmer"", ""wu-palmer"",
  'lch', ""leacock-chordorow""]:
  return similarity_by_path(sense1, sense2, option)
  elif option.lower() in [""res"", ""resnik"",
  ""jcn"",""jiang-conrath"",
  ""lin""]:
  return similarity_by_infocontent(sense1, sense2, option)","Calculates similarity based on user's choice. :param sense1: A synset. :param sense2: A synset. :param option: String, one of ('path', 'wup', 'lch', 'res', 'jcn', 'lin'). :return: A float, similarity measurement.",1,0,0,1,2,1,0,0,1,2
"async def set_passport_data_errors(self,
  user_id: base.Integer,
  errors: typing.List[types.PassportElementError]) -> base.Boolean:
  errors = prepare_arg(errors)
  payload = generate_payload(**locals())
  result = await self.request(api.Methods.SET_PASSPORT_DATA_ERRORS, payload)
  return result","Informs a user that some of the Telegram Passport elements they provided contains errors. The user will not be able to re-submit their Passport to you until the errors are fixed (the contents of the field for which you returned the error must change). Returns True on success. Use this if the data submitted by the user doesn't satisfy the standards your service requires for any reason. For example, if a birthday date seems invalid, a submitted document is blurry, a scan shows evidence of tampering, etc. Supply some details in the error message to make sure the user knows how to correct the issues. Source https://core.telegram.org/bots/api#setpassportdataerrors :param user_id: User identifier :type user_id: :obj:`base.Integer` :param errors: A JSON-serialized array describing the errors :type errors: :obj:`typing.List[types.PassportElementError]` :return: Returns True on success :rtype: :obj:`base.Boolean`",1,0,0,2,3,1,0,0,1,2
"def company_add_user(self, email, name, password, receiver, admin):
  method, url = get_URL('company_add_user')
  payload = {
  'apikey': self.config.get('apikey'),
  'logintoken': self.session.cookies.get('logintoken'),
  'email': email,
  'name': name,
  'password': password,
  'canreceivefiles': receiver,
  'admin': admin
  }
  res = getattr(self.session, method)(url, params=payload)
  if res.status_code == 200:
  return True
  hellraiser(res)",Add a user to the company account. :param email: :param name: :param password: Pass without storing in plain text :param receiver: Can user receive files :param admin: :type email: ``str`` or ``unicode`` :type name: ``str`` or ``unicode`` :type password: ``str`` or ``unicode`` :type receiver: ``bool`` :type admin: ``bool`` :rtype: ``bool``,1,0,0,1,2,2,0,0,1,3
"def generate_checkpoints(engine, crypto_factory, min_dt=None, max_dt=None,
  logger=None):
  return _generate_notebooks(remote_checkpoints,
  remote_checkpoints.c.last_modified,
  engine, crypto_factory, min_dt, max_dt, logger)","Create a generator of decrypted remote checkpoints. Checkpoints are yielded in ascending order of their timestamp. This function selects all notebook checkpoints (optionally, falling within a datetime range), decrypts them, and returns a generator yielding dicts, each containing a decoded notebook and metadata including the user, filepath, and timestamp. Parameters ---------- engine : SQLAlchemy.engine Engine encapsulating database connections. crypto_factory : function[str -> Any] A function from user_id to an object providing the interface required by PostgresContentsManager.crypto. Results of this will be used for decryption of the selected notebooks. min_dt : datetime.datetime, optional Minimum last modified datetime at which a file will be included. max_dt : datetime.datetime, optional Last modified datetime at and after which a file will be excluded. logger : Logger, optional",0,0,0,1,1,1,0,0,0,1
"def friendships(user_id, level=2):
  logging.info(""getting friends for user %s"", user_id)
  level -= 1
  try:
  for friend_id in t.friend_ids(user_id):
  yield (user_id, friend_id)
  if level > 0:
  yield from friendships(friend_id, level)
  except requests.exceptions.HTTPError as e:
  if e.response.status_code == 401:
  logging.error(""can't get friends for protected user %s"", user_id)
  else:
  raise(e)","Pass in a user_id and you will be returned a generator of friendship tuples (user_id, friend_id). By default it will return the friend of a friend network (level=2), but you can expand this by settings the level parameter to either another number. But beware, it could run for a while!",1,0,0,1,2,2,0,0,1,3
"def parse_clubs(self, clubs_page):
  user_info = self.parse_sidebar(clubs_page)
  second_col = clubs_page.find(u'div', {u'id': u'content'}).find(u'table').find(u'tr').find_all(u'td', recursive=False)[1]
  try:
  user_info[u'clubs'] = []
  club_list = second_col.find(u'ol')
  if club_list:
  clubs = club_list.find_all(u'li')
  for row in clubs:
  club_link = row.find(u'a')
  link_parts = club_link.get(u'href').split(u'?cid=')
  user_info[u'clubs'].append(self.session.club(int(link_parts[1])).set({u'name': club_link.text}))
  except:
  if not self.session.suppress_parse_exceptions:
  raise
  return user_info",Parses the DOM and returns user clubs attributes. :type clubs_page: :class:`bs4.BeautifulSoup` :param clubs_page: MAL user clubs page's DOM :rtype: dict :return: User clubs attributes.,1,0,0,1,2,1,0,0,1,2
"def new(cls, path, ckan_version, site_name, **kwargs):
  if ckan_version == 'master':
  ckan_version = 'latest'
  name, datadir, srcdir = task.new_environment_check(path, site_name, ckan_version)
  environment = cls(name, srcdir, datadir, site_name, ckan_version, **kwargs)
  environment._generate_passwords()
  return environment","Return a Environment object with settings for a new project. No directories or containers are created by this call. :params path: location for new project directory, may be relative :params ckan_version: release of CKAN to install :params site_name: The name of the site to install database and solr \ eventually. For additional keyword arguments see the __init__ method. Raises DatcatsError if directories or project with same name already exits.",1,0,0,0,1,1,0,0,0,1
"def get_from_area(self, lat_min, lon_min, lat_max, lon_max, picture_size=None, set_=None, map_filter=None):
  page_size = 100
  page = 0
  result = self._request(lat_min, lon_min, lat_max, lon_max, page * page_size, (page + 1) * page_size,
  picture_size, set_, map_filter)
  total_photos = result['count']
  if total_photos < page_size:
  return result
  page += 1
  pages = (total_photos / page_size) + 1
  while page < pages:
  new_result = self._request(lat_min, lon_min, lat_max, lon_max, page * page_size, (page + 1) * page_size,
  picture_size, set_, map_filter)
  result['photos'].extend(new_result['photos'])
  page += 1
  return result","Get all available photos for a specific bounding box :param lat_min: Minimum latitude of the bounding box :type lat_min: float :param lon_min: Minimum longitude of the bounding box :type lon_min: float :param lat_max: Maximum latitude of the bounding box :type lat_max: float :param lon_max: Maximum longitude of the bounding box :type lon_max: float :param picture_size: This can be: original, medium (*default*), small, thumbnail, square, mini_square :type picture_size: basestring :param set_: This can be: public, popular or user-id; where user-id is the specific id of a user (as integer) :type set_: basestring/int :param map_filter: Whether to return photos that look better together; when True, tries to avoid returning photos of the same location :type map_filter: bool :return: Returns the full dataset of all available photos",2,0,0,1,3,1,0,0,1,2
"def get_user_api_key(self, username, create=True):
  retval = self._database.users.find_one({""username"": username}, {""apikey"": 1})
  if ""apikey"" not in retval and create:
  apikey = self.generate_api_key()
  self._database.users.update_one({""username"": username}, {""$set"": {""apikey"": apikey}})
  elif ""apikey"" not in retval:
  apikey = None
  else:
  apikey = retval[""apikey""]
  return apikey","Get the API key of a given user. API keys are generated on demand. :param username: :param create: Create the API key if none exists yet :return: the API key assigned to the user, or None if none exists and create is False.",1,1,1,1,4,1,1,1,1,4
"def post(self, user_ids=None, usernames=None, status=None):
  return self.connection.post('user/array',
  data=dict(user_ids=user_ids,
  usernames=usernames,
  status=status))",:param user_ids: list of int of the user_ids to return :param usernames: list of str of the usernames to return :param status: str of the status :return: list of User,1,0,0,1,2,1,0,0,2,3
"def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,
  figsize=None, cmap='Spectral',
  title_fontsize=""large"", text_fontsize=""medium""):
  transformed_X = clf.transform(X)
  if ax is None:
  fig, ax = plt.subplots(1, 1, figsize=figsize)
  ax.set_title(title, fontsize=title_fontsize)
  classes = np.unique(np.array(y))
  colors = plt.cm.get_cmap(cmap)(np.linspace(0, 1, len(classes)))
  for label, color in zip(classes, colors):
  ax.scatter(transformed_X[y == label, 0], transformed_X[y == label, 1],
  alpha=0.8, lw=2, label=label, color=color)
  ax.legend(loc='best', shadow=False, scatterpoints=1,
  fontsize=text_fontsize)
  ax.set_xlabel('First Principal Component', fontsize=text_fontsize)
  ax.set_ylabel('Second Principal Component', fontsize=text_fontsize)
  ax.tick_params(labelsize=text_fontsize)
  return ax","Plots the 2-dimensional projection of PCA on a given dataset. Args: clf: Fitted PCA instance that can ``transform`` given data set into 2 dimensions. X (array-like, shape (n_samples, n_features)): Feature set to project, where n_samples is the number of samples and n_features is the number of features. y (array-like, shape (n_samples) or (n_samples, n_features)): Target relative to X for labeling. title (string, optional): Title of the generated plot. Defaults to ""PCA 2-D Projection"" ax (:class:`matplotlib.axes.Axes`, optional): The axes upon which to plot the curve. If None, the plot is drawn on a new set of axes. figsize (2-tuple, optional): Tuple denoting figure size of the plot e.g. (6, 6). Defaults to ``None``. cmap (string or :class:`matplotlib.colors.Colormap` instance, optional): Colormap used for plotting the projection. View Matplotlib Colormap documentation for available options. https://matplotlib.org/users/colormaps.html title_fontsize (string or int, optional): Matplotlib-style fontsizes. Use e.g. ""small"", ""medium"", ""large"" or integer-values. Defaults to ""large"". text_fontsize (string or int, optional): Matplotlib-style fontsizes. Use e.g. ""small"", ""medium"", ""large"" or integer-values. Defaults to ""medium"". Returns: ax (:class:`matplotlib.axes.Axes`): The axes on which the plot was drawn. Example: >>> import scikitplot.plotters as skplt >>> pca = PCA(random_state=1) >>> pca.fit(X) >>> skplt.plot_pca_2d_projection(pca, X, y) <matplotlib.axes._subplots.AxesSubplot object at 0x7fe967d64490> >>> plt.show() .. image:: _static/examples/plot_pca_2d_projection.png :align: center :alt: PCA 2D Projection",0,0,0,1,1,1,0,0,1,2
"def twisted_consume(callback, bindings=None, queues=None):
  if isinstance(bindings, dict):
  bindings = [bindings]
  callback = _check_callback(callback)
  global _twisted_service
  if _twisted_service is None:
  _twisted_service = service.FedoraMessagingServiceV2(config.conf[""amqp_url""])
  reactor.callWhenRunning(_twisted_service.startService)
  reactor.addSystemEventTrigger(
  ""before"", ""shutdown"", _twisted_service.stopService
  )
  return _twisted_service._service.factory.consume(callback, bindings, queues)","Start a consumer using the provided callback and run it using the Twisted event loop (reactor). .. note:: Callbacks run in a Twisted-managed thread pool using the :func:`twisted.internet.threads.deferToThread` API to avoid them blocking the event loop. If you wish to use Twisted APIs in your callback you must use the :func:`twisted.internet.threads.blockingCallFromThread` or :class:`twisted.internet.interfaces.IReactorFromThreads` APIs. This API expects the caller to start the reactor. Args: callback (callable): A callable object that accepts one positional argument, a :class:`.Message` or a class object that implements the ``__call__`` method. The class will be instantiated before use. bindings (dict or list of dict): Bindings to declare before consuming. This should be the same format as the :ref:`conf-bindings` configuration. queues (dict): The queue to declare and consume from. Each key in this dictionary should be a queue name to declare, and each value should be a dictionary with the ""durable"", ""auto_delete"", ""exclusive"", and ""arguments"" keys. Returns: twisted.internet.defer.Deferred: A deferred that fires with the list of one or more :class:`.Consumer` objects. Each consumer object has a :attr:`.Consumer.result` instance variable that is a Deferred that fires or errors when the consumer halts. Note that this API is meant to survive network problems, so consuming will continue until :meth:`.Consumer.cancel` is called or a fatal server error occurs. The deferred returned by this function may error back with a :class:`fedora_messaging.exceptions.BadDeclaration` if queues or bindings cannot be declared on the broker, a :class:`fedora_messaging.exceptions.PermissionException` if the user doesn't have access to the queue, or :class:`fedora_messaging.exceptions.ConnectionException` if the TLS or AMQP handshake fails.",1,0,0,0,1,1,0,0,1,2
"def list_collections(self, session=None, filter=None, **kwargs):
  if filter is not None:
  kwargs['filter'] = filter
  read_pref = ((session and session._txn_read_preference())
  or ReadPreference.PRIMARY)
  def _cmd(session, server, sock_info, slave_okay):
  return self._list_collections(
  sock_info, slave_okay, session, read_preference=read_pref,
  **kwargs)
  return self.__client._retryable_read(
  _cmd, read_pref, session)",Get a cursor over the collectons of this database. :Parameters: - `session` (optional): a :class:`~pymongo.client_session.ClientSession`. - `filter` (optional): A query document to filter the list of collections returned from the listCollections command. - `**kwargs` (optional): Optional parameters of the `listCollections command <https://docs.mongodb.com/manual/reference/command/listCollections/>`_ can be passed as keyword arguments to this method. The supported options differ by server version. :Returns: An instance of :class:`~pymongo.command_cursor.CommandCursor`. .. versionadded:: 3.6,1,0,1,1,3,2,0,0,1,3
"def validate(self, data,
  expect_header_row=True,
  ignore_lines=0,
  summarize=False,
  limit=0,
  context=None,
  report_unexpected_exceptions=True):
  problems = list()
  problem_generator = self.ivalidate(data, expect_header_row,
  ignore_lines, summarize, context,
  report_unexpected_exceptions)
  for i, p in enumerate(problem_generator):
  if not limit or i < limit:
  problems.append(p)
  return problems","Validate `data` and return a list of validation problems found. Arguments --------- `data` - any source of row-oriented data, e.g., as provided by a `csv.reader`, or a list of lists of strings, or ... `expect_header_row` - does the data contain a header row (i.e., the first record is a list of field names)? Defaults to True. `ignore_lines` - ignore n lines (rows) at the beginning of the data `summarize` - only report problem codes, no other details `limit` - report at most n problems `context` - a dictionary of any additional information to be added to any problems found - useful if problems are being aggregated from multiple validators `report_unexpected_exceptions` - value check function, value predicates, record check functions, record predicates, and other user-supplied validation functions may raise unexpected exceptions. If this argument is true, any unexpected exceptions will be reported as validation problems; if False, unexpected exceptions will be handled silently.",0,0,0,1,1,1,0,0,1,2
"def search_reports(self, search_term=None,
  enclave_ids=None,
  from_time=None,
  to_time=None,
  tags=None,
  excluded_tags=None):
  return Page.get_generator(page_generator=self._search_reports_page_generator(search_term, enclave_ids,
  from_time, to_time, tags,
  excluded_tags))","Uses the |search_reports_page| method to create a generator that returns each successive report. :param str search_term: The term to search for. If empty, no search term will be applied. Otherwise, must be at least 3 characters. :param list(str) enclave_ids: list of enclave ids used to restrict reports to specific enclaves (optional - by default reports from all of user's enclaves are returned) :param int from_time: start of time window in milliseconds since epoch (optional) :param int to_time: end of time window in milliseconds since epoch (optional) :param list(str) tags: Name (or list of names) of tag(s) to filter reports by. Only reports containing ALL of these tags will be returned. (optional) :param list(str) excluded_tags: Reports containing ANY of these tags will be excluded from the results. :return: The generator of Report objects. Note that the body attributes of these reports will be ``None``.",2,0,0,1,3,2,0,0,1,3
"def list(self, roomId=None, personId=None, personEmail=None, max=None,
  **request_parameters):
  check_type(roomId, basestring)
  check_type(personId, basestring)
  check_type(personEmail, basestring)
  check_type(max, int)
  params = dict_from_items_with_values(
  request_parameters,
  roomId=roomId,
  personId=personId,
  personEmail=personEmail,
  max=max,
  )
  items = self._session.get_items(API_ENDPOINT, params=params)
  for item in items:
  yield self._object_factory(OBJECT_TYPE, item)","List room memberships. By default, lists memberships for rooms to which the authenticated user belongs. Use query parameters to filter the response. Use `roomId` to list memberships for a room, by ID. Use either `personId` or `personEmail` to filter the results. This method supports Webex Teams's implementation of RFC5988 Web Linking to provide pagination support. It returns a generator container that incrementally yields all memberships returned by the query. The generator will automatically request additional 'pages' of responses from Webex as needed until all responses have been returned. The container makes the generator safe for reuse. A new API call will be made, using the same parameters that were specified when the generator was created, every time a new iterator is requested from the container. Args: roomId(basestring): Limit results to a specific room, by ID. personId(basestring): Limit results to a specific person, by ID. personEmail(basestring): Limit results to a specific person, by email address. max(int): Limit the maximum number of items returned from the Webex Teams service per request. **request_parameters: Additional request parameters (provides support for parameters that may be added in the future). Returns: GeneratorContainer: A GeneratorContainer which, when iterated, yields the memberships returned by the Webex Teams query. Raises: TypeError: If the parameter types are incorrect. ApiError: If the Webex Teams cloud returns an error.",2,0,0,1,3,2,0,0,1,3
"def GetSavename(default=None, **kwargs):
  args = ['--save']
  if default:
  args.append('--filename=%s' % default)
  for generic_args in kwargs_helper(kwargs):
  args.append('--%s=%s' % generic_args)
  p = run_zenity('--file-selection', *args)
  if p.wait() == 0:
  return p.stdout.read().strip().split('|')","Prompt the user for a filename to save as. This will raise a Zenity Save As Dialog. It will return the name to save a file as or None if the user hit cancel. default - The default name that should appear in the save as dialog. kwargs - Optional command line parameters for Zenity such as height, width, etc.",1,0,0,1,2,1,0,0,1,2
"def delete_user(self, id, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.delete_user_with_http_info(id, **kwargs)
  else:
  (data) = self.delete_user_with_http_info(id, **kwargs)
  return data","Deletes a user identified by id # noqa: E501 # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.delete_user(id, async_req=True) >>> result = thread.get() :param async_req bool :param str id: (required) :return: None If the method is called asynchronously, returns the request thread.",1,1,0,1,3,2,0,0,1,3
"def format_query_result(self, query_result, query_path, return_type=list, preceding_depth=None):
  if type(query_result) != return_type:
  converted_result = self.format_with_handler(query_result, return_type)
  else:
  converted_result = query_result
  converted_result = self.add_preceding_dict(converted_result, query_path, preceding_depth)
  return converted_result","Formats the query result based on the return type requested. :param query_result: (dict or str or list), yaml query result :param query_path: (str, list(str)), representing query path :param return_type: type, return type of object user desires :param preceding_depth: int, the depth to which we want to encapsulate back up config tree -1 : defaults to entire tree :return: (dict, OrderedDict, str, list), specified return type",0,0,0,1,1,1,0,0,1,2
"async def get_offers(self, **params):
 if params.get(""message""):
 params = json.loads(params.get(""message"", ""{}""))
 if not params:
 return {""error"":400, ""reason"":""Missed required fields""}
 cid = params.get(""cid"")
 public_key = params.get(""public_key"")
 coinid = params.get(""coinid"")
 if cid and coinid:
 cid = int(cid)
 database = client[coinid]
 offer_collection = database[settings.OFFER]
 content_collection = database[settings.CONTENT]
 offers = [{i:document[i] for i in document if i == ""confirmed""}
 async for document in offer_collection.find({""cid"":cid, ""confirmed"":None})]
 elif not cid:
 database = client[coinid]
 offer_collection = database[settings.OFFER]
 offers = [{i:document[i] for i in document if i == ""confirmed""}
 async for document in offer_collection.find({""public_key"":public_key,
 ""confirmed"":None})]
 return offers",Receives all users input (by cid) or output offers Accepts: - public key - cid (optional) - coinid (optional),1,0,1,1,3,2,0,1,1,4
"def ccds(self, ccdsid=None, hgnc_symbol=None, hgnc_identifier=None, limit=None, as_df=False):
  q = self.session.query(models.CCDS)
  model_queries_config = (
  (ccdsid, models.CCDS.ccdsid),
  )
  q = self.get_model_queries(q, model_queries_config)
  one_to_many_queries_config = (
  (hgnc_symbol, models.HGNC.symbol),
  (hgnc_identifier, models.HGNC.identifier)
  )
  q = self.get_one_to_many_queries(q, one_to_many_queries_config)
  return self._limit_and_df(q, limit, as_df)","Method to query :class:`.models.CCDS` objects in database :param ccdsid: Consensus CDS ID(s) :type ccdsid: str or tuple(str) or None :param hgnc_symbol: HGNC symbol(s) :type hgnc_symbol: str or tuple(str) or None :param hgnc_identifier: identifiers(s) in :class:`.models.HGNC` :type hgnc_identifier: int or tuple(int) or None :param limit: - if `isinstance(limit,int)==True` -> limit - if `isinstance(limit,tuple)==True` -> format:= tuple(page_number, results_per_page) - if limit == None -> all results :type limit: int or tuple(int) or None :param bool as_df: if `True` results are returned as :class:`pandas.DataFrame` :return: - if `as_df == False` -> list(:class:`.models.CCDS`) - if `as_df == True` -> :class:`pandas.DataFrame` :rtype: list(:class:`.models.CCDS`) or :class:`pandas.DataFrame`",1,0,1,1,3,1,0,1,1,3
"def task_add_user(self, *args, **kwargs):
  if not self.cur_task:
  return
  dialog = UserAdderDialog(task=self.cur_task)
  dialog.exec_()
  users = dialog.users
  for user in users:
  userdata = djitemdata.UserItemData(user)
  treemodel.TreeItem(userdata, self.task_user_model.root)",Add users to the current task :returns: None :rtype: None :raises: None,1,0,0,1,2,1,1,0,1,3
"def encrypt_data(self, name, plaintext, context="""", key_version=0, nonce=None, batch_input=None, type=""aes256-gcm96"",
  convergent_encryption="""", mount_point=DEFAULT_MOUNT_POINT):
  params = {
  'plaintext': plaintext,
  'context': context,
  'key_version': key_version,
  'nonce': nonce,
  'batch_input': batch_input,
  'type': type,
  'convergent_encryption': convergent_encryption,
  }
  api_path = '/v1/{mount_point}/encrypt/{name}'.format(
  mount_point=mount_point,
  name=name,
  )
  response = self._adapter.post(
  url=api_path,
  json=params,
  )
  return response.json()","Encrypt the provided plaintext using the named key. This path supports the create and update policy capabilities as follows: if the user has the create capability for this endpoint in their policies, and the key does not exist, it will be upserted with default values (whether the key requires derivation depends on whether the context parameter is empty or not). If the user only has update capability and the key does not exist, an error will be returned. Supported methods: POST: /{mount_point}/encrypt/{name}. Produces: 200 application/json :param name: Specifies the name of the encryption key to encrypt against. This is specified as part of the URL. :type name: str | unicode :param plaintext: Specifies base64 encoded plaintext to be encoded. :type plaintext: str | unicode :param context: Specifies the base64 encoded context for key derivation. This is required if key derivation is enabled for this key. :type context: str | unicode :param key_version: Specifies the version of the key to use for encryption. If not set, uses the latest version. Must be greater than or equal to the key's min_encryption_version, if set. :type key_version: int :param nonce: Specifies the base64 encoded nonce value. This must be provided if convergent encryption is enabled for this key and the key was generated with Vault 0.6.1. Not required for keys created in 0.6.2+. The value must be exactly 96 bits (12 bytes) long and the user must ensure that for any given context (and thus, any given encryption key) this nonce value is never reused. :type nonce: str | unicode :param batch_input: Specifies a list of items to be encrypted in a single batch. When this parameter is set, if the parameters 'plaintext', 'context' and 'nonce' are also set, they will be ignored. The format for the input is: [dict(context=""b64_context"", plaintext=""b64_plaintext""), ...] :type batch_input: List[dict] :param type: This parameter is required when encryption key is expected to be created. When performing an upsert operation, the type of key to create. :type type: str | unicode :param convergent_encryption: This parameter will only be used when a key is expected to be created. Whether to support convergent encryption. This is only supported when using a key with key derivation enabled and will require all requests to carry both a context and 96-bit (12-byte) nonce. The given nonce will be used in place of a randomly generated nonce. As a result, when the same context and nonce are supplied, the same ciphertext is generated. It is very important when using this mode that you ensure that all nonces are unique for a given context. Failing to do so will severely impact the ciphertext's security. :type convergent_encryption: str | unicode :param mount_point: The ""path"" the method/backend was mounted on. :type mount_point: str | unicode :return: The JSON response of the request. :rtype: requests.Response",2,0,0,2,4,1,0,0,2,3
"def role_get(name, user=None, host=None, port=None, maintenance_db=None,
  password=None, runas=None, return_password=False):
  all_users = user_list(user=user,
  host=host,
  port=port,
  maintenance_db=maintenance_db,
  password=password,
  runas=runas,
  return_password=return_password)
  try:
  return all_users.get(name, None)
  except AttributeError:
  log.error('Could not retrieve Postgres role. Is Postgres running?')
  return None",Return a dict with information about users of a Postgres server. Set return_password to True to get password hash in the result. CLI Example: .. code-block:: bash salt '*' postgres.role_get postgres,1,0,1,1,3,1,0,0,1,2
"def auth_access(self, auth_code):
  data = {
  'client_id': self.client_id,
  'client_secret': self.client_secret,
  'grant_type': 'authorization_code',
  'code': auth_code,
  'redirect_uri': self.redirect_url
  }
  return self.request(""post"", ""access_token"", data=data)","verify the fist authorization response url code response data    access_token string  UID  expires_in string access_token remind_in string access_tokenexpires_in uid string UIDuser/show access_token :param auth_code: authorize_url response code :return: normal: { ""access_token"": ""ACCESS_TOKEN"", ""expires_in"": 1234, ""remind_in"":""798114"", ""uid"":""12341234"" } mobile: { ""access_token"": ""SlAV32hkKG"", ""remind_in"": 3600, ""expires_in"": 3600 ""refresh_token"": ""QXBK19xm62"" }",1,0,0,1,2,2,0,0,2,4
"def get_bookmarks(self, folder='unread', limit=25, have=None):
  path = 'bookmarks/list'
  params = {'folder_id': folder, 'limit': limit}
  if have:
  have_concat = ','.join(str(id_) for id_ in have)
  params['have'] = have_concat
  response = self.request(path, params)
  items = response['data']
  bookmarks = []
  for item in items:
  if item.get('type') == 'error':
  raise Exception(item.get('message'))
  elif item.get('type') == 'bookmark':
  bookmarks.append(Bookmark(self, **item))
  return bookmarks","Return list of user's bookmarks. :param str folder: Optional. Possible values are unread (default), starred, archive, or a folder_id value. :param int limit: Optional. A number between 1 and 500, default 25. :param list have: Optional. A list of IDs to exclude from results :returns: List of user's bookmarks :rtype: list",2,0,0,1,3,2,0,0,1,3
"def lsdb(self, lsdb=None, url=None, hgnc_symbol=None, hgnc_identifier=None, limit=None, as_df=False):
  q = self.session.query(models.LSDB)
  model_queries_config = (
  (lsdb, models.LSDB.lsdb),
  (url, models.LSDB.url),
  )
  q = self.get_model_queries(q, model_queries_config)
  one_to_many_queries_config = (
  (hgnc_symbol, models.HGNC.symbol),
  (hgnc_identifier, models.HGNC.identifier)
  )
  q = self.get_one_to_many_queries(q, one_to_many_queries_config)
  return self._limit_and_df(q, limit, as_df)","Method to query :class:`.models.LSDB` objects in database :param lsdb: name(s) of the Locus Specific Mutation Database :type lsdb: str or tuple(str) or None :param url: URL of the Locus Specific Mutation Database :type url: str or tuple(str) or None :param hgnc_symbol: HGNC symbol(s) :type hgnc_symbol: str or tuple(str) or None :param hgnc_identifier: identifiers(s) in :class:`.models.HGNC` :type hgnc_identifier: int or tuple(int) or None :param limit: - if `isinstance(limit,int)==True` -> limit - if `isinstance(limit,tuple)==True` -> format:= tuple(page_number, results_per_page) - if limit == None -> all results :type limit: int or tuple(int) or None :param bool as_df: if `True` results are returned as :class:`pandas.DataFrame` :return: - if `as_df == False` -> list(:class:`.models.LSDB`) - if `as_df == True` -> :class:`pandas.DataFrame` :rtype: list(:class:`.models.LSDB`) or :class:`pandas.DataFrame`",1,0,1,1,3,1,0,1,1,3
"def checkUserAccess(self):
  allowed = True
  pm = getToolByName(self, ""portal_membership"")
  member = pm.getAuthenticatedMember()
  analyst = self.getAnalyst().strip()
  if analyst != _c(member.getId()):
  roles = member.getRoles()
  restrict = 'Manager' not in roles \
  and 'LabManager' not in roles \
  and 'LabClerk' not in roles \
  and 'RegulatoryInspector' not in roles \
  and self.bika_setup.getRestrictWorksheetUsersAccess()
  allowed = not restrict
  return allowed","Checks if the current user has granted access to this worksheet. Returns False if the user has no access, otherwise returns True",2,0,1,1,4,1,0,0,1,2
"def solr_advanced_search(self, query, token=None, limit=20):
  parameters = dict()
  parameters['query'] = query
  parameters['limit'] = limit
  if token:
  parameters['token'] = token
  response = self.request('midas.solr.search.advanced', parameters)
  return response",Search item metadata using Apache Solr. :param query: The Apache Lucene search query. :type query: string :param token: (optional) A valid token for the user in question. :type token: None | string :param limit: (optional) The limit of the search. :type limit: int | long :returns: The list of items that match the search query. :rtype: list[dict],1,0,1,1,3,2,0,0,1,3
"def getList(self, full_path, type = 1, dept = 0, sort = 'name', order = 'asc', startnum = 0, pagingrow = 1000, dummy = 56184):
  if type not in range(1, 6):
  print ""Error getList: `type` should be between 1 to 5""
  return False
  data = {'orgresource': full_path,
  'type': type,
  'dept': dept,
  'sort': sort,
  'order': order,
  'startnum': startnum,
  'pagingrow': pagingrow,
  'userid': self.user_id,
  'useridx': self.useridx,
  'dummy': dummy,
  }
  s, metadata = self.POST('getList', data)
  if s is True:
  return metadata
  else:
  print metadata
  return False","Get a list of files >>> nd_list = nd.getList('/', type=3) >>> print nd_list There are 5 kinds of ``type``: - 1 => only directories with idxfolder property - 2 => only files - 3 => directories and files with thumbnail info (like viewHeight, viewWidth for Image file) - 4 => only directories except idxfolder - 5 => directories and files without thumbnail info There are 5 kindes of ``sort``: - file : file type,  - length : size of file,  - date : edited date,     - credate : creation date,    - protect : protect or not,   :param full_path: The full path to get the file list. :param type: 1, 2, 3, 4 or 5 :param depth: Dept for file list :param sort: name =>  :param order: Order by (asc, desc) :return: metadata (list of dict) or False when failed to get list :metadata: - u'copyright': u'N', - u'creationdate': u'2013-05-12T21:17:23+09:00', - u'filelink': None, - u'fileuploadstatus': u'1', - u'getcontentlength': 0, - u'getlastmodified': u'2014-01-26T12:23:07+09:00', - u'href': u'/Codes/', - u'lastaccessed': u'2013-05-12T21:17:23+09:00', - u'lastmodifieduser': None, - u'priority': u'1', - u'protect': u'N', - u'resourceno': 204041859, - u'resourcetype': u'collection', - u'sharedinfo': u'F', - u'sharemsgcnt': 0, - u'shareno': 0, - u'subfoldercnt': 5, - u'thumbnailpath': u'N', - u'virusstatus': u'N'",2,0,0,1,3,2,0,0,1,3
"def search_user_group_for_facet(self, facet, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.search_user_group_for_facet_with_http_info(facet, **kwargs)
  else:
  (data) = self.search_user_group_for_facet_with_http_info(facet, **kwargs)
  return data","Lists the values of a specific facet over the customer's user groups # noqa: E501 # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.search_user_group_for_facet(facet, async_req=True) >>> result = thread.get() :param async_req bool :param str facet: (required) :param FacetSearchRequestContainer body: :return: ResponseContainerFacetResponse If the method is called asynchronously, returns the request thread.",2,0,0,1,3,2,0,0,1,3
"def console_user(username=False):
  try:
  uid = os.stat('/dev/console')[4]
  except (OSError, IndexError):
  raise CommandExecutionError('Failed to get a UID for the console user.')
  if username:
  return pwd.getpwuid(uid)[0]
  return uid","Gets the UID or Username of the current console user. :return: The uid or username of the console user. :param bool username: Whether to return the username of the console user instead of the UID. Defaults to False :rtype: Interger of the UID, or a string of the username. Raises: CommandExecutionError: If we fail to get the UID. CLI Example: .. code-block:: bash import salt.utils.mac_service salt.utils.mac_service.console_user()",0,0,0,1,1,1,0,0,1,2
"def pastdate(self, prompt, default=None):
  prompt = prompt if prompt is not None else ""Enter a past date""
  if default is not None:
  prompt += "" ["" + default.strftime('%d %m %Y') + ""]""
  prompt += ': '
  return self.input(curry(filter_pastdate, default=default), prompt)",Prompts user to input a date in the past.,1,0,0,1,2,1,0,0,1,2
"def iter_starred(self, sort=None, direction=None, number=-1, etag=None):
  from .repos import Repository
  params = {'sort': sort, 'direction': direction}
  self._remove_none(params)
  url = self.starred_urlt.expand(owner=None, repo=None)
  return self._iter(int(number), url, Repository, params, etag)","Iterate over repositories starred by this user. .. versionchanged:: 0.5 Added sort and direction parameters (optional) as per the change in GitHub's API. :param int number: (optional), number of starred repos to return. Default: -1, returns all available repos :param str sort: (optional), either 'created' (when the star was created) or 'updated' (when the repository was last pushed to) :param str direction: (optional), either 'asc' or 'desc'. Default: 'desc' :param str etag: (optional), ETag from a previous request to the same endpoint :returns: generator of :class:`Repository <github3.repos.Repository>`",2,0,1,1,4,2,0,0,1,3
"def delete_scope(self, scope):
  assert isinstance(scope, Scope), 'Scope ""{}"" is not a scope!'.format(scope.name)
  response = self._request('DELETE', self._build_url('scope', scope_id=str(scope.id)))
  if response.status_code != requests.codes.no_content:
  raise APIError(""Could not delete scope, {}: {}"".format(str(response), response.content))","Delete a scope. This will delete a scope if the client has the right to do so. Sufficient permissions to delete a scope are a superuser, a user in the `GG:Configurators` group or a user that is the Scope manager of the scope to be deleted. :param scope: Scope object to be deleted :type scope: :class: `models.Scope` :return: None :raises APIError: in case of failure in the deletion of the scope",0,0,0,1,1,2,0,0,1,3
"def get_all(rc_file='~/.odoorpcrc'):
  conf = ConfigParser()
  conf.read([os.path.expanduser(rc_file)])
  sessions = {}
  for name in conf.sections():
  sessions[name] = {
  'type': conf.get(name, 'type'),
  'host': conf.get(name, 'host'),
  'protocol': conf.get(name, 'protocol'),
  'port': conf.getint(name, 'port'),
  'timeout': conf.getfloat(name, 'timeout'),
  'user': conf.get(name, 'user'),
  'passwd': conf.get(name, 'passwd'),
  'database': conf.get(name, 'database'),
  }
  return sessions","Return all session configurations from the `rc_file` file. >>> import odoorpc >>> from pprint import pprint as pp >>> pp(odoorpc.session.get_all()) # doctest: +SKIP {'foo': {'database': 'db_name', 'host': 'localhost', 'passwd': 'password', 'port': 8069, 'protocol': 'jsonrpc', 'timeout': 120, 'type': 'ODOO', 'user': 'admin'}, ...} .. doctest:: :hide: >>> import odoorpc >>> session = '%s_session' % DB >>> odoo.save(session) >>> data = odoorpc.session.get_all() >>> data[session]['host'] == HOST True >>> data[session]['protocol'] == PROTOCOL True >>> data[session]['port'] == int(PORT) True >>> data[session]['database'] == DB True >>> data[session]['user'] == USER True >>> data[session]['passwd'] == PWD True >>> data[session]['type'] == 'ODOO' True",0,0,0,1,1,1,0,0,1,2
"def schema_exists(dbname, name, user=None,
  db_user=None, db_password=None,
  db_host=None, db_port=None):
  return bool(
  schema_get(dbname, name, user=user,
  db_user=db_user,
  db_host=db_host,
  db_port=db_port,
  db_password=db_password))",Checks if a schema exists on the Postgres server. CLI Example: .. code-block:: bash salt '*' postgres.schema_exists dbname schemaname dbname Database name we query on name Schema name we look for user The system user the operation should be performed on behalf of db_user database username if different from config or default db_password user password if any password for a specified user db_host Database host if different from config or default db_port Database port if different from config or default,1,0,1,1,3,1,0,0,1,2
"def get(self, key: str, *,
  prompt: Optional[Message_T] = None,
  arg_filters: Optional[List[Filter_T]] = None,
  **kwargs) -> Any:
  if key in self.state:
  return self.state[key]
  self.current_key = key
  self.current_arg_filters = arg_filters
  self._current_send_kwargs = kwargs
  self.pause(prompt, **kwargs)","Get an argument with a given key. If the argument does not exist in the current session, a pause exception will be raised, and the caller of the command will know it should keep the session for further interaction with the user. :param key: argument key :param prompt: prompt to ask the user :param arg_filters: argument filters for the next user input :return: the argument value",1,0,0,1,2,1,0,0,1,2
"def _get_id_from_username(self, username):
  _mask = ""mask[id, username]""
  _filter = {'users': {'username': utils.query_filter(username)}}
  user = self.list_users(_mask, _filter)
  if len(user) == 1:
  return [user[0]['id']]
  elif len(user) > 1:
  raise exceptions.SoftLayerError(""Multiple users found with the name: %s"" % username)
  else:
  raise exceptions.SoftLayerError(""Unable to find user id for %s"" % username)",Looks up a username's id :param string username: Username to lookup :returns: The id that matches username.,2,0,1,1,4,1,0,0,1,2
"def retry_on_integrity_error(self):
  session = self.session
  assert session.info.get(_ATOMIC_FLAG_SESSION_INFO_KEY), \
  'Calls to ""retry_on_integrity_error"" must be wrapped in atomic block.'
  session.flush()
  try:
  yield
  session.flush()
  except IntegrityError:
  raise DBSerializationError","Re-raise :class:`~sqlalchemy.exc.IntegrityError` as `DBSerializationError`. This is mainly useful to handle race conditions in atomic blocks. For example, even if prior to a database INSERT we have verified that there is no existing row with the given primary key, we still may get an :class:`~sqlalchemy.exc.IntegrityError` if another transaction inserted a row with this primary key in the meantime. But if we do (within an atomic block):: with db.retry_on_integrity_error(): db.session.add(instance) then if the before-mentioned race condition occurs, `DBSerializationError` will be raised instead of :class:`~sqlalchemy.exc.IntegrityError`, so that the transaction will be retried (by the atomic block), and the second time our prior-to-INSERT check will correctly detect a primary key collision. Note: :meth:`retry_on_integrity_error` triggers a session flush.",0,0,0,0,0,0,1,0,0,1
"def find_one(self, filter=None, *args, **kwargs):
  if (filter is not None and not
  isinstance(filter, collections.Mapping)):
  filter = {""_id"": filter}
  cursor = self.find(filter, *args, **kwargs)
  for result in cursor.limit(-1):
  return result
  return None","Get a single document from the database. All arguments to :meth:`find` are also valid arguments for :meth:`find_one`, although any `limit` argument will be ignored. Returns a single document, or ``None`` if no matching document is found. The :meth:`find_one` method obeys the :attr:`read_preference` of this :class:`Collection`. :Parameters: - `filter` (optional): a dictionary specifying the query to be performed OR any other type to be used as the value for a query for ``""_id""``. - `*args` (optional): any additional positional arguments are the same as the arguments to :meth:`find`. - `**kwargs` (optional): any additional keyword arguments are the same as the arguments to :meth:`find`. >>> collection.find_one(max_time_ms=100)",1,0,1,1,3,1,0,1,1,3
"def compute_auth_key(userid, password):
  import sys
  if sys.version_info >= (3, 0):
  return hashlib.sha1(b""|"".join((userid.encode(""ascii""),
  password.encode(""ascii"")))).hexdigest()
  return hashlib.sha1(""|"".join((userid, password))).hexdigest()",Compute the authentication key for freedns.afraid.org. This is the SHA1 hash of the string b'userid|password'. :param userid: ascii username :param password: ascii password :return: ascii authentication key (SHA1 at this point),0,0,0,0,0,1,0,0,1,2
"def __on_disconnect(self, client, userdata, result_code):
  if result_code:
  _logger.error(
  ""Unexpected disconnection from the MQTT server: %s (%d)"",
  paho.connack_string(result_code),
  result_code,
  )
  self.__stop_timer()
  self.__start_timer(2)
  if self.on_disconnect is not None:
  try:
  self.on_disconnect(self, result_code)
  except Exception as ex:
  _logger.exception(""Error notifying MQTT listener: %s"", ex)","Client has been disconnected from the server :param client: Client that received the message :param userdata: User data (unused) :param result_code: Disconnection reason (0: expected, 1: error)",0,0,0,1,1,1,0,0,1,2
"def prj_add_user(self, *args, **kwargs):
  if not self.cur_prj:
  return
  dialog = UserAdderDialog(project=self.cur_prj)
  dialog.exec_()
  users = dialog.users
  for user in users:
  userdata = djitemdata.UserItemData(user)
  treemodel.TreeItem(userdata, self.prj_user_model.root)
  self.cur_prj.save()",Add more users to the project. :returns: None :rtype: None :raises: None,1,1,0,1,3,1,1,0,1,3
"def pack_req(cls, is_unlock, password_md5, conn_id):
  from futuquant.common.pb.Trd_UnlockTrade_pb2 import Request
  req = Request()
  req.c2s.unlock = is_unlock
  req.c2s.pwdMD5 = password_md5
  return pack_pb_req(req, ProtoId.Trd_UnlockTrade, conn_id)",Convert from user request for trading days to PLS request,0,0,0,1,1,1,0,0,1,2
"def prompt_choices(name, choices, default=None, no_choice=('none',)):
  _choices = []
  options = []
  for choice in choices:
  options.append(choice)
  _choices.append(choice)
  while True:
  rv = prompt(name + '? - (%s)' % ', '.join(options), default)
  rv = rv.lower()
  if rv in no_choice:
  return None
  if rv in _choices:
  return rv","Grabs user input from command line from set of provided choices. :param name: prompt text :param choices: list or tuple of available choices. :param default: default value if no input provided. :param no_choice: acceptable list of strings for ""null choice""",1,0,0,1,2,1,0,0,1,2
"def get_authors_string(self, own_accts=None, replace_own=None):
  if replace_own is None:
  replace_own = settings.get('thread_authors_replace_me')
  if replace_own:
  if own_accts is None:
  own_accts = settings.get_accounts()
  authorslist = []
  for aname, aaddress in self.get_authors():
  for account in own_accts:
  if account.matches_address(aaddress):
  aname = settings.get('thread_authors_me')
  break
  if not aname:
  aname = aaddress
  if aname not in authorslist:
  authorslist.append(aname)
  return ', '.join(authorslist)
  else:
  return self._notmuch_authors_string","returns a string of comma-separated authors Depending on settings, it will substitute ""me"" for author name if address is user's own. :param own_accts: list of own accounts to replace :type own_accts: list of :class:`Account` :param replace_own: whether or not to actually do replacement :type replace_own: bool :rtype: str",1,0,0,1,2,1,0,0,1,2
"async def auth_login(
  self, username: str, password: str, timeout: DefaultNumType = _default
  ) -> SMTPResponse:
  encoded_username = base64.b64encode(username.encode(""ascii""))
  encoded_password = base64.b64encode(password.encode(""ascii""))
  async with self._command_lock:
  initial_response = await self.execute_command(
  b""AUTH"", b""LOGIN"", encoded_username, timeout=timeout
  )
  if initial_response.code != SMTPStatus.auth_continue:
  raise SMTPAuthenticationError(
  initial_response.code, initial_response.message
  )
  response = await self.execute_command(encoded_password, timeout=timeout)
  if response.code != SMTPStatus.auth_successful:
  raise SMTPAuthenticationError(response.code, response.message)
  return response","LOGIN auth sends the Base64 encoded username and password in sequence. Example:: 250 AUTH LOGIN PLAIN CRAM-MD5 auth login avlsdkfj 334 UGFzc3dvcmQ6 avlsdkfj Note that there is an alternate version sends the username as a separate command:: 250 AUTH LOGIN PLAIN CRAM-MD5 auth login 334 VXNlcm5hbWU6 avlsdkfj 334 UGFzc3dvcmQ6 avlsdkfj However, since most servers seem to support both, we send the username with the initial request.",1,0,0,1,2,1,0,0,1,2
"def get_best_span(span_start_logits: torch.Tensor, span_end_logits: torch.Tensor) -> torch.Tensor:
  if span_start_logits.dim() != 2 or span_end_logits.dim() != 2:
  raise ValueError(""Input shapes must be (batch_size, passage_length)"")
  batch_size, passage_length = span_start_logits.size()
  device = span_start_logits.device
  span_log_probs = span_start_logits.unsqueeze(2) + span_end_logits.unsqueeze(1)
  span_log_mask = torch.triu(torch.ones((passage_length, passage_length),
  device=device)).log()
  valid_span_log_probs = span_log_probs + span_log_mask
  best_spans = valid_span_log_probs.view(batch_size, -1).argmax(-1)
  span_start_indices = best_spans // passage_length
  span_end_indices = best_spans % passage_length
  return torch.stack([span_start_indices, span_end_indices], dim=-1)","This acts the same as the static method ``BidirectionalAttentionFlow.get_best_span()`` in ``allennlp/models/reading_comprehension/bidaf.py``. We keep it here so that users can directly import this function without the class. We call the inputs ""logits"" - they could either be unnormalized logits or normalized log probabilities. A log_softmax operation is a constant shifting of the entire logit vector, so taking an argmax over either one gives the same result.",1,0,0,1,2,1,0,0,1,2
"def authorization_url(self, client_id, redirect_uri, approval_prompt='auto',
  scope=None, state=None):
  return self.protocol.authorization_url(client_id=client_id,
  redirect_uri=redirect_uri,
  approval_prompt=approval_prompt,
  scope=scope, state=state)","Get the URL needed to authorize your application to access a Strava user's information. :param client_id: The numeric developer client id. :type client_id: int :param redirect_uri: The URL that Strava will redirect to after successful (or failed) authorization. :type redirect_uri: str :param approval_prompt: Whether to prompt for approval even if approval already granted to app. Choices are 'auto' or 'force'. (Default is 'auto') :type approval_prompt: str :param scope: The access scope required. Omit to imply ""public"". Valid values are 'read', 'read_all', 'profile:read_all', 'profile:write', 'profile:read_all', 'activity:read_all', 'activity:write' :type scope: str :param state: An arbitrary variable that will be returned to your application in the redirect URI. :type state: str :return: The URL to use for authorization link. :rtype: str",1,0,0,1,2,2,0,0,1,3
"def user_has(ability, get_user=import_user):
  def wrapper(func):
  @wraps(func)
  def inner(*args, **kwargs):
  from .models import Ability
  desired_ability = Ability.query.filter_by(
  name=ability).first()
  user_abilities = []
  current_user = get_user()
  for role in current_user._roles:
  user_abilities += role.abilities
  if desired_ability in user_abilities:
  return func(*args, **kwargs)
  else:
  raise Forbidden(""You do not have access"")
  return inner
  return wrapper",Takes an ability (a string name of either a role or an ability) and returns the function if the user has that ability,0,0,1,1,2,1,0,1,1,3
"def get_best_match(self, options, service_name, api_version=None):
  if not options:
  msg = ""No JSON files provided. Please check your "" + \
  ""configuration/install.""
  raise NoResourceJSONFound(msg)
  if api_version is None:
  best_version = max(options.keys())
  return options[best_version][0], best_version
  if api_version in options:
  return options[api_version][0], api_version
  for key in sorted(options.keys(), reverse=True):
  if key <= api_version:
  return options[key][0], key
  raise NoResourceJSONFound(
  ""No compatible JSON could be loaded for {0} ({1})."".format(
  service_name,
  api_version
  )
  )","Given a collection of possible service options, selects the best match. If no API version is provided, the path to the most recent API version will be returned. If an API version is provided & there is an exact match, the path to that version will be returned. If there is no exact match, an attempt will be made to find a compatible (earlier) version. In all cases, user-created files (if present) will be given preference over the default included versions. :param options: A dictionary of options. See ``.get_available_options(...)``. :type options: dict :param service_name: The name of the desired service :type service_name: string :param api_version: (Optional) The desired API version to load :type service_name: string :returns: The full path to the best matching JSON file",1,0,0,1,2,1,0,0,1,2
"def run_instances(DryRun=None, ImageId=None, MinCount=None, MaxCount=None, KeyName=None, SecurityGroups=None, SecurityGroupIds=None, UserData=None, InstanceType=None, Placement=None, KernelId=None, RamdiskId=None, BlockDeviceMappings=None, Monitoring=None, SubnetId=None, DisableApiTermination=None, InstanceInitiatedShutdownBehavior=None, PrivateIpAddress=None, Ipv6Addresses=None, Ipv6AddressCount=None, ClientToken=None, AdditionalInfo=None, NetworkInterfaces=None, IamInstanceProfile=None, EbsOptimized=None, TagSpecifications=None):
  """"""
  Launches the specified number of instances using an AMI for which you have permissions.
  You can specify a number of options, or leave the default options. The following rules apply:
  To ensure faster instance launches, break up large requests into smaller batches. For example, create 5 separate launch requests for 100 instances each instead of 1 launch request for 500 instances.
  An instance is ready for you to use when it's in the running state. You can check the state of your instance using DescribeInstances . You can tag instances and EBS volumes during launch, after launch, or both. For more information, see CreateTags and Tagging Your Amazon EC2 Resources .
  Linux instances have access to the public key of the key pair at boot. You can use this key to provide secure access to the instance. Amazon EC2 public images use this feature to provide secure access without passwords. For more information, see Key Pairs in the Amazon Elastic Compute Cloud User Guide .
  For troubleshooting, see What To Do If An Instance Immediately Terminates , and Troubleshooting Connecting to Your Instance in the Amazon Elastic Compute Cloud User Guide .
  See also: AWS API Documentation
  :example: response = client.run_instances(
  DryRun=True|False,
  ImageId='string',
  MinCount=123,
  MaxCount=123,
  KeyName='string',
  SecurityGroups=[
  'string',
  ],
  SecurityGroupIds=[
  'string',
  ],
  UserData='string',
  InstanceType='t1.micro'|'t2.nano'|'t2.micro'|'t2.small'|'t2.medium'|'t2.large'|'t2.xlarge'|'t2.2xlarge'|'m1.small'|'m1.medium'|'m1.large'|'m1.xlarge'|'m3.medium'|'m3.large'|'m3.xlarge'|'m3.2xlarge'|'m4.large'|'m4.xlarge'|'m4.2xlarge'|'m4.4xlarge'|'m4.10xlarge'|'m4.16xlarge'|'m2.xlarge'|'m2.2xlarge'|'m2.4xlarge'|'cr1.8xlarge'|'r3.large'|'r3.xlarge'|'r3.2xlarge'|'r3.4xlarge'|'r3.8xlarge'|'r4.large'|'r4.xlarge'|'r4.2xlarge'|'r4.4xlarge'|'r4.8xlarge'|'r4.16xlarge'|'x1.16xlarge'|'x1.32xlarge'|'i2.xlarge'|'i2.2xlarge'|'i2.4xlarge'|'i2.8xlarge'|'i3.large'|'i3.xlarge'|'i3.2xlarge'|'i3.4xlarge'|'i3.8xlarge'|'i3.16xlarge'|'hi1.4xlarge'|'hs1.8xlarge'|'c1.medium'|'c1.xlarge'|'c3.large'|'c3.xlarge'|'c3.2xlarge'|'c3.4xlarge'|'c3.8xlarge'|'c4.large'|'c4.xlarge'|'c4.2xlarge'|'c4.4xlarge'|'c4.8xlarge'|'cc1.4xlarge'|'cc2.8xlarge'|'g2.2xlarge'|'g2.8xlarge'|'cg1.4xlarge'|'p2.xlarge'|'p2.8xlarge'|'p2.16xlarge'|'d2.xlarge'|'d2.2xlarge'|'d2.4xlarge'|'d2.8xlarge'|'f1.2xlarge'|'f1.16xlarge',
  Placement={
  'AvailabilityZone': 'string',
  'GroupName': 'string',
  'Tenancy': 'default'|'dedicated'|'host',
  'HostId': 'string',
  'Affinity': 'string'
  },
  KernelId='string',
  RamdiskId='string',
  BlockDeviceMappings=[
  {
  'VirtualName': 'string',
  'DeviceName': 'string',
  'Ebs': {
  'SnapshotId': 'string',
  'VolumeSize': 123,
  'DeleteOnTermination': True|False,
  'VolumeType': 'standard'|'io1'|'gp2'|'sc1'|'st1',
  'Iops': 123,
  'Encrypted': True|False
  },
  'NoDevice': 'string'
  },
  ],
  Monitoring={
  'Enabled': True|False
  },
  SubnetId='string',
  DisableApiTermination=True|False,
  InstanceInitiatedShutdownBehavior='stop'|'terminate',
  PrivateIpAddress='string',
  Ipv6Addresses=[
  {
  'Ipv6Address': 'string'
  },
  ],
  Ipv6AddressCount=123,
  ClientToken='string',
  AdditionalInfo='string',
  NetworkInterfaces=[
  {
  'NetworkInterfaceId': 'string',
  'DeviceIndex': 123,
  'SubnetId': 'string',
  'Description': 'string',
  'PrivateIpAddress': 'string',
  'Groups': [
  'string',
  ],
  'DeleteOnTermination': True|False,
  'PrivateIpAddresses': [
  {
  'PrivateIpAddress': 'string',
  'Primary': True|False
  },
  ],
  'SecondaryPrivateIpAddressCount': 123,
  'AssociatePublicIpAddress': True|False,
  'Ipv6Addresses': [
  {
  'Ipv6Address': 'string'
  },
  ],
  'Ipv6AddressCount': 123
  },
  ],
  IamInstanceProfile={
  'Arn': 'string',
  'Name': 'string'
  },
  EbsOptimized=True|False,
  TagSpecifications=[
  {
  'ResourceType': 'customer-gateway'|'dhcp-options'|'image'|'instance'|'internet-gateway'|'network-acl'|'network-interface'|'reserved-instances'|'route-table'|'snapshot'|'spot-instances-request'|'subnet'|'security-group'|'volume'|'vpc'|'vpn-connection'|'vpn-gateway',
  'Tags': [
  {
  'Key': 'string',
  'Value': 'string'
  },
  ]
  },
  ]
  )
  :type DryRun: boolean
  :param DryRun: Checks whether you have the required permissions for the action, without actually making the request, and provides an error response. If you have the required permissions, the error response is DryRunOperation . Otherwise, it is UnauthorizedOperation .
  :type ImageId: string
  :param ImageId: [REQUIRED]
  The ID of the AMI, which you can get by calling DescribeImages .
  :type MinCount: integer
  :param MinCount: [REQUIRED]
  The minimum number of instances to launch. If you specify a minimum that is more instances than Amazon EC2 can launch in the target Availability Zone, Amazon EC2 launches no instances.
  Constraints: Between 1 and the maximum number you're allowed for the specified instance type. For more information about the default limits, and how to request an increase, see How many instances can I run in Amazon EC2 in the Amazon EC2 General FAQ.
  :type MaxCount: integer
  :param MaxCount: [REQUIRED]
  The maximum number of instances to launch. If you specify more instances than Amazon EC2 can launch in the target Availability Zone, Amazon EC2 launches the largest possible number of instances above MinCount .
  Constraints: Between 1 and the maximum number you're allowed for the specified instance type. For more information about the default limits, and how to request an increase, see How many instances can I run in Amazon EC2 in the Amazon EC2 FAQ.
  :type KeyName: string
  :param KeyName: The name of the key pair. You can create a key pair using CreateKeyPair or ImportKeyPair .
  Warning
  If you do not specify a key pair, you can't connect to the instance unless you choose an AMI that is configured to allow users another way to log in.
  :type SecurityGroups: list
  :param SecurityGroups: [EC2-Classic, default VPC] One or more security group names. For a nondefault VPC, you must use security group IDs instead.
  Default: Amazon EC2 uses the default security group.
  (string) --
  :type SecurityGroupIds: list
  :param SecurityGroupIds: One or more security group IDs. You can create a security group using CreateSecurityGroup .
  Default: Amazon EC2 uses the default security group.
  (string) --
  :type UserData: string
  :param UserData: The user data to make available to the instance. For more information, see Running Commands on Your Linux Instance at Launch (Linux) and Adding User Data (Windows). If you are using an AWS SDK or command line tool, Base64-encoding is performed for you, and you can load the text from a file. Otherwise, you must provide Base64-encoded text.
  This value will be base64 encoded automatically. Do not base64 encode this value prior to performing the operation.
  :type InstanceType: string
  :param InstanceType: The instance type. For more information, see Instance Types in the Amazon Elastic Compute Cloud User Guide .
  Default: m1.small
  :type Placement: dict
  :param Placement: The placement for the instance.
  AvailabilityZone (string) --The Availability Zone of the instance.
  GroupName (string) --The name of the placement group the instance is in (for cluster compute instances).
  Tenancy (string) --The tenancy of the instance (if the instance is running in a VPC). An instance with a tenancy of dedicated runs on single-tenant hardware. The host tenancy is not supported for the ImportInstance command.
  HostId (string) --The ID of the Dedicated Host on which the instance resides. This parameter is not supported for the ImportInstance command.
  Affinity (string) --The affinity setting for the instance on the Dedicated Host. This parameter is not supported for the ImportInstance command.
  :type KernelId: string
  :param KernelId: The ID of the kernel.
  Warning
  We recommend that you use PV-GRUB instead of kernels and RAM disks. For more information, see PV-GRUB in the Amazon Elastic Compute Cloud User Guide .
  :type RamdiskId: string
  :param RamdiskId: The ID of the RAM disk.
  Warning
  We recommend that you use PV-GRUB instead of kernels and RAM disks. For more information, see PV-GRUB in the Amazon Elastic Compute Cloud User Guide .
  :type BlockDeviceMappings: list
  :param BlockDeviceMappings: The block device mapping.
  Warning
  Supplying both a snapshot ID and an encryption value as arguments for block-device mapping results in an error. This is because only blank volumes can be encrypted on start, and these are not created from a snapshot. If a snapshot is the basis for the volume, it contains data by definition and its encryption status cannot be changed using this action.
  (dict) --Describes a block device mapping.
  VirtualName (string) --The virtual device name (ephemeral N). Instance store volumes are numbered starting from 0. An instance type with 2 available instance store volumes can specify mappings for ephemeral0 and ephemeral1 .The number of available instance store volumes depends on the instance type. After you connect to the instance, you must mount the volume.
  Constraints: For M3 instances, you must specify instance store volumes in the block device mapping for the instance. When you launch an M3 instance, we ignore any instance store volumes specified in the block device mapping for the AMI.
  DeviceName (string) --The device name exposed to the instance (for example, /dev/sdh or xvdh ).
  Ebs (dict) --Parameters used to automatically set up EBS volumes when the instance is launched.
  SnapshotId (string) --The ID of the snapshot.
  VolumeSize (integer) --The size of the volume, in GiB.
  Constraints: 1-16384 for General Purpose SSD (gp2 ), 4-16384 for Provisioned IOPS SSD (io1 ), 500-16384 for Throughput Optimized HDD (st1 ), 500-16384 for Cold HDD (sc1 ), and 1-1024 for Magnetic (standard ) volumes. If you specify a snapshot, the volume size must be equal to or larger than the snapshot size.
  Default: If you're creating the volume from a snapshot and don't specify a volume size, the default is the snapshot size.
  DeleteOnTermination (boolean) --Indicates whether the EBS volume is deleted on instance termination.
  VolumeType (string) --The volume type: gp2 , io1 , st1 , sc1 , or standard .
  Default: standard
  Iops (integer) --The number of I/O operations per second (IOPS) that the volume supports. For io1 , this represents the number of IOPS that are provisioned for the volume. For gp2 , this represents the baseline performance of the volume and the rate at which the volume accumulates I/O credits for bursting. For more information about General Purpose SSD baseline performance, I/O credits, and bursting, see Amazon EBS Volume Types in the Amazon Elastic Compute Cloud User Guide .
  Constraint: Range is 100-20000 IOPS for io1 volumes and 100-10000 IOPS for gp2 volumes.
  Condition: This parameter is required for requests to create io1 volumes; it is not used in requests to create gp2 , st1 , sc1 , or standard volumes.
  Encrypted (boolean) --Indicates whether the EBS volume is encrypted. Encrypted Amazon EBS volumes may only be attached to instances that support Amazon EBS encryption.
  NoDevice (string) --Suppresses the specified device included in the block device mapping of the AMI.
  :type Monitoring: dict
  :param Monitoring: The monitoring for the instance.
  Enabled (boolean) -- [REQUIRED]Indicates whether detailed monitoring is enabled. Otherwise, basic monitoring is enabled.
  :type SubnetId: string
  :param SubnetId: [EC2-VPC] The ID of the subnet to launch the instance into.
  :type DisableApiTermination: boolean
  :param DisableApiTermination: If you set this parameter to true , you can't terminate the instance using the Amazon EC2 console, CLI, or API; otherwise, you can. To change this attribute to false after launch, use ModifyInstanceAttribute . Alternatively, if you set InstanceInitiatedShutdownBehavior to terminate , you can terminate the instance by running the shutdown command from the instance.
  Default: false
  :type InstanceInitiatedShutdownBehavior: string
  :param InstanceInitiatedShutdownBehavior: Indicates whether an instance stops or terminates when you initiate shutdown from the instance (using the operating system command for system shutdown).
  Default: stop
  :type PrivateIpAddress: string
  :param PrivateIpAddress: [EC2-VPC] The primary IPv4 address. You must specify a value from the IPv4 address range of the subnet.
  Only one private IP address can be designated as primary. You can't specify this option if you've specified the option to designate a private IP address as the primary IP address in a network interface specification. You cannot specify this option if you're launching more than one instance in the request.
  :type Ipv6Addresses: list
  :param Ipv6Addresses: [EC2-VPC] Specify one or more IPv6 addresses from the range of the subnet to associate with the primary network interface. You cannot specify this option and the option to assign a number of IPv6 addresses in the same request. You cannot specify this option if you've specified a minimum number of instances to launch.
  (dict) --Describes an IPv6 address.
  Ipv6Address (string) --The IPv6 address.
  :type Ipv6AddressCount: integer
  :param Ipv6AddressCount: [EC2-VPC] A number of IPv6 addresses to associate with the primary network interface. Amazon EC2 chooses the IPv6 addresses from the range of your subnet. You cannot specify this option and the option to assign specific IPv6 addresses in the same request. You can specify this option if you've specified a minimum number of instances to launch.
  :type ClientToken: string
  :param ClientToken: Unique, case-sensitive identifier you provide to ensure the idempotency of the request. For more information, see Ensuring Idempotency .
  Constraints: Maximum 64 ASCII characters
  :type AdditionalInfo: string
  :param AdditionalInfo: Reserved.
  :type NetworkInterfaces: list
  :param NetworkInterfaces: One or more network interfaces.
  (dict) --Describes a network interface.
  NetworkInterfaceId (string) --The ID of the network interface.
  DeviceIndex (integer) --The index of the device on the instance for the network interface attachment. If you are specifying a network interface in a RunInstances request, you must provide the device index.
  SubnetId (string) --The ID of the subnet associated with the network string. Applies only if creating a network interface when launching an instance.
  Description (string) --The description of the network interface. Applies only if creating a network interface when launching an instance.
  PrivateIpAddress (string) --The private IPv4 address of the network interface. Applies only if creating a network interface when launching an instance. You cannot specify this option if you're launching more than one instance in a RunInstances request.
  Groups (list) --The IDs of the security groups for the network interface. Applies only if creating a network interface when launching an instance.
  (string) --
  DeleteOnTermination (boolean) --If set to true , the interface is deleted when the instance is terminated. You can specify true only if creating a new network interface when launching an instance.
  PrivateIpAddresses (list) --One or more private IPv4 addresses to assign to the network interface. Only one private IPv4 address can be designated as primary. You cannot specify this option if you're launching more than one instance in a RunInstances request.
  (dict) --Describes a secondary private IPv4 address for a network interface.
  PrivateIpAddress (string) -- [REQUIRED]The private IPv4 addresses.
  Primary (boolean) --Indicates whether the private IPv4 address is the primary private IPv4 address. Only one IPv4 address can be designated as primary.
  SecondaryPrivateIpAddressCount (integer) --The number of secondary private IPv4 addresses. You can't specify this option and specify more than one private IP address using the private IP addresses option. You cannot specify this option if you're launching more than one instance in a RunInstances request.
  AssociatePublicIpAddress (boolean) --Indicates whether to assign a public IPv4 address to an instance you launch in a VPC. The public IP address can only be assigned to a network interface for eth0, and can only be assigned to a new network interface, not an existing one. You cannot specify more than one network interface in the request. If launching into a default subnet, the default value is true .
  Ipv6Addresses (list) --One or more IPv6 addresses to assign to the network interface. You cannot specify this option and the option to assign a number of IPv6 addresses in the same request. You cannot specify this option if you've specified a minimum number of instances to launch.
  (dict) --Describes an IPv6 address.
  Ipv6Address (string) --The IPv6 address.
  Ipv6AddressCount (integer) --A number of IPv6 addresses to assign to the network interface. Amazon EC2 chooses the IPv6 addresses from the range of the subnet. You cannot specify this option and the option to assign specific IPv6 addresses in the same request. You can specify this option if you've specified a minimum number of instances to launch.
  :type IamInstanceProfile: dict
  :param IamInstanceProfile: The IAM instance profile.
  Arn (string) --The Amazon Resource Name (ARN) of the instance profile.
  Name (string) --The name of the instance profile.
  :type EbsOptimized: boolean
  :param EbsOptimized: Indicates whether the instance is optimized for EBS I/O. This optimization provides dedicated throughput to Amazon EBS and an optimized configuration stack to provide optimal EBS I/O performance. This optimization isn't available with all instance types. Additional usage charges apply when using an EBS-optimized instance.
  Default: false
  :type TagSpecifications: list
  :param TagSpecifications: The tags to apply to the resources during launch. You can tag instances and volumes. The specified tags are applied to all instances or volumes that are created during launch.
  (dict) --The tags to apply to a resource when the resource is being created.
  ResourceType (string) --The type of resource to tag. Currently, the resource types that support tagging on creation are instance and volume .
  Tags (list) --The tags to apply to the resource.
  (dict) --Describes a tag.
  Key (string) --The key of the tag.
  Constraints: Tag keys are case-sensitive and accept a maximum of 127 Unicode characters. May not begin with aws:
  Value (string) --The value of the tag.
  Constraints: Tag values are case-sensitive and accept a maximum of 255 Unicode characters.
  :rtype: dict
  :return: {
  'ReservationId': 'string',
  'OwnerId': 'string',
  'RequesterId': 'string',
  'Groups': [
  {
  'GroupName': 'string',
  'GroupId': 'string'
  },
  ],
  'Instances': [
  {
  'InstanceId': 'string',
  'ImageId': 'string',
  'State': {
  'Code': 123,
  'Name': 'pending'|'running'|'shutting-down'|'terminated'|'stopping'|'stopped'
  },
  'PrivateDnsName': 'string',
  'PublicDnsName': 'string',
  'StateTransitionReason': 'string',
  'KeyName': 'string',
  'AmiLaunchIndex': 123,
  'ProductCodes': [
  {
  'ProductCodeId': 'string',
  'ProductCodeType': 'devpay'|'marketplace'
  },
  ],
  'InstanceType': 't1.micro'|'t2.nano'|'t2.micro'|'t2.small'|'t2.medium'|'t2.large'|'t2.xlarge'|'t2.2xlarge'|'m1.small'|'m1.medium'|'m1.large'|'m1.xlarge'|'m3.medium'|'m3.large'|'m3.xlarge'|'m3.2xlarge'|'m4.large'|'m4.xlarge'|'m4.2xlarge'|'m4.4xlarge'|'m4.10xlarge'|'m4.16xlarge'|'m2.xlarge'|'m2.2xlarge'|'m2.4xlarge'|'cr1.8xlarge'|'r3.large'|'r3.xlarge'|'r3.2xlarge'|'r3.4xlarge'|'r3.8xlarge'|'r4.large'|'r4.xlarge'|'r4.2xlarge'|'r4.4xlarge'|'r4.8xlarge'|'r4.16xlarge'|'x1.16xlarge'|'x1.32xlarge'|'i2.xlarge'|'i2.2xlarge'|'i2.4xlarge'|'i2.8xlarge'|'i3.large'|'i3.xlarge'|'i3.2xlarge'|'i3.4xlarge'|'i3.8xlarge'|'i3.16xlarge'|'hi1.4xlarge'|'hs1.8xlarge'|'c1.medium'|'c1.xlarge'|'c3.large'|'c3.xlarge'|'c3.2xlarge'|'c3.4xlarge'|'c3.8xlarge'|'c4.large'|'c4.xlarge'|'c4.2xlarge'|'c4.4xlarge'|'c4.8xlarge'|'cc1.4xlarge'|'cc2.8xlarge'|'g2.2xlarge'|'g2.8xlarge'|'cg1.4xlarge'|'p2.xlarge'|'p2.8xlarge'|'p2.16xlarge'|'d2.xlarge'|'d2.2xlarge'|'d2.4xlarge'|'d2.8xlarge'|'f1.2xlarge'|'f1.16xlarge',
  'LaunchTime': datetime(2015, 1, 1),
  'Placement': {
  'AvailabilityZone': 'string',
  'GroupName': 'string',
  'Tenancy': 'default'|'dedicated'|'host',
  'HostId': 'string',
  'Affinity': 'string'
  },
  'KernelId': 'string',
  'RamdiskId': 'string',
  'Platform': 'Windows',
  'Monitoring': {
  'State': 'disabled'|'disabling'|'enabled'|'pending'
  },
  'SubnetId': 'string',
  'VpcId': 'string',
  'PrivateIpAddress': 'string',
  'PublicIpAddress': 'string',
  'StateReason': {
  'Code': 'string',
  'Message': 'string'
  },
  'Architecture': 'i386'|'x86_64',
  'RootDeviceType': 'ebs'|'instance-store',
  'RootDeviceName': 'string',
  'BlockDeviceMappings': [
  {
  'DeviceName': 'string',
  'Ebs': {
  'VolumeId': 'string',
  'Status': 'attaching'|'attached'|'detaching'|'detached',
  'AttachTime': datetime(2015, 1, 1),
  'DeleteOnTermination': True|False
  }
  },
  ],
  'VirtualizationType': 'hvm'|'paravirtual',
  'InstanceLifecycle': 'spot'|'scheduled',
  'SpotInstanceRequestId': 'string',
  'ClientToken': 'string',
  'Tags': [
  {
  'Key': 'string',
  'Value': 'string'
  },
  ],
  'SecurityGroups': [
  {
  'GroupName': 'string',
  'GroupId': 'string'
  },
  ],
  'SourceDestCheck': True|False,
  'Hypervisor': 'ovm'|'xen',
  'NetworkInterfaces': [
  {
  'NetworkInterfaceId': 'string',
  'SubnetId': 'string',
  'VpcId': 'string',
  'Description': 'string',
  'OwnerId': 'string',
  'Status': 'available'|'attaching'|'in-use'|'detaching',
  'MacAddress': 'string',
  'PrivateIpAddress': 'string',
  'PrivateDnsName': 'string',
  'SourceDestCheck': True|False,
  'Groups': [
  {
  'GroupName': 'string',
  'GroupId': 'string'
  },
  ],
  'Attachment': {
  'AttachmentId': 'string',
  'DeviceIndex': 123,
  'Status': 'attaching'|'attached'|'detaching'|'detached',
  'AttachTime': datetime(2015, 1, 1),
  'DeleteOnTermination': True|False
  },
  'Association': {
  'PublicIp': 'string',
  'PublicDnsName': 'string',
  'IpOwnerId': 'string'
  },
  'PrivateIpAddresses': [
  {
  'PrivateIpAddress': 'string',
  'PrivateDnsName': 'string',
  'Primary': True|False,
  'Association': {
  'PublicIp': 'string',
  'PublicDnsName': 'string',
  'IpOwnerId': 'string'
  }
  },
  ],
  'Ipv6Addresses': [
  {
  'Ipv6Address': 'string'
  },
  ]
  },
  ],
  'IamInstanceProfile': {
  'Arn': 'string',
  'Id': 'string'
  },
  'EbsOptimized': True|False,
  'SriovNetSupport': 'string',
  'EnaSupport': True|False
  },
  ]
  }
  :returns:
  DryRun (boolean) -- Checks whether you have the required permissions for the action, without actually making the request, and provides an error response. If you have the required permissions, the error response is DryRunOperation . Otherwise, it is UnauthorizedOperation .
  ImageId (string) -- [REQUIRED]
  The ID of the AMI, which you can get by calling DescribeImages .
  MinCount (integer) -- [REQUIRED]
  The minimum number of instances to launch. If you specify a minimum that is more instances than Amazon EC2 can launch in the target Availability Zone, Amazon EC2 launches no instances.
  Constraints: Between 1 and the maximum number you're allowed for the specified instance type. For more information about the default limits, and how to request an increase, see How many instances can I run in Amazon EC2 in the Amazon EC2 General FAQ.
  MaxCount (integer) -- [REQUIRED]
  The maximum number of instances to launch. If you specify more instances than Amazon EC2 can launch in the target Availability Zone, Amazon EC2 launches the largest possible number of instances above MinCount .
  Constraints: Between 1 and the maximum number you're allowed for the specified instance type. For more information about the default limits, and how to request an increase, see How many instances can I run in Amazon EC2 in the Amazon EC2 FAQ.
  KeyName (string) -- The name of the key pair. You can create a key pair using CreateKeyPair or ImportKeyPair .
  Warning
  If you do not specify a key pair, you can't connect to the instance unless you choose an AMI that is configured to allow users another way to log in.
  SecurityGroups (list) -- [EC2-Classic, default VPC] One or more security group names. For a nondefault VPC, you must use security group IDs instead.
  Default: Amazon EC2 uses the default security group.
  (string) --
  SecurityGroupIds (list) -- One or more security group IDs. You can create a security group using CreateSecurityGroup .
  Default: Amazon EC2 uses the default security group.
  (string) --
  UserData (string) -- The user data to make available to the instance. For more information, see Running Commands on Your Linux Instance at Launch (Linux) and Adding User Data (Windows). If you are using an AWS SDK or command line tool, Base64-encoding is performed for you, and you can load the text from a file. Otherwise, you must provide Base64-encoded text.
  This value will be base64 encoded automatically. Do not base64 encode this value prior to performing the operation.
  InstanceType (string) -- The instance type. For more information, see Instance Types in the Amazon Elastic Compute Cloud User Guide .
  Default: m1.small
  Placement (dict) -- The placement for the instance.
  AvailabilityZone (string) --The Availability Zone of the instance.
  GroupName (string) --The name of the placement group the instance is in (for cluster compute instances).
  Tenancy (string) --The tenancy of the instance (if the instance is running in a VPC). An instance with a tenancy of dedicated runs on single-tenant hardware. The host tenancy is not supported for the ImportInstance command.
  HostId (string) --The ID of the Dedicated Host on which the instance resides. This parameter is not supported for the ImportInstance command.
  Affinity (string) --The affinity setting for the instance on the Dedicated Host. This parameter is not supported for the ImportInstance command.
  KernelId (string) -- The ID of the kernel.
  Warning
  We recommend that you use PV-GRUB instead of kernels and RAM disks. For more information, see PV-GRUB in the Amazon Elastic Compute Cloud User Guide .
  RamdiskId (string) -- The ID of the RAM disk.
  Warning
  We recommend that you use PV-GRUB instead of kernels and RAM disks. For more information, see PV-GRUB in the Amazon Elastic Compute Cloud User Guide .
  BlockDeviceMappings (list) -- The block device mapping.
  Warning
  Supplying both a snapshot ID and an encryption value as arguments for block-device mapping results in an error. This is because only blank volumes can be encrypted on start, and these are not created from a snapshot. If a snapshot is the basis for the vol","Launches the specified number of instances using an AMI for which you have permissions. You can specify a number of options, or leave the default options. The following rules apply: To ensure faster instance launches, break up large requests into smaller batches. For example, create 5 separate launch requests for 100 instances each instead of 1 launch request for 500 instances. An instance is ready for you to use when it's in the running state. You can check the state of your instance using DescribeInstances . You can tag instances and EBS volumes during launch, after launch, or both. For more information, see CreateTags and Tagging Your Amazon EC2 Resources . Linux instances have access to the public key of the key pair at boot. You can use this key to provide secure access to the instance. Amazon EC2 public images use this feature to provide secure access without passwords. For more information, see Key Pairs in the Amazon Elastic Compute Cloud User Guide . For troubleshooting, see What To Do If An Instance Immediately Terminates , and Troubleshooting Connecting to Your Instance in the Amazon Elastic Compute Cloud User Guide . See also: AWS API Documentation :example: response = client.run_instances( DryRun=True|False, ImageId='string', MinCount=123, MaxCount=123, KeyName='string', SecurityGroups=[ 'string', ], SecurityGroupIds=[ 'string', ], UserData='string', InstanceType='t1.micro'|'t2.nano'|'t2.micro'|'t2.small'|'t2.medium'|'t2.large'|'t2.xlarge'|'t2.2xlarge'|'m1.small'|'m1.medium'|'m1.large'|'m1.xlarge'|'m3.medium'|'m3.large'|'m3.xlarge'|'m3.2xlarge'|'m4.large'|'m4.xlarge'|'m4.2xlarge'|'m4.4xlarge'|'m4.10xlarge'|'m4.16xlarge'|'m2.xlarge'|'m2.2xlarge'|'m2.4xlarge'|'cr1.8xlarge'|'r3.large'|'r3.xlarge'|'r3.2xlarge'|'r3.4xlarge'|'r3.8xlarge'|'r4.large'|'r4.xlarge'|'r4.2xlarge'|'r4.4xlarge'|'r4.8xlarge'|'r4.16xlarge'|'x1.16xlarge'|'x1.32xlarge'|'i2.xlarge'|'i2.2xlarge'|'i2.4xlarge'|'i2.8xlarge'|'i3.large'|'i3.xlarge'|'i3.2xlarge'|'i3.4xlarge'|'i3.8xlarge'|'i3.16xlarge'|'hi1.4xlarge'|'hs1.8xlarge'|'c1.medium'|'c1.xlarge'|'c3.large'|'c3.xlarge'|'c3.2xlarge'|'c3.4xlarge'|'c3.8xlarge'|'c4.large'|'c4.xlarge'|'c4.2xlarge'|'c4.4xlarge'|'c4.8xlarge'|'cc1.4xlarge'|'cc2.8xlarge'|'g2.2xlarge'|'g2.8xlarge'|'cg1.4xlarge'|'p2.xlarge'|'p2.8xlarge'|'p2.16xlarge'|'d2.xlarge'|'d2.2xlarge'|'d2.4xlarge'|'d2.8xlarge'|'f1.2xlarge'|'f1.16xlarge', Placement={ 'AvailabilityZone': 'string', 'GroupName': 'string', 'Tenancy': 'default'|'dedicated'|'host', 'HostId': 'string', 'Affinity': 'string' }, KernelId='string', RamdiskId='string', BlockDeviceMappings=[ { 'VirtualName': 'string', 'DeviceName': 'string', 'Ebs': { 'SnapshotId': 'string', 'VolumeSize': 123, 'DeleteOnTermination': True|False, 'VolumeType': 'standard'|'io1'|'gp2'|'sc1'|'st1', 'Iops': 123, 'Encrypted': True|False }, 'NoDevice': 'string' }, ], Monitoring={ 'Enabled': True|False }, SubnetId='string', DisableApiTermination=True|False, InstanceInitiatedShutdownBehavior='stop'|'terminate', PrivateIpAddress='string', Ipv6Addresses=[ { 'Ipv6Address': 'string' }, ], Ipv6AddressCount=123, ClientToken='string', AdditionalInfo='string', NetworkInterfaces=[ { 'NetworkInterfaceId': 'string', 'DeviceIndex': 123, 'SubnetId': 'string', 'Description': 'string', 'PrivateIpAddress': 'string', 'Groups': [ 'string', ], 'DeleteOnTermination': True|False, 'PrivateIpAddresses': [ { 'PrivateIpAddress': 'string', 'Primary': True|False }, ], 'SecondaryPrivateIpAddressCount': 123, 'AssociatePublicIpAddress': True|False, 'Ipv6Addresses': [ { 'Ipv6Address': 'string' }, ], 'Ipv6AddressCount': 123 }, ], IamInstanceProfile={ 'Arn': 'string', 'Name': 'string' }, EbsOptimized=True|False, TagSpecifications=[ { 'ResourceType': 'customer-gateway'|'dhcp-options'|'image'|'instance'|'internet-gateway'|'network-acl'|'network-interface'|'reserved-instances'|'route-table'|'snapshot'|'spot-instances-request'|'subnet'|'security-group'|'volume'|'vpc'|'vpn-connection'|'vpn-gateway', 'Tags': [ { 'Key': 'string', 'Value': 'string' }, ] }, ] ) :type DryRun: boolean :param DryRun: Checks whether you have the required permissions for the action, without actually making the request, and provides an error response. If you have the required permissions, the error response is DryRunOperation . Otherwise, it is UnauthorizedOperation . :type ImageId: string :param ImageId: [REQUIRED] The ID of the AMI, which you can get by calling DescribeImages . :type MinCount: integer :param MinCount: [REQUIRED] The minimum number of instances to launch. If you specify a minimum that is more instances than Amazon EC2 can launch in the target Availability Zone, Amazon EC2 launches no instances. Constraints: Between 1 and the maximum number you're allowed for the specified instance type. For more information about the default limits, and how to request an increase, see How many instances can I run in Amazon EC2 in the Amazon EC2 General FAQ. :type MaxCount: integer :param MaxCount: [REQUIRED] The maximum number of instances to launch. If you specify more instances than Amazon EC2 can launch in the target Availability Zone, Amazon EC2 launches the largest possible number of instances above MinCount . Constraints: Between 1 and the maximum number you're allowed for the specified instance type. For more information about the default limits, and how to request an increase, see How many instances can I run in Amazon EC2 in the Amazon EC2 FAQ. :type KeyName: string :param KeyName: The name of the key pair. You can create a key pair using CreateKeyPair or ImportKeyPair . Warning If you do not specify a key pair, you can't connect to the instance unless you choose an AMI that is configured to allow users another way to log in. :type SecurityGroups: list :param SecurityGroups: [EC2-Classic, default VPC] One or more security group names. For a nondefault VPC, you must use security group IDs instead. Default: Amazon EC2 uses the default security group. (string) -- :type SecurityGroupIds: list :param SecurityGroupIds: One or more security group IDs. You can create a security group using CreateSecurityGroup . Default: Amazon EC2 uses the default security group. (string) -- :type UserData: string :param UserData: The user data to make available to the instance. For more information, see Running Commands on Your Linux Instance at Launch (Linux) and Adding User Data (Windows). If you are using an AWS SDK or command line tool, Base64-encoding is performed for you, and you can load the text from a file. Otherwise, you must provide Base64-encoded text. This value will be base64 encoded automatically. Do not base64 encode this value prior to performing the operation. :type InstanceType: string :param InstanceType: The instance type. For more information, see Instance Types in the Amazon Elastic Compute Cloud User Guide . Default: m1.small :type Placement: dict :param Placement: The placement for the instance. AvailabilityZone (string) --The Availability Zone of the instance. GroupName (string) --The name of the placement group the instance is in (for cluster compute instances). Tenancy (string) --The tenancy of the instance (if the instance is running in a VPC). An instance with a tenancy of dedicated runs on single-tenant hardware. The host tenancy is not supported for the ImportInstance command. HostId (string) --The ID of the Dedicated Host on which the instance resides. This parameter is not supported for the ImportInstance command. Affinity (string) --The affinity setting for the instance on the Dedicated Host. This parameter is not supported for the ImportInstance command. :type KernelId: string :param KernelId: The ID of the kernel. Warning We recommend that you use PV-GRUB instead of kernels and RAM disks. For more information, see PV-GRUB in the Amazon Elastic Compute Cloud User Guide . :type RamdiskId: string :param RamdiskId: The ID of the RAM disk. Warning We recommend that you use PV-GRUB instead of kernels and RAM disks. For more information, see PV-GRUB in the Amazon Elastic Compute Cloud User Guide . :type BlockDeviceMappings: list :param BlockDeviceMappings: The block device mapping. Warning Supplying both a snapshot ID and an encryption value as arguments for block-device mapping results in an error. This is because only blank volumes can be encrypted on start, and these are not created from a snapshot. If a snapshot is the basis for the volume, it contains data by definition and its encryption status cannot be changed using this action. (dict) --Describes a block device mapping. VirtualName (string) --The virtual device name (ephemeral N). Instance store volumes are numbered starting from 0. An instance type with 2 available instance store volumes can specify mappings for ephemeral0 and ephemeral1 .The number of available instance store volumes depends on the instance type. After you connect to the instance, you must mount the volume. Constraints: For M3 instances, you must specify instance store volumes in the block device mapping for the instance. When you launch an M3 instance, we ignore any instance store volumes specified in the block device mapping for the AMI. DeviceName (string) --The device name exposed to the instance (for example, /dev/sdh or xvdh ). Ebs (dict) --Parameters used to automatically set up EBS volumes when the instance is launched. SnapshotId (string) --The ID of the snapshot. VolumeSize (integer) --The size of the volume, in GiB. Constraints: 1-16384 for General Purpose SSD (gp2 ), 4-16384 for Provisioned IOPS SSD (io1 ), 500-16384 for Throughput Optimized HDD (st1 ), 500-16384 for Cold HDD (sc1 ), and 1-1024 for Magnetic (standard ) volumes. If you specify a snapshot, the volume size must be equal to or larger than the snapshot size. Default: If you're creating the volume from a snapshot and don't specify a volume size, the default is the snapshot size. DeleteOnTermination (boolean) --Indicates whether the EBS volume is deleted on instance termination. VolumeType (string) --The volume type: gp2 , io1 , st1 , sc1 , or standard . Default: standard Iops (integer) --The number of I/O operations per second (IOPS) that the volume supports. For io1 , this represents the number of IOPS that are provisioned for the volume. For gp2 , this represents the baseline performance of the volume and the rate at which the volume accumulates I/O credits for bursting. For more information about General Purpose SSD baseline performance, I/O credits, and bursting, see Amazon EBS Volume Types in the Amazon Elastic Compute Cloud User Guide . Constraint: Range is 100-20000 IOPS for io1 volumes and 100-10000 IOPS for gp2 volumes. Condition: This parameter is required for requests to create io1 volumes; it is not used in requests to create gp2 , st1 , sc1 , or standard volumes. Encrypted (boolean) --Indicates whether the EBS volume is encrypted. Encrypted Amazon EBS volumes may only be attached to instances that support Amazon EBS encryption. NoDevice (string) --Suppresses the specified device included in the block device mapping of the AMI. :type Monitoring: dict :param Monitoring: The monitoring for the instance. Enabled (boolean) -- [REQUIRED]Indicates whether detailed monitoring is enabled. Otherwise, basic monitoring is enabled. :type SubnetId: string :param SubnetId: [EC2-VPC] The ID of the subnet to launch the instance into. :type DisableApiTermination: boolean :param DisableApiTermination: If you set this parameter to true , you can't terminate the instance using the Amazon EC2 console, CLI, or API; otherwise, you can. To change this attribute to false after launch, use ModifyInstanceAttribute . Alternatively, if you set InstanceInitiatedShutdownBehavior to terminate , you can terminate the instance by running the shutdown command from the instance. Default: false :type InstanceInitiatedShutdownBehavior: string :param InstanceInitiatedShutdownBehavior: Indicates whether an instance stops or terminates when you initiate shutdown from the instance (using the operating system command for system shutdown). Default: stop :type PrivateIpAddress: string :param PrivateIpAddress: [EC2-VPC] The primary IPv4 address. You must specify a value from the IPv4 address range of the subnet. Only one private IP address can be designated as primary. You can't specify this option if you've specified the option to designate a private IP address as the primary IP address in a network interface specification. You cannot specify this option if you're launching more than one instance in the request. :type Ipv6Addresses: list :param Ipv6Addresses: [EC2-VPC] Specify one or more IPv6 addresses from the range of the subnet to associate with the primary network interface. You cannot specify this option and the option to assign a number of IPv6 addresses in the same request. You cannot specify this option if you've specified a minimum number of instances to launch. (dict) --Describes an IPv6 address. Ipv6Address (string) --The IPv6 address. :type Ipv6AddressCount: integer :param Ipv6AddressCount: [EC2-VPC] A number of IPv6 addresses to associate with the primary network interface. Amazon EC2 chooses the IPv6 addresses from the range of your subnet. You cannot specify this option and the option to assign specific IPv6 addresses in the same request. You can specify this option if you've specified a minimum number of instances to launch. :type ClientToken: string :param ClientToken: Unique, case-sensitive identifier you provide to ensure the idempotency of the request. For more information, see Ensuring Idempotency . Constraints: Maximum 64 ASCII characters :type AdditionalInfo: string :param AdditionalInfo: Reserved. :type NetworkInterfaces: list :param NetworkInterfaces: One or more network interfaces. (dict) --Describes a network interface. NetworkInterfaceId (string) --The ID of the network interface. DeviceIndex (integer) --The index of the device on the instance for the network interface attachment. If you are specifying a network interface in a RunInstances request, you must provide the device index. SubnetId (string) --The ID of the subnet associated with the network string. Applies only if creating a network interface when launching an instance. Description (string) --The description of the network interface. Applies only if creating a network interface when launching an instance. PrivateIpAddress (string) --The private IPv4 address of the network interface. Applies only if creating a network interface when launching an instance. You cannot specify this option if you're launching more than one instance in a RunInstances request. Groups (list) --The IDs of the security groups for the network interface. Applies only if creating a network interface when launching an instance. (string) -- DeleteOnTermination (boolean) --If set to true , the interface is deleted when the instance is terminated. You can specify true only if creating a new network interface when launching an instance. PrivateIpAddresses (list) --One or more private IPv4 addresses to assign to the network interface. Only one private IPv4 address can be designated as primary. You cannot specify this option if you're launching more than one instance in a RunInstances request. (dict) --Describes a secondary private IPv4 address for a network interface. PrivateIpAddress (string) -- [REQUIRED]The private IPv4 addresses. Primary (boolean) --Indicates whether the private IPv4 address is the primary private IPv4 address. Only one IPv4 address can be designated as primary. SecondaryPrivateIpAddressCount (integer) --The number of secondary private IPv4 addresses. You can't specify this option and specify more than one private IP address using the private IP addresses option. You cannot specify this option if you're launching more than one instance in a RunInstances request. AssociatePublicIpAddress (boolean) --Indicates whether to assign a public IPv4 address to an instance you launch in a VPC. The public IP address can only be assigned to a network interface for eth0, and can only be assigned to a new network interface, not an existing one. You cannot specify more than one network interface in the request. If launching into a default subnet, the default value is true . Ipv6Addresses (list) --One or more IPv6 addresses to assign to the network interface. You cannot specify this option and the option to assign a number of IPv6 addresses in the same request. You cannot specify this option if you've specified a minimum number of instances to launch. (dict) --Describes an IPv6 address. Ipv6Address (string) --The IPv6 address. Ipv6AddressCount (integer) --A number of IPv6 addresses to assign to the network interface. Amazon EC2 chooses the IPv6 addresses from the range of the subnet. You cannot specify this option and the option to assign specific IPv6 addresses in the same request. You can specify this option if you've specified a minimum number of instances to launch. :type IamInstanceProfile: dict :param IamInstanceProfile: The IAM instance profile. Arn (string) --The Amazon Resource Name (ARN) of the instance profile. Name (string) --The name of the instance profile. :type EbsOptimized: boolean :param EbsOptimized: Indicates whether the instance is optimized for EBS I/O. This optimization provides dedicated throughput to Amazon EBS and an optimized configuration stack to provide optimal EBS I/O performance. This optimization isn't available with all instance types. Additional usage charges apply when using an EBS-optimized instance. Default: false :type TagSpecifications: list :param TagSpecifications: The tags to apply to the resources during launch. You can tag instances and volumes. The specified tags are applied to all instances or volumes that are created during launch. (dict) --The tags to apply to a resource when the resource is being created. ResourceType (string) --The type of resource to tag. Currently, the resource types that support tagging on creation are instance and volume . Tags (list) --The tags to apply to the resource. (dict) --Describes a tag. Key (string) --The key of the tag. Constraints: Tag keys are case-sensitive and accept a maximum of 127 Unicode characters. May not begin with aws: Value (string) --The value of the tag. Constraints: Tag values are case-sensitive and accept a maximum of 255 Unicode characters. :rtype: dict :return: { 'ReservationId': 'string', 'OwnerId': 'string', 'RequesterId': 'string', 'Groups': [ { 'GroupName': 'string', 'GroupId': 'string' }, ], 'Instances': [ { 'InstanceId': 'string', 'ImageId': 'string', 'State': { 'Code': 123, 'Name': 'pending'|'running'|'shutting-down'|'terminated'|'stopping'|'stopped' }, 'PrivateDnsName': 'string', 'PublicDnsName': 'string', 'StateTransitionReason': 'string', 'KeyName': 'string', 'AmiLaunchIndex': 123, 'ProductCodes': [ { 'ProductCodeId': 'string', 'ProductCodeType': 'devpay'|'marketplace' }, ], 'InstanceType': 't1.micro'|'t2.nano'|'t2.micro'|'t2.small'|'t2.medium'|'t2.large'|'t2.xlarge'|'t2.2xlarge'|'m1.small'|'m1.medium'|'m1.large'|'m1.xlarge'|'m3.medium'|'m3.large'|'m3.xlarge'|'m3.2xlarge'|'m4.large'|'m4.xlarge'|'m4.2xlarge'|'m4.4xlarge'|'m4.10xlarge'|'m4.16xlarge'|'m2.xlarge'|'m2.2xlarge'|'m2.4xlarge'|'cr1.8xlarge'|'r3.large'|'r3.xlarge'|'r3.2xlarge'|'r3.4xlarge'|'r3.8xlarge'|'r4.large'|'r4.xlarge'|'r4.2xlarge'|'r4.4xlarge'|'r4.8xlarge'|'r4.16xlarge'|'x1.16xlarge'|'x1.32xlarge'|'i2.xlarge'|'i2.2xlarge'|'i2.4xlarge'|'i2.8xlarge'|'i3.large'|'i3.xlarge'|'i3.2xlarge'|'i3.4xlarge'|'i3.8xlarge'|'i3.16xlarge'|'hi1.4xlarge'|'hs1.8xlarge'|'c1.medium'|'c1.xlarge'|'c3.large'|'c3.xlarge'|'c3.2xlarge'|'c3.4xlarge'|'c3.8xlarge'|'c4.large'|'c4.xlarge'|'c4.2xlarge'|'c4.4xlarge'|'c4.8xlarge'|'cc1.4xlarge'|'cc2.8xlarge'|'g2.2xlarge'|'g2.8xlarge'|'cg1.4xlarge'|'p2.xlarge'|'p2.8xlarge'|'p2.16xlarge'|'d2.xlarge'|'d2.2xlarge'|'d2.4xlarge'|'d2.8xlarge'|'f1.2xlarge'|'f1.16xlarge', 'LaunchTime': datetime(2015, 1, 1), 'Placement': { 'AvailabilityZone': 'string', 'GroupName': 'string', 'Tenancy': 'default'|'dedicated'|'host', 'HostId': 'string', 'Affinity': 'string' }, 'KernelId': 'string', 'RamdiskId': 'string', 'Platform': 'Windows', 'Monitoring': { 'State': 'disabled'|'disabling'|'enabled'|'pending' }, 'SubnetId': 'string', 'VpcId': 'string', 'PrivateIpAddress': 'string', 'PublicIpAddress': 'string', 'StateReason': { 'Code': 'string', 'Message': 'string' }, 'Architecture': 'i386'|'x86_64', 'RootDeviceType': 'ebs'|'instance-store', 'RootDeviceName': 'string', 'BlockDeviceMappings': [ { 'DeviceName': 'string', 'Ebs': { 'VolumeId': 'string', 'Status': 'attaching'|'attached'|'detaching'|'detached', 'AttachTime': datetime(2015, 1, 1), 'DeleteOnTermination': True|False } }, ], 'VirtualizationType': 'hvm'|'paravirtual', 'InstanceLifecycle': 'spot'|'scheduled', 'SpotInstanceRequestId': 'string', 'ClientToken': 'string', 'Tags': [ { 'Key': 'string', 'Value': 'string' }, ], 'SecurityGroups': [ { 'GroupName': 'string', 'GroupId': 'string' }, ], 'SourceDestCheck': True|False, 'Hypervisor': 'ovm'|'xen', 'NetworkInterfaces': [ { 'NetworkInterfaceId': 'string', 'SubnetId': 'string', 'VpcId': 'string', 'Description': 'string', 'OwnerId': 'string', 'Status': 'available'|'attaching'|'in-use'|'detaching', 'MacAddress': 'string', 'PrivateIpAddress': 'string', 'PrivateDnsName': 'string', 'SourceDestCheck': True|False, 'Groups': [ { 'GroupName': 'string', 'GroupId': 'string' }, ], 'Attachment': { 'AttachmentId': 'string', 'DeviceIndex': 123, 'Status': 'attaching'|'attached'|'detaching'|'detached', 'AttachTime': datetime(2015, 1, 1), 'DeleteOnTermination': True|False }, 'Association': { 'PublicIp': 'string', 'PublicDnsName': 'string', 'IpOwnerId': 'string' }, 'PrivateIpAddresses': [ { 'PrivateIpAddress': 'string', 'PrivateDnsName': 'string', 'Primary': True|False, 'Association': { 'PublicIp': 'string', 'PublicDnsName': 'string', 'IpOwnerId': 'string' } }, ], 'Ipv6Addresses': [ { 'Ipv6Address': 'string' }, ] }, ], 'IamInstanceProfile': { 'Arn': 'string', 'Id': 'string' }, 'EbsOptimized': True|False, 'SriovNetSupport': 'string', 'EnaSupport': True|False }, ] } :returns: DryRun (boolean) -- Checks whether you have the required permissions for the action, without actually making the request, and provides an error response. If you have the required permissions, the error response is DryRunOperation . Otherwise, it is UnauthorizedOperation . ImageId (string) -- [REQUIRED] The ID of the AMI, which you can get by calling DescribeImages . MinCount (integer) -- [REQUIRED] The minimum number of instances to launch. If you specify a minimum that is more instances than Amazon EC2 can launch in the target Availability Zone, Amazon EC2 launches no instances. Constraints: Between 1 and the maximum number you're allowed for the specified instance type. For more information about the default limits, and how to request an increase, see How many instances can I run in Amazon EC2 in the Amazon EC2 General FAQ. MaxCount (integer) -- [REQUIRED] The maximum number of instances to launch. If you specify more instances than Amazon EC2 can launch in the target Availability Zone, Amazon EC2 launches the largest possible number of instances above MinCount . Constraints: Between 1 and the maximum number you're allowed for the specified instance type. For more information about the default limits, and how to request an increase, see How many instances can I run in Amazon EC2 in the Amazon EC2 FAQ. KeyName (string) -- The name of the key pair. You can create a key pair using CreateKeyPair or ImportKeyPair . Warning If you do not specify a key pair, you can't connect to the instance unless you choose an AMI that is configured to allow users another way to log in. SecurityGroups (list) -- [EC2-Classic, default VPC] One or more security group names. For a nondefault VPC, you must use security group IDs instead. Default: Amazon EC2 uses the default security group. (string) -- SecurityGroupIds (list) -- One or more security group IDs. You can create a security group using CreateSecurityGroup . Default: Amazon EC2 uses the default security group. (string) -- UserData (string) -- The user data to make available to the instance. For more information, see Running Commands on Your Linux Instance at Launch (Linux) and Adding User Data (Windows). If you are using an AWS SDK or command line tool, Base64-encoding is performed for you, and you can load the text from a file. Otherwise, you must provide Base64-encoded text. This value will be base64 encoded automatically. Do not base64 encode this value prior to performing the operation. InstanceType (string) -- The instance type. For more information, see Instance Types in the Amazon Elastic Compute Cloud User Guide . Default: m1.small Placement (dict) -- The placement for the instance. AvailabilityZone (string) --The Availability Zone of the instance. GroupName (string) --The name of the placement group the instance is in (for cluster compute instances). Tenancy (string) --The tenancy of the instance (if the instance is running in a VPC). An instance with a tenancy of dedicated runs on single-tenant hardware. The host tenancy is not supported for the ImportInstance command. HostId (string) --The ID of the Dedicated Host on which the instance resides. This parameter is not supported for the ImportInstance command. Affinity (string) --The affinity setting for the instance on the Dedicated Host. This parameter is not supported for the ImportInstance command. KernelId (string) -- The ID of the kernel. Warning We recommend that you use PV-GRUB instead of kernels and RAM disks. For more information, see PV-GRUB in the Amazon Elastic Compute Cloud User Guide . RamdiskId (string) -- The ID of the RAM disk. Warning We recommend that you use PV-GRUB instead of kernels and RAM disks. For more information, see PV-GRUB in the Amazon Elastic Compute Cloud User Guide . BlockDeviceMappings (list) -- The block device mapping. Warning Supplying both a snapshot ID and an encryption value as arguments for block-device mapping results in an error. This is because only blank volumes can be encrypted on start, and these are not created from a snapshot. If a snapshot is the basis for the volume, it contains data by definition and its encryption status cannot be changed using this action. (dict) --Describes a block device mapping. VirtualName (string) --The virtual device name (ephemeral N). Instance store volumes are numbered starting from 0. An instance type with 2 available instance store volumes can specify mappings for ephemeral0 and ephemeral1 .The number of available instance store volumes depends on the instance type. After you connect to the instance, you must mount the volume. Constraints: For M3 instances, you must specify instance store volumes in the block device mapping for the instance. When you launch an M3 instance, we ignore any instance store volumes specified in the block device mapping for the AMI. DeviceName (string) --The device name exposed to the instance (for example, /dev/sdh or xvdh ). Ebs (dict) --Parameters used to automatically set up EBS volumes when the instance is launched. SnapshotId (string) --The ID of the snapshot. VolumeSize (integer) --The size of the volume, in GiB. Constraints: 1-16384 for General Purpose SSD (gp2 ), 4-16384 for Provisioned IOPS SSD (io1 ), 500-16384 for Throughput Optimized HDD (st1 ), 500-16384 for Cold HDD (sc1 ), and 1-1024 for Magnetic (standard ) volumes. If you specify a snapshot, the volume size must be equal to or larger than the snapshot size. Default: If you're creating the volume from a snapshot and don't specify a volume size, the default is the snapshot size. DeleteOnTermination (boolean) --Indicates whether the EBS volume is deleted on instance termination. VolumeType (string) --The volume type: gp2 , io1 , st1 , sc1 , or standard . Default: standard Iops (integer) --The number of I/O operations per second (IOPS) that the volume supports. For io1 , this represents the number of IOPS that are provisioned for the volume. For gp2 , this represents the baseline performance of the volume and the rate at which the volume accumulates I/O credits for bursting. For more information about General Purpose SSD baseline performance, I/O credits, and bursting, see Amazon EBS Volume Types in the Amazon Elastic Compute Cloud User Guide . Constraint: Range is 100-20000 IOPS for io1 volumes and 100-10000 IOPS for gp2 volumes. Condition: This parameter is required for requests to create io1 volumes; it is not used in requests to create gp2 , st1 , sc1 , or standard volumes. Encrypted (boolean) --Indicates whether the EBS volume is encrypted. Encrypted Amazon EBS volumes may only be attached to instances that support Amazon EBS encryption. NoDevice (string) --Suppresses the specified device included in the block device mapping of the AMI. Monitoring (dict) -- The monitoring for the instance. Enabled (boolean) -- [REQUIRED]Indicates whether detailed monitoring is enabled. Otherwise, basic monitoring is enabled. SubnetId (string) -- [EC2-VPC] The ID of the subnet to launch the instance into. DisableApiTermination (boolean) -- If you set this parameter to true , you can't terminate the instance using the Amazon EC2 console, CLI, or API; otherwise, you can. To change this attribute to false after launch, use ModifyInstanceAttribute . Alternatively, if you set InstanceInitiatedShutdownBehavior to terminate , you can terminate the instance by running the shutdown command from the instance. Default: false InstanceInitiatedShutdownBehavior (string) -- Indicates whether an instance stops or terminates when you initiate shutdown from the instance (using the operating system command for system shutdown). Default: stop PrivateIpAddress (string) -- [EC2-VPC] The primary IPv4 address. You must specify a value from the IPv4 address range of the subnet. Only one private IP address can be designated as primary. You can't specify this option if you've specified the option to designate a private IP address as the primary IP address in a network interface specification. You cannot specify this option if you're launching more than one instance in the request. Ipv6Addresses (list) -- [EC2-VPC] Specify one or more IPv6 addresses from the range of the subnet to associate with the primary network interface. You cannot specify this option and the option to assign a number of IPv6 addresses in the same request. You cannot specify this option if you've specified a minimum number of instances to launch. (dict) --Describes an IPv6 address. Ipv6Address (string) --The IPv6 address. Ipv6AddressCount (integer) -- [EC2-VPC] A number of IPv6 addresses to associate with the primary network interface. Amazon EC2 chooses the IPv6 addresses from the range of your subnet. You cannot specify this option and the option to assign specific IPv6 addresses in the same request. You can specify this option if you've specified a minimum number of instances to launch. ClientToken (string) -- Unique, case-sensitive identifier you provide to ensure the idempotency of the request. For more information, see Ensuring Idempotency . Constraints: Maximum 64 ASCII characters AdditionalInfo (string) -- Reserved. NetworkInterfaces (list) -- One or more network interfaces. (dict) --Describes a network interface. NetworkInterfaceId (string) --The ID of the network interface. DeviceIndex (integer) --The index of the device on the instance for the network interface attachment. If you are specifying a network interface in a RunInstances request, you must provide the device index. SubnetId (string) --The ID of the subnet associated with the network string. Applies only if creating a network interface when launching an instance. Description (string) --The description of the network interface. Applies only if creating a network interface when launching an instance. PrivateIpAddress (string) --The private IPv4 address of the network interface. Applies only if creating a network interface when launching an instance. You cannot specify this option if you're launching more than one instance in a RunInstances request. Groups (list) --The IDs of the security groups for the network interface. Applies only if creating a network interface when launching an instance. (string) -- DeleteOnTermination (boolean) --If set to true , the interface is deleted when the instance is terminated. You can specify true only if creating a new network interface when launching an instance. PrivateIpAddresses (list) --One or more private IPv4 addresses to assign to the network interface. Only one private IPv4 address can be designated as primary. You cannot specify this option if you're launching more than one instance in a RunInstances request. (dict) --Describes a secondary private IPv4 address for a network interface. PrivateIpAddress (string) -- [REQUIR",1,0,0,2,3,1,0,0,2,3
"def user_is(role, get_user=import_user):
  def wrapper(func):
  @wraps(func)
  def inner(*args, **kwargs):
  from .models import Role
  current_user = get_user()
  if role in current_user.roles:
  return func(*args, **kwargs)
  raise Forbidden(""You do not have access"")
  return inner
  return wrapper",Takes an role (a string name of either a role or an ability) and returns the function if the user has that role,1,0,0,0,1,1,0,1,1,3
"def build_access_required(function_or_param_name):
  def get_wrapper(param_name, f):
  @functools.wraps(f)
  def wrapped(*args, **kwargs):
  g.build = can_user_access_build(param_name)
  if not utils.is_production():
  time.sleep(0.5)
  return f(*args, **kwargs)
  return wrapped
  if isinstance(function_or_param_name, basestring):
  return lambda f: get_wrapper(function_or_param_name, f)
  else:
  return get_wrapper('id', function_or_param_name)",Decorator ensures user has access to the build ID in the request. May be used in two ways: @build_access_required def my_func(build): ... @build_access_required('custom_build_id_param') def my_func(build): ... Always calls the given function with the models.Build entity as the first positional argument.,1,0,1,0,2,1,0,0,1,2
"def register(self):
  user, created = self.Model.create_account(self._json_params)
  if user.api_key is None:
  raise JHTTPBadRequest('Failed to generate ApiKey for user')
  if not created:
  raise JHTTPConflict('Looks like you already have an account.')
  self.request._user = user
  headers = remember(self.request, user.username)
  return JHTTPOk('Registered', headers=headers)",Register a new user by POSTing all required data. User's `Authorization` header value is returned in `WWW-Authenticate` header.,1,1,0,1,3,2,1,0,1,4
"def spkaps(targ, et, ref, abcorr, stobs, accobs):
  targ = ctypes.c_int(targ)
  et = ctypes.c_double(et)
  ref = stypes.stringToCharP(ref)
  abcorr = stypes.stringToCharP(abcorr)
  stobs = stypes.toDoubleVector(stobs)
  accobs = stypes.toDoubleVector(accobs)
  starg = stypes.emptyDoubleVector(6)
  lt = ctypes.c_double()
  dlt = ctypes.c_double()
  libspice.spkaps_c(targ, et, ref, abcorr, stobs, accobs, starg,
  ctypes.byref(lt), ctypes.byref(dlt))
  return stypes.cVectorToPython(starg), lt.value, dlt.value","Given the state and acceleration of an observer relative to the solar system barycenter, return the state (position and velocity) of a target body relative to the observer, optionally corrected for light time and stellar aberration. All input and output vectors are expressed relative to an inertial reference frame. This routine supersedes :func:`spkapp`. SPICE users normally should call the high-level API routines :func:`spkezr` or :func:`spkez` rather than this routine. http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/spkaps_c.html :param targ: Target body. :type targ: int :param et: Observer epoch. :type et: float :param ref: Inertial reference frame of output state. :type ref: str :param abcorr: Aberration correction flag. :type abcorr: str :param stobs: State of the observer relative to the SSB. :type stobs: 6-Element Array of floats :param accobs: Acceleration of the observer relative to the SSB. :type accobs: 6-Element Array of floats :return: State of target, One way light time between observer and target, Derivative of light time with respect to time. :rtype: tuple",1,0,0,1,2,0,0,0,1,1
"def DatabaseEnabled(cls):
  if not issubclass(cls, Storable):
  raise ValueError(
  ""%s is not a subclass of gludb.datab.Storage"" % repr(cls)
  )
  cls.ensure_table = classmethod(_ensure_table)
  cls.find_one = classmethod(_find_one)
  cls.find_all = classmethod(_find_all)
  cls.find_by_index = classmethod(_find_by_index)
  cls.save = _save
  cls.delete = _delete
  return cls",Given persistence methods to classes with this annotation. All this really does is add some functions that forward to the mapped database class.,0,1,1,0,2,0,1,1,0,2
"def refresh_db(jail=None, chroot=None, root=None, force=False, **kwargs):
  salt.utils.pkg.clear_rtag(__opts__)
  cmd = _pkg(jail, chroot, root)
  cmd.append('update')
  if force:
  cmd.append('-f')
  return __salt__['cmd.retcode'](cmd, python_shell=False) == 0","Refresh PACKAGESITE contents .. note:: This function can accessed using ``pkg.update`` in addition to ``pkg.refresh_db``, to more closely match the CLI usage of ``pkg(8)``. CLI Example: .. code-block:: bash salt '*' pkg.refresh_db jail Refresh the pkg database within the specified jail chroot Refresh the pkg database within the specified chroot (ignored if ``jail`` is specified) root Refresh the pkg database within the specified root (ignored if ``jail`` is specified) force Force a full download of the repository catalog without regard to the respective ages of the local and remote copies of the catalog. CLI Example: .. code-block:: bash salt '*' pkg.refresh_db force=True",1,0,0,1,2,1,0,0,1,2
"def authorize(self, scope=None, redirect_uri=None, state=None):
  _logger.debug(""Called authorize()"")
  params = {'client_id': self.client_id}
  if scope:
  params['scope'] = scope
  if redirect_uri:
  params['redirect_uri'] = redirect_uri
  if state:
  params['state'] = state
  url = self.auth_url + 'authorize?' + urlencode(params)
  _logger.debug(""Redirecting to %s"", url)
  return redirect(url)","Redirect to GitHub and request access to a user's data. :param scope: List of `Scopes`_ for which to request access, formatted as a string or comma delimited list of scopes as a string. Defaults to ``None``, resulting in granting read-only access to public information (includes public user profile info, public repository info, and gists). For more information on this, see the examples in presented in the GitHub API `Scopes`_ documentation, or see the examples provided below. :type scope: str :param redirect_uri: `Redirect URL`_ to which to redirect the user after authentication. Defaults to ``None``, resulting in using the default redirect URL for the OAuth application as defined in GitHub. This URL can differ from the callback URL defined in your GitHub application, however it must be a subdirectory of the specified callback URL, otherwise raises a :class:`GitHubError`. For more information on this, see the examples in presented in the GitHub API `Redirect URL`_ documentation, or see the example provided below. :type redirect_uri: str :param state: An unguessable random string. It is used to protect against cross-site request forgery attacks. :type state: str For example, if we wanted to use this method to get read/write access to user profile information, in addition to read-write access to code, commit status, etc., we would need to use the `Scopes`_ ``user`` and ``repo`` when calling this method. .. code-block:: python github.authorize(scope=""user,repo"") Additionally, if we wanted to specify a different redirect URL following authorization. .. code-block:: python # Our application's callback URL is ""http://example.com/callback"" redirect_uri=""http://example.com/callback/my/path"" github.authorize(scope=""user,repo"", redirect_uri=redirect_uri) .. _Scopes: https://developer.github.com/v3/oauth/#scopes .. _Redirect URL: https://developer.github.com/v3/oauth/#redirect-urls",1,0,0,1,2,2,0,0,1,3
"def _augment_url_with_version(auth_url):
  if has_in_url_path(auth_url, [""/v2.0"", ""/v3""]):
  return auth_url
  if get_keystone_version() >= 3:
  return url_path_append(auth_url, ""/v3"")
  else:
  return url_path_append(auth_url, ""/v2.0"")","Optionally augment auth_url path with version suffix. Check if path component already contains version suffix and if it does not, append version suffix to the end of path, not erasing the previous path contents, since keystone web endpoint (like /identity) could be there. Keystone version needs to be added to endpoint because as of Kilo, the identity URLs returned by Keystone might no longer contain API versions, leaving the version choice up to the user.",1,0,0,0,1,1,0,0,1,2
"def get_instances(self, project=None, zone='us-west1-a'):
  project = self._get_project(project)
  zone = self._get_zone(zone)
  return self._compute_service.instances().list(project=project,
  zone=zone).execute()","get instances will return the (unparsed) list of instances, for functions for the user. This is primarily used by get_builders to print a list of builder instances. Parameters ========== project: specify a project, will default to environment first zone: the zone to use, defaults to us-west1-a if environment not set",1,0,0,0,1,2,0,0,1,3
"def fetch_datasets(self, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('callback'):
  return self.fetch_datasets_with_http_info(**kwargs)
  else:
  (data) = self.fetch_datasets_with_http_info(**kwargs)
  return data","List datasets as owner Fetch datasets that the currently authenticated user has access to because he or she is the owner of the dataset. This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please define a `callback` function to be invoked when receiving the response. >>> def callback_function(response): >>> pprint(response) >>> >>> thread = api.fetch_datasets(callback=callback_function) :param callback function: The callback function for asynchronous request. (optional) :param str limit: Maximum number of items to include in a page of results. :param str next: Token from previous result page to be used when requesting a subsequent page. :return: PaginatedDatasetResults If the method is called asynchronously, returns the request thread.",2,0,0,1,3,2,0,0,1,3
"def get_all_assignable_users_for_project(self, project_key, start=0, limit=50):
  url = 'rest/api/2/user/assignable/search?project={project_key}&startAt={start}&maxResults={limit}'.format(
  project_key=project_key,
  start=start,
  limit=limit)
  return self.get(url)","Provide assignable users for project :param project_key: :param start: OPTIONAL: The start point of the collection to return. Default: 0. :param limit: OPTIONAL: The limit of the number of users to return, this may be restricted by fixed system limits. Default by built-in method: 50 :return:",2,0,0,1,3,2,0,0,1,3
"def create_password_reset(cls, email, valid_for=3600) -> str:
  user = cls.where_email(email)
  if user is None:
  return None
  PasswordResetModel.delete_where_user_id(user.id)
  token = JWT().create_token({
  'code': Security.random_string(5),
  'user_id': user.id},
  token_valid_for=valid_for)
  code = Security.generate_uuid(1) + ""-"" + Security.random_string(5)
  password_reset_model = PasswordResetModel()
  password_reset_model.token = token
  password_reset_model.code = code
  password_reset_model.user_id = user.id
  password_reset_model.save()
  return code",Create a password reset request in the user_password_resets database table. Hashed code gets stored in the database. Returns unhashed reset code,1,1,1,1,4,1,1,1,1,4
"def parse_plugin_args(command_args, config_args):
  parsed_args = dict()
  for arg in command_args:
  kv = arg.split(""="")
  if len(kv) != 2:
  raise click.UsageError(""Invalid parameter '{}', must be in key=value format"".format(arg))
  parsed_args[kv[0]] = config_utils.get_truetype(kv[1])
  for arg in config_args:
  value = arg[defs.VALUE]
  value_type = arg[defs.TYPE]
  if value in parsed_args:
  config_utils.validate_field_matches_type(value, parsed_args[value], value_type,
  arg.get(defs.ITEMS), arg.get(defs.MIN), arg.get(defs.MAX))
  elif defs.DEFAULT in arg:
  parsed_args[value] = arg[defs.DEFAULT]
  elif arg[defs.REQUIRED]:
  raise exceptions.RequiredFieldMissing(value)
  return parsed_args",Parse command line arguments based on the plugin's parameters config. :param command_args: Command line arguments as provided by the user in `key=value` format. :param config_args: Plugin parameters parsed from config.json. :returns: Validated dictionary of parameters that will be passed to plugin class,1,0,0,0,1,1,0,0,1,2
"def can_create_asset_content_with_record_types(self, asset_id=None, asset_content_record_types=None):
  url_path = construct_url('authorization',
  bank_id=self._catalog_idstr)
  return self._get_request(url_path)['assetHints']['canCreate']","Tests if this user can create an ``AssetContent`` using the desired record types. While ``RepositoryManager.getAssetContentRecordTypes()`` can be used to test which records are supported, this method tests which records are required for creating a specific ``AssetContent``. Providing an empty array tests if an ``AssetContent`` can be created with no records. :param asset_id: the ``Id`` of an ``Asset`` :type asset_id: ``osid.id.Id`` :param asset_content_record_types: array of asset content record types :type asset_content_record_types: ``osid.type.Type[]`` :return: ``true`` if ``AssetContent`` creation using the specified ``Types`` is supported, ``false`` otherwise :rtype: ``boolean`` :raise: ``NullArgument`` -- ``asset_id`` or ``asset_content_record_types`` is ``null`` *compliance: mandatory -- This method must be implemented.*",2,0,1,1,4,2,0,0,1,3
"def __is_applied(self, delta):
  query = .format(
  self.upgrades_table, delta.get_version(), delta.get_checksum())
  self.cursor.execute(query)
  if not self.cursor.fetchone():
  return False
  else:
  return True",Verifies if delta file is already applied on database Parameters ---------- delta: Delta object The delta object representing the delta file Returns ------- bool True if the delta is already applied on the db False otherwise,0,0,1,0,1,0,1,1,0,2
"def find_vulnerabilities_in_cfg(
  cfg,
  definitions,
  lattice,
  blackbox_mapping,
  vulnerabilities_list,
  interactive,
  nosec_lines
 ):
  triggers = identify_triggers(
  cfg,
  definitions.sources,
  definitions.sinks,
  lattice,
  nosec_lines[cfg.filename]
  )
  for sink in triggers.sinks:
  for source in triggers.sources:
  vulnerability, interactive = get_vulnerability(
  source,
  sink,
  triggers,
  lattice,
  cfg,
  interactive,
  blackbox_mapping
  )
  if vulnerability:
  vulnerabilities_list.append(vulnerability)",Find vulnerabilities in a cfg. Args: cfg(CFG): The CFG to find vulnerabilities in. definitions(trigger_definitions_parser.Definitions): Source and sink definitions. lattice(Lattice): the lattice we're analysing. blackbox_mapping(dict): A map of blackbox functions containing whether or not they propagate taint. vulnerabilities_list(list): That we append to when we find vulnerabilities. interactive(bool): determines if we ask the user about blackbox functions not in the mapping file.,1,0,0,1,2,1,0,0,1,2
"def send_media_group(chat_id, media,
  reply_to_message_id=None, disable_notification=False,
  **kwargs):
  files = []
  if len(media) < 2 or len(media) > 10:
  raise ValueError('media must contain between 2 and 10 InputMedia items')
  for i, entry in media:
  if isinstance(entry.media, InputFile):
  files.append(entry.media)
  media[i].media = ""attach://{}"".format(entry[1][0])
  params = dict(
  chat_id=chat_id,
  media=json.dumps(media)
  )
  params.update(
  _clean_params(
  reply_to_message_id=reply_to_message_id,
  disable_notification=disable_notification,
  )
  )
  return TelegramBotRPCRequest('sendMediaGroup', params=params, files=files, on_result=lambda result: [Message.from_result(message) for message in result], **kwargs)","Use this method to send a group of photos or videos as an album. On success, an array of the sent Messages is returned. :param chat_id: Unique identifier for the target chat or username of the target channel (in the format @channelusername) :param media: A list of InputMedia objects to be sent, must include 210 items :param reply_to_message_id: If the message is a reply, ID of the original message :param disable_notification: Sends the messages silently. Users will receive a notification with no sound. :param kwargs: Args that get passed down to :class:`TelegramBotRPCRequest` :type chat_id: int or str :type media: `list` of :class:`InputMedia` :type reply_to_message_id: int :returns: On success, an array of the sent Messages is returned. :rtype: TelegramBotRPCRequest",2,0,0,1,3,1,0,0,2,3
"def _calc_avg_and_last_val(self, has_no_column, sum_existing_columns):
  sum_no_columns = len(has_no_column)
  columns_left = self.ALLOWED_COLUMNS - sum_existing_columns
  if sum_no_columns == 0:
  columns_avg = columns_left
  else:
  columns_avg = int(columns_left / sum_no_columns)
  remainder = columns_left - (columns_avg * sum_no_columns)
  columns_for_last_element = columns_avg + remainder
  return columns_avg, columns_for_last_element","Calculate the average of all columns and return a rounded down number. Store the remainder and add it to the last row. Could be implemented better. If the enduser wants more control, he can also just add the amount of columns. Will work fine with small number (<4) of items in a row. :param has_no_column: :param sum_existing_columns: :return: average, columns_for_last_element",0,0,0,1,1,0,0,0,0,0
"def line_transformation_suggestor(line_transformation, line_filter=None):
  def suggestor(lines):
  for line_number, line in enumerate(lines):
  if line_filter and not line_filter(line):
  continue
  candidate = line_transformation(line)
  if candidate is None:
  yield Patch(line_number)
  else:
  yield Patch(line_number, new_lines=[candidate])
  return suggestor","Returns a suggestor (a function that takes a list of lines and yields patches) where suggestions are the result of line-by-line transformations. @param line_transformation Function that, given a line, returns another line with which to replace the given one. If the output line is different from the input line, the user will be prompted about whether to make the change. If the output is None, this means ""I don't have a suggestion, but the user should still be asked if zhe wants to edit the line."" @param line_filter Given a line, returns True or False. If False, a line is ignored (as if line_transformation returned the line itself for that line).",1,0,0,1,2,1,0,0,1,2
"def level(self):
  level_key = ""player_level""
  if level_key in self._api[""response""]:
  return self._api[""response""][level_key]
  try:
  lvl = api.interface(""IPlayerService"").GetSteamLevel(steamid=self.id64)[""response""][level_key]
  self._api[""response""][level_key] = lvl
  return lvl
  except:
  return -1","Returns the the user's profile level, note that this runs a separate request because the profile level data isn't in the standard player summary output even though it should be. Which is also why it's not implemented as a separate class. You won't need this output and not the profile output",1,0,0,1,2,2,0,0,1,3
"def boolbox(msg=""Shall I continue?"", title="" "",
  choices=(""[Y]es"", ""[N]o""), image=None,
  default_choice='Yes', cancel_choice='No'):
  if len(choices) != 2:
  raise AssertionError(
  'boolbox takes exactly 2 choices! Consider using indexbox instead'
  )
  reply = bb.buttonbox(msg=msg,
  title=title,
  choices=choices,
  image=image,
  default_choice=default_choice,
  cancel_choice=cancel_choice)
  if reply == choices[0]:
  return True
  else:
  return False","Display a boolean msgbox. The returned value is calculated this way:: if the first choice is chosen, or if the dialog is cancelled: returns True else: returns False :param str msg: the msg to be displayed :param str title: the window title :param list choices: a list or tuple of the choices to be displayed :param str image: Filename of image to display :param str default_choice: The choice you want highlighted when the gui appears :param str cancel_choice: If the user presses the 'X' close, which button should be pressed :return: True if first button pressed or dialog is cancelled, False if second button is pressed",1,0,0,0,1,1,0,0,1,2
"def selected_impact_function_constraints(self):
  hazard = self.step_fc_functions1.selected_value(
  layer_purpose_hazard['key'])
  exposure = self.step_fc_functions1.selected_value(
  layer_purpose_exposure['key'])
  hazard_geometry = self.step_fc_functions2.selected_value(
  layer_purpose_hazard['key'])
  exposure_geometry = self.step_fc_functions2.selected_value(
  layer_purpose_exposure['key'])
  return hazard, exposure, hazard_geometry, exposure_geometry","Obtain impact function constraints selected by user. :returns: Tuple of metadata of hazard, exposure, hazard layer geometry and exposure layer geometry :rtype: tuple",1,0,0,1,2,1,0,0,1,2
"def create_branch(self, project_key, repository, name, start_point, message=""""):
  url = 'rest/api/1.0/projects/{projectKey}/repos/{repository}/branches'.format(projectKey=project_key,
  repository=repository)
  data = {
  ""name"": name,
  ""startPoint"": start_point,
  ""message"": message
  }
  return self.post(url, data=data)","Creates a branch using the information provided in the request. The authenticated user must have REPO_WRITE permission for the context repository to call this resource. :param project_key: The project matching the projectKey supplied in the resource path as shown in URL. :type project_key: str :param repository: Name of repository where branch is created (i.e. ""my_repo""). :type repository: str :param name: Name of branch to create (i.e. ""my_branch""). :type name: str :param start_point: Name of branch to branch from. :type start_point: str :param message: Branch message. :type message: str :return: 200 - application/json (repository) 401 - application/json (errors) 404 - application/json (errors) :rtype: requests.Response",1,0,0,2,3,1,0,0,1,2
"def rmlst(databasepath, credentials):
  logging.info('Downloading rMLST database')
  completefile = os.path.join(databasepath, 'rMLST', 'complete')
  if not os.path.isfile(completefile):
  args = MetadataObject()
  args.path = databasepath
  args.logging = logging
  args.credentials = credentials
  get_rmlst.Get(args)
  with open(completefile, 'w') as complete:
  complete.write('\n'.join(glob(os.path.join(databasepath, 'rMLST', '*'))))",Get the most up-to-date profiles and alleles from pubmlst. Note that you will need the necessary access token and secret for this to work :param databasepath: path to use to save the database :param credentials: path to folder containing accessory token and secret.txt files,1,0,0,1,2,1,0,0,1,2
"def submit_recording(raw_data_json):
  url = ""http://www.martin-thoma.de/write-math/classify/index.php""
  headers = {'User-Agent': 'Mozilla/5.0',
  'Content-Type': 'application/x-www-form-urlencoded'}
  payload = {'drawnJSON': raw_data_json}
  s = requests.Session()
  req = requests.Request('POST', url, headers=headers, data=payload)
  prepared = req.prepare()
  s.send(prepared)",Submit a recording to the database on write-math.com. Parameters ---------- raw_data_json : str Raw data in JSON format Raises ------ requests.exceptions.ConnectionError If the internet connection is lost.,0,0,0,1,1,1,0,0,1,2
"def clear_cache(temp_dir=None):
  with memory_lock:
  _module_values['last_update'] = None
  _module_values['certs'] = None
  ca_path, temp = _ca_path(temp_dir)
  if temp:
  with path_lock:
  if os.path.exists(ca_path):
  os.remove(ca_path)","Clears any cached info that was exported from the OS trust store. This will ensure the latest changes are returned from calls to get_list() and get_path(), but at the expense of re-exporting and parsing all certificates. :param temp_dir: The temporary directory to cache the CA certs in on OS X and Windows. Needs to have secure permissions so other users can not modify the contents. Must be the same value passed to get_path().",1,0,0,0,1,1,0,0,0,1
"def parse_input(self):
  app_split = self.args[0].split('.')
  app = app_split[0]
  model_name = app_split[1].lower()
  self.model = get_model(app, model_name)
  self.field = self.args[1]","Go through the user input, get/validate some important values.",1,0,1,0,2,1,0,0,1,2
"def search_users(self, user, startAt=0, maxResults=50, includeActive=True, includeInactive=False):
  params = {
  'username': user,
  'includeActive': includeActive,
  'includeInactive': includeInactive}
  return self._fetch_pages(User, None, 'user/search', startAt, maxResults, params)","Get a list of user Resources that match the specified search string. :param user: a string to match usernames, name or email against. :type user: str :param startAt: index of the first user to return. :type startAt: int :param maxResults: maximum number of users to return. If maxResults evaluates as False, it will try to get all items in batches. :type maxResults: int :param includeActive: If true, then active users are included in the results. (Default: True) :type includeActive: bool :param includeInactive: If true, then inactive users are included in the results. (Default: False) :type includeInactive: bool :rtype: ResultList",2,0,0,1,3,2,0,0,1,3
"def update_last_view(self, app_id, attributes):
  if not isinstance(attributes, dict):
  raise TypeError('Must be of type dict')
  attribute_data = json.dumps(attributes)
  return self.transport.PUT(url='/view/app/{}/last'.format(app_id),
  body=attribute_data, type='application/json')",Updates the last view for the active user :param app_id: the app id :param attributes: the body of the request in dictionary format,1,0,0,1,2,1,0,0,2,3
"def ls(github_user, template, long_format):
  github_urls = temple.ls.ls(github_user, template=template)
  for ssh_path, info in github_urls.items():
  if long_format:
  print(ssh_path, '-', info['description'] or '(no project description found)')
  else:
  print(ssh_path)","List packages created with temple. Enter a github user or organization to list all templates under the user or org. Using a template path as the second argument will list all projects that have been started with that template. Use ""-l"" to print the Github repository descriptions of templates or projects.",1,0,0,1,2,1,0,0,1,2
"def capture_update_from_model(cls, table_name, record_id, *, update_fields=()):
  include_cols = ()
  if update_fields:
  model_cls = get_connected_model_for_table_name(table_name)
  include_cols = cls._fieldnames_to_colnames(model_cls, update_fields)
  raw_query = sql.SQL().format(
  schema=sql.Identifier(settings.HEROKU_CONNECT_SCHEMA),
  table_name=sql.Identifier(table_name),
  include_cols=sql.SQL(', ').join(sql.Identifier(col) for col in include_cols),
  )
  params = {'record_id': record_id, 'table_name': table_name}
  result_qs = TriggerLog.objects.raw(raw_query, params)
  return list(result_qs)","Create a fresh update record from the current model state in the database. For read-write connected models, this will lead to the attempted update of the values of a corresponding object in Salesforce. Args: table_name (str): The name of the table backing the connected model (without schema) record_id (int): The primary id of the connected model update_fields (Iterable[str]): If given, the names of fields that will be included in the write record Returns: A list of the created TriggerLog entries (usually one). Raises: LookupError: if ``table_name`` does not belong to a connected model",0,1,1,2,4,0,1,1,1,3
"def get_form(self, form_class=None):
  form = super(LinesUpdateModalBasket, self).get_form(form_class)
  initial = form.initial
  initial['type_tax'] = self.object.product_final.product.tax.pk
  initial['tax'] = self.object.tax_basket
  initial['price'] = float(self.object.price_base_basket) * (1 + (self.object.tax_basket / 100))
  return form","if self.__is_pack: options = [] lang = get_language_database() for option in SalesLineBasketOption.objects.filter(line_budget__pk=self.__line_pk): initial['packs[{}]'.format(option.product_option.pk)] = option.product_final.pk a = { 'id': option.product_option.pk, 'label': getattr(option.product_option, lang).name, 'products': list(option.product_option.products_pack.all().values('pk').annotate(name=F('{}__name'.format(lang)))), 'selected': option.product_final.pk, } options.append(a) # compatibility with GenForeignKey initial['packs'] = json.dumps({'__JSON_DATA__': options})",1,0,1,1,3,1,0,1,1,3
"def add(self, level, message, extra_tags=''):
  if not message:
  return
  level = int(level)
  if level < self.level:
  return
  if level not in stored_messages_settings.STORE_LEVELS or self.user.is_anonymous():
  return super(StorageMixin, self).add(level, message, extra_tags)
  self.added_new = True
  m = self.backend.create_message(level, message, extra_tags)
  self.backend.archive_store([self.user], m)
  self._queued_messages.append(m)","If the message level was configured for being stored and request.user is not anonymous, save it to the database. Otherwise, let some other class handle the message. Notice: controls like checking the message is not empty and the level is above the filter need to be performed here, but it could happen they'll be performed again later if the message does not need to be stored.",1,1,0,1,3,1,1,0,0,2
"def decorate_arguments(self, arguments):
  arguments = list(arguments)
  for i, value in enumerate(arguments):
  is_constraint_file = (i >= 1 and match_option(arguments[i - 1], '-c', '--constraint'))
  is_requirement_file = (i >= 1 and match_option(arguments[i - 1], '-r', '--requirement'))
  if not is_constraint_file and not is_requirement_file and os.path.isfile(value):
  arguments[i] = '%s
  return arguments","Change pathnames of local files into ``file://`` URLs with ``#md5=...`` fragments. :param arguments: The command line arguments to ``pip install ...`` (a list of strings). :returns: A copy of the command line arguments with pathnames of local files rewritten to ``file://`` URLs. When pip-accel calls pip to download missing distribution archives and the user specified the pathname of a local distribution archive on the command line, pip will (by default) *not* copy the archive into the download directory if an archive for the same package name and version is already present. This can lead to the confusing situation where the user specifies a local distribution archive to install, a different (older) archive for the same package and version is present in the download directory and `pip-accel` installs the older archive instead of the newer archive. To avoid this confusing behavior, the :func:`decorate_arguments()` method rewrites the command line arguments given to ``pip install`` so that pathnames of local archives are changed into ``file://`` URLs that include a fragment with the hash of the file's contents. Here's an example: - Local pathname: ``/tmp/pep8-1.6.3a0.tar.gz`` - File URL: ``file:///tmp/pep8-1.6.3a0.tar.gz#md5=19cbf0b633498ead63fb3c66e5f1caf6`` When pip fills the download directory and encounters a previously cached distribution archive it will check the hash, realize the contents have changed and replace the archive in the download directory.",1,0,0,0,1,1,0,0,1,2
"def append_records(self, owner, id, stream_id, body, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('callback'):
  return self.append_records_with_http_info(owner, id, stream_id, body, **kwargs)
  else:
  (data) = self.append_records_with_http_info(owner, id, stream_id, body, **kwargs)
  return data","Append records to a stream. This endpoint appends JSON data to a stream associated with a dataset. Streams don't need to be created before you can append data to it. They will be created on-demand, the first time they are used. Multiple records can be appended at once by using JSON-L (`application/json-l`) as the request content type. Currently, data uploaded to a dataset via a stream is not immediatelly processed. Instead, it is processed automatically at least once a day or as a result of a \""Sync files\"" endpoint invocation. Once processed, the contents of a stream will appear as part of the respective dataset as a `.jsonl` file (e.g. `my-stream` will produce a file named `my-stream.jsonl`). This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please define a `callback` function to be invoked when receiving the response. >>> def callback_function(response): >>> pprint(response) >>> >>> thread = api.append_records(owner, id, stream_id, body, callback=callback_function) :param callback function: The callback function for asynchronous request. (optional) :param str owner: User name and unique identifier of the creator of a dataset or project. For example, in the URL: [https://data.world/jonloyens/an-intro-to-dataworld-dataset](https://data.world/jonloyens/an-intro-to-dataworld-dataset), jonloyens is the unique identifier of the owner. (required) :param str id: Dataset unique identifier. For example, in the URL:[https://data.world/jonloyens/an-intro-to-dataworld-dataset](https://data.world/jonloyens/an-intro-to-dataworld-dataset), an-intro-to-dataworld-dataset is the unique identifier of the dataset. (required) :param str stream_id: Stream unique identifier as defined by the user the first time the stream was used. Only lower case letters, numbers and dashes are allowed. Maximum length of 95 characters. (required) :param object body: (required) :return: None If the method is called asynchronously, returns the request thread.",1,0,0,1,2,1,0,0,1,2
"def get_all(self, callsign, timestamp=timestamp_now):
  callsign_data = self._lookup_callsign(callsign, timestamp)
  try:
  cqz = self._lookuplib.lookup_zone_exception(callsign, timestamp)
  callsign_data[const.CQZ] = cqz
  except KeyError:
  pass
  return callsign_data","Lookup a callsign and return all data available from the underlying database Args: callsign (str): Amateur Radio callsign timestamp (datetime, optional): datetime in UTC (tzinfo=pytz.UTC) Returns: dict: Dictionary containing the callsign specific data Raises: KeyError: Callsign could not be identified Example: The following code returns all available information from the country-files.com database for the callsign ""DH1TW"" >>> from pyhamtools import LookupLib, Callinfo >>> my_lookuplib = LookupLib(lookuptype=""countryfile"") >>> cic = Callinfo(my_lookuplib) >>> cic.get_all(""DH1TW"") { 'country': 'Fed. Rep. of Germany', 'adif': 230, 'continent': 'EU', 'latitude': 51.0, 'longitude': -10.0, 'cqz': 14, 'ituz': 28 } Note: The content of the returned data depends entirely on the injected :py:class:`LookupLib` (and the used database). While the country-files.com provides for example the ITU Zone, Clublog doesn't. Consequently, the item ""ituz"" would be missing with Clublog (API or XML) :py:class:`LookupLib`.",0,0,0,1,1,1,0,0,1,2
"def event_text_key(self, event):
  char = event.char
  if not char or char not in string.ascii_letters:
  return
  converted_char = invert_shift(char)
  log.debug(""convert keycode %s - char %s to %s"", event.keycode, repr(char), converted_char)
  self.text.insert(tkinter.INSERT, converted_char)
  return ""break""","So a ""invert shift"" for user inputs: Convert all lowercase letters to uppercase and vice versa.",1,0,0,1,2,1,0,0,1,2
"def patch_runtime_class(self, name, body, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.patch_runtime_class_with_http_info(name, body, **kwargs)
  else:
  (data) = self.patch_runtime_class_with_http_info(name, body, **kwargs)
  return data","partially update the specified RuntimeClass This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.patch_runtime_class(name, body, async_req=True) >>> result = thread.get() :param async_req bool :param str name: name of the RuntimeClass (required) :param object body: (required) :param str pretty: If 'true', then the output is pretty printed. :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint. This field is required for apply requests (application/apply-patch) but optional for non-apply patch types (JsonPatch, MergePatch, StrategicMergePatch). :param bool force: Force is going to \""force\"" Apply requests. It means user will re-acquire conflicting fields owned by other people. Force flag must be unset for non-apply patch requests. :return: V1beta1RuntimeClass If the method is called asynchronously, returns the request thread.",1,0,0,1,2,1,0,0,1,2
"def service_add(service_rootid=None, service_name=None, triggerid=None, **kwargs):
  conn_args = _login(**kwargs)
  ret = {}
  try:
  if conn_args:
  method = 'service.create'
  params = {'name': service_name}
  params = _params_extend(params, _ignore_name=True, **kwargs)
  params.setdefault('algorithm', 1)
  params.setdefault('showsla', 1)
  params.setdefault('goodsla', 99.7)
  params.setdefault('sortorder', 1)
  if service_rootid:
  params.setdefault('parentid', service_rootid)
  if triggerid:
  params.setdefault('triggerid', triggerid)
  ret = _query(method, params, conn_args['url'], conn_args['auth'])
  return ret['result'] if ret['result'] else False
  else:
  raise KeyError
  except KeyError:
  return ret",".. versionadded:: Fluorine Create service under service with id specified as parameter. .. note:: https://www.zabbix.com/documentation/3.4/manual/api/reference/service/create :param service_rootid: Service id under which service should be added :param service_name: Name of new service :param triggerid: Optional - ID of trigger which should be watched in service :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring) :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring) :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring) :return: Service details, False if service could not be added or on failure. CLI Example: .. code-block:: bash salt '*' zabbix.service_add 11 'My service' 11111",1,0,0,1,2,2,0,0,2,4
"def add_user_to_group(self, username, groupname, raise_on_error=False):
  data = {
  'name': groupname,
  }
  response = self._post(self.rest_url + ""/user/group/direct"",
  params={""username"": username,},
  data=json.dumps(data))
  if response.status_code == 201:
  return True
  if raise_on_error:
  raise RuntimeError(response.json()['message'])
  return False","Add a user to a group :param username: The username to assign to the group :param groupname: The group name into which to assign the user :return: True on success, False on failure.",2,1,0,1,4,1,0,0,2,3
"def auth_interactive(self, username, handler, submethods=""""):
  if (not self.active) or (not self.initial_kex_done):
  raise SSHException(""No existing session"")
  my_event = threading.Event()
  self.auth_handler = AuthHandler(self)
  self.auth_handler.auth_interactive(
  username, handler, my_event, submethods
  )
  return self.auth_handler.wait_for_response(my_event)","Authenticate to the server interactively. A handler is used to answer arbitrary questions from the server. On many servers, this is just a dumb wrapper around PAM. This method will block until the authentication succeeds or fails, peroidically calling the handler asynchronously to get answers to authentication questions. The handler may be called more than once if the server continues to ask questions. The handler is expected to be a callable that will handle calls of the form: ``handler(title, instructions, prompt_list)``. The ``title`` is meant to be a dialog-window title, and the ``instructions`` are user instructions (both are strings). ``prompt_list`` will be a list of prompts, each prompt being a tuple of ``(str, bool)``. The string is the prompt and the boolean indicates whether the user text should be echoed. A sample call would thus be: ``handler('title', 'instructions', [('Password:', False)])``. The handler should return a list or tuple of answers to the server's questions. If the server requires multi-step authentication (which is very rare), this method will return a list of auth types permissible for the next step. Otherwise, in the normal case, an empty list is returned. :param str username: the username to authenticate as :param callable handler: a handler for responding to server questions :param str submethods: a string list of desired submethods (optional) :return: list of auth types permissible for the next stage of authentication (normally empty). :raises: `.BadAuthenticationType` -- if public-key authentication isn't allowed by the server for this user :raises: `.AuthenticationException` -- if the authentication failed :raises: `.SSHException` -- if there was a network error .. versionadded:: 1.5",2,0,0,1,3,1,0,0,1,2
"def add_subjects_to_account_group(self, account_id, group_id, body, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('asynchronous'):
  return self.add_subjects_to_account_group_with_http_info(account_id, group_id, body, **kwargs)
  else:
  (data) = self.add_subjects_to_account_group_with_http_info(account_id, group_id, body, **kwargs)
  return data","Add members to a group. # noqa: E501 An endpoint for adding users and API keys to groups. **Example usage:** `curl -X POST https://api.us-east-1.mbedcloud.com/v3/accounts/{accountID}/policy-groups/{groupID} -d '{\""users\"": [0162056a9a1586f30242590700000000,0117056a9a1586f30242590700000000]\""}' -H 'content-type: application/json' -H 'Authorization: Bearer API_KEY'` # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass asynchronous=True >>> thread = api.add_subjects_to_account_group(account_id, group_id, body, asynchronous=True) >>> result = thread.get() :param asynchronous bool :param str account_id: Account ID. (required) :param str group_id: The ID of the group to be updated. (required) :param SubjectList body: A list of users and API keys to be added to the group. (required) :return: UpdatedResponse If the method is called asynchronously, returns the request thread.",1,0,0,2,3,1,0,0,1,2
"def get_scopes_for(self, user_provided_scopes):
  if user_provided_scopes is None:
  user_provided_scopes = [app_part for app_part in self._oauth_scopes]
  elif isinstance(user_provided_scopes, str):
  user_provided_scopes = [user_provided_scopes]
  if not isinstance(user_provided_scopes, (list, tuple)):
  raise ValueError(
  ""'user_provided_scopes' must be a list or a tuple of strings"")
  scopes = set()
  for app_part in user_provided_scopes:
  for scope in self._oauth_scopes.get(app_part, [(app_part,)]):
  scopes.add(self._prefix_scope(scope))
  return list(scopes)","Returns a list of scopes needed for each of the scope_helpers provided, by adding the prefix to them if required :param user_provided_scopes: a list of scopes or scope helpers :type user_provided_scopes: list or tuple or str :return: scopes with url prefix added :rtype: list :raises ValueError: if unexpected datatype of scopes are passed",1,0,0,1,2,1,0,0,1,2
"def move_users(self, user_id_list, group_id):
  return self.post(
  url=""https://api.weixin.qq.com/cgi-bin/groups/members/batchupdate"",
  data={
  ""openid_list"": user_id_list,
  ""to_groupid"": group_id
  }
  )", :param user_id_list:  ID 50 :param group_id:  ID :return:  JSON ,1,0,0,2,3,1,0,0,2,3
"def _GetVisitSource(self, visit_identifier, cache, database):
  sync_cache_results = cache.GetResults('sync')
  if not sync_cache_results:
  result_set = database.Query(self._SYNC_CACHE_QUERY)
  cache.CacheQueryResults(result_set, 'sync', 'id', ('source',))
  sync_cache_results = cache.GetResults('sync')
  if sync_cache_results and visit_identifier:
  results = sync_cache_results.get(visit_identifier, None)
  if results:
  return results[0]
  return None",Retrieves a visit source type based on the identifier. Args: visit_identifier (str): identifier from the visits table for the particular record. cache (SQLiteCache): cache which contains cached results from querying the visit_source table. database (SQLiteDatabase): database. Returns: int: visit source type or None if no visit source type was found for the identifier.,1,0,1,1,3,1,0,1,1,3
"def group_required(self, group):
  def decorator(view):
  @functools.wraps(view)
  def decorated(*args, **kwargs):
  log.info(""Trying to get access to resource: %s protected by group: %s"", view.__name__, group)
  if request.method == 'POST':
  token = request.form['token']
  if self.development or self.group_authenticated(token, group):
  return view(*args, **kwargs)
  else:
  log.warning(""User has not been authorized to get access to resource: %s"", view.__name__)
  else:
  log.error(""Bad request type! Expected 'POST', actual '%s'"", request.method)
  return abort(403)
  return decorated
  return decorator",Decorator which checks if user is in group Decorator for Flask's view which blocks requests from not authenticated users or if user is not member of specified group :param group: group's name,2,0,1,2,5,1,0,0,1,2
"def get_all_course_submission_information(course_id, item_type, read_replica=True):
  submission_qs = Submission.objects
  if read_replica:
  submission_qs = _use_read_replica(submission_qs)
  query = submission_qs.select_related('student_item__scoresummary__latest__submission').filter(
  student_item__course_id=course_id,
  student_item__item_type=item_type,
  ).iterator()
  for submission in query:
  student_item = submission.student_item
  serialized_score = {}
  if hasattr(student_item, 'scoresummary'):
  latest_score = student_item.scoresummary.latest
  if (not latest_score.is_hidden()) and latest_score.submission.uuid == submission.uuid:
  serialized_score = ScoreSerializer(latest_score).data
  yield (
  StudentItemSerializer(student_item).data,
  SubmissionSerializer(submission).data,
  serialized_score
  )","For the given course, get all student items of the given item type, all the submissions for those itemes, and the latest scores for each item. If a submission was given a score that is not the latest score for the relevant student item, it will still be included but without score. Args: course_id (str): The course that we are getting submissions from. item_type (str): The type of items that we are getting submissions for. read_replica (bool): Try to use the database's read replica if it's available. Yields: A tuple of three dictionaries representing: (1) a student item with the following fields: student_id course_id student_item item_type (2) a submission with the following fields: student_item attempt_number submitted_at created_at answer (3) a score with the following fields, if one exists and it is the latest score: (if both conditions are not met, an empty dict is returned here) student_item submission points_earned points_possible created_at submission_uuid",2,0,1,1,4,1,0,1,1,3
"def _cookiecutter_configs_have_changed(template, old_version, new_version):
  temple.check.is_git_ssh_path(template)
  repo_path = temple.utils.get_repo_path(template)
  github_client = temple.utils.GithubClient()
  api = '/repos/{}/contents/cookiecutter.json'.format(repo_path)
  old_config_resp = github_client.get(api, params={'ref': old_version})
  old_config_resp.raise_for_status()
  new_config_resp = github_client.get(api, params={'ref': new_version})
  new_config_resp.raise_for_status()
  return old_config_resp.json()['content'] != new_config_resp.json()['content']","Given an old version and new version, check if the cookiecutter.json files have changed When the cookiecutter.json files change, it means the user will need to be prompted for new context Args: template (str): The git SSH path to the template old_version (str): The git SHA of the old version new_version (str): The git SHA of the new version Returns: bool: True if the cookiecutter.json files have been changed in the old and new versions",1,0,0,0,1,1,0,0,1,2
"def answer_pre_checkout_query(self, pre_checkout_query_id, ok, error_message=None):
  assert_type_or_raise(pre_checkout_query_id, unicode_type, parameter_name=""pre_checkout_query_id"")
  assert_type_or_raise(ok, bool, parameter_name=""ok"")
  assert_type_or_raise(error_message, None, unicode_type, parameter_name=""error_message"")
  result = self.do(""answerPreCheckoutQuery"", pre_checkout_query_id=pre_checkout_query_id, ok=ok, error_message=error_message)
  if self.return_python_objects:
  logger.debug(""Trying to parse {data}"".format(data=repr(result)))
  try:
  return from_array_list(bool, result, list_level=0, is_builtin=True)
  except TgApiParseException:
  logger.debug(""Failed parsing as primitive bool"", exc_info=True)
  raise TgApiParseException(""Could not parse result."")
  return result","Once the user has confirmed their payment and shipping details, the Bot API sends the final confirmation in the form of an Update with the field pre_checkout_query. Use this method to respond to such pre-checkout queries. On success, True is returned. Note: The Bot API must receive an answer within 10 seconds after the pre-checkout query was sent. https://core.telegram.org/bots/api#answerprecheckoutquery Parameters: :param pre_checkout_query_id: Unique identifier for the query to be answered :type pre_checkout_query_id: str|unicode :param ok: Specify True if everything is alright (goods are available, etc.) and the bot is ready to proceed with the order. Use False if there are any problems. :type ok: bool Optional keyword parameters: :param error_message: Required if ok is False. Error message in human readable form that explains the reason for failure to proceed with the checkout (e.g. ""Sorry, somebody just bought the last of our amazing black T-shirts while you were busy filling out your payment details. Please choose a different color or garment!""). Telegram will display this message to the user. :type error_message: str|unicode Returns: :return: On success, True is returned :rtype: bool",1,0,0,1,2,1,0,0,1,2
"def get_picture(self, login=None, **kwargs):
  _login = kwargs.get(
  'login',
  login or self._login
  )
  _activities_url = PICTURE_URL.format(login=_login)
  return self._request_api(url=_activities_url).content",Get a user's picture. :param str login: Login of the user to check :return: JSON,2,0,0,1,3,2,0,0,1,3
"def get_reply_to_names(self, mention):
  mention_list = [user['screen_name'] for user in mention['entities']['user_mentions']]
  mention_list.append(mention['user']['screen_name'])
  reply_to_names = set(mention_list)
  reply_to_names.discard(self.screen_name)
  return sorted(list(reply_to_names))","Get a sorted list of unique usernames mentioned in the message, excluding the bot's own name :param mention: JSON mention object from twitter :return: list of usernames",1,0,0,1,2,1,0,0,1,2
"def write_keywords(layer, keywords):
  if not isinstance(layer, QgsMapLayer):
  raise Exception(
  tr('The layer is not a QgsMapLayer : {type}').format(
  type=type(layer)))
  source = layer.source()
  write_iso19115_metadata(source, keywords)","Write keywords for a datasource. This is a wrapper method that will 'do the right thing' to store keywords for the given datasource. In particular, if the datasource is remote (e.g. a database connection) it will write the keywords from the keywords store. :param layer: A QGIS QgsMapLayer instance. :type layer: qgis.core.QgsMapLayer :param keywords: A dict containing all the keywords to be written for the layer. :type keywords: dict :raises: UnsupportedProviderError",1,0,0,0,1,1,0,0,0,1
"def pin_chat_message(chat_id, message_id, disable_notification=None, **kwargs):
  params = dict(
  chat_id=chat_id,
  message_id=message_id
  )
  params.update(
  _clean_params(
  disable_notification=disable_notification,
  )
  )
  return TelegramBotRPCRequest('pinChatMessage', params=params, on_result=lambda result: result, **kwargs)","Use this method to pin a message in a supergroup or a channel. The bot must be an administrator in the chat for this to work and must have the can_pin_messages admin right in the supergroup or can_edit_messages admin right in the channel. :param chat_id: Unique identifier for the target chat or username of the target channel (in the format @channelusername) :param message_id: Identifier of a message to pin :param disable_notification: Pass True, if it is not necessary to send a notification to all chat members about the new pinned message. Notifications are always disabled in channels. :param kwargs: Args that get passed down to :class:`TelegramBotRPCRequest` :return: Returns True on success. :rtype: bool",1,0,0,2,3,1,0,0,2,3
"def get_credential(self, service, username):
  if username is not None:
  password = self.get_password(service, username)
  if password is not None:
  return credentials.SimpleCredential(
  username,
  password,
  )
  return None",Gets the username and password for the service. Returns a Credential instance. The *username* argument is optional and may be omitted by the caller or ignored by the backend. Callers must use the returned username.,1,0,0,1,2,1,0,0,1,2
"def get_permission_requests(parser, token):
  return PermissionsForObjectNode.handle_token(parser, token,
  approved=False,
  name='""permission_requests""')","Retrieves all permissions requests associated with the given obj and user and assigns the result to a context variable. Syntax:: {% get_permission_requests obj %} {% for perm in permissions %} {{ perm }} {% endfor %} {% get_permission_requests obj as ""my_permissions"" %} {% get_permission_requests obj for request.user as ""my_permissions"" %}",1,0,0,1,2,1,0,0,1,2
"def fit(
  self,
  interactions,
  user_features=None,
  item_features=None,
  sample_weight=None,
  epochs=1,
  num_threads=1,
  verbose=False,
  ):
  self._reset_state()
  return self.fit_partial(
  interactions,
  user_features=user_features,
  item_features=item_features,
  sample_weight=sample_weight,
  epochs=epochs,
  num_threads=num_threads,
  verbose=verbose,
  )","Fit the model. For details on how to use feature matrices, see the documentation on the :class:`lightfm.LightFM` class. Arguments --------- interactions: np.float32 coo_matrix of shape [n_users, n_items] the matrix containing user-item interactions. Will be converted to numpy.float32 dtype if it is not of that type. user_features: np.float32 csr_matrix of shape [n_users, n_user_features], optional Each row contains that user's weights over features. item_features: np.float32 csr_matrix of shape [n_items, n_item_features], optional Each row contains that item's weights over features. sample_weight: np.float32 coo_matrix of shape [n_users, n_items], optional matrix with entries expressing weights of individual interactions from the interactions matrix. Its row and col arrays must be the same as those of the interactions matrix. For memory efficiency its possible to use the same arrays for both weights and interaction matrices. Defaults to weight 1.0 for all interactions. Not implemented for the k-OS loss. epochs: int, optional number of epochs to run num_threads: int, optional Number of parallel computation threads to use. Should not be higher than the number of physical cores. verbose: bool, optional whether to print progress messages. If `tqdm` is installed, a progress bar will be displayed instead. Returns ------- LightFM instance the fitted model",1,0,0,1,2,1,0,0,1,2
"def register(self, user_data, base_confirm_url='', send_welcome=True):
  user = self.__model__(**user_data)
  schema = RegisterSchema()
  valid = schema.process(user)
  if not valid:
  return valid
  db.session.add(user)
  db.session.commit()
  if not user.id:
  return False
  if send_welcome:
  self.send_welcome_message(user, base_confirm_url)
  events.register_event.send(user)
  return user","Register user Accepts user data, validates it and performs registration. Will send a welcome message with a confirmation link on success. :param user_data: dic, populate user with data :param send_welcome: bool, whether to send welcome or skip it (testing) :param base_confirm_url: str, base confirmation link url :return: boiler.user.models.User",1,1,0,1,3,1,1,1,0,3
"def transfer_request_notifications(user):
  orgs = [o for o in user.organizations if o.is_member(user)]
  notifications = []
  qs = Transfer.objects(recipient__in=[user] + orgs, status='pending')
  qs = qs.only('id', 'created', 'subject')
  for transfer in qs.no_dereference():
  notifications.append((transfer.created, {
  'id': transfer.id,
  'subject': {
  'class': transfer.subject['_cls'].lower(),
  'id': transfer.subject['_ref'].id
  }
  }))
  return notifications",Notify user about pending transfer requests,1,0,1,1,3,1,0,1,1,3
"def missing(self, field):
  try:
  matches = self._find_by_field(field, print_found=False)
  except AssertionError:
  return
  for found in matches:
  self.log_json(
  found[""reality""],
  ""\n\nExpected '%s' to not exist, but it is:"" % (field),
  )
  raise AssertionError(
  ""Expected '%s' to not exist, but it does."" % (field)
  )","*Asserts the field does not exist.* The field consists of parts separated by spaces, the parts being object property names or array indices starting from 0, and the root being the instance created by the last request (see `Output` for it). For asserting deeply nested properties or multiple objects at once, [http://goessner.net/articles/JsonPath|JSONPath] can be used with [https://github.com/h2non/jsonpath-ng#jsonpath-syntax|supported JSONPath expressions], the root being the response body. *Examples* | `GET` | /users/1 | # https://jsonplaceholder.typicode.com/users/1 | | `Missing` | response body password | | `Missing` | $.password | | `Missing` | $..geo.elevation | # response body address geo elevation | | `GET` | /users | # https://jsonplaceholder.typicode.com/users | | `Missing` | response body 0 password | | `Missing` | $[*].password | | `Missing` | $[*]..geo.elevation |",2,0,0,0,2,1,0,0,1,2
"def sip(self, sip_url, username=None, password=None, url=None, method=None,
  status_callback_event=None, status_callback=None,
  status_callback_method=None, **kwargs):
  return self.nest(Sip(
  sip_url,
  username=username,
  password=password,
  url=url,
  method=method,
  status_callback_event=status_callback_event,
  status_callback=status_callback,
  status_callback_method=status_callback_method,
  **kwargs
  ))",Create a <Sip> element :param sip_url: SIP URL :param username: SIP Username :param password: SIP Password :param url: Action URL :param method: Action URL method :param status_callback_event: Status callback events :param status_callback: Status callback URL :param status_callback_method: Status callback URL method :param kwargs: additional attributes :returns: <Sip> element,1,0,0,1,2,2,0,0,2,4
"def submit(self, txn, timeout=None):
  url = ENDPOINT_SUBMIT.format(self._url).encode()
  data = txn._marshal()
  obj = yield self._post(url, data, timeout)
  header, responses = validate_client_submit_response(obj)
  if obj.get(u'succeeded', False):
  returnValue(Success(header, responses))
  else:
  raise Failed(header, responses)","Submit a transaction. Processes multiple requests in a single transaction. A transaction increments the revision of the key-value store and generates events with the same revision for every completed request. It is not allowed to modify the same key several times within one transaction. From google paxosdb paper: Our implementation hinges around a powerful primitive which we call MultiOp. All other database operations except for iteration are implemented as a single call to MultiOp. A MultiOp is applied atomically and consists of three components: 1. A list of tests called guard. Each test in guard checks a single entry in the database. It may check for the absence or presence of a value, or compare with a given value. Two different tests in the guard may apply to the same or different entries in the database. All tests in the guard are applied and MultiOp returns the results. If all tests are true, MultiOp executes t op (see item 2 below), otherwise it executes f op (see item 3 below). 2. A list of database operations called t op. Each operation in the list is either an insert, delete, or lookup operation, and applies to a single database entry. Two different operations in the list may apply to the same or different entries in the database. These operations are executed if guard evaluates to true. 3. A list of database operations called f op. Like t op, but executed if guard evaluates to false. :param txn: The transaction to submit. :type txn: instance of :class:`txaioetcd.Transaction` :param timeout: Request timeout in seconds. :type timeout: int :returns: An instance of :class:`txaioetcd.Success` or an exception of :class:`txioetcd.Failed` or :class:`txaioetcd.Error` :rtype: instance of :class:`txaioetcd.Success`, :class:`txaioetcd.Failed` or :class:`txaioetcd.Error`",1,1,1,2,5,1,0,0,1,2
"def get_users_of_group(self, group_id, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('asynchronous'):
  return self.get_users_of_group_with_http_info(group_id, **kwargs)
  else:
  (data) = self.get_users_of_group_with_http_info(group_id, **kwargs)
  return data","Get users of a group. # noqa: E501 An endpoint for listing the users of a group with details. **Example usage:** `curl https://api.us-east-1.mbedcloud.com/v3/policy-groups/{group-id}/users -H 'Authorization: Bearer API_KEY'` # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass asynchronous=True >>> thread = api.get_users_of_group(group_id, asynchronous=True) >>> result = thread.get() :param asynchronous bool :param str group_id: The ID of the group whose users are retrieved. (required) :param int limit: The number of results to return (2-1000), default is 50. :param str after: The entity ID to fetch after the given one. :param str order: The order of the records based on creation time, ASC or DESC; by default ASC :param str include: Comma separated additional data to return. Currently supported: total_count :param str status__eq: An optional filter for getting users by status. :param str status__in: An optional filter for getting users with a specified set of statuses. :param str status__nin: An optional filter for excluding users with a specified set of statuses. :return: UserInfoRespList If the method is called asynchronously, returns the request thread.",2,0,0,1,3,2,0,0,1,3
"def copy(source, dest, name=None, shallow=False, without_attrs=False, log=None,
  if_exists='raise', dry_run=False, **create_kws):
  _check_dest_is_group(dest)
  with _LogWriter(log) as log:
  n_copied, n_skipped, n_bytes_copied = _copy(
  log, source, dest, name=name, root=True, shallow=shallow,
  without_attrs=without_attrs, if_exists=if_exists, dry_run=dry_run,
  **create_kws
  )
  _log_copy_summary(log, dry_run, n_copied, n_skipped, n_bytes_copied)
  return n_copied, n_skipped, n_bytes_copied","Copy the `source` array or group into the `dest` group. Parameters ---------- source : group or array/dataset A zarr group or array, or an h5py group or dataset. dest : group A zarr or h5py group. name : str, optional Name to copy the object to. shallow : bool, optional If True, only copy immediate children of `source`. without_attrs : bool, optional Do not copy user attributes. log : callable, file path or file-like object, optional If provided, will be used to log progress information. if_exists : {'raise', 'replace', 'skip', 'skip_initialized'}, optional How to handle arrays that already exist in the destination group. If 'raise' then a CopyError is raised on the first array already present in the destination group. If 'replace' then any array will be replaced in the destination. If 'skip' then any existing arrays will not be copied. If 'skip_initialized' then any existing arrays with all chunks initialized will not be copied (not available when copying to h5py). dry_run : bool, optional If True, don't actually copy anything, just log what would have happened. **create_kws Passed through to the create_dataset method when copying an array/dataset. Returns ------- n_copied : int Number of items copied. n_skipped : int Number of items skipped. n_bytes_copied : int Number of bytes of data that were actually copied. Examples -------- Here's an example of copying a group named 'foo' from an HDF5 file to a Zarr group:: >>> import h5py >>> import zarr >>> import numpy as np >>> source = h5py.File('data/example.h5', mode='w') >>> foo = source.create_group('foo') >>> baz = foo.create_dataset('bar/baz', data=np.arange(100), chunks=(50,)) >>> spam = source.create_dataset('spam', data=np.arange(100, 200), chunks=(30,)) >>> zarr.tree(source) /  foo   bar   baz (100,) int64  spam (100,) int64 >>> dest = zarr.group() >>> from sys import stdout >>> zarr.copy(source['foo'], dest, log=stdout) copy /foo copy /foo/bar copy /foo/bar/baz (100,) int64 all done: 3 copied, 0 skipped, 800 bytes copied (3, 0, 800) >>> dest.tree() # N.B., no spam /  foo  bar  baz (100,) int64 >>> source.close() The ``if_exists`` parameter provides options for how to handle pre-existing data in the destination. Here are some examples of these options, also using ``dry_run=True`` to find out what would happen without actually copying anything:: >>> source = zarr.group() >>> dest = zarr.group() >>> baz = source.create_dataset('foo/bar/baz', data=np.arange(100)) >>> spam = source.create_dataset('foo/spam', data=np.arange(1000)) >>> existing_spam = dest.create_dataset('foo/spam', data=np.arange(1000)) >>> from sys import stdout >>> try: ... zarr.copy(source['foo'], dest, log=stdout, dry_run=True) ... except zarr.CopyError as e: ... print(e) ... copy /foo copy /foo/bar copy /foo/bar/baz (100,) int64 an object 'spam' already exists in destination '/foo' >>> zarr.copy(source['foo'], dest, log=stdout, if_exists='replace', dry_run=True) copy /foo copy /foo/bar copy /foo/bar/baz (100,) int64 copy /foo/spam (1000,) int64 dry run: 4 copied, 0 skipped (4, 0, 0) >>> zarr.copy(source['foo'], dest, log=stdout, if_exists='skip', dry_run=True) copy /foo copy /foo/bar copy /foo/bar/baz (100,) int64 skip /foo/spam (1000,) int64 dry run: 3 copied, 1 skipped (3, 1, 0) Notes ----- Please note that this is an experimental feature. The behaviour of this function is still evolving and the default behaviour and/or parameters may change in future versions.",1,0,0,1,2,1,0,0,0,1
"def integer_entry(self, prompt, message=None, min=None, max=None, rofi_args=None, **kwargs):
  if (min is not None) and (max is not None) and not (max > min):
  raise ValueError(""Maximum limit has to be more than the minimum limit."")
  def integer_validator(text):
  error = None
  try:
  value = int(text)
  except ValueError:
  return None, ""Please enter an integer value.""
  if (min is not None) and (value < min):
  return None, ""The minimum allowable value is {0:d}."".format(min)
  if (max is not None) and (value > max):
  return None, ""The maximum allowable value is {0:d}."".format(max)
  return value, None
  return self.generic_entry(prompt, integer_validator, message, rofi_args, **kwargs)","Prompt the user to enter an integer. Parameters ---------- prompt: string Prompt to display to the user. message: string, optional Message to display under the entry line. min, max: integer, optional Minimum and maximum values to allow. If None, no limit is imposed. Returns ------- integer, or None if the dialog is cancelled.",1,0,0,1,2,1,0,0,1,2
"def locale(self) -> tornado.locale.Locale:
  if not hasattr(self, ""_locale""):
  loc = self.get_user_locale()
  if loc is not None:
  self._locale = loc
  else:
  self._locale = self.get_browser_locale()
  assert self._locale
  return self._locale","The locale for the current session. Determined by either `get_user_locale`, which you can override to set the locale based on, e.g., a user preference stored in a database, or `get_browser_locale`, which uses the ``Accept-Language`` header. .. versionchanged: 4.1 Added a property setter.",0,0,0,1,1,1,0,0,1,2
"def get_data_sharing_consent(username, enterprise_customer_uuid, course_id=None, program_uuid=None):
  EnterpriseCustomer = apps.get_model('enterprise', 'EnterpriseCustomer')
  try:
  if course_id:
  return get_course_data_sharing_consent(username, course_id, enterprise_customer_uuid)
  return get_program_data_sharing_consent(username, program_uuid, enterprise_customer_uuid)
  except EnterpriseCustomer.DoesNotExist:
  return None","Get the data sharing consent object associated with a certain user, enterprise customer, and other scope. :param username: The user that grants consent :param enterprise_customer_uuid: The consent requester :param course_id (optional): A course ID to which consent may be related :param program_uuid (optional): A program to which consent may be related :return: The data sharing consent object, or None if the enterprise customer for the given UUID does not exist.",1,0,1,1,3,1,0,1,1,3
"def commit(self):
  self._check_state()
  database = self._session._database
  api = database.spanner_api
  metadata = _metadata_with_prefix(database.name)
  txn_options = TransactionOptions(read_write=TransactionOptions.ReadWrite())
  response = api.commit(
  self._session.name,
  self._mutations,
  single_use_transaction=txn_options,
  metadata=metadata,
  )
  self.committed = _pb_timestamp_to_datetime(response.commit_timestamp)
  return self.committed",Commit mutations to the database. :rtype: datetime :returns: timestamp of the committed changes.,0,0,0,2,2,1,1,0,1,3
"def maybe_canonicalize_exe_path(exe_name, iocontext):
  has_sep = (os.path.sep in exe_name
  or (os.path.altsep is not None and os.path.altsep in exe_name))
  if has_sep and iocontext.dir is not None and not os.path.isabs(exe_name):
  return os.path.realpath(exe_name)
  else:
  return exe_name","There's a tricky interaction between exe paths and `dir`. Exe paths can be relative, and so we have to ask: Is an exe path interpreted relative to the parent's cwd, or the child's? The answer is that it's platform dependent! >.< (Windows uses the parent's cwd, but because of the fork-chdir-exec pattern, Unix usually uses the child's.) We want to use the parent's cwd consistently, because that saves the caller from having to worry about whether `dir` will have side effects, and because it's easy for the caller to use path.join if they want to. That means that when `dir` is in use, we need to detect exe names that are relative paths, and absolutify them. We want to do that as little as possible though, both because canonicalization can fail, and because we prefer to let the caller control the child's argv[0]. We never want to absolutify a name like ""emacs"", because that's probably a program in the PATH rather than a local file. So we look for slashes in the name to determine what's a filepath and what isn't. Note that anything given as a Path will always have a slash by the time we get here, because stringify_with_dot_if_path prepends a ./ to them when they're relative. This leaves the case where Windows users might pass a local file like ""foo.bat"" as a string, which we can't distinguish from a global program name. However, because the Windows has the preferred ""relative to parent's cwd"" behavior already, this case actually works without our help. (The thing Windows users have to watch out for instead is local files shadowing global program names, which I don't think we can or should prevent.)",0,0,0,0,0,1,0,0,1,2
"def get_users_indexed_by_lang():
  result = {}
  users = TransUser.objects.filter(active=True).select_related('user')
  for user in users:
  for lang in user.languages.all():
  if lang.code not in result:
  result[lang.code] = set()
  result[lang.code].add(user)
  return result",Return all the translator users indexed by lang :return:,1,0,1,1,3,1,0,1,1,3
"def can_create_catalog_with_record_types(self, catalog_record_types):
  if self._catalog_session is not None:
  return self._catalog_session.can_create_catalog_with_record_types(catalog_record_types=catalog_record_types)
  return True","Tests if this user can create a single ``Catalog`` using the desired record types. While ``CatalogingManager.getCatalogRecordTypes()`` can be used to examine which records are supported, this method tests which record(s) are required for creating a specific ``Catalog``. Providing an empty array tests if a ``Catalog`` can be created with no records. arg: catalog_record_types (osid.type.Type[]): array of catalog record types return: (boolean) - ``true`` if ``Catalog`` creation using the specified record ``Types`` is supported, ``false`` otherwise raise: NullArgument - ``catalog_record_types`` is ``null`` *compliance: mandatory -- This method must be implemented.*",1,0,0,1,2,2,0,0,1,3
"def create_default_database(reset: bool = False) -> GraphDatabaseInterface:
  import sqlalchemy
  from sqlalchemy.ext.declarative import declarative_base
  from sqlalchemy.orm import sessionmaker
  from sqlalchemy.pool import StaticPool
  Base = declarative_base()
  engine = sqlalchemy.create_engine(""sqlite:///SpotifyArtistGraph.db"", poolclass=StaticPool)
  Session = sessionmaker(bind=engine)
  dbi: GraphDatabaseInterface = create_graph_database_interface(
  sqlalchemy, Session(), Base, sqlalchemy.orm.relationship
  )
  if reset:
  Base.metadata.drop_all(engine)
  Base.metadata.create_all(engine)
  return dbi",Creates and returns a default SQLAlchemy database interface to use. Arguments: reset (bool): Whether to reset the database if it happens to exist already.,1,1,0,0,2,0,1,1,0,2
"def user_warning(self, message, caption='Warning!'):
  dlg = wx.MessageDialog(self, message, caption,
  wx.OK | wx.CANCEL | wx.ICON_WARNING)
  if self.show_dlg(dlg) == wx.ID_OK:
  continue_bool = True
  else:
  continue_bool = False
  dlg.Destroy()
  return continue_bool","Shows a dialog that warns the user about some action Parameters ---------- message : message to display to user caption : title for dialog (default: ""Warning!"") Returns ------- continue_bool : True or False",1,0,0,1,2,1,0,0,1,2
"def increase_posts_count(sender, instance, **kwargs):
  if instance.poster is None:
  return
  profile, dummy = ForumProfile.objects.get_or_create(user=instance.poster)
  increase_posts_count = False
  if instance.pk:
  try:
  old_instance = instance.__class__._default_manager.get(pk=instance.pk)
  except ObjectDoesNotExist:
  increase_posts_count = True
  old_instance = None
  if old_instance and old_instance.approved is False and instance.approved is True:
  increase_posts_count = True
  elif instance.approved:
  increase_posts_count = True
  if increase_posts_count:
  profile.posts_count = F('posts_count') + 1
  profile.save()",Increases the member's post count after a post save. This receiver handles the update of the profile related to the user who is the poster of the forum post being created or updated.,0,1,1,1,3,1,1,1,1,4
"def list_upgrades(refresh=True, **kwargs):
  if salt.utils.data.is_true(refresh):
  refresh_db(full=True)
  upgrades = {}
  lines = __salt__['cmd.run_stdout'](""/bin/pkg list -Huv"").splitlines()
  for line in lines:
  upgrades[_ips_get_pkgname(line)] = _ips_get_pkgversion(line)
  return upgrades","Lists all packages available for update. When run in global zone, it reports only upgradable packages for the global zone. When run in non-global zone, it can report more upgradable packages than ``pkg update -vn``, because ``pkg update`` hides packages that require newer version of ``pkg://solaris/entire`` (which means that they can be upgraded only from the global zone). If ``pkg://solaris/entire`` is found in the list of upgrades, then the global zone should be updated to get all possible updates. Use ``refresh=True`` to refresh the package database. refresh : True Runs a full package database refresh before listing. Set to ``False`` to disable running the refresh. .. versionchanged:: 2017.7.0 In previous versions of Salt, ``refresh`` defaulted to ``False``. This was changed to default to ``True`` in the 2017.7.0 release to make the behavior more consistent with the other package modules, which all default to ``True``. CLI Example: .. code-block:: bash salt '*' pkg.list_upgrades salt '*' pkg.list_upgrades refresh=False",1,0,0,1,2,1,0,0,1,2
"def fetchUserInfo(self, *user_ids):
  threads = self.fetchThreadInfo(*user_ids)
  users = {}
  for id_, thread in threads.items():
  if thread.type == ThreadType.USER:
  users[id_] = thread
  else:
  raise FBchatUserError(""Thread {} was not a user"".format(thread))
  return users","Get users' info from IDs, unordered .. warning:: Sends two requests, to fetch all available info! :param user_ids: One or more user ID(s) to query :return: :class:`models.User` objects, labeled by their ID :rtype: dict :raises: FBchatException if request failed",2,0,0,1,3,2,0,0,1,3
"def _get_base_model(self):
  if self.model_name == 'inception_v3':
  return InceptionV3(weights='imagenet', include_top=False)
  elif self.model_name == 'xception':
  return Xception(weights='imagenet', include_top=False)
  elif self.model_name == 'vgg16':
  return VGG16(weights='imagenet', include_top=False)
  elif self.model_name == 'vgg19':
  return VGG19(weights='imagenet', include_top=False)
  elif self.model_name == 'resnet50':
  return ResNet50(weights='imagenet', include_top=False)
  else:
  raise ValueError('Cannot find base model %s' % self.model_name)",:return: base model from Keras based on user-supplied model name,0,0,0,1,1,1,0,0,1,2
"def patch_namespaced_deployment(self, name, namespace, body, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.patch_namespaced_deployment_with_http_info(name, namespace, body, **kwargs)
  else:
  (data) = self.patch_namespaced_deployment_with_http_info(name, namespace, body, **kwargs)
  return data","partially update the specified Deployment This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.patch_namespaced_deployment(name, namespace, body, async_req=True) >>> result = thread.get() :param async_req bool :param str name: name of the Deployment (required) :param str namespace: object name and auth scope, such as for teams and projects (required) :param object body: (required) :param str pretty: If 'true', then the output is pretty printed. :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint. This field is required for apply requests (application/apply-patch) but optional for non-apply patch types (JsonPatch, MergePatch, StrategicMergePatch). :param bool force: Force is going to \""force\"" Apply requests. It means user will re-acquire conflicting fields owned by other people. Force flag must be unset for non-apply patch requests. :return: V1Deployment If the method is called asynchronously, returns the request thread.",2,0,0,2,4,1,0,0,1,2
"def get_log_events(self, login=None, **kwargs):
  _login = kwargs.get(
  'login',
  login
  )
  log_events_url = GSA_EVENTS_URL.format(login=_login)
  return self._request_api(url=log_events_url).json()",Get a user's log events. :param str login: User's login (Default: self._login) :return: JSON,2,0,0,1,3,2,0,0,1,3
"def get_all(self, category='', count=-1, fields='', filter='', padding=0, query='', reference_uri='',
  sort='', start=0, user_query='', view=''):
  uri = self.URI + '?'
  uri += self.__list_or_str_to_query(category, 'category')
  uri += self.__list_or_str_to_query(fields, 'fields')
  uri += self.__list_or_str_to_query(filter, 'filter')
  uri += self.__list_or_str_to_query(padding, 'padding')
  uri += self.__list_or_str_to_query(query, 'query')
  uri += self.__list_or_str_to_query(reference_uri, 'referenceUri')
  uri += self.__list_or_str_to_query(sort, 'sort')
  uri += self.__list_or_str_to_query(user_query, 'userQuery')
  uri += self.__list_or_str_to_query(view, 'view')
  uri = uri.replace('?&', '?')
  return self._client.get_all(start=start, count=count, uri=uri)","Gets a list of index resources based on optional sorting and filtering and is constrained by start and count parameters. Args: category (str or list): Category of resources. Multiple Category parameters are applied with OR condition. count (int): The number of resources to return. A count of -1 requests all items. The actual number of items in the response might differ from the requested count if the sum of start and count exceeds the total number of items. fields (str): Specifies which fields should be returned in the result set. filter (list or str): A general filter/query string to narrow the list of items returned. The default is no filter; all resources are returned. padding (int): Number of resources to be returned before the reference URI resource. query (str): A general query string to narrow the list of resources returned. The default is no query - all resources are returned. reference_uri (str): Load one page of resources, pagination is applied with reference to referenceUri provided. sort (str): The sort order of the returned data set. By default, the sort order is based on create time with the oldest entry first. start (int): The first item to return, using 0-based indexing. If not specified, the default is 0 - start with the first available item. user_query (str): Free text Query string to search the resources. This will match the string in any field that is indexed. view (str): Return a specific subset of the attributes of the resource or collection, by specifying the name of a predefined view. Returns: list: A list of index resources.",1,0,0,1,2,2,0,0,1,3
"def fetch_liked_projects(self, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('callback'):
  return self.fetch_liked_projects_with_http_info(**kwargs)
  else:
  (data) = self.fetch_liked_projects_with_http_info(**kwargs)
  return data","List liked projects Fetch projects that the currently authenticated user likes. This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please define a `callback` function to be invoked when receiving the response. >>> def callback_function(response): >>> pprint(response) >>> >>> thread = api.fetch_liked_projects(callback=callback_function) :param callback function: The callback function for asynchronous request. (optional) :return: PaginatedProjectResults If the method is called asynchronously, returns the request thread.",2,0,0,1,3,2,0,0,1,3
"async def prompt(self, text=None):
  if self.sess is None:
  hist = FileHistory(s_common.getSynPath('cmdr_history'))
  self.sess = PromptSession(history=hist)
  if text is None:
  text = self.cmdprompt
  with patch_stdout():
  retn = await self.sess.prompt(text, async_=True, vi_mode=self.vi_mode, enable_open_in_editor=True)
  return retn",Prompt for user input from stdin.,1,0,0,0,1,1,0,0,1,2
"def calcparams_cec(effective_irradiance, temp_cell,
  alpha_sc, a_ref, I_L_ref, I_o_ref, R_sh_ref, R_s,
  Adjust, EgRef=1.121, dEgdT=-0.0002677,
  irrad_ref=1000, temp_ref=25):
  return calcparams_desoto(effective_irradiance, temp_cell,
  alpha_sc*(1.0 - Adjust/100),
  a_ref, I_L_ref, I_o_ref,
  R_sh_ref, R_s,
  EgRef=1.121, dEgdT=-0.0002677,
  irrad_ref=1000, temp_ref=25)","Calculates five parameter values for the single diode equation at effective irradiance and cell temperature using the CEC model described in [1]. The CEC model differs from the De soto et al. model [3] by the parameter Adjust. The five values returned by calcparams_cec can be used by singlediode to calculate an IV curve. Parameters ---------- effective_irradiance : numeric The irradiance (W/m2) that is converted to photocurrent. temp_cell : numeric The average cell temperature of cells within a module in C. alpha_sc : float The short-circuit current temperature coefficient of the module in units of A/C. a_ref : float The product of the usual diode ideality factor (n, unitless), number of cells in series (Ns), and cell thermal voltage at reference conditions, in units of V. I_L_ref : float The light-generated current (or photocurrent) at reference conditions, in amperes. I_o_ref : float The dark or diode reverse saturation current at reference conditions, in amperes. R_sh_ref : float The shunt resistance at reference conditions, in ohms. R_s : float The series resistance at reference conditions, in ohms. Adjust : float The adjustment to the temperature coefficient for short circuit current, in percent EgRef : float The energy bandgap at reference temperature in units of eV. 1.121 eV for crystalline silicon. EgRef must be >0. For parameters from the SAM CEC module database, EgRef=1.121 is implicit for all cell types in the parameter estimation algorithm used by NREL. dEgdT : float The temperature dependence of the energy bandgap at reference conditions in units of 1/K. May be either a scalar value (e.g. -0.0002677 as in [3]) or a DataFrame (this may be useful if dEgdT is a modeled as a function of temperature). For parameters from the SAM CEC module database, dEgdT=-0.0002677 is implicit for all cell types in the parameter estimation algorithm used by NREL. irrad_ref : float (optional, default=1000) Reference irradiance in W/m^2. temp_ref : float (optional, default=25) Reference cell temperature in C. Returns ------- Tuple of the following results: photocurrent : numeric Light-generated current in amperes saturation_current : numeric Diode saturation curent in amperes resistance_series : float Series resistance in ohms resistance_shunt : numeric Shunt resistance in ohms nNsVth : numeric The product of the usual diode ideality factor (n, unitless), number of cells in series (Ns), and cell thermal voltage at specified effective irradiance and cell temperature. References ---------- [1] A. Dobos, ""An Improved Coefficient Calculator for the California Energy Commission 6 Parameter Photovoltaic Module Model"", Journal of Solar Energy Engineering, vol 134, 2012. [2] System Advisor Model web page. https://sam.nrel.gov. [3] W. De Soto et al., ""Improvement and validation of a model for photovoltaic array performance"", Solar Energy, vol 80, pp. 78-88, 2006. See Also -------- calcparams_desoto singlediode retrieve_sam",0,0,0,0,0,1,0,0,1,2
"def sync(self, since=None, timeout_ms=30000, filter=None,
  full_state=None, set_presence=None):
  request = {
  ""timeout"": int(timeout_ms)
  }
  if since:
  request[""since""] = since
  if filter:
  request[""filter""] = filter
  if full_state:
  request[""full_state""] = json.dumps(full_state)
  if set_presence:
  request[""set_presence""] = set_presence
  return self._send(""GET"", ""/sync"", query_params=request,
  api_path=MATRIX_V2_API_PATH)","Perform a sync request. Args: since (str): Optional. A token which specifies where to continue a sync from. timeout_ms (int): Optional. The time in milliseconds to wait. filter (int|str): Either a Filter ID or a JSON string. full_state (bool): Return the full state for every room the user has joined Defaults to false. set_presence (str): Should the client be marked as ""online"" or"" offline""",2,0,1,1,4,2,0,0,1,3
"def parse_auth_response(auth_response):
  auth_dict = dict()
  LOG.debug('Authentication Headers %s', auth_response.headers)
  try:
  auth_dict['os_token'] = auth_response.headers['x-auth-token']
  auth_dict['storage_url'] = urlparse.urlparse(
  auth_response.headers['x-storage-url']
  )
  except KeyError as exp:
  raise exceptions.AuthenticationProblem(
  'No token was found in the authentication response. Please'
  ' check your auth URL, your credentials, and your set auth'
  ' version. Auth Headers: [ %s ] Error: [ %s ]',
  auth_response.headers,
  exp
  )
  else:
  return auth_dict","Parse the auth response and return the tenant, token, and username. :param auth_response: the full object returned from an auth call :returns: ``dict``",1,0,0,0,1,1,0,0,1,2
"def _satisfies_wolfe(val_0,
  val_c,
  f_lim,
  sufficient_decrease_param,
  curvature_param):
  exact_wolfe_suff_dec = (sufficient_decrease_param * val_0.df >=
  (val_c.f - val_0.f) / val_c.x)
  wolfe_curvature = val_c.df >= curvature_param * val_0.df
  exact_wolfe = exact_wolfe_suff_dec & wolfe_curvature
  approx_wolfe_applies = val_c.f <= f_lim
  approx_wolfe_suff_dec = ((2 * sufficient_decrease_param - 1) * val_0.df
  >= val_c.df)
  approx_wolfe = approx_wolfe_applies & approx_wolfe_suff_dec & wolfe_curvature
  is_satisfied = exact_wolfe | approx_wolfe
  return is_satisfied","Checks whether the Wolfe or approx Wolfe conditions are satisfied. The Wolfe conditions are a set of stopping criteria for an inexact line search algorithm. Let f(a) be the function value along the search direction and df(a) the derivative along the search direction evaluated a distance 'a'. Here 'a' is the distance along the search direction. The Wolfe conditions are: ```None f(a) <= f(0) + delta * a * df(0) (Armijo/Sufficient decrease condition) df(a) >= sigma * df(0) (Weak curvature condition) ``` `delta` and `sigma` are two user supplied parameters satisfying: `0 < delta < sigma <= 1.`. In the following, delta is called `sufficient_decrease_param` and sigma is called `curvature_param`. On a finite precision machine, the Wolfe conditions are difficult to satisfy when one is close to the minimum. Hence, Hager-Zhang propose replacing the sufficient decrease condition with the following condition on the derivative in the vicinity of a minimum. ```None df(a) <= (2 * delta - 1) * df(0) (Approx Wolfe sufficient decrease) ``` This condition is only used if one is near the minimum. This is tested using ```None f(a) <= f(0) + epsilon * |f(0)| ``` The following function checks both the Wolfe and approx Wolfe conditions. Here, `epsilon` is a small positive constant. In the following, the argument `f_lim` corresponds to the product: epsilon * |f(0)|. Args: val_0: A namedtuple, as returned by value_and_gradients_function evaluated at 0. val_c: A namedtuple, as returned by value_and_gradients_function evaluated at the point to be tested. f_lim: Scalar `Tensor` of real dtype. The function value threshold for the approximate Wolfe conditions to be checked. sufficient_decrease_param: Positive scalar `Tensor` of real dtype. Bounded above by the curvature param. Corresponds to 'delta' in the terminology of [Hager and Zhang (2006)][2]. curvature_param: Positive scalar `Tensor` of real dtype. Bounded above by `1.`. Corresponds to 'sigma' in the terminology of [Hager Zhang (2005)][1]. Returns: is_satisfied: A scalar boolean `Tensor` which is True if either the Wolfe or approximate Wolfe conditions are satisfied.",0,0,0,0,0,1,0,0,1,2
"def input_loop():
  while mpstate.status.exit != True:
  try:
  if mpstate.status.exit != True:
  line = input(mpstate.rl.prompt)
  except EOFError:
  mpstate.status.exit = True
  sys.exit(1)
  mpstate.input_queue.put(line)",wait for user input,1,0,0,0,1,1,0,0,1,2
"def calendar_tuple(jd_float, offset=0.0):
  jd_float = _to_array(jd_float)
  whole, fraction = divmod(jd_float + 0.5, 1.0)
  whole = whole.astype(int)
  year, month, day = calendar_date(whole)
  hour, hfrac = divmod(fraction * 24.0, 1.0)
  minute, second = divmod(hfrac * 3600.0, 60.0)
  return year, month, day, hour.astype(int), minute.astype(int), second","Return a (year, month, day, hour, minute, second.fraction) tuple. The `offset` is added to the time before it is split into its components. This is useful if the user is going to round the result before displaying it. If the result is going to be displayed as seconds, for example, set `offset` to half a second and then throw away the fraction; if the result is going to be displayed as minutes, set `offset` to thirty seconds and then throw away the seconds; and so forth.",0,0,0,1,1,1,0,0,1,2
"def sendeof(self):
  if hasattr(termios, 'VEOF'):
  char = termios.tcgetattr(self.child_fd)[6][termios.VEOF]
  else:
  char = chr(4)
  self.send(char)","This sends an EOF to the child. This sends a character which causes the pending parent output buffer to be sent to the waiting child program without waiting for end-of-line. If it is the first character of the line, the read() in the user program returns 0, which signifies end-of-file. This means to work as expected a sendeof() has to be called at the beginning of a line. This method does not send a newline. It is the responsibility of the caller to ensure the eof is sent at the beginning of a line.",0,0,0,1,1,1,0,0,1,2
"def list(
  self, application_id, filter_hostname=None, filter_ids=None,
  page=None):
  filters = [
  'filter[hostname]={0}'.format(filter_hostname) if filter_hostname else None,
  'filter[ids]={0}'.format(','.join([str(app_id) for app_id in filter_ids])) if filter_ids else None,
  'page={0}'.format(page) if page else None
  ]
  return self._get(
  url='{root}applications/{application_id}/instances.json'.format(
  root=self.URL,
  application_id=application_id
  ),
  headers=self.headers,
  params=self.build_param_string(filters)
  )","This API endpoint returns a paginated list of instances associated with the given application. Application instances can be filtered by hostname, or the list of application instance IDs. :type application_id: int :param application_id: Application ID :type filter_hostname: str :param filter_hostname: Filter by server hostname :type filter_ids: list of ints :param filter_ids: Filter by application instance ids :type page: int :param page: Pagination index :rtype: dict :return: The JSON response of the API, with an additional 'pages' key if there are paginated results :: { ""application_instances"": [ { ""id"": ""integer"", ""application_name"": ""string"", ""host"": ""string"", ""port"": ""integer"", ""language"": ""integer"", ""health_status"": ""string"", ""application_summary"": { ""response_time"": ""float"", ""throughput"": ""float"", ""error_rate"": ""float"", ""apdex_score"": ""float"" }, ""end_user_summary"": { ""response_time"": ""float"", ""throughput"": ""float"", ""apdex_score"": ""float"" }, ""links"": { ""application"": ""integer"", ""application_host"": ""integer"", ""server"": ""integer"" } } ], ""pages"": { ""last"": { ""url"": ""https://api.newrelic.com/v2/applications/{application_id}/instances.json?page=2"", ""rel"": ""last"" }, ""next"": { ""url"": ""https://api.newrelic.com/v2/applications/{application_id}/instances.json?page=2"", ""rel"": ""next"" } } }",2,0,0,1,3,2,0,0,1,3
"def account_info(remote, resp):
  gh = github3.login(token=resp['access_token'])
  me = gh.me()
  return dict(
  user=dict(
  email=_extract_email(gh),
  profile=dict(
  username=me.login,
  full_name=me.name,
  ),
  ),
  external_id=str(me.id),
  external_method='github'
  )","Retrieve remote account information used to find local user. It returns a dictionary with the following structure: .. code-block:: python { 'user': { 'email': '...', 'profile': { 'username': '...', 'full_name': '...', } }, 'external_id': 'github-unique-identifier', 'external_method': 'github', } Information inside the user dictionary are available for other modules. For example, they are used from the module invenio-userprofiles to fill the user profile. :param remote: The remote application. :param resp: The response. :returns: A dictionary with the user information.",1,0,0,1,2,2,0,0,1,3
"def login(self):
  auth_data = dict()
  auth_data['apikey'] = self.api_key
  auth_data['username'] = self.username
  auth_data['userkey'] = self.account_identifier
  auth_resp = requests_util.run_request('post', self.API_BASE_URL + '/login', data=json.dumps(auth_data),
  headers=self.__get_header())
  if auth_resp.status_code == 200:
  auth_resp_data = self.parse_raw_response(auth_resp)
  self.__token = auth_resp_data['token']
  self.__auth_time = datetime.now()
  self.is_authenticated = True
  else:
  raise AuthenticationFailedException('Authentication failed!')","This method performs the login on TheTVDB given the api key, user name and account identifier. :return: None",1,0,0,1,2,2,0,0,2,4
"def confirm(action, default=None, skip=False):
  MAX_ITERATIONS = 3
  if skip:
  return default
  else:
  defaults = {
  None: ('y','n'),
  True: ('Y','n'),
  False: ('y','N'),
  }
  y, n = defaults[default]
  prompt = text_type('{action}? ({y}/{n})').format(**locals())
  choice = None
  try:
  if default is None:
  cnt = 1
  while not choice and cnt < MAX_ITERATIONS:
  choice = safe_input(prompt)
  cnt += 1
  else:
  choice = safe_input(prompt)
  except KeyboardInterrupt:
  return None
  if choice in ('yes', 'y', 'Y'):
  return True
  if choice in ('no', 'n', 'N'):
  return False
  if default is not None:
  return default
  return None","A shortcut for typical confirmation prompt. :param action: a string describing the action, e.g. ""Apply changes"". A question mark will be appended. :param default: `bool` or `None`. Determines what happens when user hits :kbd:`Enter` without typing in a choice. If `True`, default choice is ""yes"". If `False`, it is ""no"". If `None` the prompt keeps reappearing until user types in a choice (not necessarily acceptable) or until the number of iteration reaches the limit. Default is `None`. :param skip: `bool`; if `True`, no interactive prompt is used and default choice is returned (useful for batch mode). Default is `False`. Usage:: def delete(key, silent=False): item = db.get(Item, args.key) if confirm('Delete '+item.title, default=True, skip=silent): item.delete() print('Item deleted.') else: print('Operation cancelled.') Returns `None` on `KeyboardInterrupt` event.",1,0,0,1,2,1,0,0,1,2
"def get_user_autocompletions(ctx, args, incomplete, cmd_param):
  results = []
  if isinstance(cmd_param.type, Choice):
  results = [(c, None)
  for c in cmd_param.type.choices if str(c).startswith(incomplete)]
  elif cmd_param.autocompletion is not None:
  dynamic_completions = cmd_param.autocompletion(ctx=ctx,
  args=args,
  incomplete=incomplete)
  results = [c if isinstance(c, tuple) else (c, None)
  for c in dynamic_completions]
  return results",:param ctx: context associated with the parsed command :param args: full list of args :param incomplete: the incomplete text to autocomplete :param cmd_param: command definition :return: all the possible user-specified completions for the param,1,0,0,1,2,1,0,0,1,2
"def list_user_logins_users(self, user_id):
  path = {}
  data = {}
  params = {}
  path[""user_id""] = user_id
  self.logger.debug(""GET /api/v1/users/{user_id}/logins with query params: {params} and form data: {data}"".format(params=params, data=data, **path))
  return self.generic_request(""GET"", ""/api/v1/users/{user_id}/logins"".format(**path), data=data, params=params, all_pages=True)","List user logins. Given a user ID, return that user's logins for the given account.",2,0,0,1,3,2,0,0,1,3
"def create_replication_group(ReplicationGroupId=None, ReplicationGroupDescription=None, PrimaryClusterId=None, AutomaticFailoverEnabled=None, NumCacheClusters=None, PreferredCacheClusterAZs=None, NumNodeGroups=None, ReplicasPerNodeGroup=None, NodeGroupConfiguration=None, CacheNodeType=None, Engine=None, EngineVersion=None, CacheParameterGroupName=None, CacheSubnetGroupName=None, CacheSecurityGroupNames=None, SecurityGroupIds=None, Tags=None, SnapshotArns=None, SnapshotName=None, PreferredMaintenanceWindow=None, Port=None, NotificationTopicArn=None, AutoMinorVersionUpgrade=None, SnapshotRetentionLimit=None, SnapshotWindow=None, AuthToken=None):
  pass","Creates a Redis (cluster mode disabled) or a Redis (cluster mode enabled) replication group. A Redis (cluster mode disabled) replication group is a collection of cache clusters, where one of the cache clusters is a read/write primary and the others are read-only replicas. Writes to the primary are asynchronously propagated to the replicas. A Redis (cluster mode enabled) replication group is a collection of 1 to 15 node groups (shards). Each node group (shard) has one read/write primary node and up to 5 read-only replica nodes. Writes to the primary are asynchronously propagated to the replicas. Redis (cluster mode enabled) replication groups partition the data across node groups (shards). When a Redis (cluster mode disabled) replication group has been successfully created, you can add one or more read replicas to it, up to a total of 5 read replicas. You cannot alter a Redis (cluster mode enabled) replication group after it has been created. However, if you need to increase or decrease the number of node groups (console: shards), you can avail yourself of ElastiCache for Redis' enhanced backup and restore. For more information, see Restoring From a Backup with Cluster Resizing in the ElastiCache User Guide . See also: AWS API Documentation :example: response = client.create_replication_group( ReplicationGroupId='string', ReplicationGroupDescription='string', PrimaryClusterId='string', AutomaticFailoverEnabled=True|False, NumCacheClusters=123, PreferredCacheClusterAZs=[ 'string', ], NumNodeGroups=123, ReplicasPerNodeGroup=123, NodeGroupConfiguration=[ { 'Slots': 'string', 'ReplicaCount': 123, 'PrimaryAvailabilityZone': 'string', 'ReplicaAvailabilityZones': [ 'string', ] }, ], CacheNodeType='string', Engine='string', EngineVersion='string', CacheParameterGroupName='string', CacheSubnetGroupName='string', CacheSecurityGroupNames=[ 'string', ], SecurityGroupIds=[ 'string', ], Tags=[ { 'Key': 'string', 'Value': 'string' }, ], SnapshotArns=[ 'string', ], SnapshotName='string', PreferredMaintenanceWindow='string', Port=123, NotificationTopicArn='string', AutoMinorVersionUpgrade=True|False, SnapshotRetentionLimit=123, SnapshotWindow='string', AuthToken='string' ) :type ReplicationGroupId: string :param ReplicationGroupId: [REQUIRED] The replication group identifier. This parameter is stored as a lowercase string. Constraints: A name must contain from 1 to 20 alphanumeric characters or hyphens. The first character must be a letter. A name cannot end with a hyphen or contain two consecutive hyphens. :type ReplicationGroupDescription: string :param ReplicationGroupDescription: [REQUIRED] A user-created description for the replication group. :type PrimaryClusterId: string :param PrimaryClusterId: The identifier of the cache cluster that serves as the primary for this replication group. This cache cluster must already exist and have a status of available . This parameter is not required if NumCacheClusters , NumNodeGroups , or ReplicasPerNodeGroup is specified. :type AutomaticFailoverEnabled: boolean :param AutomaticFailoverEnabled: Specifies whether a read-only replica is automatically promoted to read/write primary if the existing primary fails. If true , Multi-AZ is enabled for this replication group. If false , Multi-AZ is disabled for this replication group. AutomaticFailoverEnabled must be enabled for Redis (cluster mode enabled) replication groups. Default: false Note ElastiCache Multi-AZ replication groups is not supported on: Redis versions earlier than 2.8.6. Redis (cluster mode disabled): T1 and T2 node types. Redis (cluster mode enabled): T2 node types. :type NumCacheClusters: integer :param NumCacheClusters: The number of clusters this replication group initially has. This parameter is not used if there is more than one node group (shard). You should use ReplicasPerNodeGroup instead. If AutomaticFailoverEnabled is true , the value of this parameter must be at least 2. If AutomaticFailoverEnabled is false you can omit this parameter (it will default to 1), or you can explicitly set it to a value between 2 and 6. The maximum permitted value for NumCacheClusters is 6 (primary plus 5 replicas). :type PreferredCacheClusterAZs: list :param PreferredCacheClusterAZs: A list of EC2 Availability Zones in which the replication group's cache clusters are created. The order of the Availability Zones in the list is the order in which clusters are allocated. The primary cluster is created in the first AZ in the list. This parameter is not used if there is more than one node group (shard). You should use NodeGroupConfiguration instead. Note If you are creating your replication group in an Amazon VPC (recommended), you can only locate cache clusters in Availability Zones associated with the subnets in the selected subnet group. The number of Availability Zones listed must equal the value of NumCacheClusters . Default: system chosen Availability Zones. (string) -- :type NumNodeGroups: integer :param NumNodeGroups: An optional parameter that specifies the number of node groups (shards) for this Redis (cluster mode enabled) replication group. For Redis (cluster mode disabled) either omit this parameter or set it to 1. Default: 1 :type ReplicasPerNodeGroup: integer :param ReplicasPerNodeGroup: An optional parameter that specifies the number of replica nodes in each node group (shard). Valid values are 0 to 5. :type NodeGroupConfiguration: list :param NodeGroupConfiguration: A list of node group (shard) configuration options. Each node group (shard) configuration has the following: Slots, PrimaryAvailabilityZone, ReplicaAvailabilityZones, ReplicaCount. If you're creating a Redis (cluster mode disabled) or a Redis (cluster mode enabled) replication group, you can use this parameter to individually configure each node group (shard), or you can omit this parameter. (dict) --node group (shard) configuration options. Each node group (shard) configuration has the following: Slots , PrimaryAvailabilityZone , ReplicaAvailabilityZones , ReplicaCount . Slots (string) --A string that specifies the keyspace for a particular node group. Keyspaces range from 0 to 16,383. The string is in the format startkey-endkey . Example: '0-3999' ReplicaCount (integer) --The number of read replica nodes in this node group (shard). PrimaryAvailabilityZone (string) --The Availability Zone where the primary node of this node group (shard) is launched. ReplicaAvailabilityZones (list) --A list of Availability Zones to be used for the read replicas. The number of Availability Zones in this list must match the value of ReplicaCount or ReplicasPerNodeGroup if not specified. (string) -- :type CacheNodeType: string :param CacheNodeType: The compute and memory capacity of the nodes in the node group (shard). Valid node types are as follows: General purpose: Current generation: cache.t2.micro , cache.t2.small , cache.t2.medium , cache.m3.medium , cache.m3.large , cache.m3.xlarge , cache.m3.2xlarge , cache.m4.large , cache.m4.xlarge , cache.m4.2xlarge , cache.m4.4xlarge , cache.m4.10xlarge Previous generation: cache.t1.micro , cache.m1.small , cache.m1.medium , cache.m1.large , cache.m1.xlarge Compute optimized: cache.c1.xlarge Memory optimized: Current generation: cache.r3.large , cache.r3.xlarge , cache.r3.2xlarge , cache.r3.4xlarge , cache.r3.8xlarge Previous generation: cache.m2.xlarge , cache.m2.2xlarge , cache.m2.4xlarge Notes: All T2 instances are created in an Amazon Virtual Private Cloud (Amazon VPC). Redis backup/restore is not supported for Redis (cluster mode disabled) T1 and T2 instances. Backup/restore is supported on Redis (cluster mode enabled) T2 instances. Redis Append-only files (AOF) functionality is not supported for T1 or T2 instances. For a complete listing of node types and specifications, see Amazon ElastiCache Product Features and Details and either Cache Node Type-Specific Parameters for Memcached or Cache Node Type-Specific Parameters for Redis . :type Engine: string :param Engine: The name of the cache engine to be used for the cache clusters in this replication group. :type EngineVersion: string :param EngineVersion: The version number of the cache engine to be used for the cache clusters in this replication group. To view the supported cache engine versions, use the DescribeCacheEngineVersions operation. Important: You can upgrade to a newer engine version (see Selecting a Cache Engine and Version ) in the ElastiCache User Guide , but you cannot downgrade to an earlier engine version. If you want to use an earlier engine version, you must delete the existing cache cluster or replication group and create it anew with the earlier engine version. :type CacheParameterGroupName: string :param CacheParameterGroupName: The name of the parameter group to associate with this replication group. If this argument is omitted, the default cache parameter group for the specified engine is used. If you are running Redis version 3.2.4 or later, only one node group (shard), and want to use a default parameter group, we recommend that you specify the parameter group by name. To create a Redis (cluster mode disabled) replication group, use CacheParameterGroupName=default.redis3.2 . To create a Redis (cluster mode enabled) replication group, use CacheParameterGroupName=default.redis3.2.cluster.on . :type CacheSubnetGroupName: string :param CacheSubnetGroupName: The name of the cache subnet group to be used for the replication group. Warning If you're going to launch your cluster in an Amazon VPC, you need to create a subnet group before you start creating a cluster. For more information, see Subnets and Subnet Groups . :type CacheSecurityGroupNames: list :param CacheSecurityGroupNames: A list of cache security group names to associate with this replication group. (string) -- :type SecurityGroupIds: list :param SecurityGroupIds: One or more Amazon VPC security groups associated with this replication group. Use this parameter only when you are creating a replication group in an Amazon Virtual Private Cloud (Amazon VPC). (string) -- :type Tags: list :param Tags: A list of cost allocation tags to be added to this resource. A tag is a key-value pair. A tag key must be accompanied by a tag value. (dict) --A cost allocation Tag that can be added to an ElastiCache cluster or replication group. Tags are composed of a Key/Value pair. A tag with a null Value is permitted. Key (string) --The key for the tag. May not be null. Value (string) --The tag's value. May be null. :type SnapshotArns: list :param SnapshotArns: A list of Amazon Resource Names (ARN) that uniquely identify the Redis RDB snapshot files stored in Amazon S3. The snapshot files are used to populate the new replication group. The Amazon S3 object name in the ARN cannot contain any commas. The new replication group will have the number of node groups (console: shards) specified by the parameter NumNodeGroups or the number of node groups configured by NodeGroupConfiguration regardless of the number of ARNs specified here. Note This parameter is only valid if the Engine parameter is redis . Example of an Amazon S3 ARN: arn:aws:s3:::my_bucket/snapshot1.rdb (string) -- :type SnapshotName: string :param SnapshotName: The name of a snapshot from which to restore data into the new replication group. The snapshot status changes to restoring while the new replication group is being created. Note This parameter is only valid if the Engine parameter is redis . :type PreferredMaintenanceWindow: string :param PreferredMaintenanceWindow: Specifies the weekly time range during which maintenance on the cache cluster is performed. It is specified as a range in the format ddd:hh24:mi-ddd:hh24:mi (24H Clock UTC). The minimum maintenance window is a 60 minute period. Valid values for ddd are: Specifies the weekly time range during which maintenance on the cluster is performed. It is specified as a range in the format ddd:hh24:mi-ddd:hh24:mi (24H Clock UTC). The minimum maintenance window is a 60 minute period. Valid values for ddd are: sun mon tue wed thu fri sat Example: sun:23:00-mon:01:30 :type Port: integer :param Port: The port number on which each member of the replication group accepts connections. :type NotificationTopicArn: string :param NotificationTopicArn: The Amazon Resource Name (ARN) of the Amazon Simple Notification Service (SNS) topic to which notifications are sent. Note The Amazon SNS topic owner must be the same as the cache cluster owner. :type AutoMinorVersionUpgrade: boolean :param AutoMinorVersionUpgrade: This parameter is currently disabled. :type SnapshotRetentionLimit: integer :param SnapshotRetentionLimit: The number of days for which ElastiCache retains automatic snapshots before deleting them. For example, if you set SnapshotRetentionLimit to 5, a snapshot that was taken today is retained for 5 days before being deleted. Note This parameter is only valid if the Engine parameter is redis . Default: 0 (i.e., automatic backups are disabled for this cache cluster). :type SnapshotWindow: string :param SnapshotWindow: The daily time range (in UTC) during which ElastiCache begins taking a daily snapshot of your node group (shard). Example: 05:00-09:00 If you do not specify this parameter, ElastiCache automatically chooses an appropriate time range. Note This parameter is only valid if the Engine parameter is redis . :type AuthToken: string :param AuthToken: Reserved parameter. The password used to access a password protected server. Password constraints: Must be only printable ASCII characters. Must be at least 16 characters and no more than 128 characters in length. Cannot contain any of the following characters: '/', ''', or '@'. For more information, see AUTH password at Redis. :rtype: dict :return: { 'ReplicationGroup': { 'ReplicationGroupId': 'string', 'Description': 'string', 'Status': 'string', 'PendingModifiedValues': { 'PrimaryClusterId': 'string', 'AutomaticFailoverStatus': 'enabled'|'disabled' }, 'MemberClusters': [ 'string', ], 'NodeGroups': [ { 'NodeGroupId': 'string', 'Status': 'string', 'PrimaryEndpoint': { 'Address': 'string', 'Port': 123 }, 'Slots': 'string', 'NodeGroupMembers': [ { 'CacheClusterId': 'string', 'CacheNodeId': 'string', 'ReadEndpoint': { 'Address': 'string', 'Port': 123 }, 'PreferredAvailabilityZone': 'string', 'CurrentRole': 'string' }, ] }, ], 'SnapshottingClusterId': 'string', 'AutomaticFailover': 'enabled'|'disabled'|'enabling'|'disabling', 'ConfigurationEndpoint': { 'Address': 'string', 'Port': 123 }, 'SnapshotRetentionLimit': 123, 'SnapshotWindow': 'string', 'ClusterEnabled': True|False, 'CacheNodeType': 'string' } } :returns: Redis versions earlier than 2.8.6. Redis (cluster mode disabled):T1 and T2 cache node types. Redis (cluster mode enabled): T1 node types.",2,0,0,2,4,1,0,0,2,3
"def read_file(self, filename):
  logger.info(""Reading file: %s"", format_path(filename))
  contents = self.context.read_file(filename)
  num_lines = len(contents.splitlines())
  logger.debug(""Read %s from %s."",
  pluralize(num_lines, 'line'),
  format_path(filename))
  return contents.rstrip()",Read a text file and provide feedback to the user. :param filename: The pathname of the file to read (a string). :returns: The contents of the file (a string).,1,0,0,1,2,1,0,0,1,2
"def generate_date_tail_boost_queries(
  field, timedeltas_and_boosts, relative_to=None):
  relative_to = relative_to or datetime.datetime.now()
  times = {}
  for timedelta, boost in timedeltas_and_boosts.items():
  date = (relative_to - timedelta).date()
  times[date] = boost
  times = sorted(times.items(), key=lambda i: i[0])
  queries = []
  for (x, time) in enumerate(times):
  kwargs = {""field"": field, ""boost"": time[1]}
  if x == 0:
  kwargs[""lte""] = time[0]
  else:
  kwargs[""gt""] = time[0]
  if x < len(times) - 1:
  kwargs[""lte""] = times[x + 1][0]
  if kwargs[""boost""] > 0:
  q = RangeQuery()
  q.add_range(**kwargs)
  queries.append(q)
  return queries","Generate a list of RangeQueries usable to boost the scores of more recent documents. Example: ``` queries = generate_date_tail_boost_queries(""publish_date"", { timedelta(days=90): 1, timedelta(days=30): 2, timedelta(days=10): 4, }) s = Search(BoolQuery(must=..., should=queries)) # ... ``` Refs: http://elasticsearch-users.115913.n3.nabble.com/Boost-recent-documents-td2126107.html#a2126317 :param field: field name to generate the queries against :param timedeltas_and_boosts: dictionary of timedelta instances and their boosts. Negative or zero boost values will not generate rangequeries. :type timedeltas_and_boosts: dict[timedelta, float] :param relative_to: Relative to this datetime (may be None for ""now"") :return: List of RangeQueries",0,0,0,1,1,1,0,0,1,2
"def execute(self, command=None, container_id=None, sudo=False, stream=False):
  sudo = self._get_sudo(sudo)
  container_id = self.get_container_id(container_id)
  cmd = self._init_command('exec')
  cmd.append(container_id)
  if command != None:
  if not isinstance(command, list):
  command = [command]
  cmd = cmd + command
  if stream:
  return stream_command(cmd, sudo=sudo)
  return self._run_command(cmd, sudo=sudo, quiet=True)","execute a command to a container instance based on container_id Parameters ========== container_id: the container_id to delete command: the command to execute to the container sudo: whether to issue the command with sudo (or not) a container started with sudo will belong to the root user If started by a user, the user needs to control deleting it stream: if True, return an iterate to iterate over results of exec. default is False, will return full output as string. Returns ======= return_code: the return code from the delete command. 0 indicates a successful delete, 255 indicates not.",1,0,0,1,2,1,0,0,1,2
"def sample_pws(self, n, asperdist=True):
  if asperdist:
  sample = np.random.choice(
  self._freq_list.shape[0], size=n, p=self._freq_list/self._totalf
  )
  else:
  sample = np.random.choice(len(self._T), size=n)
  return (self._T.restore_key(i) for i in sample)","Returns n passwords sampled from this password dataset. if asperdist is True, then returns the password sampled according the password histogram distribution (with replacement). Passwords are always sampled with replacement. TODO: The sample users, instead of passwords perse.",1,0,0,1,2,1,0,0,1,2
"def make_model_sources_image(shape, model, source_table, oversample=1):
  image = np.zeros(shape, dtype=np.float64)
  y, x = np.indices(shape)
  params_to_set = []
  for param in source_table.colnames:
  if param in model.param_names:
  params_to_set.append(param)
  init_params = {param: getattr(model, param) for param in params_to_set}
  try:
  for i, source in enumerate(source_table):
  for param in params_to_set:
  setattr(model, param, source[param])
  if oversample == 1:
  image += model(x, y)
  else:
  image += discretize_model(model, (0, shape[1]),
  (0, shape[0]), mode='oversample',
  factor=oversample)
  finally:
  for param, value in init_params.items():
  setattr(model, param, value)
  return image","Make an image containing sources generated from a user-specified model. Parameters ---------- shape : 2-tuple of int The shape of the output 2D image. model : 2D astropy.modeling.models object The model to be used for rendering the sources. source_table : `~astropy.table.Table` Table of parameters for the sources. Each row of the table corresponds to a source whose model parameters are defined by the column names, which must match the model parameter names. Column names that do not match model parameters will be ignored. Model parameters not defined in the table will be set to the ``model`` default value. oversample : float, optional The sampling factor used to discretize the models on a pixel grid. If the value is 1.0 (the default), then the models will be discretized by taking the value at the center of the pixel bin. Note that this method will not preserve the total flux of very small sources. Otherwise, the models will be discretized by taking the average over an oversampled grid. The pixels will be oversampled by the ``oversample`` factor. Returns ------- image : 2D `~numpy.ndarray` Image containing model sources. See Also -------- make_random_models_table, make_gaussian_sources_image Examples -------- .. plot:: :include-source: from collections import OrderedDict from astropy.modeling.models import Moffat2D from photutils.datasets import (make_random_models_table, make_model_sources_image) model = Moffat2D() n_sources = 10 shape = (100, 100) param_ranges = [('amplitude', [100, 200]), ('x_0', [0, shape[1]]), ('y_0', [0, shape[0]]), ('gamma', [5, 10]), ('alpha', [1, 2])] param_ranges = OrderedDict(param_ranges) sources = make_random_models_table(n_sources, param_ranges, random_state=12345) data = make_model_sources_image(shape, model, sources) plt.imshow(data)",0,0,0,1,1,1,0,0,1,2
"def get_dir_indices(msg, dirs):
  import os
  usage = ('\nEnter numbers preceeding paths seperated by commas (e.g. '
  '`0,2,3`).\nTo select all paths type `all`.\nSingle directories '
  'can also be entered (e.g. `0`)\n\n')
  dirs_str = ['{:2} {:60}\n'.format(i, p) for i, p in enumerate(dirs)]
  dirs_str = ''.join(dirs_str)
  input_dirs = recursive_input(''.join([msg, usage, dirs_str, '\n']), str)
  if ',' in input_dirs:
  input_dir_indices = [int(x.strip()) for x in input_dirs.split(',')]
  elif 'all' in input_dirs:
  input_dir_indices = range(len(dirs))
  else:
  try:
  input_dir_indices = [int(input_dirs.strip()),]
  except:
  raise ValueError('Could not determine input type for input: '
  '{}'.format(input_dirs))
  return input_dir_indices",Return path(s) indices of directory list from user input Args ---- msg: str String with message to display before pass selection input dir_list: array-like list of paths to be displayed and selected from Return ------ input_dir_indices: array-like list of index positions of user selected path from input,1,0,0,1,2,1,0,0,1,2
"def fetch_all_images(self, type=None, private=None):
  r
  params = {}
  if type is not None:
  params[""type""] = type
  if private is not None:
  params[""private""] = 'true' if private else 'false'
  return map(self._image, self.paginate('/v2/images', 'images',
  params=params))","r"""""" Returns a generator that yields all of the images available to the account :param type: the type of images to fetch: ``""distribution""``, ``""application""``, or all (`None`); default: `None` :type type: string or None :param bool private: whether to only return the user's private images; default: return all images :rtype: generator of `Image`\ s :raises DOAPIError: if the API endpoint replies with an error",2,0,0,1,3,2,0,0,1,3
"def decide_which_users_to_annotate(centrality_vector,
  number_to_annotate,
  already_annotated,
  node_to_id):
  centrality_vector = np.asarray(centrality_vector)
  ind = np.argsort(np.squeeze(centrality_vector))
  if centrality_vector.size > 1:
  reversed_ind = ind[::-1]
  else:
  reversed_ind = list()
  reversed_ind = reversed_ind.append(ind)
  user_id_list = list()
  append_user_id = user_id_list.append
  counter = 0
  for node in reversed_ind:
  user_twitter_id = node_to_id[node]
  if user_twitter_id not in already_annotated:
  append_user_id(user_twitter_id)
  counter += 1
  if counter >= number_to_annotate:
  break
  return user_id_list","Sorts a centrality vector and returns the Twitter user ids that are to be annotated. Inputs: - centrality_vector: A numpy array vector, that contains the centrality values for all users. - number_to_annotate: The number of users to annotate. - already_annotated: A python set of user twitter ids that have already been annotated. - node_to_id: A python dictionary that maps graph nodes to user twitter ids. Output: - user_id_list: A python list of Twitter user ids.",1,0,0,1,2,1,0,0,1,2
"async def _run_sql(database, operation, *args, **kwargs):
  __log__.debug((operation, args, kwargs))
  with peewee.__exception_wrapper__:
  cursor = await database.cursor_async()
  try:
  await cursor.execute(operation, *args, **kwargs)
  except:
  await cursor.release()
  raise
  return cursor",Run SQL operation (query or command) against database.,0,1,1,0,2,0,0,1,0,1
"def load_hgnc(adapter, genes=None, ensembl_lines=None, hgnc_lines=None, exac_lines=None, mim2gene_lines=None,
  genemap_lines=None, hpo_lines=None, transcripts_lines=None, build='37', omim_api_key=''):
  gene_objs = load_hgnc_genes(
  adapter=adapter,
  genes = genes,
  ensembl_lines=ensembl_lines,
  hgnc_lines=hgnc_lines,
  exac_lines=exac_lines,
  mim2gene_lines=mim2gene_lines,
  genemap_lines=genemap_lines,
  hpo_lines=hpo_lines,
  build=build,
  omim_api_key=omim_api_key,
  )
  ensembl_genes = {}
  for gene_obj in gene_objs:
  ensembl_genes[gene_obj['ensembl_id']] = gene_obj
  transcript_objs = load_transcripts(
  adapter=adapter,
  transcripts_lines=transcripts_lines,
  build=build,
  ensembl_genes=ensembl_genes)",Load Genes and transcripts into the database If no resources are provided the correct ones will be fetched. Args: adapter(scout.adapter.MongoAdapter) genes(dict): If genes are already parsed ensembl_lines(iterable(str)): Lines formated with ensembl gene information hgnc_lines(iterable(str)): Lines with gene information from genenames.org exac_lines(iterable(str)): Lines with information pLi-scores from ExAC mim2gene(iterable(str)): Lines with map from omim id to gene symbol genemap_lines(iterable(str)): Lines with information of omim entries hpo_lines(iterable(str)): Lines information about map from hpo terms to genes transcripts_lines(iterable): iterable with ensembl transcript lines build(str): What build to use. Defaults to '37',1,1,0,0,2,1,0,0,1,2
"def delete_multiple_users(self, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.delete_multiple_users_with_http_info(**kwargs)
  else:
  (data) = self.delete_multiple_users_with_http_info(**kwargs)
  return data","Deletes multiple users # noqa: E501 # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.delete_multiple_users(async_req=True) >>> result = thread.get() :param async_req bool :param list[str] body: identifiers of list of users which should be deleted :return: ResponseContainerListString If the method is called asynchronously, returns the request thread.",1,1,0,1,3,2,0,0,1,3
"def determine_result(self, returncode, returnsignal, output, isTimeout):
  if not output:
  return 'ERROR - no output'
  last = output[-1]
  if isTimeout:
  return 'TIMEOUT'
  if returncode != 0:
  return 'ERROR - Pre-run'
  if last is None:
  return 'ERROR - no output'
  elif 'result: true' in last:
  return result.RESULT_TRUE_PROP
  elif 'result: false' in last:
  return result.RESULT_FALSE_REACH
  else:
  return result.RESULT_UNKNOWN","Parse the output of the tool and extract the verification result. This method always needs to be overridden. If the tool gave a result, this method needs to return one of the benchexec.result.RESULT_* strings. Otherwise an arbitrary string can be returned that will be shown to the user and should give some indication of the failure reason (e.g., ""CRASH"", ""OUT_OF_MEMORY"", etc.).",0,0,0,1,1,1,0,0,1,2
"def other_gene_name(self, type_=None, name=None, entry_name=None, limit=None, as_df=None):
  q = self.session.query(models.OtherGeneName)
  model_queries_config = (
  (type_, models.OtherGeneName.type_),
  (name, models.OtherGeneName.name),
  )
  q = self.get_model_queries(q, model_queries_config)
  q = self.get_one_to_many_queries(q, ((entry_name, models.Entry.name),))
  return self._limit_and_df(q, limit, as_df)","Method to query :class:`.models.OtherGeneName` objects in database :param type_: type(s) of gene name e.g. *synonym* :type type_: str or tuple(str) or None :param name: other gene name(s) :type name: str or tuple(str) or None :param entry_name: name(s) in :class:`.models.Entry` :type entry_name: str or tuple(str) or None :param limit: - if `isinstance(limit,int)==True` -> limit - if `isinstance(limit,tuple)==True` -> format:= tuple(page_number, results_per_page) - if limit == None -> all results :type limit: int or tuple(int) or None :param bool as_df: if `True` results are returned as :class:`pandas.DataFrame` :return: - if `as_df == False` -> list(:class:`.models.OtherGeneName`) - if `as_df == True` -> :class:`pandas.DataFrame` :rtype: list(:class:`.models.OtherGeneName`) or :class:`pandas.DataFrame`",0,0,1,1,2,1,0,1,1,3
"def line(x_fn, y_fn, *, options={}, **interact_params):
  fig = options.get('_fig', False) or _create_fig(options=options)
  [line] = (_create_marks(fig=fig, marks=[bq.Lines], options=options))
  _add_marks(fig, [line])
  def wrapped(**interact_params):
  x_data = util.maybe_call(x_fn, interact_params, prefix='x')
  line.x = x_data
  y_bound = util.maybe_curry(y_fn, x_data)
  line.y = util.maybe_call(y_bound, interact_params, prefix='y')
  controls = widgets.interactive(wrapped, **interact_params)
  return widgets.VBox([controls, fig])","Generates an interactive line chart that allows users to change the parameters of the inputs x_fn and y_fn. Args: x_fn (Array | (*args -> Array str | Array int | Array float)): If array, uses array values for x-coordinates. If function, must take parameters to interact with and return an array of strings or numbers. These will become the x-coordinates of the line plot. y_fn (Array | (Array, *args -> Array int | Array float)): If array, uses array values for y-coordinates. If function, must take in the output of x_fn as its first parameter and optionally other parameters to interact with. Must return an array of numbers. These will become the y-coordinates of the line plot. Kwargs: {options} interact_params (dict): Keyword arguments in the same format as `ipywidgets.interact`. One argument is required for each argument of both `x_fn` and `y_fn`. If `x_fn` and `y_fn` have conflicting parameter names, prefix the corresponding kwargs with `x__` and `y__`. Returns: VBox with two children: the interactive controls and the figure. >>> line([1, 2, 3], [4, 7, 10]) VBox(...) >>> def x_values(max): return np.arange(0, max) >>> def y_values(xs, sd): ... return xs + np.random.normal(len(xs), scale=sd) >>> line(x_values, y_values, max=(10, 50), sd=(1, 10)) VBox(...)",1,0,0,1,2,1,0,0,1,2
"def start_app(self, package_name, activity=None, stop=False):
  _pattern = re.compile(r'TotalTime: (\d+)')
  if activity is None:
  self.adb_shell(['monkey', '-p', package_name, '-c',
  'android.intent.category.LAUNCHER', '1'])
  else:
  args = ['-W']
  if stop:
  args.append('-S')
  output = self.adb_shell(
  ['am', 'start'] + args + ['-n', '%s/%s' % (package_name, activity)])
  m = _pattern.search(output)
  if m:
  return int(m.group(1))/1000.0","Start application Args: - package_name (string): like com.example.app1 - activity (string): optional, activity name Returns time used (unit second), if activity is not None Document: usage: adb shell am start -D: enable debugging -W: wait for launch to complete --start-profiler <FILE>: start profiler and send results to <FILE> --sampling INTERVAL: use sample profiling with INTERVAL microseconds between samples (use with --start-profiler) -P <FILE>: like above, but profiling stops when app goes idle -R: repeat the activity launch <COUNT> times. Prior to each repeat, the top activity will be finished. -S: force stop the target app before starting the activity --opengl-trace: enable tracing of OpenGL functions --user <USER_ID> | current: Specify which user to run as; if not specified then run as the current user.",0,0,0,1,1,1,0,0,1,2
"def get_user_of_group(config, fas, groupname):
  if not _cache.is_configured:
  _cache.configure(**config['fmn.rules.cache'])
  key = cache_key_generator(get_user_of_group, groupname)
  def creator():
  if not fas:
  return set()
  return set([u.username for u in fas.group_members(groupname)])
  return _cache.get_or_create(key, creator)",Return the list of users in the specified group. :arg config: a dict containing the fedmsg config :arg fas: a fedora.client.fas2.AccountSystem object instanciated and loged into FAS. :arg groupname: the name of the group for which we want to retrieve the members. :return: a list of FAS user members of the specified group.,2,0,0,1,3,1,0,0,0,1
"def can_fetch (self, useragent, url):
  log.debug(LOG_CHECK, ""%r check allowance for:\n user agent: %r\n url: %r ..."", self.url, useragent, url)
  if not isinstance(useragent, str):
  useragent = useragent.encode(""ascii"", ""ignore"")
  if not isinstance(url, str):
  url = url.encode(""ascii"", ""ignore"")
  if self.disallow_all:
  log.debug(LOG_CHECK, "" ... disallow all."")
  return False
  if self.allow_all:
  log.debug(LOG_CHECK, "" ... allow all."")
  return True
  url = urllib.quote(urlparse.urlparse(urllib.unquote(url))[2]) or ""/""
  for entry in self.entries:
  if entry.applies_to(useragent):
  return entry.allowance(url)
  if self.default_entry is not None:
  return self.default_entry.allowance(url)
  log.debug(LOG_CHECK, "" ... agent not found, allow."")
  return True","Using the parsed robots.txt decide if useragent can fetch url. @return: True if agent can fetch url, else False @rtype: bool",1,0,0,0,1,1,0,0,1,2
"def make_fpfList(options):
  user_values = options.fpf
  defaults = ['0.0001', '0.001', '0.01', '0.05']
  if user_values:
  for fpf in user_values:
  if fpf not in defaults:
  defaults.append(fpf)
  defaults.sort()
  return defaults",aggregate default fpf values and user-defined fpf values where enrichment factor calculations will be attempted. :param options: SplitInput object :return defaults: list includes default fpf values & unique user-defined values,0,0,0,1,1,1,0,0,1,2
"def get_authentic_node_name(self, node_name: str) -> Optional[str]:
  vertex: IGraphVertex = None
  try:
  vertex: IGraphVertex = self._wrapped_graph.vs.find(node_name)
  except ValueError:
  pass
  if vertex is None:
  try:
  vertex: IGraphVertex = self._wrapped_graph.vs[int(node_name)]
  except ValueError:
  return None
  except IndexError:
  return None
  try:
  return vertex[""name""]
  except KeyError:
  return str(vertex.index)","Returns the exact, authentic node name for the given node name if a node corresponding to the given name exists in the graph (maybe not locally yet) or `None` otherwise. By default, this method checks whether a node with the given name exists locally in the graph and return `node_name` if it does or `None` otherwise. In `Graph` extensions that are used by applications where the user can enter potentially incorrect node names, this method should be overridden to improve usability. Arguments: node_name (str): The node name to return the authentic node name for. Returns: The authentic name of the node corresponding to the given node name or `None` if no such node exists.",1,0,1,1,3,1,0,0,1,2
"def get_file_contents(source_path: str) -> str:
  open_funcs = [
  functools.partial(codecs.open, source_path, encoding='utf-8'),
  functools.partial(open, source_path, 'r')
  ]
  for open_func in open_funcs:
  try:
  with open_func() as f:
  return f.read()
  except Exception:
  pass
  return (
  'raise IOError(""Unable to load step file at: {}"")'
  .format(source_path)
  )","Loads the contents of the source into a string for execution using multiple loading methods to handle cross-platform encoding edge cases. If none of the load methods work, a string is returned that contains an error function response that will be displayed when the step is run alert the user to the error. :param source_path: Path of the step file to load.",1,0,0,1,2,1,0,0,1,2
"def date_range(self):
  try:
  days = int(self.days)
  except ValueError:
  exit_after_echo(QUERY_DAYS_INVALID)
  if days < 1:
  exit_after_echo(QUERY_DAYS_INVALID)
  start = datetime.today()
  end = start + timedelta(days=days)
  return (
  datetime.strftime(start, '%Y-%m-%d'),
  datetime.strftime(end, '%Y-%m-%d')
  )",Generate date range according to the `days` user input.,1,0,0,1,2,1,0,0,1,2
"def get_user_name(self, uid, return_none_on_error=True):
  response = self.raw_command(netfn=0x06, command=0x46, data=(uid,))
  if 'error' in response:
  if return_none_on_error:
  return None
  raise Exception(response['error'])
  name = None
  if 'data' in response:
  data = response['data']
  if len(data) == 16:
  n = ''.join(chr(data[i]) for i in range(0, len(data)))
  n = n.rstrip(""\x00"")
  if len(n) > 0:
  name = n
  return name",Get user name :param uid: user number [1:16] :param return_none_on_error: return None on error TODO: investigate return code on error,1,0,0,1,2,1,0,0,1,2
"def commit(self):
  self._check_state()
  database = self._session._database
  api = database.spanner_api
  metadata = _metadata_with_prefix(database.name)
  response = api.commit(
  self._session.name,
  self._mutations,
  transaction_id=self._transaction_id,
  metadata=metadata,
  )
  self.committed = _pb_timestamp_to_datetime(response.commit_timestamp)
  del self._session._transaction
  return self.committed",Commit mutations to the database. :rtype: datetime :returns: timestamp of the committed changes. :raises ValueError: if there are no mutations to commit.,0,1,0,0,1,1,1,0,1,3
"def parse_discovery_service_response(url="""", query="""",
  returnIDParam=""entityID""):
  if url:
  part = urlparse(url)
  qsd = parse_qs(part[4])
  elif query:
  qsd = parse_qs(query)
  else:
  qsd = {}
  try:
  return qsd[returnIDParam][0]
  except KeyError:
  return """"","Deal with the response url from a Discovery Service :param url: the url the user was redirected back to or :param query: just the query part of the URL. :param returnIDParam: This is where the identifier of the IdP is place if it was specified in the query. Default is 'entityID' :return: The IdP identifier or """" if none was given",1,0,0,1,2,2,0,0,1,3
"def get_issue_generator(self, user_id, project_id, project_name):
  user_tasks_data = self.call_api(
  ""/projects/"" + six.text_type(project_id) + ""/user-tasks"")
  for key, task in enumerate(user_tasks_data):
  assigned_task = self.get_task_dict(project_id, key, task)
  if assigned_task:
  log.debug(
  "" Adding '"" + assigned_task['description'] +
  ""' to task list."")
  yield assigned_task","Approach: 1. Get user ID from bugwarriorrc file 2. Get list of tickets from /user-tasks for a given project 3. For each ticket/task returned from #2, get ticket/task info and check if logged-in user is primary (look at `is_owner` and `user_id`)",2,0,0,1,3,2,0,0,1,3
"def setup_server_users(server):
  log_info(""Checking if there are any users that need to be added for ""
  ""server '%s'..."" % server.id)
  seed_users = server.get_seed_users()
  count_new_users = 0
  if not server.is_slave():
  count_new_users += setup_server_admin_users(server)
  for dbname, db_seed_users in seed_users.items():
  if dbname in [""admin"", ""local""]:
  continue
  count_new_users += setup_server_db_users(server, dbname, db_seed_users)
  if count_new_users > 0:
  log_info(""Added %s users."" % count_new_users)
  else:
  log_verbose(""Did not add any new users."")",Seeds all users returned by get_seed_users() IF there are no users seed yet i.e. system.users collection is empty,0,1,1,0,2,1,1,0,0,2
"def v1_subfolder_add(request, response, kvlclient,
  fid, sfid, cid, subid=None):
  if subid is not None:
  assert '@' not in subid
  path = [
  urllib.unquote(fid),
  urllib.unquote(sfid),
  cid + (('@' + subid) if subid is not None else ''),
  ]
  path = '/'.join(path)
  new_folders(kvlclient, request).put(path)
  response.status = 201","Adds a subtopic to a subfolder for the current user. The route for this endpoint is: ``PUT /dossier/v1/folder/<fid>/subfolder/<sfid>/<cid>/<subid>``. ``fid`` is the folder identifier, e.g., ``My_Folder``. ``sfid`` is the subfolder identifier, e.g., ``My_Subtopic``. ``cid`` and ``subid`` are the content id and subtopic id of the subtopic being added to the subfolder. If the subfolder does not already exist, it is created automatically. N.B. An empty subfolder cannot exist! If the subtopic was added successfully, ``201`` status is returned. (Temporarily, the ""current user"" can be set via the ``annotator_id`` query parameter.)",1,0,0,1,2,1,1,0,1,3
"def find_raw_batches(self, *args, **kwargs):
  if ""session"" in kwargs:
  raise ConfigurationError(
  ""find_raw_batches does not support sessions"")
  return RawBatchCursor(self, *args, **kwargs)","Query the database and retrieve batches of raw BSON. Similar to the :meth:`find` method but returns a :class:`~pymongo.cursor.RawBatchCursor`. This example demonstrates how to work with raw batches, but in practice raw batches should be passed to an external library that can decode BSON into another data type, rather than used with PyMongo's :mod:`bson` module. >>> import bson >>> cursor = db.test.find_raw_batches() >>> for batch in cursor: ... print(bson.decode_all(batch)) .. note:: find_raw_batches does not support sessions. .. versionadded:: 3.6",0,0,1,0,1,1,0,0,1,2
"def descendants(self, node, relations=None, reflexive=False):
  if reflexive:
  decs = self.descendants(node, relations, reflexive=False)
  decs.append(node)
  return decs
  g = None
  if relations is None:
  g = self.get_graph()
  else:
  g = self.get_filtered_graph(relations)
  if node in g:
  return list(nx.descendants(g, node))
  else:
  return []","Returns all descendants of specified node. The default implementation is to use networkx, but some implementations of the Ontology class may use a database or service backed implementation, for large graphs. Arguments --------- node : str identifier for node in ontology reflexive : bool if true, return query node in graph relations : list relation (object property) IDs used to filter Returns ------- list[str] descendant node IDs",0,0,0,1,1,1,0,0,1,2
"def get_user(user=None):
  if user is None:
  user = getSecurityManager().getUser()
  elif isinstance(user, MemberData):
  user = user.getUser()
  elif isinstance(user, basestring):
  user = get_member_by_login_name(get_portal(), user, False)
  if user:
  user = user.getUser()
  return user","Get the user object :param user: A user id, memberdata object or None for the current user :returns: Plone User (PlonePAS) / Propertied User (PluggableAuthService)",1,0,1,1,3,1,0,1,1,3
"def ip_link_add(session, name, type_='loopback', lladdr='00:00:00:00:00:00'):
  intf = ip_link_show(session, ifname=name)
  if intf:
  LOG.debug('Interface ""%s"" already exists: %s', intf.ifname, intf)
  return intf
  if type_ == 'ethernet':
  intf = Interface(
  ifname=name,
  flags=DEFAULT_ETH_FLAGS,
  ifmtu=DEFAULT_ETH_MTU,
  ifmtu6=DEFAULT_ETH_MTU,
  hw_addr=lladdr)
  else:
  intf = Interface(
  ifname=name,
  inet='127.0.0.1/8',
  inet6='::1/128')
  session.add(intf)
  return intf","Adds an interface record into Zebra protocol service database. The arguments are similar to ""ip link add"" command of iproute2. :param session: Session instance connecting to database. :param name: Name of interface. :param type_: Type of interface. 'loopback' or 'ethernet'. :param lladdr: Link layer address. Mostly MAC address. :return: Instance of added record or already existing record.",1,0,0,1,2,1,1,0,0,2
"def draft(self, **kwargs):
  tmos_ver = self._meta_data['bigip']._meta_data['tmos_version']
  legacy = kwargs.pop('legacy', False)
  if LooseVersion(tmos_ver) < LooseVersion('12.1.0') or legacy:
  raise DraftPolicyNotSupportedInTMOSVersion(
  ""Drafting on this version of BIG-IP is not supported""
  )
  kwargs = dict(
  createDraft=True
  )
  super(Policy, self)._modify(**kwargs)
  get_kwargs = {
  'name': self.name,
  'partition': self.partition,
  'uri_as_parts': True,
  'subPath': 'Drafts'
  }
  base_uri = self._meta_data['container']._meta_data['uri']
  session = self._meta_data['bigip']._meta_data['icr_session']
  response = session.get(base_uri, **get_kwargs)
  json_data = response.json()
  self._local_update(json_data)
  self._activate_URI(json_data['selfLink'])","Allows for easily re-drafting a policy After a policy has been created, it was not previously possible to re-draft the published policy. This method makes it possible for a user with existing, published, policies to create drafts from them so that they are modifiable. See https://github.com/F5Networks/f5-common-python/pull/1099 :param kwargs: :return:",0,0,1,0,1,1,1,0,1,3
"def write_result(self, data):
  data['custom_timers'] = ujson.dumps(data['custom_timers'])
  self.results.append(data)
  if len(self.results) >= 150:
  with db.execution_context():
  with db.atomic():
  Result.insert_many(self.results).execute()
  del self.results[:]",Write the results received to the database :param dict data: the data to save in database :return: None,0,1,0,0,1,0,1,1,0,2
"def _get_db_options(args):
  import optik, getpass,sys
  from optik import OptionParser
  parser=OptionParser()
  parser.add_option(""-d"",""--database"",
  action=""store"", type=""string"", dest=""database"",
  default=""cfht"",
  help=""Name of the SYBASE database containing TABLE"",
  metavar=""FILE"")
  parser.add_option(""-u"",""--user"",
  action=""store"", type=""string"", dest=""user"",
  default=getpass.getuser(),
  help=""User name to access db with"",
  metavar=""USER"")
  (opt, unused_args) = parser.parse_args(args)
  return opt.database,opt.user,unused_args",Parse through a command line of arguments to over-ride the values in the users .dbrc file. If no user name is given then the environment variable $USERNAME is used. If $USERNAME is not defined then prompt for input.,1,0,0,1,2,1,0,0,1,2
"def connect(host, username, password, port=443, verify=False, debug=False):
  context = ssl.SSLContext(ssl.PROTOCOL_TLSv1_2)
  if not verify:
  context.verify_mode = ssl.CERT_NONE
  requests.packages.urllib3.disable_warnings()
  try:
  si = SmartConnect(
  host=host,
  user=username,
  pwd=password,
  port=port,
  sslContext=context
  )
  atexit.register(Disconnect, si)
  return si.RetrieveContent()
  except IOError as e:
  print('I/O error({0}): {1}'.format(e.errno, e.strerror))
  except vmodl.MethodFault as e:
  print('Connection could not be established', file=sys.stderr)
  raise ConnectionError('Connection could not be established')
  print('Caught vmodl fault: ', e.msg, file=sys.stderr)
  if debug:
  traceback.print_exc()
  except Exception as e:
  print('Caught exception:', str(e), file=sys.stderr)
  if debug:
  traceback.print_exc()",Connect to a vCenter via the API :param host: Hostname or IP of the vCenter :type host: str or unicode :param username: Username :type user: str or unicode :param password: Password :type user: str or unicode :param port: Port on which the vCenter API is running (default: 443) :type port: int :param verify: Whether to verify SSL certs upon connection (default: False) :type verify: bool :param debug: Debug option (default: False) :type debug: bool :return: Content :rtype: vim.ServiceInstanceContent,1,0,0,2,3,1,0,0,1,2
"def call(self, op_name, query=None, **kwargs):
  LOG.debug(kwargs)
  kwargs = {k: v for k, v in kwargs.items() if v}
  if query:
  query = jmespath.compile(query)
  if self.client.can_paginate(op_name):
  paginator = self.client.get_paginator(op_name)
  results = paginator.paginate(**kwargs)
  data = results.build_full_result()
  else:
  op = getattr(self.client, op_name)
  data = op(**kwargs)
  if query:
  data = query.search(data)
  return data",Make a request to a method in this client. The response data is returned from this call as native Python data structures. This method differs from just calling the client method directly in the following ways: * It automatically handles the pagination rather than relying on a separate pagination method call. * You can pass an optional jmespath query and this query will be applied to the data returned from the low-level call. This allows you to tailor the returned data to be exactly what you want. * It automatically gets rid of uneeded parameters (with None values) :type op_name: str :param op_name: The name of the request you wish to make. :type query: str :param query: A jmespath query that will be applied to the data returned by the operation prior to returning it to the user. :type kwargs: keyword arguments :param kwargs: Additional keyword arguments you want to pass to the method when making the request.,1,0,0,1,2,2,0,0,1,3
"def pubsubhubbub(self, mode, topic, callback, secret=''):
  from re import match
  m = match('https?://[\w\d\-\.\:]+/\w+/[\w\._-]+/events/\w+', topic)
  status = False
  if mode and topic and callback and m:
  data = [('hub.mode', mode), ('hub.topic', topic),
  ('hub.callback', callback)]
  if secret:
  data.append(('hub.secret', secret))
  url = self._build_url('hub')
  status = self._boolean(self._post(url, data=data, json=False), 204,
  404)
  return status","Create/update a pubsubhubbub hook. :param str mode: (required), accepted values: ('subscribe', 'unsubscribe') :param str topic: (required), form: https://github.com/:user/:repo/events/:event :param str callback: (required), the URI that receives the updates :param str secret: (optional), shared secret key that generates a SHA1 HMAC of the payload content. :returns: bool",1,0,0,1,2,1,0,0,1,2
"def get(self, *args, **kwargs):
  kwargs = self._inject_request_kwargs(kwargs)
  url = reddit_url(*args)
  r = requests.get(url, **kwargs)
  if r.status_code == 200:
  thing = self._thingify(json.loads(r.content), path=urlparse(r.url).path)
  return thing
  else:
  raise BadResponse(r)","Sends a GET request to a reddit path determined by ``args``. Basically ``.get('foo', 'bar', 'baz')`` will GET http://www.reddit.com/foo/bar/baz/.json. ``kwargs`` supplied will be passed to :meth:`requests.get` after having ``user_agent`` and ``cookies`` injected. Injection only occurs if they don't already exist. Returns :class:`things.Blob` object or a subclass of :class:`things.Blob`, or raises :class:`exceptions.BadResponse` if not a 200 Response. :param \*args: strings that will form the path to GET :param \*\*kwargs: extra keyword arguments to be passed to :meth:`requests.get`",1,0,0,1,2,2,0,0,1,3
"def plot_reg(xvals, yvals, poly, x_label=""x"", y_label=""y"", data_label=""data"",
  reg_label=""regression line"", fname=None):
  import matplotlib.pyplot as plt
  plt.plot(xvals, yvals, ""bo"", label=data_label)
  if not (poly is None):
  plt.plot(xvals, np.polyval(poly, xvals), ""r-"", label=reg_label)
  plt.xlabel(x_label)
  plt.ylabel(y_label)
  plt.legend(loc=""best"")
  if fname is None:
  plt.show()
  else:
  plt.savefig(fname)
  plt.close()","Helper function to plot trend lines for line-fitting approaches. This function will show a plot through ``plt.show()`` and close it after the window has been closed by the user. Args: xvals (list/array of float): list of x-values yvals (list/array of float): list of y-values poly (list/array of float): polynomial parameters as accepted by ``np.polyval`` Kwargs: x_label (str): label of the x-axis y_label (str): label of the y-axis data_label (str): label of the data reg_label(str): label of the regression line fname (str): file name (if not None, the plot will be saved to disc instead of showing it though ``plt.show()``)",1,0,0,1,2,1,0,0,1,2
"def recordModelProgress(self, modelID, modelParams, modelParamsHash, results,
  completed, completionReason, matured, numRecords):
  if results is None:
  metricResult = None
  else:
  metricResult = results[1].values()[0]
  errScore = self._resultsDB.update(modelID=modelID,
  modelParams=modelParams,modelParamsHash=modelParamsHash,
  metricResult=metricResult, completed=completed,
  completionReason=completionReason, matured=matured,
  numRecords=numRecords)
  self.logger.debug('Received progress on model %d: completed: %s, '
  'cmpReason: %s, numRecords: %d, errScore: %s' ,
  modelID, completed, completionReason, numRecords, errScore)
  (bestModelID, bestResult) = self._resultsDB.bestModelIdAndErrScore()
  self.logger.debug('Best err score seen so far: %s on model %s' % \
  (bestResult, bestModelID))","Record or update the results for a model. This is called by the HSW whenever it gets results info for another model, or updated results on a model that is still running. The first time this is called for a given modelID, the modelParams will contain the params dict for that model and the modelParamsHash will contain the hash of the params. Subsequent updates of the same modelID will have params and paramsHash values of None (in order to save overhead). The Hypersearch object should save these results into it's own working memory into some table, which it then uses to determine what kind of new models to create next time createModels() is called. Parameters: ---------------------------------------------------------------------- modelID: ID of this model in models table modelParams: params dict for this model, or None if this is just an update of a model that it already previously reported on. See the comments for the createModels() method for a description of this dict. modelParamsHash: hash of the modelParams dict, generated by the worker that put it into the model database. results: tuple containing (allMetrics, optimizeMetric). Each is a dict containing metricName:result pairs. . May be none if we have no results yet. completed: True if the model has completed evaluation, False if it is still running (and these are online results) completionReason: One of the ClientJobsDAO.CMPL_REASON_XXX equates matured: True if this model has matured. In most cases, once a model matures, it will complete as well. The only time a model matures and does not complete is if it's currently the best model and we choose to keep it running to generate predictions. numRecords: Number of records that have been processed so far by this model.",0,1,1,0,2,1,1,0,0,2
"def is_result_edition_allowed(self, analysis_brain):
  if not self.is_analysis_edition_allowed(analysis_brain):
  return False
  obj = api.get_object(analysis_brain)
  if not obj.getDetectionLimitOperand():
  return True
  if obj.getDetectionLimitSelector():
  if not obj.getAllowManualDetectionLimit():
  return False
  return True","Checks if the edition of the result field is allowed :param analysis_brain: Brain that represents an analysis :return: True if the user can edit the result field, otherwise False",1,0,1,1,3,1,0,0,1,2
"def suggest_path(root_dir):
  if not root_dir:
  return [os.path.abspath(os.sep), '~', os.curdir, os.pardir]
  if '~' in root_dir:
  root_dir = os.path.expanduser(root_dir)
  if not os.path.exists(root_dir):
  root_dir, _ = os.path.split(root_dir)
  return list_path(root_dir)","List all files and subdirectories in a directory. If the directory is not specified, suggest root directory, user directory, current and parent directory. :param root_dir: string: directory to list :return: list",1,0,0,1,2,1,0,0,1,2
"def execute(self, using=None):
  if not using:
  using = self.get_connection()
  inserted_entities = {}
  for klass in self.orders:
  number = self.quantities[klass]
  if klass not in inserted_entities:
  inserted_entities[klass] = []
  for i in range(0, number):
  entity = self.entities[klass].execute(using, inserted_entities)
  inserted_entities[klass].append(entity)
  return inserted_entities",Populate the database using all the Entity classes previously added. :param using A Django database connection name :rtype: A list of the inserted PKs,1,1,0,1,3,0,1,1,0,2
"def make_gffutils_db(gtf, db):
  import gffutils
  out_db = gffutils.create_db(gtf,
  db,
  keep_order=True,
  infer_gene_extent=False)
  return out_db",Make database for gffutils. Parameters ---------- gtf : str Path to Gencode gtf file. db : str Path to save database to. Returns ------- out_db : gffutils.FeatureDB gffutils feature database.,1,1,0,0,2,0,0,0,0,0
"def watch(self, pipeline=None, full_document='default', resume_after=None,
  max_await_time_ms=None, batch_size=None, collation=None,
  start_at_operation_time=None, session=None):
  cursor_class = create_class_with_framework(
  AgnosticChangeStream, self._framework, self.__module__)
  return cursor_class(self, pipeline, full_document, resume_after,
  max_await_time_ms, batch_size, collation,
  start_at_operation_time, session)","Watch changes on this cluster. Returns a :class:`~MotorChangeStream` cursor which iterates over changes on all databases in this cluster. Introduced in MongoDB 4.0. See the documentation for :meth:`MotorCollection.watch` for more details and examples. :Parameters: - `pipeline` (optional): A list of aggregation pipeline stages to append to an initial ``$changeStream`` stage. Not all pipeline stages are valid after a ``$changeStream`` stage, see the MongoDB documentation on change streams for the supported stages. - `full_document` (optional): The fullDocument option to pass to the ``$changeStream`` stage. Allowed values: 'default', 'updateLookup'. Defaults to 'default'. When set to 'updateLookup', the change notification for partial updates will include both a delta describing the changes to the document, as well as a copy of the entire document that was changed from some time after the change occurred. - `resume_after` (optional): The logical starting point for this change stream. - `max_await_time_ms` (optional): The maximum time in milliseconds for the server to wait for changes before responding to a getMore operation. - `batch_size` (optional): The maximum number of documents to return per batch. - `collation` (optional): The :class:`~pymongo.collation.Collation` to use for the aggregation. - `start_at_operation_time` (optional): If provided, the resulting change stream will only return changes that occurred at or after the specified :class:`~bson.timestamp.Timestamp`. Requires MongoDB >= 4.0. - `session` (optional): a :class:`~pymongo.client_session.ClientSession`. :Returns: A :class:`~MotorChangeStream`. .. versionadded:: 2.0 .. mongodoc:: changeStreams",1,0,1,1,3,1,0,0,1,2
"def request(self, endpoint, method=""GET"", params=None):
  if endpoint.startswith(""http""):
  url = endpoint
  else:
  url = ""%s/%s.json"" % (self.api_url, endpoint)
  if method != ""GET"":
  if self.api_key is None:
  raise SafecastPyAuthError(""Require an api_key"")
  url = url + ""?api_key={0}"".format(self.api_key)
  content = self._request(url, method=method, params=params, api_call=url)
  return content","Return dict of response received from Safecast's API :param endpoint: (required) Full url or Safecast API endpoint (e.g. measurements/users) :type endpoint: string :param method: (optional) Method of accessing data, either GET, POST, PUT or DELETE. (default GET) :type method: string :param params: (optional) Dict of parameters (if any) accepted the by Safecast API endpoint you are trying to access (default None) :type params: dict or None :rtype: dict",1,0,0,1,2,2,0,0,1,3
"def alterar(
  self,
  id_interface,
  nome,
  protegida,
  descricao,
  id_ligacao_front,
  id_ligacao_back,
  tipo=None,
  vlan=None):
  if not is_valid_int_param(id_interface):
  raise InvalidParameterError(
  u'Interface id is invalid or was not informed.')
  url = 'interface/' + str(id_interface) + '/'
  interface_map = dict()
  interface_map['nome'] = nome
  interface_map['protegida'] = protegida
  interface_map['descricao'] = descricao
  interface_map['id_ligacao_front'] = id_ligacao_front
  interface_map['id_ligacao_back'] = id_ligacao_back
  interface_map['tipo'] = tipo
  interface_map['vlan'] = vlan
  code, xml = self.submit({'interface': interface_map}, 'PUT', url)
  return self.response(code, xml)","Edit an interface by its identifier. Equipment identifier is not changed. :param nome: Interface name. :param protegida: Indication of protected ('0' or '1'). :param descricao: Interface description. :param id_ligacao_front: Front end link interface identifier. :param id_ligacao_back: Back end link interface identifier. :param id_interface: Interface identifier. :return: None :raise InvalidParameterError: The parameters interface id, nome and protegida are none or invalid. :raise NomeInterfaceDuplicadoParaEquipamentoError: There is already an interface with this name for this equipment. :raise InterfaceNaoExisteError: Front link interface and/or back link interface doesn't exist. :raise DataBaseError: Networkapi failed to access the database. :raise XMLError: Networkapi failed to generate the XML response.",2,1,0,1,4,1,0,0,2,3
"def create_key(self, title, key):
  json = None
  if title and key:
  data = {'title': title, 'key': key}
  url = self._build_url('keys', base_url=self._api)
  json = self._json(self._post(url, data=data), 201)
  return Key(json, self) if json else None","Create a deploy key. :param str title: (required), title of key :param str key: (required), key text :returns: :class:`Key <github3.users.Key>` if successful, else None",1,0,0,2,3,2,0,0,2,4
"def _serve_environment(self, request):
  return http_util.Respond(
  request,
  {
  'data_location': self._logdir or self._db_uri,
  'mode': 'db' if self._db_uri else 'logdir',
  'window_title': self._window_title,
  },
  'application/json')",Serve a JSON object containing some base properties used by the frontend. * data_location is either a path to a directory or an address to a database (depending on which mode TensorBoard is running in). * window_title is the title of the TensorBoard web page.,2,0,1,1,4,1,0,0,1,2
"def multiple_choice(question, choices, answers):
  if not isinstance(answers, (int, collections.Iterable)):
  raise TypeError(
  'The `answers` arg is expected to be of type '
  '(int | iterable int) but got {} instead.'.format(type(answers))
  )
  @curry
  def check_answer(index, button):
  is_correct = (
  index == answers if isinstance(answers, int) else index in answers
  )
  button.style.button_color = GREEN if is_correct else RED
  answer_choices = []
  for index, choice in enumerate(choices):
  button = widgets.Button(
  layout=widgets.Layout(width='20px', height='20px', padding='0')
  )
  button.on_click(check_answer(index))
  button_and_question = widgets.HBox(
  [button, widgets.HTML(TEXT_STYLE.format(choice))],
  layout=widgets.Layout(align_items='center')
  )
  answer_choices.append(button_and_question)
  question_html = [widgets.HTML(TEXT_STYLE.format(question))]
  display(widgets.VBox(question_html + answer_choices))","Generates a multiple choice question that allows the user to select an answer choice and shows whether choice was correct. Args: question (str): Question text displayed above choices. choices (list str): Answer choices that user can select. answers (int | iterable int): Either an integer or iterable of integers. Each integer in answers corresponds to the index of the correct choice in `choices`. Returns: None >>> multiple_choice(question=""What is 10 + 2 * 5?"", ... choices=['12', '60', '20'], ... answers=2) #doctest: +SKIP <What is 10 + 2 * 5?> <Button> <12> <Button> <60> <Button> <20> (Correct) >>> multiple_choice(question=""Select all prime numbers."", ... choices=['12', '3', '31'], ... answers=[1, 2]) #doctest: +SKIP <Select all prime numbers.> <Button> <12> <Button> <3> (Correct) <Button> <31> (Correct)",1,0,0,1,2,1,0,0,1,2
"def write_settings(settings):
  if not os.access(DATA_DIR, os.W_OK): return False
  try:
  f = open(DATA_DIR + os.sep + SETTINGS_FILE, 'w')
  f.writelines(json.dumps(settings, indent=0))
  f.close()
  os.chmod(os.path.abspath(DATA_DIR + os.sep + SETTINGS_FILE), 0o777)
  except IOError:
  return False
  else:
  return True",Saves user's settings @returns True on success @returns False on failure,0,0,0,1,1,1,0,0,0,1
"def get_chat(self, chat_id):
  assert_type_or_raise(chat_id, (int, unicode_type), parameter_name=""chat_id"")
  result = self.do(""getChat"", chat_id=chat_id)
  if self.return_python_objects:
  logger.debug(""Trying to parse {data}"".format(data=repr(result)))
  from pytgbot.api_types.receivable.peer import Chat
  try:
  return Chat.from_array(result)
  except TgApiParseException:
  logger.debug(""Failed parsing as api_type Chat"", exc_info=True)
  raise TgApiParseException(""Could not parse result."")
  return result","Use this method to get up to date information about the chat (current name of the user for one-on-one conversations, current username of a user, group or channel, etc.). Returns a Chat object on success. https://core.telegram.org/bots/api#getchat Parameters: :param chat_id: Unique identifier for the target chat or username of the target supergroup or channel (in the format @channelusername) :type chat_id: int | str|unicode Returns: :return: Returns a Chat object on success :rtype: pytgbot.api_types.receivable.peer.Chat",2,0,0,1,3,2,0,0,1,3
"def set_user_project_permission(self, project_id, user_id, auth_role):
  put_data = {
  ""auth_role[id]"": auth_role
  }
  return self._put(""/projects/"" + project_id + ""/permissions/"" + user_id, put_data,
  content_type=ContentType.form)",Send PUT request to /projects/{project_id}/permissions/{user_id/ with auth_role value. :param project_id: str uuid of the project :param user_id: str uuid of the user :param auth_role: str project role eg 'project_admin' :return: requests.Response containing the successful result,1,0,0,1,2,1,0,0,2,3
"async def update(self, **params):
  assert_or_raise(all(k in self._update_parameters for k in params.keys()),
  NameError,
  'Wrong parameter given')
  res = await self.connection('PUT',
  'tournaments/{}'.format(self._id),
  'tournament',
  **params)
  self._refresh_from_json(res)","update some parameters of the tournament Use this function if you want to update multiple options at once, but prefer helpers functions like :func:`allow_attachments`, :func:`set_start_date`... |methcoro| Args: params: one or more of: ``name`` ``tournament_type`` ``url`` ``subdomain`` ``description`` ``open_signup`` ``hold_third_place_match`` ``pts_for_match_win`` ``pts_for_match_tie`` ``pts_for_game_win`` ``pts_for_game_tie`` ``pts_for_bye`` ``swiss_rounds`` ``ranked_by`` ``rr_pts_for_match_win`` ``rr_pts_for_match_tie`` ``rr_pts_for_game_win`` ``rr_pts_for_game_tie`` ``accept_attachments`` ``hide_forum`` ``show_rounds`` ``private`` ``notify_users_when_matches_open`` ``notify_users_when_the_tournament_ends`` ``sequential_pairings`` ``signup_cap`` ``start_at`` ``check_in_duration`` ``grand_finals_modifier`` Raises: APIException",2,0,0,2,4,1,0,0,1,2
"def _concat_queries(queries, operators='__and__'):
  if not queries:
  raise ValueError('Expected some `queries`, got {}.'.format(queries))
  if len(queries) == 1:
  return queries[0]
  if isinstance(operators, str):
  operators = [operators] * (len(queries) - 1)
  if len(queries) - 1 != len(operators):
  raise ValueError('Expected `operators` to be a string or a list with the same'
  ' length as `field_names` ({}), got {}.'.format(len(queries),
  operators))
  first, rest, end = queries[0], queries[1:-1], queries[-1:][0]
  bigop = getattr(first, operators[0])
  for i, q in enumerate(rest):
  bigop = getattr(bigop(q), operators[i])
  return bigop(end)",Create a tinyDB Query object that is the concatenation of each query in `queries`. The concatenation operator is taken from `operators`. Parameters ---------- queries: list of tinydb.Query The list of tinydb.Query to be joined. operators: str or list of str List of binary operators to join `queries` into one query. Check TinyDB.Query class for possible choices. Returns ------- query: tinydb.database.Query,0,0,1,0,1,1,0,0,1,2
"def put_name(self, type_, id_, name):
  cachefile = self.filename(type_, id_)
  dirname = os.path.dirname(cachefile)
  try:
  os.makedirs(dirname)
  except OSError as e:
  if e.errno != errno.EEXIST:
  raise
  with open(cachefile, 'w') as f:
  f.write(name)","Write a cached name to disk. :param type_: str, ""user"" or ""tag"" :param id_: int, eg. 123456 :returns: None",1,0,0,0,1,0,0,0,0,0
"def list_groups(self, filtr, url_prefix, auth, session, send_opts):
  req = self.get_group_request(
  'GET', 'application/json', url_prefix, auth)
  if filtr is not None:
  if not filtr == 'member' and not filtr == 'maintainer':
  raise RuntimeError(
  'filtr must be either ""member"", ""maintainer"", or None.')
  req.params = {'filter': filtr}
  prep = session.prepare_request(req)
  resp = session.send(prep, **send_opts)
  if resp.status_code == 200:
  resp_json = resp.json()
  return resp_json['groups']
  msg = ('List groups failed, got HTTP response: ({}) - {}'.format(
  resp.status_code, resp.text))
  raise HTTPError(msg, request = req, response = resp)",Get the groups the logged in user is a member of. Optionally filter by 'member' or 'maintainer'. Args: filtr (string|None): ['member'|'maintainer'] or defaults to None. url_prefix (string): Protocol + host such as https://api.theboss.io auth (string): Token to send in the request header. session (requests.Session): HTTP session to use for request. send_opts (dictionary): Additional arguments to pass to session.send(). Returns: (list[string]): List of group names. Raises: requests.HTTPError on failure.,2,0,0,1,3,2,0,0,1,3
"def handle_unexpected_redirect_to_login_page(self, response):
  next_url = URLObject(response.url).query_dict.get(""next"")
  login_url = (
  URLObject(""http://"")
  .with_hostname(self.domain)
  .with_port(self.port)
  .with_path(LOGIN_API_PATH)
  )
  if next_url:
  login_url = login_url.set_query_param(""next"", next_url)
  credentials = {
  ""email"": self.login_email,
  ""password"": self.login_password,
  }
  headers = {
  b""X-CSRFToken"": get_csrf_token(response),
  }
  yield scrapy.FormRequest(
  login_url,
  formdata=credentials,
  headers=headers,
  callback=self.after_login,
  errback=self.handle_error
  )","This method is called if the crawler has been unexpectedly logged out. If that happens, and the crawler requests a page that requires a logged-in user, the crawler will be redirected to a login page, with the originally-requested URL as the `next` query parameter. This method simply causes the crawler to log back in using the saved email and password credentials. We rely on the fact that the login page will redirect the user to the URL in the `next` query parameter if the login is successful -- this will allow the crawl to resume where it left off. This is method is very much like the `get_initial_login()` method, but the callback is `self.after_login` instead of `self.after_initial_login`.",1,0,0,1,2,1,0,0,1,2
"def sendTo(self, loc, usr=None):
  if not loc in self._messages:
  return False
  if not self.usr and not usr:
  return False
  if self.usr: usr = self.usr
  pg = usr.getPage(""http://www.neopets.com/iteminfo.phtml?obj_id="" + str(self.id))
  form = pg.form(name=""item_form"")
  form['action'] = loc
  pg = form.submit()
  return self._messages[loc] in pg.content","Transfer's an item from user's inventory to another inventory, returns result Parameters: loc (str) -- Location to send the item to (see Item.SHOP, Item.SDB, etc.) usr (User) -- User who has the item Returns bool - True if successful, false otherwise",2,0,0,1,3,1,0,0,1,2
"def list_user_comments(self, topic_id, user_alias=None):
  user_alias = user_alias or self.api.user_alias
  comment_start = 0
  results = []
  while comment_start is not None:
  comments = self.list_comments(topic_id, comment_start)
  results += [item for item in comments['results'] if item['author_alias'] == user_alias]
  comment_start = comments['next_start']
  return results", :param topic_id: ID :param user_alias: ID :return: ,2,0,0,1,3,2,0,0,1,3
"def validate_users(self, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.validate_users_with_http_info(**kwargs)
  else:
  (data) = self.validate_users_with_http_info(**kwargs)
  return data","Returns valid users and invalid identifiers from the given list # noqa: E501 # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.validate_users(async_req=True) >>> result = thread.get() :param async_req bool :param list[str] body: :return: ResponseContainerValidatedUsersDTO If the method is called asynchronously, returns the request thread.",2,0,0,1,3,2,0,0,1,3
"def get_calendar_for_object(self, obj, distinction=''):
  calendar_list = self.get_calendars_for_object(obj, distinction)
  if len(calendar_list) == 0:
  raise Calendar.DoesNotExist(""Calendar does not exist."")
  elif len(calendar_list) > 1:
  raise AssertionError(""More than one calendars were found."")
  else:
  return calendar_list[0]","This function gets a calendar for an object. It should only return one calendar. If the object has more than one calendar related to it (or more than one related to it under a distinction if a distinction is defined) an AssertionError will be raised. If none are returned it will raise a DoesNotExistError. >>> user = User.objects.get(username='tony') >>> try: ... Calendar.objects.get_calendar_for_object(user) ... except Calendar.DoesNotExist: ... print(""failed"") ... failed Now if we add a calendar it should return the calendar >>> calendar = Calendar(name='My Cal') >>> calendar.save() >>> calendar.create_relation(user) >>> Calendar.objects.get_calendar_for_object(user) <Calendar: My Cal> Now if we add one more calendar it should raise an AssertionError because there is more than one related to it. If you would like to get more than one calendar for an object you should use get_calendars_for_object (see below). >>> calendar = Calendar(name='My 2nd Cal') >>> calendar.save() >>> calendar.create_relation(user) >>> try: ... Calendar.objects.get_calendar_for_object(user) ... except AssertionError: ... print(""failed"") ... failed",1,0,1,1,3,1,0,1,1,3
"async def authenticate_token_service_url(auth_header: str, credentials: CredentialProvider, service_url: str, channel_id: str) -> ClaimsIdentity:
  identity = await asyncio.ensure_future(
  ChannelValidation.authenticate_token(auth_header, credentials, channel_id))
  service_url_claim = identity.get_claim_value(ChannelValidation.SERVICE_URL_CLAIM)
  if service_url_claim != service_url:
  raise Exception('Unauthorized. service_url claim do not match.')
  return identity","Validate the incoming Auth Header Validate the incoming Auth Header as a token sent from the Bot Framework Service. A token issued by the Bot Framework emulator will FAIL this check. :param auth_header: The raw HTTP header in the format: 'Bearer [longString]' :type auth_header: str :param credentials: The user defined set of valid credentials, such as the AppId. :type credentials: CredentialProvider :param service_url: Claim value that must match in the identity. :type service_url: str :return: A valid ClaimsIdentity. :raises Exception:",2,0,0,1,3,1,0,0,1,2
"def antonym(phrase, format=""json""):
  base_url = Vocabulary.__get_api_link(""bighugelabs"")
  url = base_url.format(word=phrase)
  json_obj = Vocabulary.__return_json(url)
  if not json_obj:
  return False
  result = []
  visited = {}
  idx = 0
  for key in json_obj.keys():
  antonyms = json_obj[key].get('ant', False)
  if not antonyms:
  continue
  for antonym in antonyms:
  if visited.get(antonym, False):
  continue
  result.append({'seq': idx, 'text': antonym})
  idx += 1
  visited[antonym] = True
  if not result:
  return False
  return Response().respond(result, format)","queries the bighugelabs API for the antonym. The results include - ""syn"" (synonym) - ""ant"" (antonym) - ""rel"" (related terms) - ""sim"" (similar terms) - ""usr"" (user suggestions) But currently parsing only the antonym as I have already done - synonym (using glosbe API) :param phrase: word for which antonym is to be found :param format: response structure type. Defaults to: ""json"" :returns: returns a json object :raises KeyError: returns False when no antonyms are found",2,0,0,1,3,1,0,0,1,2
"def get_notifications(self, login=None, **kwargs):
  _login = kwargs.get(
  'login',
  login or self._login
  )
  _notif_url = NOTIF_URL.format(login=_login)
  return self._request_api(url=_notif_url).json()",Get the current notifications of a user. :return: JSON,2,0,0,1,3,2,0,0,1,3
"def check_acceptable_xmx(xmx_string):
  acceptable_xmx = True
  acceptable_suffixes = ['K', 'M', 'G']
  if xmx_string[-1].upper() not in acceptable_suffixes:
  acceptable_xmx = False
  logging.error('ERROR: Memory must be specified as K (kilobytes), M (megabytes), or G (gigabytes). Your specified '
  'suffix was {}.'.format(xmx_string[-1]))
  if '.' in xmx_string:
  acceptable_xmx = False
  logging.error('ERROR: Xmx strings must be integers, floating point numbers are not accepted.')
  if not str.isdigit(xmx_string[:-1]):
  acceptable_xmx = False
  logging.error('ERROR: The amount of memory requested was not an integer.')
  return acceptable_xmx","BBTools can have their memory set manually. This will check that the memory setting is actually valid :param xmx_string: The users requested XMX, as a string. :return: True if the Xmx string will be accepted by BBTools, otherwise false.",0,0,0,1,1,1,0,0,1,2
"def followed_streams(self, limit=25, offset=0):
  r = self.kraken_request('GET', 'streams/followed',
  params={'limit': limit,
  'offset': offset})
  return models.Stream.wrap_search(r)",Return the streams the current user follows. Needs authorization ``user_read``. :param limit: maximum number of results :type limit: :class:`int` :param offset: offset for pagination :type offset: :class:`int` :returns: A list of streams :rtype: :class:`list`of :class:`models.Stream` instances :raises: :class:`exceptions.NotAuthorizedError`,1,0,0,1,2,2,0,0,1,3
"def mkdir_chown(paths, user_group=None, permissions='ug=rwX,o=rX', create_parent=True, check_if_exists=False, recursive=False):
  def _generate_str(path):
  mkdir_str = mkdir(path, create_parent, check_if_exists)
  chown_str = chown(user_group, path, recursive) if user_group else None
  chmod_str = chmod(permissions, path, recursive) if permissions else None
  return ' && '.join(n for n in (mkdir_str, chown_str, chmod_str) if n)
  if isinstance(paths, (tuple, list)):
  return '; '.join((_generate_str(path) for path in paths))
  return _generate_str(paths)","Generates a unix command line for creating a directory and assigning permissions to it. Shortcut to a combination of :func:`~mkdir`, :func:`~chown`, and :func:`~chmod`. Note that if `check_if_exists` has been set to ``True``, and the directory is found, `mkdir` is not called, but `user_group` and `permissions` are still be applied. :param paths: Can be a single path string, or a list or tuple of path strings. :type paths: unicode | str | tuple[unicode | str] | list[unicode | str] :param: Optional owner of the directory. For notation, see :func:`~get_user_group`. :type user_group: unicode | str | int | tuple :param permissions: Optional permission mode, in any notation accepted by the unix `chmod` command. Default is ``ug=rwX,o=rX``. :type permissions: unicode | str :param create_parent: Parent directories are created if not present (`-p` argument to `mkdir`). :type create_parent: bool :param check_if_exists: Prior to creating the directory, checks if it already exists. :type check_if_exists: bool :param recursive: Apply permissions and owner change recursively. :type recursive: bool :return: Unix shell command line. :rtype: unicode | str",1,0,0,1,2,1,0,0,1,2
"def connect(jclassname, driver_args, jars=None, libs=None):
  if _gateway_is_running():
  gateway = java_gateway.JavaGateway()
  else:
  driver_args = [driver_args] if isinstance(driver_args, str) else driver_args
  if jars:
  classpath = os.pathsep.join(jars) if isinstance(jars, list) else jars
  else:
  classpath = None
  if libs:
  javaopts = libs if isinstance(libs, list) else [libs]
  else:
  javaopts = []
  gateway = java_gateway.JavaGateway.launch_gateway(
  port=25333, classpath=classpath, javaopts=javaopts, die_on_exit=True)
  java_gateway.java_import(gateway.jvm, 'java.sql.DriverManager')
  gateway.jvm.Class.forName(jclassname)
  connection = gateway.jvm.DriverManager.getConnection(*driver_args)
  if _converters is None:
  types_map = {}
  for type in gateway.jvm.Class.forName(""java.sql.Types"").getFields():
  types_map[type.getName()] = type.getInt(None)
  _init_converters(types_map)
  return Connection(connection, _converters)",Open a connection to a database using a JDBC driver and return a Connection instance. jclassname: Full qualified Java class name of the JDBC driver. driver_args: Argument or sequence of arguments to be passed to the Java DriverManager.getConnection method. Usually the database URL. See http://docs.oracle.com/javase/6/docs/api/java/sql/DriverManager.html for more details jars: Jar filename or sequence of filenames for the JDBC driver libs: Dll/so filenames or sequence of dlls/sos used as shared library by the JDBC driver,0,0,0,1,1,1,0,0,1,2
"def execute(self, processProtocol, command, env={},
  path=None, uid=None, gid=None, usePTY=0, childFDs=None):
  sshCommand = (command if isinstance(command, SSHCommand)
  else SSHCommand(command, self.precursor, path))
  commandLine = sshCommand.getCommandLine()
  connectionDeferred = self.getConnection(uid)
  connectionDeferred.addCallback(connectProcess, processProtocol,
  commandLine, env, usePTY, childFDs)
  return connectionDeferred","Execute a process on the remote machine using SSH @param processProtocol: the ProcessProtocol instance to connect @param executable: the executable program to run @param args: the arguments to pass to the process @param env: environment variables to request the remote ssh server to set @param path: the remote path to start the remote process on @param uid: user id or username to connect to the ssh server with @param gid: this is not used for remote ssh processes @param usePTY: wither to request a pty for the process @param childFDs: file descriptors to use for stdin, stdout and stderr",1,0,0,0,1,1,0,0,1,2
"def connect_get_namespaced_service_proxy_with_path(self, name, namespace, path, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.connect_get_namespaced_service_proxy_with_path_with_http_info(name, namespace, path, **kwargs)
  else:
  (data) = self.connect_get_namespaced_service_proxy_with_path_with_http_info(name, namespace, path, **kwargs)
  return data","connect_get_namespaced_service_proxy_with_path # noqa: E501 connect GET requests to proxy of Service # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.connect_get_namespaced_service_proxy_with_path(name, namespace, path, async_req=True) >>> result = thread.get() :param async_req bool :param str name: name of the ServiceProxyOptions (required) :param str namespace: object name and auth scope, such as for teams and projects (required) :param str path: path to the resource (required) :param str path2: Path is the part of URLs that include service endpoints, suffixes, and parameters to use for the current proxy request to service. For example, the whole request URL is http://localhost/api/v1/namespaces/kube-system/services/elasticsearch-logging/_search?q=user:kimchy. Path is _search?q=user:kimchy. :return: str If the method is called asynchronously, returns the request thread.",1,0,0,1,2,1,0,0,1,2
"def invite_by_email(self, email, user, organization, **kwargs):
  try:
  invitee = self.user_model.objects.get(email__iexact=email)
  except self.user_model.DoesNotExist:
  invitee = None
  user_invitation = self.invitation_model.objects.create(
  invitee=invitee,
  invitee_identifier=email.lower(),
  invited_by=user,
  organization=organization,
  )
  self.send_invitation(user_invitation)
  return user_invitation",Primary interface method by which one user invites another to join Args: email: request: **kwargs: Returns: an invitation instance Raises: MultipleObjectsReturned if multiple matching users are found,1,1,1,1,4,1,1,1,1,4
"def query_yes_no(question, default=""yes""):
  valid = {""yes"": Answers.YES, ""y"": Answers.YES, ""ye"": Answers.YES,
  ""no"": Answers.NO, ""n"": Answers.NO}
  if default is None:
  prompt = "" [y/n] ""
  elif default == ""yes"":
  prompt = "" [Y/n] ""
  elif default == ""no"":
  prompt = "" [y/N] ""
  else:
  raise ValueError(""invalid default answer: '%s'"" % default)
  while True:
  sys.stdout.write(question + prompt)
  choice = input().lower()
  if default is not None and choice == '':
  return valid[default]
  elif choice in valid:
  return valid[choice]
  else:
  sys.stdout.write(""Please respond with 'yes' or 'no' ""
  ""(or 'y' or 'n').\n"")","Ask a yes/no question via raw_input() and return their answer. ""question"" is a string that is presented to the user. ""default"" is the presumed answer if the user just hits <Enter>. It must be ""yes"" (the default), ""no"" or None (meaning an answer is required of the user). The return value is one of Answers.YES or Answers.NO. Copied (and modified) from http://stackoverflow.com/questions/3041986/python-command-line-yes-no-input",1,0,0,1,2,1,0,0,1,2
"def add_multiple(self, *users):
  guid = uuid.uuid4()
  for i, user_ in enumerate(users):
  user_['guid'] = '{}-{}'.format(guid, i)
  payload = {'members': users}
  url = utils.urljoin(self.url, 'add')
  response = self.session.post(url, json=payload)
  return MembershipRequest(self, *users, group_id=self.group_id,
  **response.data)","Add multiple users to the group at once. Each given user must be a dictionary containing a nickname and either an email, phone number, or user_id. :param args users: the users to add :return: a membership request :rtype: :class:`MembershipRequest`",2,0,0,2,4,1,0,0,2,3
"async def create_conversation(self, reference: ConversationReference, logic):
  try:
  if reference.service_url is None:
  raise TypeError('BotFrameworkAdapter.create_conversation(): reference.service_url cannot be None.')
  parameters = ConversationParameters(bot=reference.bot)
  client = self.create_connector_client(reference.service_url)
  resource_response = await client.conversations.create_conversation(parameters)
  request = TurnContext.apply_conversation_reference(Activity(), reference, is_incoming=True)
  request.conversation = ConversationAccount(id=resource_response.id)
  if resource_response.service_url:
  request.service_url = resource_response.service_url
  context = self.create_context(request)
  return await self.run_middleware(context, logic)
  except Exception as e:
  raise e",Starts a new conversation with a user. This is typically used to Direct Message (DM) a member of a group. :param reference: :param logic: :return:,1,0,0,2,3,1,0,0,1,2
"def message(self, user, message, **kwargs):
  payload = {""message"": message, ""user"": user, ""token"": self.token}
  for key, value in kwargs.iteritems():
  if key not in Pushover.message_keywords:
  raise ValueError(""{0}: invalid message parameter"".format(key))
  elif key == ""timestamp"" and value is True:
  payload[key] = int(time.time())
  elif key == ""sound"" and value not in self.sounds:
  raise ValueError(""{0}: invalid sound"".format(value))
  else:
  payload[key] = value
  return MessageRequest(payload)","Send `message` to the user specified by `user`. It is possible to specify additional properties of the message by passing keyword arguments. The list of valid keywords is ``title, priority, sound, callback, timestamp, url, url_title, device, retry, expire and html`` which are described in the Pushover API documentation. For convenience, you can simply set ``timestamp=True`` to set the timestamp to the current timestamp. An image can be attached to a message by passing a file-like object to the `attachment` keyword argument. This method returns a :class:`MessageRequest` object.",1,0,0,1,2,1,0,0,1,2
"def authenticate_redirect(self, callback_uri=None,
  ask_for=[""name"", ""email"", ""language"", ""username""]):
  callback_uri = callback_uri or request.url
  args = self._openid_args(callback_uri, ax_attrs=ask_for)
  return redirect(self._OPENID_ENDPOINT +
  (""&"" if ""?"" in self._OPENID_ENDPOINT else ""?"") +
  urllib.urlencode(args))","Performs a redirect to the authentication URL for this service. After authentication, the service will redirect back to the given callback URI. We request the given attributes for the authenticated user by default (name, email, language, and username). If you don't need all those attributes for your app, you can request fewer with the |ask_for| keyword argument.",2,0,0,1,3,2,0,0,1,3
"def absl_flags():
  flags_dict = flags.FLAGS.flags_by_module_dict()
  def _relevant_module(module_name):
  if __package__ and __package__ in module_name:
  return True
  if module_name == sys.argv[0]:
  return True
  return False
  return {
  flag.name: flag.value for module, flags in flags_dict.items()
  for flag in flags if _relevant_module(module)}","Extracts absl-py flags that the user has specified and outputs their key-value mapping. By default, extracts only those flags in the current __package__ and mainfile. Useful to put into a trial's param_map.",0,0,0,1,1,1,0,0,1,2
"def look_up(self, _from, _localField, _foreignField, _as):
  query = {
  'from': _from,
  'localField': _localField,
  'foreignField': _foreignField,
  'as': _as
  }
  self._q.append({'$lookup': query})
  return self",Adds look up stage at query (left outer join) :param _from: Specifies the collection in the same database to perform the join with. :param _localField: Specifies the field from the documents input to the $lookup stage. :param _foreignField: Specifies the field from the documents in the from collection. :param _as: Specifies the name of the new array field to add to the input documents. :return: The current object,0,0,1,0,1,0,0,0,1,1
"def kill_running_submission(self, submissionid, user_check=True):
  submission = self.get_submission(submissionid, user_check)
  if not submission:
  return False
  if ""jobid"" not in submission:
  return False
  return self._client.kill_job(submission[""jobid""])","Attempt to kill the remote job associated with this submission id. :param submissionid: :param user_check: Check if the current user owns this submission :return: True if the job was killed, False if an error occurred",1,0,1,2,4,1,0,0,1,2
"def list_user_page_views(self, user_id, end_time=None, start_time=None):
  path = {}
  data = {}
  params = {}
  path[""user_id""] = user_id
  if start_time is not None:
  params[""start_time""] = start_time
  if end_time is not None:
  params[""end_time""] = end_time
  self.logger.debug(""GET /api/v1/users/{user_id}/page_views with query params: {params} and form data: {data}"".format(params=params, data=data, **path))
  return self.generic_request(""GET"", ""/api/v1/users/{user_id}/page_views"".format(**path), data=data, params=params, all_pages=True)","List user page views. Return the user's page view history in json format, similar to the available CSV download. Pagination is used as described in API basics section. Page views are returned in descending order, newest to oldest.",2,0,0,1,3,2,0,0,1,3
"def init(uri, echo=False):
  global ENGINE, _METADATA, JOBS_TABLE, METADATA_TABLE, LOGS_TABLE
  ENGINE = sqlalchemy.create_engine(uri, echo=echo, convert_unicode=True)
  _METADATA = sqlalchemy.MetaData(ENGINE)
  JOBS_TABLE = _init_jobs_table()
  METADATA_TABLE = _init_metadata_table()
  LOGS_TABLE = _init_logs_table()
  _METADATA.create_all(ENGINE)","Initialise the database. Initialise the sqlalchemy engine, metadata and table objects that we use to connect to the database. Create the database and the database tables themselves if they don't already exist. :param uri: the sqlalchemy database URI :type uri: string :param echo: whether or not to have the sqlalchemy engine log all statements to stdout :type echo: bool",0,1,0,0,1,0,1,1,0,2
"def file_chooser(prompt_text = ""Enter File: "", default=None, filearg=[], filekwarg={}):
  try:
  import readline, rlcomplete
  completer = rlcomplete.PathCompleter()
  readline.set_completer_delims(completer.delims)
  readline.parse_and_bind(""tab: complete"")
  readline.set_completer(completer.complete)
  except ImportError:
  pass
  while True:
  f = raw_input(prompt_text)
  if f == '': return default
  f = os.path.expanduser(f)
  if len(f) != 0 and f[0] == os.path.sep:
  f = os.path.abspath(f)
  try:
  return open(f, *filearg, **filekwarg)
  except IOError as e:
  stderr.write(ERROR_MESSAGE % (""unable to open %s : %s"" % (f, e)))",A simple tool to get a file from the user. Takes keyworded arguemnts and passes them to open(). If the user enters nothing the function will return the ``default`` value. Otherwise it continues to prompt the user until it get's a decent response. filekwarg may contain arguements passed on to ``open()``.,1,0,0,1,2,1,0,0,1,2
"def eval(w,t,x,msk,s):
  assertType(w, (str, int, long))
  assertType(t, (str, int, long))
  assertType(x, (str, int, long))
  kw = genKw(w,msk,s)
  beta = hashG1(t, x)
  y = beta*kw
  return y,kw,beta","Pythia server-side computation of intermediate PRF output. @w: ensemble key selector (any string, e.g. webserver ID) @t: tweak (any string, e.g. user ID) @x: message (any string) @msk: Pythia server's master secret key @s: state value from Pythia server's key table @returns: (y, kw, dummy=None) where: y: intermediate result kw: secret key bound to w (needed for proof) beta: H(kw,t,x) (needed for proof)",1,0,0,0,1,0,0,0,1,1
"def initialize(self, name, reuse=False):
  user = self._kwargs['user']
  password = self._kwargs['password']
  host = self._kwargs['host']
  port = self._kwargs['port']
  if '-' in name:
  self.error(""dabase name '%s' cannot contain '-' characters"" % name)
  return CODE_VALUE_ERROR
  try:
  Database.create(user, password, name, host, port)
  db = Database(user, password, name, host, port)
  self.__load_countries(db)
  except DatabaseExists as e:
  if not reuse:
  self.error(str(e))
  return CODE_DATABASE_EXISTS
  except DatabaseError as e:
  self.error(str(e))
  return CODE_DATABASE_ERROR
  except LoadError as e:
  Database.drop(user, password, name, host, port)
  self.error(str(e))
  return CODE_LOAD_ERROR
  return CMD_SUCCESS","Create an empty Sorting Hat registry. This method creates a new database including the schema of Sorting Hat. Any attempt to create a new registry over an existing instance will produce an error, except if reuse=True. In that case, the database will be reused, assuming the database schema is correct (it won't be created in this case). :param name: name of the database :param reuse: reuse database if it already exists",1,1,1,0,3,1,1,1,0,3
"def get_item(self, item_number, raw=False):
  if not isinstance(item_number, int):
  item_number = int(item_number)
  suburl = ""v0/item/{}.json"".format(item_number)
  try:
  item_data = self._make_request(suburl)
  except requests.HTTPError as e:
  hn_logger.exception('Faulted on item request for item {}, with status {}'.format(item_number, e.errno))
  raise e
  if not item_data:
  raise ValueError('Item id {} not found!'.format(item_number))
  return item_data if raw else HackerNewsItem(**item_data)","Get a dictionary or object with info about the given item number from the Hacker News API. Item can be a poll, story, comment or possibly other entry. Will raise an requests.HTTPError if we got a non-200 response back. Will raise a ValueError if a item_number that can not be converted to int was passed in, or the server has no information for that item number. (Possible) response parameters: ""id"" -> The item's unique id. Required. ""deleted"" -> true if the item is deleted. ""type"" -> The type of item. One of ""job"", ""story"", ""comment"", ""poll"", or ""pollopt"". ""by"" -> The username of the item's author. ""time"" -> Creation date of the item, in Unix Time. ""text"" -> The comment, Ask HN, or poll text. HTML. ""dead"" -> true if the item is dead. ""parent"" -> The item's parent. For comments, either another comment or the relevant story. For pollopts, the relevant poll. ""kids"" -> The ids of the item's comments, in ranked display order. ""url"" -> The URL of the story. ""score"" -> The story's score, or the votes for a pollopt. ""title"" -> The title of the story or poll. ""parts"" -> A list of related pollopts, in display order. :param item_number: an integer number for the HN item requested :param raw: (optional): If true, return the raw decoded JSON dict, if False, return a nice object with keywords as attributes. Default if False. :return: A dictionary with relevant info about the item, if successful.",2,0,0,1,3,2,0,0,1,3
"def raw_input(self, prompt=''):
  if self.has_readline:
  self.set_readline_completer()
  prompt = py3compat.cast_bytes_py2(prompt)
  try:
  line = py3compat.str_to_unicode(self.raw_input_original(prompt))
  except ValueError:
  warn(""\n********\nYou or a %run:ed script called sys.stdin.close()""
  "" or sys.stdout.close()!\nExiting IPython!\n"")
  self.ask_exit()
  return """"
  if self.autoindent:
  if num_ini_spaces(line) > self.indent_current_nsp:
  line = line[self.indent_current_nsp:]
  self.indent_current_nsp = 0
  return line","Write a prompt and read a line. The returned line does not include the trailing newline. When the user enters the EOF key sequence, EOFError is raised. Optional inputs: - prompt(''): a string to be printed to prompt the user. - continue_prompt(False): whether this line is the first one or a continuation in a sequence of inputs.",1,0,0,1,2,1,0,0,1,2
"def get_package_changes(self, feed_id, continuation_token=None, batch_size=None):
  route_values = {}
  if feed_id is not None:
  route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')
  query_parameters = {}
  if continuation_token is not None:
  query_parameters['continuationToken'] = self._serialize.query('continuation_token', continuation_token, 'long')
  if batch_size is not None:
  query_parameters['batchSize'] = self._serialize.query('batch_size', batch_size, 'int')
  response = self._send(http_method='GET',
  location_id='323a0631-d083-4005-85ae-035114dfb681',
  version='5.0-preview.1',
  route_values=route_values,
  query_parameters=query_parameters)
  return self._deserialize('PackageChangesResponse', response)","GetPackageChanges. [Preview API] Get a batch of package changes made to a feed. The changes returned are 'most recent change' so if an Add is followed by an Update before you begin enumerating, you'll only see one change in the batch. While consuming batches using the continuation token, you may see changes to the same package version multiple times if they are happening as you enumerate. :param str feed_id: Name or Id of the feed. :param long continuation_token: A continuation token which acts as a bookmark to a previously retrieved change. This token allows the user to continue retrieving changes in batches, picking up where the previous batch left off. If specified, all the changes that occur strictly after the token will be returned. If not specified or 0, iteration will start with the first change. :param int batch_size: Number of package changes to fetch. The default value is 1000. The maximum value is 2000. :rtype: :class:`<PackageChangesResponse> <azure.devops.v5_0.feed.models.PackageChangesResponse>`",2,0,0,1,3,2,0,0,1,3
"def create(self, title, teamId=None, **request_parameters):
  check_type(title, basestring)
  check_type(teamId, basestring)
  post_data = dict_from_items_with_values(
  request_parameters,
  title=title,
  teamId=teamId,
  )
  json_data = self._session.post(API_ENDPOINT, json=post_data)
  return self._object_factory(OBJECT_TYPE, json_data)",Create a room. The authenticated user is automatically added as a member of the room. Args: title(basestring): A user-friendly name for the room. teamId(basestring): The team ID with which this room is associated. **request_parameters: Additional request parameters (provides support for parameters that may be added in the future). Returns: Room: A Room with the details of the created room. Raises: TypeError: If the parameter types are incorrect. ApiError: If the Webex Teams cloud returns an error.,1,0,0,2,3,1,0,0,2,3
"def resolve_url(url, desktop_user_agent=None, mobile_user_agent=None):
  if not desktop_user_agent:
  desktop_user_agent = DESKTOP_USER_AGENT
  if not mobile_user_agent:
  mobile_user_agent = MOBILE_USER_AGENT
  input_urls = set()
  parsed = urlparse(url_with_protocol(url))
  netloc = parsed.netloc
  if netloc.startswith('www.'):
  netloc = netloc[4:]
  input_urls.add('http://%s%s' % (netloc, parsed.path if parsed.path else '/'))
  input_urls.add('http://www.%s%s' % (netloc, parsed.path if parsed.path else '/'))
  resolved_urls = set()
  for input_url in input_urls:
  desktop_request = requests.get(input_url, headers={'User-Agent': desktop_user_agent})
  resolved_urls.add(desktop_request.url)
  mobile_request = requests.get(input_url, headers={'User-Agent': mobile_user_agent})
  resolved_urls.add(mobile_request.url)
  return list(resolved_urls)",Url Resolver Given a url a list of resolved urls is returned for desktop and mobile user agents,2,0,0,1,3,1,0,0,1,2
"def get_networks(context, limit=None, sorts=['id'], marker=None,
  page_reverse=False, filters=None, fields=None):
  LOG.info(""get_networks for tenant %s with filters %s, fields %s"" %
  (context.tenant_id, filters, fields))
  filters = filters or {}
  nets = db_api.network_find(context, limit, sorts, marker, page_reverse,
  join_subnets=True, **filters) or []
  nets = [v._make_network_dict(net, fields=fields) for net in nets]
  return nets",Retrieve a list of networks. The contents of the list depends on the identity of the user making the request (as indicated by the context) as well as any filters. : param context: neutron api request context : param filters: a dictionary with keys that are valid keys for a network as listed in the RESOURCE_ATTRIBUTE_MAP object in neutron/api/v2/attributes.py. Values in this dictiontary are an iterable containing values that will be used for an exact match comparison for that value. Each result returned by this function will have matched one of the values for each key in filters. : param fields: a list of strings that are valid keys in a network dictionary as listed in the RESOURCE_ATTRIBUTE_MAP object in neutron/api/v2/attributes.py. Only these fields will be returned.,1,0,1,1,3,1,0,1,1,3
"def getElementsCustomFilter(self, filterFunc):
  elements = []
  for child in self.children:
  if filterFunc(child) is True:
  elements.append(child)
  elements += child.getElementsCustomFilter(filterFunc)
  return TagCollection(elements)","getElementsCustomFilter - Searches children of this tag for those matching a provided user function @param filterFunc <function> - A function or lambda expression that should return ""True"" if the passed node matches criteria. @return - TagCollection of matching results @see getFirstElementCustomFilter",1,0,0,1,2,1,0,0,1,2
"def modify_cluster(ClusterIdentifier=None, ClusterType=None, NodeType=None, NumberOfNodes=None, ClusterSecurityGroups=None, VpcSecurityGroupIds=None, MasterUserPassword=None, ClusterParameterGroupName=None, AutomatedSnapshotRetentionPeriod=None, PreferredMaintenanceWindow=None, ClusterVersion=None, AllowVersionUpgrade=None, HsmClientCertificateIdentifier=None, HsmConfigurationIdentifier=None, NewClusterIdentifier=None, PubliclyAccessible=None, ElasticIp=None, EnhancedVpcRouting=None):
  pass","Modifies the settings for a cluster. For example, you can add another security or parameter group, update the preferred maintenance window, or change the master user password. Resetting a cluster password or modifying the security groups associated with a cluster do not need a reboot. However, modifying a parameter group requires a reboot for parameters to take effect. For more information about managing clusters, go to Amazon Redshift Clusters in the Amazon Redshift Cluster Management Guide . You can also change node type and the number of nodes to scale up or down the cluster. When resizing a cluster, you must specify both the number of nodes and the node type even if one of the parameters does not change. See also: AWS API Documentation :example: response = client.modify_cluster( ClusterIdentifier='string', ClusterType='string', NodeType='string', NumberOfNodes=123, ClusterSecurityGroups=[ 'string', ], VpcSecurityGroupIds=[ 'string', ], MasterUserPassword='string', ClusterParameterGroupName='string', AutomatedSnapshotRetentionPeriod=123, PreferredMaintenanceWindow='string', ClusterVersion='string', AllowVersionUpgrade=True|False, HsmClientCertificateIdentifier='string', HsmConfigurationIdentifier='string', NewClusterIdentifier='string', PubliclyAccessible=True|False, ElasticIp='string', EnhancedVpcRouting=True|False ) :type ClusterIdentifier: string :param ClusterIdentifier: [REQUIRED] The unique identifier of the cluster to be modified. Example: examplecluster :type ClusterType: string :param ClusterType: The new cluster type. When you submit your cluster resize request, your existing cluster goes into a read-only mode. After Amazon Redshift provisions a new cluster based on your resize requirements, there will be outage for a period while the old cluster is deleted and your connection is switched to the new cluster. You can use DescribeResize to track the progress of the resize request. Valid Values: multi-node | single-node :type NodeType: string :param NodeType: The new node type of the cluster. If you specify a new node type, you must also specify the number of nodes parameter. When you submit your request to resize a cluster, Amazon Redshift sets access permissions for the cluster to read-only. After Amazon Redshift provisions a new cluster according to your resize requirements, there will be a temporary outage while the old cluster is deleted and your connection is switched to the new cluster. When the new connection is complete, the original access permissions for the cluster are restored. You can use DescribeResize to track the progress of the resize request. Valid Values: ds1.xlarge | ds1.8xlarge | ds2.xlarge | ds2.8xlarge | dc1.large | dc1.8xlarge . :type NumberOfNodes: integer :param NumberOfNodes: The new number of nodes of the cluster. If you specify a new number of nodes, you must also specify the node type parameter. When you submit your request to resize a cluster, Amazon Redshift sets access permissions for the cluster to read-only. After Amazon Redshift provisions a new cluster according to your resize requirements, there will be a temporary outage while the old cluster is deleted and your connection is switched to the new cluster. When the new connection is complete, the original access permissions for the cluster are restored. You can use DescribeResize to track the progress of the resize request. Valid Values: Integer greater than 0 . :type ClusterSecurityGroups: list :param ClusterSecurityGroups: A list of cluster security groups to be authorized on this cluster. This change is asynchronously applied as soon as possible. Security groups currently associated with the cluster, and not in the list of groups to apply, will be revoked from the cluster. Constraints: Must be 1 to 255 alphanumeric characters or hyphens First character must be a letter Cannot end with a hyphen or contain two consecutive hyphens (string) -- :type VpcSecurityGroupIds: list :param VpcSecurityGroupIds: A list of virtual private cloud (VPC) security groups to be associated with the cluster. (string) -- :type MasterUserPassword: string :param MasterUserPassword: The new password for the cluster master user. This change is asynchronously applied as soon as possible. Between the time of the request and the completion of the request, the MasterUserPassword element exists in the PendingModifiedValues element of the operation response. Note Operations never return the password, so this operation provides a way to regain access to the master user account for a cluster if the password is lost. Default: Uses existing setting. Constraints: Must be between 8 and 64 characters in length. Must contain at least one uppercase letter. Must contain at least one lowercase letter. Must contain one number. Can be any printable ASCII character (ASCII code 33 to 126) except ' (single quote), ' (double quote), , /, @, or space. :type ClusterParameterGroupName: string :param ClusterParameterGroupName: The name of the cluster parameter group to apply to this cluster. This change is applied only after the cluster is rebooted. To reboot a cluster use RebootCluster . Default: Uses existing setting. Constraints: The cluster parameter group must be in the same parameter group family that matches the cluster version. :type AutomatedSnapshotRetentionPeriod: integer :param AutomatedSnapshotRetentionPeriod: The number of days that automated snapshots are retained. If the value is 0, automated snapshots are disabled. Even if automated snapshots are disabled, you can still create manual snapshots when you want with CreateClusterSnapshot . If you decrease the automated snapshot retention period from its current value, existing automated snapshots that fall outside of the new retention period will be immediately deleted. Default: Uses existing setting. Constraints: Must be a value from 0 to 35. :type PreferredMaintenanceWindow: string :param PreferredMaintenanceWindow: The weekly time range (in UTC) during which system maintenance can occur, if necessary. If system maintenance is necessary during the window, it may result in an outage. This maintenance window change is made immediately. If the new maintenance window indicates the current time, there must be at least 120 minutes between the current time and end of the window in order to ensure that pending changes are applied. Default: Uses existing setting. Format: ddd:hh24:mi-ddd:hh24:mi, for example wed:07:30-wed:08:00 . Valid Days: Mon | Tue | Wed | Thu | Fri | Sat | Sun Constraints: Must be at least 30 minutes. :type ClusterVersion: string :param ClusterVersion: The new version number of the Amazon Redshift engine to upgrade to. For major version upgrades, if a non-default cluster parameter group is currently in use, a new cluster parameter group in the cluster parameter group family for the new version must be specified. The new cluster parameter group can be the default for that cluster parameter group family. For more information about parameters and parameter groups, go to Amazon Redshift Parameter Groups in the Amazon Redshift Cluster Management Guide . Example: 1.0 :type AllowVersionUpgrade: boolean :param AllowVersionUpgrade: If true , major version upgrades will be applied automatically to the cluster during the maintenance window. Default: false :type HsmClientCertificateIdentifier: string :param HsmClientCertificateIdentifier: Specifies the name of the HSM client certificate the Amazon Redshift cluster uses to retrieve the data encryption keys stored in an HSM. :type HsmConfigurationIdentifier: string :param HsmConfigurationIdentifier: Specifies the name of the HSM configuration that contains the information the Amazon Redshift cluster can use to retrieve and store keys in an HSM. :type NewClusterIdentifier: string :param NewClusterIdentifier: The new identifier for the cluster. Constraints: Must contain from 1 to 63 alphanumeric characters or hyphens. Alphabetic characters must be lowercase. First character must be a letter. Cannot end with a hyphen or contain two consecutive hyphens. Must be unique for all clusters within an AWS account. Example: examplecluster :type PubliclyAccessible: boolean :param PubliclyAccessible: If true , the cluster can be accessed from a public network. Only clusters in VPCs can be set to be publicly available. :type ElasticIp: string :param ElasticIp: The Elastic IP (EIP) address for the cluster. Constraints: The cluster must be provisioned in EC2-VPC and publicly-accessible through an Internet gateway. For more information about provisioning clusters in EC2-VPC, go to Supported Platforms to Launch Your Cluster in the Amazon Redshift Cluster Management Guide. :type EnhancedVpcRouting: boolean :param EnhancedVpcRouting: An option that specifies whether to create the cluster with enhanced VPC routing enabled. To create a cluster that uses enhanced VPC routing, the cluster must be in a VPC. For more information, see Enhanced VPC Routing in the Amazon Redshift Cluster Management Guide. If this option is true , enhanced VPC routing is enabled. Default: false :rtype: dict :return: { 'Cluster': { 'ClusterIdentifier': 'string', 'NodeType': 'string', 'ClusterStatus': 'string', 'ModifyStatus': 'string', 'MasterUsername': 'string', 'DBName': 'string', 'Endpoint': { 'Address': 'string', 'Port': 123 }, 'ClusterCreateTime': datetime(2015, 1, 1), 'AutomatedSnapshotRetentionPeriod': 123, 'ClusterSecurityGroups': [ { 'ClusterSecurityGroupName': 'string', 'Status': 'string' }, ], 'VpcSecurityGroups': [ { 'VpcSecurityGroupId': 'string', 'Status': 'string' }, ], 'ClusterParameterGroups': [ { 'ParameterGroupName': 'string', 'ParameterApplyStatus': 'string', 'ClusterParameterStatusList': [ { 'ParameterName': 'string', 'ParameterApplyStatus': 'string', 'ParameterApplyErrorDescription': 'string' }, ] }, ], 'ClusterSubnetGroupName': 'string', 'VpcId': 'string', 'AvailabilityZone': 'string', 'PreferredMaintenanceWindow': 'string', 'PendingModifiedValues': { 'MasterUserPassword': 'string', 'NodeType': 'string', 'NumberOfNodes': 123, 'ClusterType': 'string', 'ClusterVersion': 'string', 'AutomatedSnapshotRetentionPeriod': 123, 'ClusterIdentifier': 'string', 'PubliclyAccessible': True|False, 'EnhancedVpcRouting': True|False }, 'ClusterVersion': 'string', 'AllowVersionUpgrade': True|False, 'NumberOfNodes': 123, 'PubliclyAccessible': True|False, 'Encrypted': True|False, 'RestoreStatus': { 'Status': 'string', 'CurrentRestoreRateInMegaBytesPerSecond': 123.0, 'SnapshotSizeInMegaBytes': 123, 'ProgressInMegaBytes': 123, 'ElapsedTimeInSeconds': 123, 'EstimatedTimeToCompletionInSeconds': 123 }, 'HsmStatus': { 'HsmClientCertificateIdentifier': 'string', 'HsmConfigurationIdentifier': 'string', 'Status': 'string' }, 'ClusterSnapshotCopyStatus': { 'DestinationRegion': 'string', 'RetentionPeriod': 123, 'SnapshotCopyGrantName': 'string' }, 'ClusterPublicKey': 'string', 'ClusterNodes': [ { 'NodeRole': 'string', 'PrivateIPAddress': 'string', 'PublicIPAddress': 'string' }, ], 'ElasticIpStatus': { 'ElasticIp': 'string', 'Status': 'string' }, 'ClusterRevisionNumber': 'string', 'Tags': [ { 'Key': 'string', 'Value': 'string' }, ], 'KmsKeyId': 'string', 'EnhancedVpcRouting': True|False, 'IamRoles': [ { 'IamRoleArn': 'string', 'ApplyStatus': 'string' }, ] } } :returns: available creating deleting final-snapshot hardware-failure incompatible-hsm incompatible-network incompatible-parameters incompatible-restore modifying rebooting renaming resizing rotating-keys storage-full updating-hsm",2,0,0,2,4,1,0,0,2,3
"def validate_params(valid_options, params):
  if not params:
  return
  data_filter = ['data', 'source', 'external_url', 'embed']
  multiple_data = [key for key in params.keys() if key in data_filter]
  if len(multiple_data) > 1:
  raise Exception(""You can't mix and match data parameters"")
  disallowed_fields = [key for key in params.keys() if key not in valid_options]
  if disallowed_fields:
  field_strings = "","".join(disallowed_fields)
  raise Exception(""{} are not allowed fields"".format(field_strings))","Helps us validate the parameters for the request :param valid_options: a list of strings of valid options for the api request :param params: a dict, the key-value store which we really only care about the key which has tells us what the user is using for the API request :returns: None or throws an exception if the validation fails",1,0,0,1,2,1,0,0,1,2
"def shareItem(sharedItem, toRole=None, toName=None, shareID=None,
  interfaces=ALL_IMPLEMENTED):
  warnings.warn(""Use Role.shareItem() instead of sharing.shareItem()."",
  PendingDeprecationWarning,
  stacklevel=2)
  if toRole is None:
  if toName is not None:
  toRole = getPrimaryRole(sharedItem.store, toName, True)
  else:
  toRole = getEveryoneRole(sharedItem.store)
  return toRole.shareItem(sharedItem, shareID, interfaces)","Share an item with a given role. This provides a way to expose items to users for later retrieval with L{Role.getShare}. This API is slated for deprecation. Prefer L{Role.shareItem} in new code. @param sharedItem: an item to be shared. @param toRole: a L{Role} instance which represents the group that has access to the given item. May not be specified if toName is also specified. @param toName: a unicode string which uniquely identifies a L{Role} in the same store as the sharedItem. @param shareID: a unicode string. If provided, specify the ID under which the shared item will be shared. @param interfaces: a list of Interface objects which specify the methods and attributes accessible to C{toRole} on C{sharedItem}. @return: a L{Share} which records the ability of the given role to access the given item.",1,0,0,1,2,1,0,1,1,3
"def send_badge_messages(self, badge_award):
  user_message = getattr(badge_award.badge, ""user_message"", None)
  if callable(user_message):
  message = user_message(badge_award)
  else:
  message = user_message
  if message is not None:
  badge_award.user.message_set.create(message=message)","If the Badge class defines a message, send it to the user who was just awarded the badge.",1,0,1,1,3,1,0,0,1,2
"def get_user_best(self, username, *, mode=OsuMode.osu, limit=50):
  return self._make_req(endpoints.USER_BEST, dict(
  k=self.key,
  u=username,
  type=_username_type(username),
  m=mode.value,
  limit=limit
  ), JsonList(SoloScore))","Get a user's best scores. Parameters ---------- username : str or int A `str` representing the user's username, or an `int` representing the user's id. mode : :class:`osuapi.enums.OsuMode` The osu! game mode for which to look up. Defaults to osu!standard. limit The maximum number of results to return. Defaults to 50, maximum 100.",1,0,0,1,2,2,0,0,1,3
"def to_xdr_amount(value):
  if not isinstance(value, str):
  raise NotValidParamError(""Value of type '{}' must be of type String, but got {}"".format(value, type(value)))
  try:
  amount = int((Decimal(value) * ONE).to_integral_exact(context=Context(traps=[Inexact])))
  except decimal.Inexact:
  raise NotValidParamError(""Value of '{}' must have at most 7 digits after the decimal."".format(value))
  except decimal.InvalidOperation:
  raise NotValidParamError(""Value of '{}' must represent a positive number."".format(value))
  return amount","Converts an amount to the appropriate value to send over the network as a part of an XDR object. Each asset amount is encoded as a signed 64-bit integer in the XDR structures. An asset amount unit (that which is seen by end users) is scaled down by a factor of ten million (10,000,000) to arrive at the native 64-bit integer representation. For example, the integer amount value 25,123,456 equals 2.5123456 units of the asset. This scaling allows for seven decimal places of precision in human-friendly amount units. This static method correctly multiplies the value by the scaling factor in order to come to the integer value used in XDR structures. See `Stellar's documentation on Asset Precision <https://www.stellar.org/developers/guides/concepts/assets.html#amount-precision-and-representation>`_ for more information. :param str value: The amount to convert to an integer for XDR serialization.",1,0,0,1,2,1,0,0,1,2
"def _get_real_user(user, anon_user=None):
  if hasattr(user, ""_get_current_object""):
  user = user._get_current_object()
  if callable(user):
  user = user()
  if anon_user and isinstance(user, anon_user):
  return None
  return user","Given a ""user"" that could be: * a real user object * a function that returns a real user object * a LocalProxy to a real user object (like Flask-Login's ``current_user``) This function returns the real user object, regardless of which we have.",1,0,1,1,3,1,0,1,1,3
"def find(self, filter={}, fields=None, skip=0, limit=None, sort=None):
  if not fields:
  self.__ensure_columns()
  fields = self.columns
  query_obj = Query(source=self.name, filter=filter, fields=fields, skip=skip, limit=limit, sort=sort)
  return QuerySet(cursor=self.cursor, query=query_obj)","Searches the table using the filters provided. :Examples: >>> users = user_table.find({'id': {'$in': [10, 20]}, 'age': {'$gt': 20}}) # Complex query >>> user_count = len(users) >>> for user in users: >>> # Do something... >>> print user.id >>> >>> users = user_table.find({}, sort=[('age', monsql.ASCENDING)]) # sort by age Also support complex operators: >>> {a: 1} # a == 1 >>> {a: {$gt: 1}} # a > 1 >>> {a: {$gte: 1}} # a >= 1 >>> {a: {$lt: 1}} # a < 1 >>> {a: {$lte: 1}} # a <= 1 >>> {a: {$eq: 1}} # a == 1 >>> {a: {$in: [1, 2]}} # a == 1 or a == 2 >>> {a: {$contains: '123'}} # a like %123% >>> {$not: condition} # !(condition) >>> {$and: [condition1, condition2, ...]} # condition1 and condition2 >>> {$or: [condition1, condition2, ...]} # condition1 or condition2 :Parameters: - query(dict): specify the WHERE clause. One example is {""name"": ""..."", ""id"": ...} - fields: specify what fields are needed - skip, limit: both integers, skip without defining limit is meaningless - sort: A list, each element is a two-item tuple, with the first item be the column name and the second item be either monsql.ASCENDING or monsql.DESCENDING :Return: a QuerySet object",1,0,1,1,3,1,0,1,1,3
"def vote_poll(
  self,
  chat_id: Union[int, str],
  message_id: id,
  option: int
  ) -> bool:
  poll = self.get_messages(chat_id, message_id).poll
  self.send(
  functions.messages.SendVote(
  peer=self.resolve_peer(chat_id),
  msg_id=message_id,
  options=[poll.options[option].data]
  )
  )
  return True","Use this method to vote a poll. Args: chat_id (``int`` | ``str``): Unique identifier (int) or username (str) of the target chat. For your personal cloud (Saved Messages) you can simply use ""me"" or ""self"". For a contact that exists in your Telegram address book you can use his phone number (str). message_id (``int``): Unique poll message identifier inside this chat. option (``int``): Index of the poll option you want to vote for (0 to 9). Returns: On success, True is returned. Raises: :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.",2,0,0,2,4,1,0,0,1,2
"async def get_chat_administrators(self, chat_id: typing.Union[base.Integer, base.String]
  ) -> typing.List[types.ChatMember]:
  payload = generate_payload(**locals())
  result = await self.request(api.Methods.GET_CHAT_ADMINISTRATORS, payload)
  return [types.ChatMember(**chatmember) for chatmember in result]","Use this method to get a list of administrators in a chat. Source: https://core.telegram.org/bots/api#getchatadministrators :param chat_id: Unique identifier for the target chat or username of the target supergroup or channel :type chat_id: :obj:`typing.Union[base.Integer, base.String]` :return: On success, returns an Array of ChatMember objects that contains information about all chat administrators except other bots. If the chat is a group or a supergroup and no administrators were appointed, only the creator will be returned. :rtype: :obj:`typing.List[types.ChatMember]`",2,0,0,1,3,2,0,0,1,3
"def add_editor(self, username, _delete=False, *args, **kwargs):
  url = self.reddit_session.config['wiki_page_editor']
  url = url.format(subreddit=six.text_type(self.subreddit),
  method='del' if _delete else 'add')
  data = {'page': self.page,
  'username': six.text_type(username)}
  return self.reddit_session.request_json(url, data=data, *args,
  **kwargs)","Add an editor to this wiki page. :param username: The name or Redditor object of the user to add. :param _delete: If True, remove the user as an editor instead. Please use :meth:`remove_editor` rather than setting it manually. Additional parameters are passed into :meth:`~praw.__init__.BaseReddit.request_json`.",1,0,0,1,2,2,0,0,2,4
"def get_sqlserver_product_version(engine: ""Engine"") -> Tuple[int]:
  assert is_sqlserver(engine), (
  ""Only call get_sqlserver_product_version() for Microsoft SQL Server ""
  ""instances.""
  )
  sql = ""SELECT CAST(SERVERPROPERTY('ProductVersion') AS VARCHAR)""
  rp = engine.execute(sql)
  row = rp.fetchone()
  dotted_version = row[0]
  return tuple(int(x) for x in dotted_version.split("".""))","Gets SQL Server version information. Attempted to use ``dialect.server_version_info``: .. code-block:: python from sqlalchemy import create_engine url = ""mssql+pyodbc://USER:PASSWORD@ODBC_NAME"" engine = create_engine(url) dialect = engine.dialect vi = dialect.server_version_info Unfortunately, ``vi == ()`` for an SQL Server 2014 instance via ``mssql+pyodbc``. It's also ``None`` for a ``mysql+pymysql`` connection. So this seems ``server_version_info`` is a badly supported feature. So the only other way is to ask the database directly. The problem is that this requires an :class:`Engine` or similar. (The initial hope was to be able to use this from within SQL compilation hooks, to vary the SQL based on the engine version. Still, this isn't so bad.) We could use either .. code-block:: sql SELECT @@version; -- returns a human-readable string SELECT SERVERPROPERTY('ProductVersion'); -- better The ``pyodbc`` interface will fall over with ``ODBC SQL type -150 is not yet supported`` with that last call, though, meaning that a ``VARIANT`` is coming back, so we ``CAST`` as per the source below.",1,0,1,1,3,1,0,1,1,3
"def compute_objective_value(objective_func, parameters, data=None, cl_runtime_info=None):
  return objective_func.evaluate({'data': data, 'parameters': Array(parameters, 'mot_float_type', mode='r')},
  parameters.shape[0], use_local_reduction=True, cl_runtime_info=cl_runtime_info)","Calculate and return the objective function value of the given model for the given parameters. Args: objective_func (mot.lib.cl_function.CLFunction): A CL function with the signature: .. code-block:: c double <func_name>(local const mot_float_type* const x, void* data, local mot_float_type* objective_list); parameters (ndarray): The parameters to use in the evaluation of the model, an (d, p) matrix with d problems and p parameters. data (mot.lib.kernel_data.KernelData): the user provided data for the ``void* data`` pointer. cl_runtime_info (mot.configuration.CLRuntimeInfo): the runtime information Returns: ndarray: vector matrix with per problem the objective function value",0,0,0,1,1,1,0,0,1,2
"def getName(self):
  r
  if self.__name:
  return self.__name
  elif self.__parent:
  par = self.__parent()
  if par:
  return par.__lookup(self)
  else:
  return None
  elif (len(self) == 1 and
  len(self.__tokdict) == 1 and
  next(iter(self.__tokdict.values()))[0][1] in (0,-1)):
  return next(iter(self.__tokdict.keys()))
  else:
  return None","r"""""" Returns the results name for this token expression. Useful when several different expressions might match at a particular location. Example:: integer = Word(nums) ssn_expr = Regex(r""\d\d\d-\d\d-\d\d\d\d"") house_number_expr = Suppress('#') + Word(nums, alphanums) user_data = (Group(house_number_expr)(""house_number"") | Group(ssn_expr)(""ssn"") | Group(integer)(""age"")) user_info = OneOrMore(user_data) result = user_info.parseString(""22 111-22-3333 #221B"") for item in result: print(item.getName(), ':', item[0]) prints:: age : 22 ssn : 111-22-3333 house_number : 221B",0,0,0,1,1,1,0,0,1,2
"def user_data_dir(appname=None, appauthor=None, version=None, roaming=False, as_path=False):
  r
  if system == ""win32"":
  if appauthor is None:
  appauthor = appname
  const = roaming and ""CSIDL_APPDATA"" or ""CSIDL_LOCAL_APPDATA""
  path = os.path.normpath(_get_win_folder(const))
  if appname:
  if appauthor is not False:
  path = os.path.join(path, appauthor, appname)
  else:
  path = os.path.join(path, appname)
  elif system == 'darwin' and not os.getenv('XDG_DATA_HOME'):
  path = os.path.expanduser('~/Library/Application Support/')
  if appname:
  path = os.path.join(path, appname)
  else:
  path = os.getenv('XDG_DATA_HOME') or os.path.expanduser(""~/.local/share"")
  if appname:
  path = os.path.join(path, appname)
  if appname and version:
  path = os.path.join(path, version)
  if as_path:
  path = Path(path)
  return path","r""""""Return full path to the user-specific data dir for this application. ""appname"" is the name of application. If None, just the system directory is returned. ""appauthor"" (only used on Windows) is the name of the appauthor or distributing body for this application. Typically it is the owning company name. This falls back to appname. You may pass False to disable it. ""version"" is an optional version path element to append to the path. You might want to use this if you want multiple versions of your app to be able to run independently. If used, this would typically be ""<major>.<minor>"". Only applied when appname is present. ""roaming"" (boolean, default False) can be set True to use the Windows roaming appdata directory. That means that for users on a Windows network setup for roaming profiles, this user data will be sync'd on login. See <http://technet.microsoft.com/en-us/library/cc766489(WS.10).aspx> for a discussion of issues. ""as_path"" (boolean, default False) can be set to True to get pathlib.Path objects instead of plain strings. On python 2.7 you need to ""pip install pathlib2"". Typical user data directories are: Mac OS X: ~/Library/Application Support/<AppName> Unix: ~/.local/share/<AppName> # or in $XDG_DATA_HOME, if defined Win XP (not roaming): C:\Documents and Settings\<username>\Application Data\<AppAuthor>\<AppName> Win XP (roaming): C:\Documents and Settings\<username>\Local Settings\Application Data\<AppAuthor>\<AppName> Win 7 (not roaming): C:\Users\<username>\AppData\Local\<AppAuthor>\<AppName> Win 7 (roaming): C:\Users\<username>\AppData\Roaming\<AppAuthor>\<AppName> For Unix, we follow the XDG spec and support $XDG_DATA_HOME. That means, by default ""~/.local/share/<AppName>"".",0,0,0,1,1,1,0,0,1,2
"async def edit_message_reply_markup(self,
  chat_id: typing.Union[base.Integer, base.String, None] = None,
  message_id: typing.Union[base.Integer, None] = None,
  inline_message_id: typing.Union[base.String, None] = None,
  reply_markup: typing.Union[types.InlineKeyboardMarkup,
  None] = None) -> types.Message or base.Boolean:
  reply_markup = prepare_arg(reply_markup)
  payload = generate_payload(**locals())
  result = await self.request(api.Methods.EDIT_MESSAGE_REPLY_MARKUP, payload)
  if isinstance(result, bool):
  return result
  return types.Message(**result)","Use this method to edit only the reply markup of messages sent by the bot or via the bot (for inline bots). Source: https://core.telegram.org/bots/api#editmessagereplymarkup :param chat_id: Required if inline_message_id is not specified Unique identifier for the target chat or username of the target channel :type chat_id: :obj:`typing.Union[base.Integer, base.String, None]` :param message_id: Required if inline_message_id is not specified. Identifier of the sent message :type message_id: :obj:`typing.Union[base.Integer, None]` :param inline_message_id: Required if chat_id and message_id are not specified. Identifier of the inline message :type inline_message_id: :obj:`typing.Union[base.String, None]` :param reply_markup: A JSON-serialized object for an inline keyboard :type reply_markup: :obj:`typing.Union[types.InlineKeyboardMarkup, None]` :return: On success, if edited message is sent by the bot, the edited Message is returned, otherwise True is returned. :rtype: :obj:`typing.Union[types.Message, base.Boolean]`",1,0,0,2,3,1,0,0,2,3
"def visit_addresses(frame, return_addresses):
  outdict = dict()
  for real_name, (pyname, address) in return_addresses.items():
  if address:
  xrval = frame[pyname].loc[address]
  if xrval.size > 1:
  outdict[real_name] = xrval
  else:
  outdict[real_name] = float(np.squeeze(xrval.values))
  else:
  outdict[real_name] = frame[pyname]
  return outdict","Visits all of the addresses, returns a new dict which contains just the addressed elements Parameters ---------- frame return_addresses: a dictionary, keys will be column names of the resulting dataframe, and are what the user passed in as 'return_columns'. Values are a tuple: (py_name, {coords dictionary}) which tells us where to look for the value to put in that specific column. Returns ------- outdict: dictionary",0,0,0,0,0,0,0,0,1,1
"def atomic(self, func):
  @wraps(func)
  def wrapper(*args, **kwargs):
  session = self.session
  session_info = session.info
  if session_info.get(_ATOMIC_FLAG_SESSION_INFO_KEY):
  return func(*args, **kwargs)
  f = retry_on_deadlock(session)(func)
  session_info[_ATOMIC_FLAG_SESSION_INFO_KEY] = True
  try:
  result = f(*args, **kwargs)
  session.flush()
  session.expunge_all()
  session.commit()
  return result
  except Exception:
  session.rollback()
  raise
  finally:
  session_info[_ATOMIC_FLAG_SESSION_INFO_KEY] = False
  return wrapper","A decorator that wraps a function in an atomic block. Example:: db = CustomSQLAlchemy() @db.atomic def f(): write_to_db('a message') return 'OK' assert f() == 'OK' This code defines the function ``f``, which is wrapped in an atomic block. Wrapping a function in an atomic block gives several guarantees: 1. The database transaction will be automatically committed if the function returns normally, and automatically rolled back if the function raises unhandled exception. 2. When the transaction is committed, all objects in ``db.session`` will be expunged. This means that no lazy loading will be performed on them. 3. If a transaction serialization error occurs during the execution of the function, the function will be re-executed. (It might be re-executed several times.) Atomic blocks can be nested, but in this case the outermost block takes full control of transaction's life-cycle, and inner blocks do nothing.",0,1,0,1,2,1,1,0,0,2
"def _check_db_exists(self, instance):
  dsn, host, username, password, database, driver = self._get_access_info(instance, self.DEFAULT_DB_KEY)
  context = ""{} - {}"".format(host, database)
  if self.existing_databases is None:
  cursor = self.get_cursor(instance, None, self.DEFAULT_DATABASE)
  try:
  self.existing_databases = {}
  cursor.execute(DATABASE_EXISTS_QUERY)
  for row in cursor:
  self.existing_databases[row.name] = True
  except Exception as e:
  self.log.error(""Failed to check if database {} exists: {}"".format(database, e))
  return False, context
  finally:
  self.close_cursor(cursor)
  return database in self.existing_databases, context",Check if the database we're targeting actually exists If not then we won't do any checks This allows the same config to be installed on many servers but fail gracefully,0,1,1,0,2,1,0,1,0,2
"def send_miniprogrampage_message(
  self, user_id, title, appid, pagepath, thumb_media_id, kf_account=None
  ):
  data = {
  ""touser"": user_id,
  ""msgtype"": ""miniprogrampage"",
  ""miniprogrampage"": {
  ""title"": title,
  ""appid"": appid,
  ""pagepath"": pagepath,
  ""thumb_media_id"": thumb_media_id
  }
  }
  if kf_account is not None:
  data[""customservice""] = {""kf_account"": kf_account}
  return self.post(
  url=""https://api.weixin.qq.com/cgi-bin/message/custom/send"",
  data=data
  )", :param user_id:  ID    `Message`  source :param title:   :param appid:  appid appid  :param pagepath:  app.json  pages/index/index?foo=bar :param thumb_media_id:  ID 520*416 :param kf_account:  :return:  JSON ,1,0,0,2,3,1,0,0,2,3
"def insert_paraphrase_information(germanet_db, wiktionary_files):
  num_paraphrases = 0
  lexunits = {}
  for filename in wiktionary_files:
  paraphrases = read_paraphrase_file(filename)
  num_paraphrases += len(paraphrases)
  for paraphrase in paraphrases:
  if paraphrase['lexUnitId'] not in lexunits:
  lexunits[paraphrase['lexUnitId']] = \
  germanet_db.lexunits.find_one(
  {'id': paraphrase['lexUnitId']})
  lexunit = lexunits[paraphrase['lexUnitId']]
  if 'paraphrases' not in lexunit:
  lexunit['paraphrases'] = []
  lexunit['paraphrases'].append(paraphrase)
  for lexunit in lexunits.values():
  germanet_db.lexunits.save(lexunit)
  print('Inserted {0} wiktionary paraphrases.'.format(num_paraphrases))",Reads in the given GermaNet relation file and inserts its contents into the given MongoDB database. Arguments: - `germanet_db`: a pymongo.database.Database object - `wiktionary_files`:,0,1,1,1,3,1,1,1,1,4
"def get_all_submissions(course_id, item_id, item_type, read_replica=True):
  submission_qs = Submission.objects
  if read_replica:
  submission_qs = _use_read_replica(submission_qs)
  query = submission_qs.select_related('student_item').filter(
  student_item__course_id=course_id,
  student_item__item_id=item_id,
  student_item__item_type=item_type,
  ).order_by('student_item__student_id', '-submitted_at', '-id').iterator()
  for unused_student_id, row_iter in itertools.groupby(query, operator.attrgetter('student_item.student_id')):
  submission = next(row_iter)
  data = SubmissionSerializer(submission).data
  data['student_id'] = submission.student_item.student_id
  yield data","For the given item, get the most recent submission for every student who has submitted. This may return a very large result set! It is implemented as a generator for efficiency. Args: course_id, item_id, item_type (string): The values of the respective student_item fields to filter the submissions by. read_replica (bool): If true, attempt to use the read replica database. If no read replica is available, use the default database. Yields: Dicts representing the submissions with the following fields: student_item student_id attempt_number submitted_at created_at answer Raises: Cannot fail unless there's a database error, but may return an empty iterable.",0,0,1,1,2,1,0,1,1,3
"def usergroup_update(usrgrpid, **kwargs):
  conn_args = _login(**kwargs)
  ret = {}
  try:
  if conn_args:
  method = 'usergroup.update'
  params = {""usrgrpid"": usrgrpid}
  params = _params_extend(params, **kwargs)
  ret = _query(method, params, conn_args['url'], conn_args['auth'])
  return ret['result']['usrgrpids']
  else:
  raise KeyError
  except KeyError:
  return ret",".. versionadded:: 2016.3.0 Update existing user group .. note:: This function accepts all standard user group properties: keyword argument names differ depending on your zabbix version, see here__. .. __: https://www.zabbix.com/documentation/2.4/manual/api/reference/usergroup/object#user_group :param usrgrpid: ID of the user group to update. :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring) :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring) :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring) :return: IDs of the updated user group, False on failure. CLI Example: .. code-block:: bash salt '*' zabbix.usergroup_update 8 name=guestsRenamed",1,0,0,1,2,1,0,0,1,2
"def drop_words(text, threshold=2, to_lower=True, delimiters=DEFAULT_DELIMITERS,
  stop_words=None):
  _raise_error_if_not_sarray(text, ""text"")
  sf = _turicreate.SFrame({'docs': text})
  fe = _feature_engineering.RareWordTrimmer(features='docs',
  threshold=threshold,
  to_lower=to_lower,
  delimiters=delimiters,
  stopwords=stop_words,
  output_column_prefix=None)
  tokens = fe.fit_transform(sf)
  return tokens['docs']","Remove words that occur below a certain number of times in an SArray. This is a common method of cleaning text before it is used, and can increase the quality and explainability of the models learned on the transformed data. RareWordTrimmer can be applied to all the string-, dictionary-, and list-typed columns in an SArray. * **string** : The string is first tokenized. By default, all letters are first converted to lower case, then tokenized by space characters. Each token is taken to be a word, and the words occurring below a threshold number of times across the entire column are removed, then the remaining tokens are concatenated back into a string. * **list** : Each element of the list must be a string, where each element is assumed to be a token. The remaining tokens are then filtered by count occurrences and a threshold value. * **dict** : The method first obtains the list of keys in the dictionary. This list is then processed as a standard list, except the value of each key must be of integer type and is considered to be the count of that key. Parameters ---------- text : SArray[str | dict | list] The input text data. threshold : int, optional The count below which words are removed from the input. stop_words: list[str], optional A manually specified list of stop words, which are removed regardless of count. to_lower : bool, optional Indicates whether to map the input strings to lower case before counting. delimiters: list[string], optional A list of delimiter characters for tokenization. By default, the list is defined to be the list of space characters. The user can define any custom list of single-character delimiters. Alternatively, setting `delimiters=None` will use a Penn treebank type tokenization, which is better at handling punctuations. (See reference below for details.) Returns ------- out : SArray. An SArray with words below a threshold removed. See Also -------- count_ngrams, tf_idf, tokenize, References ---------- - `Penn treebank tokenization <https://web.archive.org/web/19970614072242/http://www.cis.upenn.edu:80/~treebank/tokenization.html>`_ Examples -------- .. sourcecode:: python >>> import turicreate # Create input data >>> sa = turicreate.SArray([""The quick brown fox jumps in a fox like way."", ""Word word WORD, word!!!word""]) # Run drop_words >>> turicreate.text_analytics.drop_words(sa) dtype: str Rows: 2 ['fox fox', 'word word'] # Run drop_words with Penn treebank style tokenization to handle # punctuations >>> turicreate.text_analytics.drop_words(sa, delimiters=None) dtype: str Rows: 2 ['fox fox', 'word word word'] # Run drop_words with dictionary input >>> sa = turicreate.SArray([{'alice bob': 1, 'Bob alice': 2}, {'a dog': 0, 'a dog cat': 5}]) >>> turicreate.text_analytics.drop_words(sa) dtype: dict Rows: 2 [{'bob alice': 2}, {'a dog cat': 5}] # Run drop_words with list input >>> sa = turicreate.SArray([['one', 'bar bah', 'One'], ['a dog', 'a dog cat', 'A DOG']]) >>> turicreate.text_analytics.drop_words(sa) dtype: list Rows: 2 [['one', 'one'], ['a dog', 'a dog']]",2,0,0,1,3,1,0,0,1,2
"def createAndStartSwarm(client, clientInfo="""", clientKey="""", params="""",
  minimumWorkers=None, maximumWorkers=None,
  alreadyRunning=False):
  if minimumWorkers is None:
  minimumWorkers = Configuration.getInt(
  ""nupic.hypersearch.minWorkersPerSwarm"")
  if maximumWorkers is None:
  maximumWorkers = Configuration.getInt(
  ""nupic.hypersearch.maxWorkersPerSwarm"")
  return ClientJobsDAO.get().jobInsert(
  client=client,
  cmdLine=""$HYPERSEARCH"",
  clientInfo=clientInfo,
  clientKey=clientKey,
  alreadyRunning=alreadyRunning,
  params=params,
  minimumWorkers=minimumWorkers,
  maximumWorkers=maximumWorkers,
  jobType=ClientJobsDAO.JOB_TYPE_HS)","Create and start a swarm job. Args: client - A string identifying the calling client. There is a small limit for the length of the value. See ClientJobsDAO.CLIENT_MAX_LEN. clientInfo - JSON encoded dict of client specific information. clientKey - Foreign key. Limited in length, see ClientJobsDAO._initTables. params - JSON encoded dict of the parameters for the job. This can be fetched out of the database by the worker processes based on the jobID. minimumWorkers - The minimum workers to allocate to the swarm. Set to None to use the default. maximumWorkers - The maximum workers to allocate to the swarm. Set to None to use the swarm default. Set to 0 to use the maximum scheduler value. alreadyRunning - Insert a job record for an already running process. Used for testing.",1,1,0,0,2,1,0,1,1,3
"def server_error(request, template_name='500.html'):
  try:
  rendered_page = get_response_page(
  request,
  http.HttpResponseServerError,
  'icekit/response_pages/500.html',
  abstract_models.RESPONSE_HTTP500
  )
  if rendered_page is not None:
  return rendered_page
  except Exception:
  pass
  return defaults.server_error(request, template_name)",Custom 500 error handler. The exception clause is so broad to capture any 500 errors that may have been generated from getting the response page e.g. if the database was down. If they were not handled they would cause a 500 themselves and form an infinite loop. If no ResponsePage exists for with type ``RESPONSE_HTTP500`` then the default template render view will be used. Templates: :template:`500.html` Context: page A ResponsePage with type ``RESPONSE_HTTP500`` if it exists.,0,0,1,1,2,1,0,0,1,2
"def add_relation(self, parent, child, level, parent_func=None,
  child_func=None):
  if isinstance(parent, six.string_types):
  parent = self[parent]
  if isinstance(child, six.string_types):
  child = self[child]
  c = self.conn.cursor()
  c.execute(, (parent.id, child.id, level))
  if parent_func is not None:
  parent = parent_func(parent, child)
  self._update(parent, c)
  if child_func is not None:
  child = child_func(parent, child)
  self._update(child, c)
  self.conn.commit()
  return self","Manually add relations to the database. Parameters ---------- parent : str or Feature instance Parent feature to add. child : str or Feature instance Child feature to add level : int Level of the relation. For example, if parent is a gene and child is an mRNA, then you might want level to be 1. But if child is an exon, then level would be 2. parent_func, child_func : callable These optional functions control how attributes are updated in the database. They both have the signature `func(parent, child)` and must return a [possibly modified] Feature instance. For example, we could add the child's database id as the ""child"" attribute in the parent:: def parent_func(parent, child): parent.attributes['child'] = child.id and add the parent's ""gene_id"" as the child's ""Parent"" attribute:: def child_func(parent, child): child.attributes['Parent'] = parent['gene_id'] Returns ------- FeatureDB object with new relations added.",1,1,1,0,3,0,1,1,0,2
"def login(self, username=None, password=None):
  if (username is None) and (password is None):
  auth_token = self.__login_guest()
  elif (username is not None) and (password is not None):
  auth_token, fullName = self.__login_credentials(username, password)
  self.logger.info(""You are logged as {}"".format(fullName))
  else:
  raise ValueError(""you have to specify both username and password or nothing"")
  if auth_token is not None:
  self.auth_token = auth_token
  else:
  raise ConnectionError(""Impossible to retrieve the authentication token"")","Before doing any remote operation, the user has to login to the GMQL serivice. This can be done in the two following ways: * Guest mode: the user has no credentials and uses the system only as a temporary guest * Authenticated mode: the users has credentials and a stable remote account If neither username and password are specified, the user enters the system as a guest. If both are specified and they correspond to an existent user, the user enters as an authenticated user :param username: (optional) :param password: (optional) :return: None",2,0,0,2,4,2,0,0,1,3
"def do_GET(self, ):
  urld = {self.extract_site_url: 'extract_token_site.html',
  self.success_site_url: 'success_site.html'}
  site = urld.get(self.path)
  if not site:
  log.debug(""Requesting false url on login server."")
  self.send_error(404)
  return
  log.debug('Requesting the login server. Responding with %s.', urld)
  self._set_headers()
  self._write_html(site)","Handle GET requests If the path is '/', a site which extracts the token will be generated. This will redirect the user to the '/sucess' page, which shows a success message. :returns: None :rtype: None :raises: None",1,0,0,2,3,1,0,0,1,2
"def create(self, unique_name=values.unset, friendly_name=values.unset,
  identity=values.unset, deployment_sid=values.unset,
  enabled=values.unset):
  data = values.of({
  'UniqueName': unique_name,
  'FriendlyName': friendly_name,
  'Identity': identity,
  'DeploymentSid': deployment_sid,
  'Enabled': enabled,
  })
  payload = self._version.create(
  'POST',
  self._uri,
  data=data,
  )
  return DeviceInstance(self._version, payload, fleet_sid=self._solution['fleet_sid'], )","Create a new DeviceInstance :param unicode unique_name: A unique, addressable name of this Device. :param unicode friendly_name: A human readable description for this Device. :param unicode identity: An identifier of the Device user. :param unicode deployment_sid: The unique SID of the Deployment group. :param bool enabled: The enabled :returns: Newly created DeviceInstance :rtype: twilio.rest.preview.deployed_devices.fleet.device.DeviceInstance",1,0,0,2,3,1,0,0,2,3
"def dumpload(self, site=None, role=None):
  r = self.database_renderer(site=site, role=role)
  r.run('pg_dump -c --host={host_string} --username={db_user} '
  '--blobs --format=c {db_name} -n public | '
  'pg_restore -U {db_postgresql_postgres_user} --create '
  '--dbname={db_name}')","Dumps and loads a database snapshot simultaneously. Requires that the destination server has direct database access to the source server. This is better than a serial dump+load when: 1. The network connection is reliable. 2. You don't need to save the dump file. The benefits of this over a dump+load are: 1. Usually runs faster, since the load and dump happen in parallel. 2. Usually takes up less disk space since no separate dump file is downloaded.",0,1,1,0,2,1,0,0,0,1
"def search_unit(current):
  current.output = {
  'results': [],
  'status': 'OK',
  'code': 201
  }
  for user in UnitModel(current).objects.search_on(*settings.MESSAGING_UNIT_SEARCH_FIELDS,
  contains=current.input['query']):
  current.output['results'].append((user.name, user.key))","Search on units for subscribing it's users to a channel .. code-block:: python # request: { 'view':'_zops_search_unit', 'query': string, } # response: { 'results': [('name', 'key'), ], 'status': 'OK', 'code': 200 }",1,0,1,1,3,1,0,1,1,3
"def set_postmortem_debugger(cls, cmdline,
  auto = None, hotkey = None, bits = None):
  if bits is None:
  bits = cls.bits
  elif bits not in (32, 64):
  raise NotImplementedError(""Unknown architecture (%r bits)"" % bits)
  if bits == 32 and cls.bits == 64:
  keyname = 'HKLM\\SOFTWARE\\Wow6432Node\\Microsoft\\Windows NT\\CurrentVersion\\AeDebug'
  else:
  keyname = 'HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\AeDebug'
  key = cls.registry[keyname]
  if cmdline is not None:
  key['Debugger'] = cmdline
  if auto is not None:
  key['Auto'] = int(bool(auto))
  if hotkey is not None:
  key['UserDebuggerHotkey'] = int(hotkey)","Sets the postmortem debugging settings in the Registry. @warning: This method requires administrative rights. @see: L{get_postmortem_debugger} @type cmdline: str @param cmdline: Command line to the new postmortem debugger. When the debugger is invoked, the first ""%ld"" is replaced with the process ID and the second ""%ld"" is replaced with the event handle. Don't forget to enclose the program filename in double quotes if the path contains spaces. @type auto: bool @param auto: Set to C{True} if no user interaction is allowed, C{False} to prompt a confirmation dialog before attaching. Use C{None} to leave this value unchanged. @type hotkey: int @param hotkey: Virtual key scan code for the user defined hotkey. Use C{0} to disable the hotkey. Use C{None} to leave this value unchanged. @type bits: int @param bits: Set to C{32} for the 32 bits debugger, or C{64} for the 64 bits debugger. Set to {None} for the default (L{System.bits}). @rtype: tuple( str, bool, int ) @return: Previously defined command line and auto flag. @raise WindowsError: Raises an exception on error.",1,0,0,1,2,1,0,0,1,2
"def run(self, args=None):
  if not args:
  args = self.parse(sys.argv[1:])
  if getattr(args, 'verbose', False):
  self.logger.setLevel(logging.DEBUG)
  try:
  if hasattr(args, 'run'):
  args.run(self, args)
  else:
  self.__main__(args)
  except Exception as e:
  import traceback
  self.logger.debug(traceback.format_exc())
  self.logger.error(str(e))
  if self.raise_exceptions:
  raise
  sys.exit(2)",Runs the main command or sub command based on user input,1,0,0,1,2,1,0,0,1,2
"def available_deprecated(supported_name):
  def wrapper(func):
  func_name = func.__name__
  renamed_method(func_name, supported_name)
  @wraps(func)
  def inner(*args, **kwargs):
  warn('adapter:{}'.format(func_name))
  return func(*args, **kwargs)
  return available(inner)
  return wrapper","A decorator that marks a function as available, but also prints a deprecation warning. Use like @available_deprecated('my_new_method') def my_old_method(self, arg, model_name=None): args = compatability_shim(arg) return self.my_new_method(*args, model_name=None) To make `adapter.my_old_method` available but also print out a warning on use directing users to `my_new_method`.",1,0,0,1,2,1,0,0,0,1
"def following_qs(self, user, *models, **kwargs):
  qs = self.filter(user=user)
  ctype_filters = Q()
  for model in models:
  check(model)
  ctype_filters |= Q(content_type=ContentType.objects.get_for_model(model))
  qs = qs.filter(ctype_filters)
  flag = kwargs.get('flag', '')
  if flag:
  qs = qs.filter(flag=flag)
  return qs.fetch_generic_relations('follow_object')","Returns a queryset of actors that the given user is following (eg who im following). Items in the list can be of any model unless a list of restricted models are passed. Eg following(user, User) will only return users following the given user",1,0,1,1,3,1,0,1,1,3
"def go_to_line(self):
  editor = self.get_current_editor()
  if not editor:
  return False
  line, state = QInputDialog.getInt(self, ""Goto Line Number"", ""Line number:"", min=1)
  if not state:
  return False
  LOGGER.debug(""> Chosen line number: '{0}'."".format(line))
  return editor.go_to_line(line)",Moves current **Script_Editor_tabWidget** Widget tab Model editor cursor to user defined line. :return: Method success. :rtype: bool :note: May require user interaction.,1,0,0,1,2,1,0,0,1,2
"async def continue_dialog(self):
  if self.active_dialog != None:
  dialog = await self.find_dialog(self.active_dialog.id)
  if not dialog:
  raise Exception(""DialogContext.continue_dialog(): Can't continue dialog. A dialog with an id of '%s' wasn't found."" % active_dialog.id)
  return await dialog.continue_dialog(self)
  else:
  return DialogTurnResult(DialogTurnStatus.Empty)","Continues execution of the active dialog, if there is one, by passing the context object to its `Dialog.continue_dialog()` method. You can check `turn_context.responded` after the call completes to determine if a dialog was run and a reply was sent to the user. :return:",2,0,0,1,3,1,0,0,1,2
"def set_database_path(dbfolder):
  configpath = get_configpath()
  try:
  d = get_config()
  except IOError:
  d = configparser.ConfigParser()
  d['pyciss_db'] = {}
  d['pyciss_db']['path'] = dbfolder
  with configpath.open('w') as f:
  d.write(f)
  print(""Saved database path into {}."".format(configpath))",Use to write the database path into the config. Parameters ---------- dbfolder : str or pathlib.Path Path to where pyciss will store the ISS images it downloads and receives.,0,1,1,0,2,1,0,0,0,1
"def get_or_load_name(self, type_, id_, method):
  name = self.get_name(type_, id_)
  if name is not None:
  defer.returnValue(name)
  instance = yield method(id_)
  if instance is None:
  defer.returnValue(None)
  self.put_name(type_, id_, instance.name)
  defer.returnValue(instance.name)","read-through cache for a type of object's name. If we don't have a cached name for this type/id, then we will query the live Koji server and store the value before returning. :param type_: str, ""user"" or ""tag"" :param id_: int, eg. 123456 :param method: function to call if this value is not in the cache. This method must return a deferred that fires with an object with a "".name"" attribute. :returns: deferred that when fired returns a str, or None",1,0,1,1,3,0,0,0,0,0
"async def reset_state(self, *,
  chat: typing.Union[str, int, None] = None,
  user: typing.Union[str, int, None] = None,
  with_data: typing.Optional[bool] = True):
  chat, user = self.check_address(chat=chat, user=user)
  await self.set_state(chat=chat, user=user, state=None)
  if with_data:
  await self.set_data(chat=chat, user=user, data={})","Reset state for user in chat. You may desire to use this method when finishing conversations. Chat or user is always required. If one of this is not presented, you have to set missing value based on the provided one. :param chat: :param user: :param with_data: :return:",0,1,1,0,2,1,0,0,1,2
"def plot_op(fn, inputs=[], outputs=[]):
 global COUNT, ht
 if not isinstance(outputs,list):
 outputs=[outputs]
 for tensor in outputs:
 if tensor.op.type is not 'Placeholder':
 raise Error('Output nodes must be Placeholders')
 op=PlotOp(fn, COUNT, inputs, outputs)
 op_store.add_op(op)
 COUNT+=1
 if outputs:
 return outputs[0]
 else:
 return op",User-exposed api method for constructing a python_node Args: fn: python function that computes some np.ndarrays given np.ndarrays as inputs. it can have arbitrary side effects. inputs: array of tf.Tensors (optional). These are where fn derives its values from outputs: tf.Placeholder nodes (optional). These are constructed by the user (which allows the user to plug them into other ht.Ops or tf.Ops). The outputs of fn are mapped to each of the output placeholders. raises an Error if fn cannot map,1,0,0,0,1,1,0,0,1,2
"def str(password,
  opslimit=OPSLIMIT_INTERACTIVE,
  memlimit=MEMLIMIT_INTERACTIVE):
  return nacl.bindings.crypto_pwhash_str_alg(password,
  opslimit,
  memlimit,
  ALG)","Hashes a password with a random salt, using the memory-hard argon2i construct and returning an ascii string that has all the needed info to check against a future password The default settings for opslimit and memlimit are those deemed correct for the interactive user login case. :param bytes password: :param int opslimit: :param int memlimit: :rtype: bytes .. versionadded:: 1.2",1,0,0,0,1,1,0,0,0,1
"def get_next_step(self):
  if self.parent.is_selected_layer_keywordless:
  self.parent.parent_step = self
  self.parent.existing_keywords = None
  self.parent.set_mode_label_to_keywords_creation()
  new_step = self.parent.step_kw_purpose
  else:
  if layers_intersect(self.parent.hazard_layer,
  self.parent.exposure_layer):
  new_step = self.parent.step_fc_agglayer_origin
  else:
  new_step = self.parent.step_fc_disjoint_layers
  return new_step",Find the proper step when user clicks the Next button. :returns: The step to be switched to :rtype: WizardStep instance or None,1,0,0,1,2,1,0,0,1,2
"async def create_object(model, **data):
  warnings.warn(""create_object() is deprecated, Manager.create() ""
  ""should be used instead"",
  DeprecationWarning)
  obj = model(**data)
  pk = await insert(model.insert(**dict(obj.__data__)))
  if obj._pk is None:
  obj._pk = pk
  return obj",Create object asynchronously. :param model: mode class :param data: data for initializing object :return: new object saved to database,0,1,0,0,1,1,1,1,0,3
"def normalize_mask(mask, is_micro):
  if mask is None:
  return None
  try:
  mask = int(mask)
  except ValueError:
  raise MaskError('Invalid data mask ""{0}"". Must be an integer or a string which represents an integer value.'.format(mask))
  if is_micro:
  if not 0 <= mask < 4:
  raise MaskError('Invalid data mask ""{0}"" for Micro QR Code. Must be in range 0 .. 3'.format(mask))
  else:
  if not 0 <= mask < 8:
  raise MaskError('Invalid data mask ""{0}"". Must be in range 0 .. 7'.format(mask))
  return mask",\ Normalizes the (user specified) mask. :param mask: A mask constant :type mask: int or None :param bool is_micro: Indicates if the mask is meant to be used for a Micro QR Code.,0,0,0,0,0,1,0,0,1,2
"def get_user_application_data_directory():
  system_application_data_directory = get_system_application_data_directory()
  if not foundations.common.path_exists(system_application_data_directory):
  LOGGER.error(
  ""!> Undefined or non existing system Application data directory, using 'HOME' directory as fallback!"")
  system_application_data_directory = Environment(""HOME"").get_value()
  if not foundations.common.path_exists(system_application_data_directory):
  temporary_directory = get_temporary_directory()
  LOGGER.error(""!> Undefined or non existing 'HOME' directory, using system temporary directory as fallback!"")
  system_application_data_directory = temporary_directory
  return os.path.join(system_application_data_directory, Constants.provider_directory,
  Constants.application_directory)","| Returns the user Application directory. | The difference between :func:`get_user_application_data_directory` and :func:`get_system_application_data_directory` definitions is that :func:`get_user_application_data_directory` definition will append :attr:`foundations.globals.constants.Constants.provider_directory` and :attr:`foundations.globals.constants.Constants.application_directory` attributes values to the path returned. | If the user Application directory is not available, the function will fallback to system temporary directory. Examples directories:: - 'C:\\Users\\$USER\\AppData\\Roaming\\Provider\\Application' on Windows 7. - 'C:\\Documents and Settings\\$USER\\Application Data\\Provider\\Application' on Windows XP. - '/Users/$USER/Library/Preferences/Provider/Application' on Mac Os X. - '/home/$USER/.Provider/Application' on Linux. :return: User Application directory. :rtype: unicode",0,0,0,0,0,1,0,0,1,2
"def get_user_recent(self, username, *, mode=OsuMode.osu, limit=10):
  return self._make_req(endpoints.USER_RECENT, dict(
  k=self.key,
  u=username,
  type=_username_type(username),
  m=mode.value,
  limit=limit
  ), JsonList(RecentScore))","Get a user's most recent scores, within the last 24 hours. Parameters ---------- username : str or int A `str` representing the user's username, or an `int` representing the user's id. mode : :class:`osuapi.enums.OsuMode` The osu! game mode for which to look up. Defaults to osu!standard. limit The maximum number of results to return. Defaults to 10, maximum 50.",2,0,0,1,3,2,0,0,1,3
"async def message_from_token(self, token: Text, payload: Any) \
  -> Optional[BaseMessage]:
  methods = [
  self._message_from_sr,
  self._message_from_token,
  ]
  for method in methods:
  msg = method(token, payload)
  if msg:
  return msg",There is two ways of getting a FB user: either with a signed request or either with a platform token. Both are tried out.,1,0,0,0,1,1,0,0,1,2
"def getAsGeoJson(self, session):
  statement = .format(self.geometryColumnName,
  self.tableName,
  self.id)
  result = session.execute(statement)
  for row in result:
  return row.json",Retrieve the geometry in GeoJSON format. This method is a veneer for an SQL query that calls the ``ST_AsGeoJSON()`` function on the geometry column. Args: session (:mod:`sqlalchemy.orm.session.Session`): SQLAlchemy session object bound to PostGIS enabled database. Returns: str: GeoJSON string representation of geometry.,1,0,1,1,3,0,0,1,1,2
"def send_user_pin(self, user_token, pin, skip_validation=False):
  if not skip_validation:
  validate_pin(pin)
  response = _request('PUT',
  url=self.url_v1('/user/pins/' + pin['id']),
  user_agent=self.user_agent,
  user_token=user_token,
  json=pin,
  )
  _raise_for_status(response)",Send a user pin. :param str user_token: The token of the user. :param dict pin: The pin. :param bool skip_validation: Whether to skip the validation. :raises pypebbleapi.schemas.DocumentError: If the validation process failed. :raises `requests.exceptions.HTTPError`: If an HTTP error occurred.,1,0,0,1,2,2,0,0,1,3
"def add_member(self, address, **kwargs):
  path = 'membership/%s' % self.id
  kwargs[""address""] = address
  if ""group_name"" not in kwargs and self.default_name:
  kwargs[""group_name""] = self.default_name
  response_data = self.api.request(path, kwargs)
  if 'user_id' in response_data:
  user_id = response_data['user_id']
  return FiestaUser(user_id, address=address, groups=[self])
  return None","Add a member to a group. All Fiesta membership options can be passed in as keyword arguments. Some valid options include: - `group_name`: Since each member can access a group using their own name, you can override the `group_name` in this method. By default, the group will have the name specified on the class level `default_name` property. - `display_name` is the full name of the user that they will see throughout the UI if this is a new account. - `welcome_message` should be a dictionary specified according to the docs. If you set it to ``False``, no message will be sent. See http://docs.fiesta.cc/list-management-api.html#message for formatting details. .. seealso:: `Fiesta API documentation <http://docs.fiesta.cc/list-management-api.html#adding-members>`_",1,0,0,2,3,2,0,0,2,4
"def hold(model: Model, reducer: Optional[Callable] = None) -> Iterator[list]:
  if not isinstance(model, Model):
  raise TypeError(""Expected a Model, not %r."" % model)
  events = []
  restore = model.__dict__.get(""_notify_model_views"")
  model._notify_model_views = lambda e: events.extend(e)
  try:
  yield events
  finally:
  if restore is None:
  del model._notify_model_views
  else:
  model._notify_model_views = restore
  events = tuple(events)
  if reducer is not None:
  events = tuple(map(Data, reducer(model, events)))
  model._notify_model_views(events)","Temporarilly withold change events in a modifiable list. All changes that are captured within a ""hold"" context are forwarded to a list which is yielded to the user before being sent to views of the given ``model``. If desired, the user may modify the list of events before the context is left in order to change the events that are ultimately sent to the model's views. Parameters: model: The model object whose change events will be temporarilly witheld. reducer: A function for modifying the events list at the end of the context. Its signature is ``(model, events) -> new_events`` where ``model`` is the given model, ``events`` is the complete list of events produced in the context, and the returned ``new_events`` is a list of events that will actuall be distributed to views. Notes: All changes witheld from views will be sent as a single notification. For example if you view a :class:`specate.mvc.models.List` and its ``append()`` method is called three times within a :func:`hold` context, Examples: Note how the event from ``l.append(1)`` is omitted from the printed statements. .. code-block:: python from spectate import mvc l = mvc.List() mvc.view(d, lambda d, e: list(map(print, e))) with mvc.hold(l) as events: l.append(1) l.append(2) del events[0] .. code-block:: text {'index': 1, 'old': Undefined, 'new': 2}",1,0,0,1,2,1,0,0,0,1
"def confirm(prompt=None, resp=False):
  if prompt is None:
  prompt = 'Confirm'
  if resp:
  prompt = '%s [%s]|%s: ' % (prompt, 'y', 'n')
  else:
  prompt = '%s [%s]|%s: ' % (prompt, 'n', 'y')
  while True:
  ans = raw_input(prompt)
  if not ans:
  return resp
  if ans not in ['y', 'Y', 'n', 'N']:
  print 'please enter y or n.'
  continue
  if ans == 'y' or ans == 'Y':
  return True
  if ans == 'n' or ans == 'N':
  return False","Prompts user for confirmation. :param prompt: String to display to user. :param resp: Default response value. :return: Boolean response from user, or default value.",1,0,0,1,2,1,0,0,1,2
"def AuthenticateSessionId(self, username, password):
  self.__setAuthenticationMethod__('authenticating_session_id')
  parameters = {'username':username, 'password':password}
  if self.__SenseApiCall__(""/login.json"", ""POST"", parameters = parameters):
  try:
  response = json.loads(self.__response__)
  except:
  self.__setAuthenticationMethod__('not_authenticated')
  self.__error__ = ""notjson""
  return False
  try:
  self.__session_id__ = response['session_id']
  self.__setAuthenticationMethod__('session_id')
  return True
  except:
  self.__setAuthenticationMethod__('not_authenticated')
  self.__error__ = ""no session_id""
  return False
  else:
  self.__setAuthenticationMethod__('not_authenticated')
  self.__error__ = ""api call unsuccessful""
  return False",Authenticate using a username and password. The SenseApi object will store the obtained session_id internally until a call to LogoutSessionId is performed. @param username (string) - CommonSense username @param password (string) - MD5Hash of CommonSense password @return (bool) - Boolean indicating whether AuthenticateSessionId was successful,2,0,0,0,2,2,0,0,2,4
"def format_db_selection(selection, engine=None):
  if selection is None:
  return ''
  selections = []
  for col, op_, value in parse_column_filters(selection):
  if engine and engine.name == 'postgresql':
  col = '""%s""' % col
  try:
  opstr = [key for key in OPERATORS if OPERATORS[key] is op_][0]
  except KeyError:
  raise ValueError(""Cannot format database 'WHERE' command with ""
  ""selection operator %r"" % op_)
  selections.append('{0} {1} {2!r}'.format(col, opstr, value))
  if selections:
  return 'WHERE %s' % ' AND '.join(selections)
  return ''",Format a column filter selection as a SQL database WHERE string,0,0,0,1,1,0,0,1,1,2
"def getProcDict(self, fields=('user', 'cmd',), threads=False, **kwargs):
  stats = {}
  field_list = list(fields)
  num_cols = len(field_list)
  if threads:
  key = 'spid'
  else:
  key = 'pid'
  try:
  key_idx = field_list.index(key)
  except ValueError:
  field_list.append(key)
  key_idx = len(field_list) - 1
  result = self.getProcList(field_list, threads, **kwargs)
  if result is not None:
  headers = result['headers'][:num_cols]
  lines = result['stats']
  if len(lines) > 1:
  for cols in lines:
  stats[cols[key_idx]] = dict(zip(headers, cols[:num_cols]))
  return stats
  else:
  return None","Execute ps command with custom output format with columns format with columns from fields, and return result as a nested dictionary with the key PID or SPID. The Standard Format Specifiers from ps man page must be used for the fields parameter. @param fields: Fields included in the output. Default: user, cmd (PID or SPID column is included by default.) @param threads: If True, include threads in output. @param **kwargs: Keyword variables are used for filtering the results depending on the values of the columns. Each keyword must correspond to a field name with an optional suffix: field: Field equal to value or in list of values. field_ic: Field equal to value or in list of values, using case insensitive comparison. field_regex: Field matches regex value or matches with any regex in list of values. field_ic_regex: Field matches regex value or matches with any regex in list of values using case insensitive match. @return: Nested dictionary indexed by: PID for process info. SPID for thread info.",1,0,0,1,2,1,0,0,1,2
"async def answer_inline_query(self, inline_query_id: base.String,
  results: typing.List[types.InlineQueryResult],
  cache_time: typing.Union[base.Integer, None] = None,
  is_personal: typing.Union[base.Boolean, None] = None,
  next_offset: typing.Union[base.String, None] = None,
  switch_pm_text: typing.Union[base.String, None] = None,
  switch_pm_parameter: typing.Union[base.String, None] = None) -> base.Boolean:
  results = prepare_arg(results)
  payload = generate_payload(**locals())
  result = await self.request(api.Methods.ANSWER_INLINE_QUERY, payload)
  return result","Use this method to send answers to an inline query. No more than 50 results per query are allowed. Source: https://core.telegram.org/bots/api#answerinlinequery :param inline_query_id: Unique identifier for the answered query :type inline_query_id: :obj:`base.String` :param results: A JSON-serialized array of results for the inline query :type results: :obj:`typing.List[types.InlineQueryResult]` :param cache_time: The maximum amount of time in seconds that the result of the inline query may be cached on the server. Defaults to 300. :type cache_time: :obj:`typing.Union[base.Integer, None]` :param is_personal: Pass True, if results may be cached on the server side only for the user that sent the query. By default, results may be returned to any user who sends the same query :type is_personal: :obj:`typing.Union[base.Boolean, None]` :param next_offset: Pass the offset that a client should send in the next query with the same text to receive more results. Pass an empty string if there are no more results or if you dont support pagination. Offset length cant exceed 64 bytes. :type next_offset: :obj:`typing.Union[base.String, None]` :param switch_pm_text: If passed, clients will display a button with specified text that switches the user to a private chat with the bot and sends the bot a start message with the parameter switch_pm_parameter :type switch_pm_text: :obj:`typing.Union[base.String, None]` :param switch_pm_parameter: Deep-linking parameter for the /start message sent to the bot when user presses the switch button. 1-64 characters, only A-Z, a-z, 0-9, _ and - are allowed. :type switch_pm_parameter: :obj:`typing.Union[base.String, None]` :return: On success, True is returned :rtype: :obj:`base.Boolean`",1,0,0,1,2,1,0,0,2,3
"def create_jwt(self, expires_in=None):
  s = utils.sign_jwt(data={""id"": self.user.id},
  secret_key=get_jwt_secret(),
  salt=get_jwt_salt(),
  expires_in=expires_in or get_jwt_ttl())
  return s","Create a secure timed JWT token that can be passed. It save the user id, which later will be used to retrieve the data :param user: AuthUser, the user's object :param expires_in: - time in second for the token to expire :return: string",1,0,0,1,2,1,0,0,1,2
"def choose(msg, items, attr):
  if len(items) == 1:
  return items[0]
  print()
  for index, i in enumerate(items):
  name = attr(i) if callable(attr) else getattr(i, attr)
  print(' %s: %s' % (index, name))
  print()
  while True:
  try:
  inp = input('%s: ' % msg)
  if any(s in inp for s in (':', '::', '-')):
  idx = slice(*map(lambda x: int(x.strip()) if x.strip() else None, inp.split(':')))
  return items[idx]
  else:
  return items[int(inp)]
  except (ValueError, IndexError):
  pass","Command line helper to display a list of choices, asking the user to choose one of the options.",1,0,0,1,2,1,0,0,1,2
"def deletegroupmember(self, group_id, user_id):
  request = requests.delete(
  '{0}/{1}/members/{2}'.format(self.groups_url, group_id, user_id),
  headers=self.headers, verify=self.verify_ssl, auth=self.auth, timeout=self.timeout)
  if request.status_code == 200:
  return True",Delete a group member :param group_id: group id to remove the member from :param user_id: user id :return: always true,0,0,0,1,1,1,0,0,1,2
"def x_parse_error(err, content, source):
  lineno, column = err.position
  if ""<doc>"" in content:
  column -= 5
  start = lineno - (1 if lineno == 1 else 2)
  lines = []
  tcontent = content.replace(""<doc>"", """").replace(""</doc>"", """").split('\n')
  for context in IT.islice(tcontent, start, lineno):
  lines.append(context.strip())
  last = wrap_line(lines.pop(), column)
  lines.extend(last)
  caret = '{:=>{}}'.format('^', len(last[-1]))
  if source is not None:
  err.msg = '\nIn: {}\n{}\n{}\n{}'.format(source, err, '\n'.join(lines), caret)
  else:
  err.msg = '{}\n{}\n{}'.format(err, '\n'.join(lines), caret)
  raise err",Explains the specified ParseError instance to show the user where the error happened in their XML.,1,0,0,1,2,1,0,0,1,2
"def map_grounding(stmts_in, **kwargs):
  from indra.preassembler.grounding_mapper import GroundingMapper
  from indra.preassembler.grounding_mapper import gm as grounding_map
  from indra.preassembler.grounding_mapper import \
  default_agent_map as agent_map
  logger.info('Mapping grounding on %d statements...' % len(stmts_in))
  do_rename = kwargs.get('do_rename')
  gm = kwargs.get('grounding_map', grounding_map)
  if do_rename is None:
  do_rename = True
  gm = GroundingMapper(gm, agent_map, use_deft=kwargs.get('use_deft', True))
  stmts_out = gm.map_agents(stmts_in, do_rename=do_rename)
  dump_pkl = kwargs.get('save')
  if dump_pkl:
  dump_statements(stmts_out, dump_pkl)
  return stmts_out","Map grounding using the GroundingMapper. Parameters ---------- stmts_in : list[indra.statements.Statement] A list of statements to map. do_rename : Optional[bool] If True, Agents are renamed based on their mapped grounding. grounding_map : Optional[dict] A user supplied grounding map which maps a string to a dictionary of database IDs (in the format used by Agents' db_refs). use_deft : Optional[bool] If True, Deft will be attempted to be used for acronym disambiguation. Default: True save : Optional[str] The name of a pickle file to save the results (stmts_out) into. Returns ------- stmts_out : list[indra.statements.Statement] A list of mapped statements.",1,0,0,1,2,1,0,0,0,1
"def detect_build(snps):
  def lookup_build_with_snp_pos(pos, s):
  try:
  return s.loc[s == pos].index[0]
  except:
  return None
  build = None
  rsids = [""rs3094315"", ""rs11928389"", ""rs2500347"", ""rs964481"", ""rs2341354""]
  df = pd.DataFrame(
  {
  36: [742429, 50908372, 143649677, 27566744, 908436],
  37: [752566, 50927009, 144938320, 27656823, 918573],
  38: [817186, 50889578, 148946169, 27638706, 983193],
  },
  index=rsids,
  )
  for rsid in rsids:
  if rsid in snps.index:
  build = lookup_build_with_snp_pos(snps.loc[rsid].pos, df.loc[rsid])
  if build is not None:
  break
  return build","Detect build of SNPs. Use the coordinates of common SNPs to identify the build / assembly of a genotype file that is being loaded. Notes ----- rs3094315 : plus strand in 36, 37, and 38 rs11928389 : plus strand in 36, minus strand in 37 and 38 rs2500347 : plus strand in 36 and 37, minus strand in 38 rs964481 : plus strand in 36, 37, and 38 rs2341354 : plus strand in 36, 37, and 38 Parameters ---------- snps : pandas.DataFrame SNPs to add Returns ------- int detected build of SNPs, else None References ---------- ..[1] Yates et. al. (doi:10.1093/bioinformatics/btu613), http://europepmc.org/search/?query=DOI:10.1093/bioinformatics/btu613 ..[2] Zerbino et. al. (doi.org/10.1093/nar/gkx1098), https://doi.org/10.1093/nar/gkx1098 ..[3] Sherry ST, Ward MH, Kholodov M, Baker J, Phan L, Smigielski EM, Sirotkin K. dbSNP: the NCBI database of genetic variation. Nucleic Acids Res. 2001 Jan 1;29(1):308-11. ..[4] Database of Single Nucleotide Polymorphisms (dbSNP). Bethesda (MD): National Center for Biotechnology Information, National Library of Medicine. dbSNP accession: rs3094315, rs11928389, rs2500347, rs964481, and rs2341354 (dbSNP Build ID: 151). Available from: http://www.ncbi.nlm.nih.gov/SNP/",1,0,0,0,1,0,0,0,1,1
"def get_team(self, name):
  res = self.get_teams(name)
  if res[0] == False:
  return res
  for t in res[1]:
  if t['name'] == name:
  return [True, t]
  return [False, 'Could not find team']","**Description** Return the team with the specified team name, if it is present. **Arguments** - **name**: the name of the team to return **Success Return Value** The requested team. **Example** `examples/user_team_mgmt.py <https://github.com/draios/python-sdc-client/blob/master/examples/user_team_mgmt.py>`_",2,0,1,1,4,1,0,0,1,2
"def print_info(self):
  d = dir(self)
  self.plugins = []
  for key in d:
  if key.startswith(""info_""):
  self.plugins.append(key)
  for key in self.plugins:
  if self.echo:
  Console.ok(""> {0}"".format(key.replace(""_"", "" "", 1)))
  exec(""self.%s()"" % key)",prints some info that the user may find useful,1,0,0,1,2,1,0,0,1,2
"def check_for_new_files_in_home(self):
  if not self._user:
  return None
  try:
  created_files = set(self._listdir(self._home_dir)).difference(self._home_dir_content)
  except (subprocess.CalledProcessError, IOError):
  created_files = []
  if created_files:
  logging.warning('The tool created the following files in %s, '
  'this may influence later runs:\n\t%s',
  self._home_dir, '\n\t'.join(created_files))
  return created_files","Check that the user account's home directory now does not contain more files than when this instance was created, and warn otherwise. Does nothing if no user account was given to RunExecutor. @return set of newly created files",0,0,0,0,0,1,0,0,1,2
"def logout(request):
  user = getattr(request, 'user', None)
  if hasattr(user, 'is_authenticated') and not user.is_authenticated:
  user = None
  user_logged_out.send(sender=user.__class__, request=request, user=user)
  language = request.session.get(LANGUAGE_SESSION_KEY)
  request.session.flush()
  if language is not None:
  request.session[LANGUAGE_SESSION_KEY] = language
  if hasattr(request, 'user'):
  request.user = MojAnonymousUser()",Removes the authenticated user's ID from the request and flushes their session data.,1,0,1,0,2,1,0,1,0,2
"def BLASTresults(rid, format_type=""Tabular"", \
  hitlist_size= None, alignments=None, \
  ncbi_gi = None, format_object=None,\
  baseURL=""http://blast.ncbi.nlm.nih.gov""):
  URL=baseURL+""/Blast.cgi?""
  URL=URL+""RID=""+str(rid)+""&FORMAT_TYPE=""+str(format_type)
  for o in [ hitlist_size, alignments,\
  ncbi_gi, format_object]:
  if o:
  URL=URL+""&""+ variablename(var) +""=""+str(o)
  URL=URL+""&CMD=Get""
  response=requests.get(url = URL)
  response=response.content
  if format_type==""Tabular"":
  result=response.split(""\n"")
  result=[ s.split(""\t"") for s in result][6:]
  header=result[:7]
  content=result[7:]
  fields=header[5][0].strip(""
  result=pd.DataFrame(content,columns=fields)
  response=result[:int(header[-1][0].split("" "")[1])]
  return response","Retrieves results for an RID. :param rid: BLAST search request identifier. Allowed values: The Request ID (RID) returned when the search was submitted :param format_type: Report type. Allowed values: HTML, Text, XML, XML2, JSON2, or Tabular. Tabular is the default. :param hitlist_size: Number of databases sequences to keep. Allowed values: Integer greater than zero. :param alignments: Number of alignments to print (applies to HTML and Text). Allowed values: Integer greater than zero. :param ncbi_gi: Show NCBI GIs in report. Allowed values: T or F. :param format_object: Object type. Allowed values: SearchInfo (status check) or Alignment (report formatting). :param baseURL: server url. Default=http://blast.ncbi.nlm.nih.gov :returns: the result of a BLAST query. If format_type=""Tabular"" it will parse the content into a Pandas dataframe.",2,0,0,1,3,1,0,0,1,2
"def select(options=None):
  if not options:
  return None
  width = len(str(len(options)))
  for x,option in enumerate(options):
  sys.stdout.write('{:{width}}) {}\n'.format(x+1,option, width=width))
  sys.stdout.write('{:>{width}} '.format('
  sys.stdout.flush()
  if sys.stdin.isatty():
  try:
  response = raw_input().strip()
  except (EOFError, KeyboardInterrupt):
  response = ''
  else:
  sys.stdin = open(""/dev/tty"")
  try:
  response = ''
  while True:
  response += sys.stdin.read(1)
  if response.endswith('\n'):
  break
  except (EOFError, KeyboardInterrupt):
  sys.stdout.flush()
  pass
  try:
  response = int(response) - 1
  except ValueError:
  return None
  if response < 0 or response >= len(options):
  return None
  return options[response]","pass in a list of options, promt the user to select one, and return the selected option or None",1,0,0,1,2,1,0,0,1,2
"def process_text(text, out_file='sofia_output.json', auth=None):
  text_json = {'text': text}
  if not auth:
  user, password = _get_sofia_auth()
  else:
  user, password = auth
  if not user or not password:
  raise ValueError('Could not use SOFIA web service since'
  ' authentication information is missing. Please'
  ' set SOFIA_USERNAME and SOFIA_PASSWORD in the'
  ' INDRA configuration file or as environmental'
  ' variables.')
  json_response, status_code, process_status = \
  _text_processing(text_json=text_json, user=user, password=password)
  if process_status != 'Done' or status_code != 200:
  return None
  if out_file:
  with open(out_file, 'w') as fh:
  json.dump(json_response, fh, indent=1)
  return process_json(json_response)","Return processor by processing text given as a string. Parameters ---------- text : str A string containing the text to be processed with Sofia. out_file : Optional[str] The path to a file to save the reader's output into. Default: sofia_output.json auth : Optional[list] A username/password pair for the Sofia web service. If not given, the SOFIA_USERNAME and SOFIA_PASSWORD values are loaded from either the INDRA config or the environment. Returns ------- sp : indra.sources.sofia.processor.SofiaProcessor A SofiaProcessor object which has a list of extracted INDRA Statements as its statements attribute. If the API did not process the text, None is returned.",1,0,0,1,2,1,0,0,1,2
"def createAccount(self, short_name, author_name=None, author_url=None):
  r = self.make_method(""createAccount"", {
  ""short_name"": short_name,
  ""author_name"": author_name,
  ""author_url"": author_url
  })
  self.access_token = r['access_token']
  return r","Creates Telegraph account :param short_name: Required. Account name, helps users with several accounts remember which they are currently using. Displayed to the user above the ""Edit/Publish"" button on Telegra.ph, other users don't see this name. :type short_name: str :param author_name: Optional. Default author name used when creating new articles. :type author_name: str :param author_url: Optional. Default profile link, opened when users click on the author's name below the title. Can be any link, not necessarily to a Telegram profile or channel. :type author_url: str :returns: Account object with the regular fields and an additional access_token field.",2,0,0,1,3,2,0,0,1,3
"def schedule_start(self) -> bool:
  if self.last_status != self.STATUSES.CREATED:
  return False
  upstream_trigger_check = self.check_upstream_trigger()
  if not upstream_trigger_check and self.is_upstream_done:
  self.on_upstream_failed()
  return False
  if not self.pipeline_run.check_concurrency():
  return True
  if not self.check_concurrency():
  return True
  self.on_scheduled()
  self.start()
  return False","Schedule the task: check first if the task can start: 1. we check that the task is still in the CREATED state. 2. we check that the upstream dependency is met. 3. we check that pipeline can start a new task; i.e. we check the concurrency of the pipeline. 4. we check that operation can start a new instance; i.e. we check the concurrency of the operation. -> If all checks pass we schedule the task start it. -> 1. If the operation is not in created status, nothing to do. -> 2. If the upstream dependency check is not met, two use cases need to be validated: * The upstream dependency is not met but could be met in the future, because some ops are still CREATED/SCHEDULED/RUNNING/... in this case nothing need to be done, every time an upstream operation finishes, it will notify all the downstream ops including this one. * The upstream dependency is not met and could not be met at all. In this case we need to mark the task with `UPSTREAM_FAILED`. -> 3. If the pipeline has reached it's concurrency limit, we just delay schedule based on the interval/time delay defined by the user. The pipeline scheduler will keep checking until the task can be scheduled or stopped. -> 4. If the operation has reached it's concurrency limit, Same as above we keep trying based on an interval defined by the user. Returns: boolean: Whether to try to schedule this operation run in the future or not.",0,0,1,0,1,1,0,0,0,1
"def _write_cert_to_database(ca_name, cert, cacert_path=None, status='V'):
  set_ca_path(cacert_path)
  ca_dir = '{0}/{1}'.format(cert_base_path(), ca_name)
  index_file, expire_date, serial_number, subject = _get_basic_info(
  ca_name,
  cert,
  ca_dir)
  index_data = '{0}\t{1}\t\t{2}\tunknown\t{3}'.format(
  status,
  expire_date,
  serial_number,
  subject
  )
  with salt.utils.files.fopen(index_file, 'a+') as ofile:
  ofile.write(salt.utils.stringutils.to_str(index_data))",write out the index.txt database file in the appropriate directory to track certificates ca_name name of the CA cert certificate to be recorded,0,0,0,0,0,1,0,0,0,1
"def sold_out_and_unregistered(context):
  user = user_for_context(context)
  if hasattr(user, ""attendee"") and user.attendee.completed_registration:
  return None
  ticket_category = settings.TICKET_PRODUCT_CATEGORY
  categories = available_categories(context)
  return ticket_category not in [cat.id for cat in categories]","If the current user is unregistered, returns True if there are no products in the TICKET_PRODUCT_CATEGORY that are available to that user. If there *are* products available, the return False. If the current user *is* registered, then return None (it's not a pertinent question for people who already have a ticket).",1,0,1,0,2,1,0,1,1,3
"def first_solar_spectral_loss(self, pw, airmass_absolute):
  if 'first_solar_spectral_coefficients' in \
  self.module_parameters.keys():
  coefficients = \
  self.module_parameters['first_solar_spectral_coefficients']
  module_type = None
  else:
  module_type = self._infer_cell_type()
  coefficients = None
  return atmosphere.first_solar_spectral_correction(pw,
  airmass_absolute,
  module_type,
  coefficients)","Use the :py:func:`first_solar_spectral_correction` function to calculate the spectral loss modifier. The model coefficients are specific to the module's cell type, and are determined by searching for one of the following keys in self.module_parameters (in order): 'first_solar_spectral_coefficients' (user-supplied coefficients) 'Technology' - a string describing the cell type, can be read from the CEC module parameter database 'Material' - a string describing the cell type, can be read from the Sandia module database. Parameters ---------- pw : array-like atmospheric precipitable water (cm). airmass_absolute : array-like absolute (pressure corrected) airmass. Returns ------- modifier: array-like spectral mismatch factor (unitless) which can be multiplied with broadband irradiance reaching a module's cells to estimate effective irradiance, i.e., the irradiance that is converted to electrical current.",0,0,0,1,1,1,0,0,1,2
"def create_project_transfer(self, project_id, to_user_ids):
  data = {
  ""to_users[][id]"": to_user_ids,
  }
  return self._post(""/projects/"" + project_id + ""/transfers"", data,
  content_type=ContentType.form)",Send POST request to initiate transfer of a project to the specified user ids :param project_id: str uuid of the project :param to_users: list of user uuids to receive the project :return: requests.Response containing the successful result,1,0,0,2,3,1,0,0,1,2
"def python_op(fn, inputs=None, outputs=None):
 global COUNT
 if not isinstance(outputs,list):
 outputs=[outputs]
 for tensor in outputs:
 if tensor.op.type != 'Placeholder':
 raise TypeError('Output nodes must be Placeholders')
 op=PythonOp('Python', fn, COUNT, inputs, outputs)
 op_store.add_op(op)
 COUNT+=1
 if outputs:
 return outputs[0]
 else:
 return op",User-exposed api method for constructing a python_node Args: fn: python function that computes some np.ndarrays given np.ndarrays as inputs. it can have arbitrary side effects. inputs: array of tf.Tensors (optional). These are where fn derives its values from outputs: tf.Placeholder nodes (optional). These are constructed by the user (which allows the user to plug them into other ht.Ops or tf.Ops). The outputs of fn are mapped to each of the output placeholders. raises an Error if fn cannot map,1,0,0,1,2,1,0,0,1,2
"def get_glacier_poly():
  rgi_fn = os.path.join(datadir, 'rgi60/regions/rgi60_merge.shp')
  if not os.path.exists(rgi_fn):
  cmd = ['get_rgi.sh',]
  sys.exit(""Missing rgi glacier data source. If already downloaded, specify correct datadir. If not, run `%s` to download"" % cmd[0])
  return rgi_fn",Calls external shell script `get_rgi.sh` to fetch: Randolph Glacier Inventory (RGI) glacier outline shapefiles Full RGI database: rgi50.zip is 410 MB The shell script will unzip and merge regional shp into single global shp http://www.glims.org/RGI/,1,0,0,0,1,1,0,0,1,2
"def save_record(self, agent_id, t_step, key, value):
  value = self.convert(key, value)
  self._tups.append(Record(agent_id=agent_id,
  t_step=t_step,
  key=key,
  value=value))
  if len(self._tups) > 100:
  self.flush_cache()",Save a collection of records to the database. Database writes are cached.,0,1,0,0,1,0,1,0,0,1
"def deprecated(removal_version, hint_message=None, subject=None, ensure_stderr=False):
  validate_deprecation_semver(removal_version, 'removal version')
  def decorator(func):
  if not inspect.isfunction(func):
  raise BadDecoratorNestingError('The @deprecated decorator must be applied innermost of all '
  'decorators.')
  func_full_name = '{}.{}'.format(func.__module__, func.__name__)
  @wraps(func)
  def wrapper(*args, **kwargs):
  warn_or_error(removal_version, subject or func_full_name, hint_message,
  ensure_stderr=ensure_stderr)
  return func(*args, **kwargs)
  return wrapper
  return decorator","Marks a function or method as deprecated. A removal version must be supplied and it must be greater than the current 'pantsbuild.pants' version. When choosing a removal version there is a natural tension between the code-base, which benefits from short deprecation cycles, and the user-base which may prefer to deal with deprecations less frequently. As a rule of thumb, if the hint message can fully convey corrective action succinctly and you judge the impact to be on the small side (effects custom tasks as opposed to effecting BUILD files), lean towards the next release version as the removal version; otherwise, consider initiating a discussion to win consensus on a reasonable removal version. :param str removal_version: The pantsbuild.pants version which will remove the deprecated function. :param str hint_message: An optional hint pointing to alternatives to the deprecation. :param str subject: The name of the subject that has been deprecated for logging clarity. Defaults to the name of the decorated function/method. :param bool ensure_stderr: Forwarded to `ensure_stderr` in warn_or_error(). :raises DeprecationApplicationError if the @deprecation is applied improperly.",0,0,0,1,1,1,0,0,0,1
"def has_any_roles(self, *roles):
  roles = map(utils.slugify, list(roles))
  return True \
  if AuthUserRole.query() \
  .join(AuthUser) \
  .filter(AuthUserRole.name.in_(roles)) \
  .filter(AuthUser.id == self.id) \
  .count() \
  else False",Check if user has any of the roles requested :param roles: tuple of roles string :return: bool,1,0,1,0,2,1,0,1,1,3
"def insert(
  self,
  table_name,
  obj=None,
  database=None,
  overwrite=False,
  partition=None,
  values=None,
  validate=True,
  ):
  table = self.table(table_name, database=database)
  return table.insert(
  obj=obj,
  overwrite=overwrite,
  partition=partition,
  values=values,
  validate=validate,
  )","Insert into existing table. See ImpalaTable.insert for other parameters. Parameters ---------- table_name : string database : string, default None Examples -------- >>> table = 'my_table' >>> con.insert(table, table_expr) # doctest: +SKIP # Completely overwrite contents >>> con.insert(table, table_expr, overwrite=True) # doctest: +SKIP",1,1,1,0,3,0,1,0,0,1
"def upload_file(self, api_token, file_path, **kwargs):
  params = {
  'token': api_token,
  'file_name': os.path.basename(file_path)
  }
  with open(file_path, 'rb') as f:
  files = {'file': f}
  return self._post('upload_file', params, files, **kwargs)",Upload a file suitable to be passed as a file_attachment. :param api_token: The user's login api_token. :type api_token: str :param file_path: The path of the file to be uploaded. :type file_path: str :return: The HTTP response to the request. :rtype: :class:`requests.Response`,1,0,0,1,2,1,0,0,2,3
"def delete_tag(self, project, repository, tag_name):
  url = 'rest/git/1.0/projects/{project}/repos/{repository}/tags/{tag}'.format(project=project,
  repository=repository,
  tag=tag_name)
  return self.delete(url)",Creates a tag using the information provided in the {@link RestCreateTagRequest request} The authenticated user must have REPO_WRITE permission for the context repository to call this resource. :param project: :param repository: :param tag_name: :return:,1,0,0,1,2,1,0,0,1,2
"def auth(self, request):
  service = UserService.objects.get(user=request.user, name='ServiceWallabag')
  callback_url = '%s://%s%s' % (request.scheme, request.get_host(), reverse('wallabag_callback'))
  params = {'username': service.username,
  'password': service.password,
  'client_id': service.client_id,
  'client_secret': service.client_secret}
  access_token = Wall.get_token(host=service.host, **params)
  request.session['oauth_token'] = access_token
  return callback_url",let's auth the user to the Service :param request: request object :return: callback url :rtype: string that contains the url to redirect after auth,1,0,1,0,2,1,0,1,1,3
"def process_pc_pathsfromto(source_genes, target_genes, neighbor_limit=1,
  database_filter=None):
  model = pcc.graph_query('pathsfromto', source_genes,
  target_genes, neighbor_limit=neighbor_limit,
  database_filter=database_filter)
  if model is not None:
  return process_model(model)","Returns a BiopaxProcessor for a PathwayCommons paths-from-to query. The paths-from-to query finds the paths from a set of source genes to a set of target genes. http://www.pathwaycommons.org/pc2/#graph http://www.pathwaycommons.org/pc2/#graph_kind Parameters ---------- source_genes : list A list of HGNC gene symbols that are the sources of paths being searched for. Examples: ['BRAF', 'RAF1', 'ARAF'] target_genes : list A list of HGNC gene symbols that are the targets of paths being searched for. Examples: ['MAP2K1', 'MAP2K2'] neighbor_limit : Optional[int] The number of steps to limit the length of the paths between the source genes and target genes being queried. Default: 1 database_filter : Optional[list] A list of database identifiers to which the query is restricted. Examples: ['reactome'], ['biogrid', 'pid', 'psp'] If not given, all databases are used in the query. For a full list of databases see http://www.pathwaycommons.org/pc2/datasources Returns ------- bp : BiopaxProcessor A BiopaxProcessor containing the obtained BioPAX model in bp.model.",0,0,1,1,2,1,0,1,1,3
"def getCancelURL(self):
  if not self.return_to:
  raise NoReturnToError
  if self.immediate:
  raise ValueError(""Cancel is not an appropriate response to ""
  ""immediate mode requests."")
  response = Message(self.message.getOpenIDNamespace())
  response.setArg(OPENID_NS, 'mode', 'cancel')
  return response.toURL(self.return_to)","Get the URL to cancel this request. Useful for creating a ""Cancel"" button on a web form so that operation can be carried out directly without another trip through the server. (Except you probably want to make another trip through the server so that it knows that the user did make a decision. Or you could simulate this method by doing C{.answer(False).encodeToURL()}) @returntype: str @returns: The return_to URL with openid.mode = cancel. @raises NoReturnError: when I do not have a return_to.",1,0,0,1,2,1,0,0,1,2
"def _updateModelDBResults(self):
  metrics = self._getMetrics()
  reportDict = dict([(k,metrics[k]) for k in self._reportMetricLabels])
  metrics = self._getMetrics()
  optimizeDict = dict()
  if self._optimizeKeyPattern is not None:
  optimizeDict[self._optimizedMetricLabel] = \
  metrics[self._optimizedMetricLabel]
  results = json.dumps((metrics , optimizeDict))
  self._jobsDAO.modelUpdateResults(self._modelID, results=results,
  metricValue=optimizeDict.values()[0],
  numRecords=(self._currentRecordIndex + 1))
  self._logger.debug(
  ""Model Results: modelID=%s; numRecords=%s; results=%s"" % \
  (self._modelID, self._currentRecordIndex + 1, results))
  return",Retrieves the current results and updates the model's record in the Model database.,0,1,0,0,1,1,1,0,1,3
"def get_user_home(self, user):
  user_home = self._conf['user_home']
  if user_home:
  if callable(user_home):
  return user_home(user)
  elif isinstance(user_home, six.string_types):
  if '/' in user_home:
  return user_home
  else:
  mod, func = user_home.rsplit(""."", 1)
  return getattr(import_module(mod), func)(user)
  raise ValueError('The user_home setting must be either a string '
  'or a callable object (e.g. a function).')
  else:
  return self.get_absolute_url()","Returns the default URL for a particular user. This method can be used to customize where a user is sent when they log in, etc. By default it returns the value of :meth:`get_absolute_url`. An alternative function can be supplied to customize this behavior by specifying a either a URL or a function which returns a URL via the ``""user_home""`` key in ``HORIZON_CONFIG``. Each of these would be valid:: {""user_home"": ""/home"",} # A URL {""user_home"": ""my_module.get_user_home"",} # Path to a function {""user_home"": lambda user: ""/"" + user.name,} # A function {""user_home"": None,} # Will always return the default dashboard This can be useful if the default dashboard may not be accessible to all users. When user_home is missing from HORIZON_CONFIG, it will default to the settings.LOGIN_REDIRECT_URL value.",0,0,0,1,1,1,0,0,1,2
"def authenticate(self, request, **credentials):
  username = credentials.get('username')
  password = credentials.get('password')
  login_server = credentials.get('login_server')
  port = credentials.get('port')
  user_model = get_user_model()
  try:
  user = user_model.objects.get(username=username)
  except user_model.DoesNotExist:
  return None
  try:
  response = poplib.POP3_SSL(host=login_server, port=port)
  response.user(user=username)
  password_string = response.pass_(pswd=password)
  if b'OK' in password_string:
  response.quit()
  return user
  except poplib.error_proto:
  return None
  except (ValueError, TypeError) as e:
  raise e",Returns user for credentials provided if credentials are valid. Returns ``None`` otherwise. :param request: HttpRequest instance :param credentials: keyword arguments :return: user object,1,0,1,1,3,1,0,1,1,3
"def has_csv_permission(self, request, obj=None):
  if getattr(settings, 'DJANGO_EXPORTS_REQUIRE_PERM', None):
  opts = self.opts
  codename = '%s_%s' % ('csv', opts.object_name.lower())
  return request.user.has_perm(""%s.%s"" % (opts.app_label, codename))
  return True","Returns True if the given request has permission to add an object. Can be overridden by the user in subclasses. By default, we assume all staff users can use this action unless `DJANGO_EXPORTS_REQUIRE_PERM` is set to True in your django settings.",1,0,0,0,1,1,0,0,1,2
"def register(self, email, username, password, first_name, last_name, birthday=""1974-11-20"", captcha_result=None):
  self.username = username
  self.password = password
  register_message = sign_up.RegisterRequest(email, username, password, first_name, last_name, birthday, captcha_result,
  self.device_id_override, self.android_id_override)
  log.info(""[+] Sending sign up request (name: {} {}, email: {})..."".format(first_name, last_name, email))
  return self._send_xmpp_element(register_message)",Sends a register request to sign up a new user to kik with the given details.,1,0,0,1,2,1,0,0,1,2
"def add_user_to_group(self, username, group):
  url = self._options['server'] + '/rest/api/latest/group/user'
  x = {'groupname': group}
  y = {'name': username}
  payload = json.dumps(y)
  r = json_loads(self._session.post(url, params=x, data=payload))
  if 'name' not in r or r['name'] != group:
  return False
  else:
  return r","Add a user to an existing group. :param username: Username that will be added to specified group. :type username: str :param group: Group that the user will be added to. :type group: str :return: json response from Jira server for success or a value that evaluates as False in case of failure. :rtype: Union[bool,Dict[str,Any]]",2,0,0,2,4,1,0,0,2,3
"def send_media(self, media_id, user_ids, text='', thread_id=None):
  user_ids = _get_user_ids(self, user_ids)
  if not isinstance(text, str) and not isinstance(user_ids, (list, str)):
  self.logger.error('Text must be an string, user_ids must be an list or string')
  return False
  if self.reached_limit('messages'):
  self.logger.info(""Out of messages for today."")
  return False
  media = self.get_media_info(media_id)
  media = media[0] if isinstance(media, list) else media
  self.delay('message')
  if self.api.send_direct_item(
  'media_share',
  user_ids,
  text=text,
  thread=thread_id,
  media_type=media.get('media_type'),
  media_id=media.get('id')
  ):
  self.total['messages'] += 1
  return True
  self.logger.info(""Message to {user_ids} wasn't sent"".format(user_ids=user_ids))
  return False",:param media_id: :param self: bot :param text: text of message :param user_ids: list of user_ids for creating group or one user_id for send to one person :param thread_id: thread_id,1,0,0,1,2,2,0,0,2,4
"def get_crossmatch_catalogues_column_map(
  dbConn,
  log):
  log.debug('starting the ``get_crossmatch_catalogues_column_map`` function')
  sqlQuery = u % locals()
  rows = readquery(
  log=log,
  sqlQuery=sqlQuery,
  dbConn=dbConn,
  quiet=False
  )
  colMaps = {}
  for row in rows:
  colMaps[row[""view_name""]] = row
  log.debug('completed the ``get_crossmatch_catalogues_column_map`` function')
  return colMaps","*Query the sherlock-catalogues helper tables to generate a map of the important columns of each catalogue* Within your sherlock-catalogues database you need to manually map the inhomogeneous column-names from the sherlock-catalogues to an internal homogeneous name-set which includes *ra*, *dec*, *redshift*, *object name*, *magnitude*, *filter* etc. The column-name map is set within the two database helper tables called `tcs_helper_catalogue_views_info` and `tcs_helper_catalogue_views_info`. See the *'Checklist for Adding A New Reference Catalogue to the Sherlock Catalogues Database'* for more information. .. todo:: - write a checklist for adding a new catalogue to the sherlock database and reference it from here (use the image below of the tcs_helper_catalogue_views_info table) .. image:: https://farm5.staticflickr.com/4604/38429536400_eafa991580_o.png :width: 200 px **Key Arguments:** - ``dbConn`` -- the sherlock-catalogues database connection - ``log`` -- logger **Return:** - ``colMaps`` -- dictionary of dictionaries with the name of the database-view (e.g. `tcs_view_agn_milliquas_v4_5`) as the key and the column-name dictary map as value (`{view_name: {columnMap}}`). **Usage:** To collect the column map dictionary of dictionaries from the catalogues database, use the ``get_crossmatch_catalogues_column_map`` function: .. code-block:: python from sherlock.commonutils import get_crossmatch_catalogues_column_map colMaps = get_crossmatch_catalogues_column_map( log=log, dbConn=cataloguesDbConn )",0,0,1,0,1,1,0,1,1,3
"def tenant_quota_usages(request, tenant_id=None, targets=None):
  if not tenant_id:
  tenant_id = request.user.project_id
  disabled_quotas = get_disabled_quotas(request, targets)
  usages = QuotaUsage()
  futurist_utils.call_functions_parallel(
  (_get_tenant_compute_usages,
  [request, usages, disabled_quotas, tenant_id]),
  (_get_tenant_network_usages,
  [request, usages, disabled_quotas, tenant_id]),
  (_get_tenant_volume_usages,
  [request, usages, disabled_quotas, tenant_id]))
  return usages","Get our quotas and construct our usage object. :param tenant_id: Target tenant ID. If no tenant_id is provided, a the request.user.project_id is assumed to be used. :param targets: A tuple of quota names to be retrieved. If unspecified, all quota and usage information is retrieved.",2,0,0,1,3,1,0,1,1,3
"def get_known_host_entries(user,
  hostname,
  config=None,
  port=None,
  fingerprint_hash_type=None):
  full = _get_known_hosts_file(config=config, user=user)
  if isinstance(full, dict):
  return full
  ssh_hostname = _hostname_and_port_to_ssh_hostname(hostname, port)
  cmd = ['ssh-keygen', '-F', ssh_hostname, '-f', full]
  lines = __salt__['cmd.run'](cmd,
  ignore_retcode=True,
  python_shell=False).splitlines()
  known_host_entries = list(
  _parse_openssh_output(lines,
  fingerprint_hash_type=fingerprint_hash_type)
  )
  return known_host_entries if known_host_entries else None",".. versionadded:: 2018.3.0 Return information about known host entries from the configfile, if any. If there are no entries for a matching hostname, return None. CLI Example: .. code-block:: bash salt '*' ssh.get_known_host_entries <user> <hostname>",0,0,1,0,1,1,0,0,1,2
"def display_for_value(value, request=None):
  from is_core.utils.compatibility import admin_display_for_value
  if request and isinstance(value, Model):
  return render_model_object_with_link(request, value)
  else:
  return (
  (value and ugettext('Yes') or ugettext('No')) if isinstance(value, bool) else admin_display_for_value(value)
  )",Converts humanized value examples: boolean True/Talse ==> Yes/No objects ==> object display name with link if current user has permissions to see the object datetime ==> in localized format,1,0,0,1,2,1,0,0,1,2
"def write_tree(self):
  mdb = MemoryDB()
  entries = self._entries_sorted()
  binsha, tree_items = write_tree_from_cache(entries, mdb, slice(0, len(entries)))
  mdb.stream_copy(mdb.sha_iter(), self.repo.odb)
  root_tree = Tree(self.repo, binsha, path='')
  root_tree._cache = tree_items
  return root_tree",Writes this index to a corresponding Tree object into the repository's object database and return it. :return: Tree object representing this index :note: The tree will be written even if one or more objects the tree refers to does not yet exist in the object database. This could happen if you added Entries to the index directly. :raise ValueError: if there are no entries in the cache :raise UnmergedEntriesError:,0,1,0,0,1,0,0,1,0,1
"def invite_by_email(self, email, sender=None, request=None, **kwargs):
  try:
  user = self.user_model.objects.get(email=email)
  except self.user_model.DoesNotExist:
  if ""username"" in inspect.getargspec(
  self.user_model.objects.create_user
  ).args:
  user = self.user_model.objects.create(
  username=self.get_username(),
  email=email,
  password=self.user_model.objects.make_random_password(),
  )
  else:
  user = self.user_model.objects.create(
  email=email, password=self.user_model.objects.make_random_password()
  )
  user.is_active = False
  user.save()
  self.send_invitation(user, sender, **kwargs)
  return user",Creates an inactive user with the information we know and then sends an invitation email for that user to complete registration. If your project uses email in a different way then you should make to extend this method as it only checks the `email` attribute for Users.,1,1,1,1,4,1,1,1,0,3
"def get_available_ip4(self, id_network):
  if not is_valid_int_param(id_network):
  raise InvalidParameterError(
  u'Network identifier is invalid or was not informed.')
  url = 'ip/availableip4/' + str(id_network) + ""/""
  code, xml = self.submit(None, 'GET', url)
  return self.response(code, xml)",Get a available IP in the network ipv4 :param id_network: Network identifier. Integer value and greater than zero. :return: Dictionary with the following structure: :: {'ip': {'ip': < available_ip >}} :raise IpNotAvailableError: Network dont have available IP for insert a new IP :raise NetworkIPv4NotFoundError: Network is not found :raise UserNotAuthorizedError: User dont have permission to get a available IP :raise InvalidParameterError: Network identifier is null or invalid. :raise XMLError: Networkapi failed to generate the XML response. :raise DataBaseError: Networkapi failed to access the database.,2,0,0,1,3,2,0,0,1,3
"def usergroup_delete(usergroupids, **kwargs):
  conn_args = _login(**kwargs)
  ret = {}
  try:
  if conn_args:
  method = 'usergroup.delete'
  if not isinstance(usergroupids, list):
  usergroupids = [usergroupids]
  params = usergroupids
  ret = _query(method, params, conn_args['url'], conn_args['auth'])
  return ret['result']['usrgrpids']
  else:
  raise KeyError
  except KeyError:
  return ret",".. versionadded:: 2016.3.0 :param usergroupids: IDs of the user groups to delete :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring) :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring) :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring) :return: IDs of the deleted user groups. CLI Example: .. code-block:: bash salt '*' zabbix.usergroup_delete 28",1,0,0,2,3,2,0,0,1,3
"def convert_from_bytes(pdf_file, dpi=200, output_folder=None, first_page=None, last_page=None,
  fmt='ppm', thread_count=1, userpw=None, use_cropbox=False, strict=False, transparent=False,
  output_file=str(uuid.uuid4()), poppler_path=None):
  fh, temp_filename = tempfile.mkstemp()
  try:
  with open(temp_filename, 'wb') as f:
  f.write(pdf_file)
  f.flush()
  return convert_from_path(f.name, dpi=dpi, output_folder=output_folder,
  first_page=first_page, last_page=last_page, fmt=fmt, thread_count=thread_count,
  userpw=userpw, use_cropbox=use_cropbox, strict=strict, transparent=transparent,
  output_file=output_file, poppler_path=poppler_path)
  finally:
  os.close(fh)
  os.remove(temp_filename)","Description: Convert PDF to Image will throw whenever one of the condition is reached Parameters: pdf_file -> Bytes representing the PDF file dpi -> Image quality in DPI poppler_path -> Path to look for poppler binaries output_folder -> Write the resulting images to a folder (instead of directly in memory) first_page -> First page to process last_page -> Last page to process before stopping fmt -> Output image format thread_count -> How many threads we are allowed to spawn for processing userpw -> PDF's password use_cropbox -> Use cropbox instead of mediabox strict -> When a Syntax Error is thrown, it will be raised as an Exception transparent -> Output with a transparent background instead of a white one. output_file -> What is the output filename poppler_path -> Path to look for poppler binaries",0,0,0,1,1,1,0,0,1,2
"def connect_head_namespaced_service_proxy_with_path(self, name, namespace, path, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.connect_head_namespaced_service_proxy_with_path_with_http_info(name, namespace, path, **kwargs)
  else:
  (data) = self.connect_head_namespaced_service_proxy_with_path_with_http_info(name, namespace, path, **kwargs)
  return data","connect_head_namespaced_service_proxy_with_path # noqa: E501 connect HEAD requests to proxy of Service # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.connect_head_namespaced_service_proxy_with_path(name, namespace, path, async_req=True) >>> result = thread.get() :param async_req bool :param str name: name of the ServiceProxyOptions (required) :param str namespace: object name and auth scope, such as for teams and projects (required) :param str path: path to the resource (required) :param str path2: Path is the part of URLs that include service endpoints, suffixes, and parameters to use for the current proxy request to service. For example, the whole request URL is http://localhost/api/v1/namespaces/kube-system/services/elasticsearch-logging/_search?q=user:kimchy. Path is _search?q=user:kimchy. :return: str If the method is called asynchronously, returns the request thread.",1,0,0,1,2,1,0,0,1,2
"async def call_command(bot: NoneBot, ctx: Context_T,
  name: Union[str, CommandName_T], *,
  current_arg: str = '',
  args: Optional[CommandArgs_T] = None,
  check_perm: bool = True,
  disable_interaction: bool = False) -> bool:
  cmd = _find_command(name)
  if not cmd:
  return False
  session = CommandSession(bot, ctx, cmd, current_arg=current_arg, args=args)
  return await _real_run_command(session, context_id(session.ctx),
  check_perm=check_perm,
  disable_interaction=disable_interaction)","Call a command internally. This function is typically called by some other commands or ""handle_natural_language"" when handling NLPResult object. Note: If disable_interaction is not True, after calling this function, any previous command session will be overridden, even if the command being called here does not need further interaction (a.k.a asking the user for more info). :param bot: NoneBot instance :param ctx: message context :param name: command name :param current_arg: command current argument string :param args: command args :param check_perm: should check permission before running command :param disable_interaction: disable the command's further interaction :return: the command is successfully called",1,0,0,0,1,1,0,0,1,2
"def default_token_user_loader(self, token):
  try:
  data = self.decode_token(token)
  except jwt.exceptions.DecodeError as e:
  raise x.JwtDecodeError(str(e))
  except jwt.ExpiredSignatureError as e:
  raise x.JwtExpired(str(e))
  user = self.get(data['user_id'])
  if not user:
  msg = 'No user with such id [{}]'
  raise x.JwtNoUser(msg.format(data['user_id']))
  if user.is_locked():
  msg = 'This account is locked'
  raise x.AccountLocked(msg, locked_until=user.locked_until)
  if self.require_confirmation and not user.email_confirmed:
  msg = 'Please confirm your email address [{}]'
  raise x.EmailNotConfirmed(
  msg.format(user.email_secure),
  email=user.email
  )
  if not token == user._token:
  raise x.JwtTokenMismatch('The token does not match our records')
  return user","Default token user loader Accepts a token and decodes it checking signature and expiration. Then loads user by id from the token to see if account is not locked. If all is good, returns user record, otherwise throws an exception. :param token: str, token string :return: boiler.user.models.User",1,0,0,1,2,1,0,0,1,2
"async def copy_from_query(self, query, *args, output,
  timeout=None, format=None, oids=None,
  delimiter=None, null=None, header=None,
  quote=None, escape=None, force_quote=None,
  encoding=None):
  opts = self._format_copy_opts(
  format=format, oids=oids, delimiter=delimiter,
  null=null, header=header, quote=quote, escape=escape,
  force_quote=force_quote, encoding=encoding
  )
  if args:
  query = await utils._mogrify(self, query, args)
  copy_stmt = 'COPY ({query}) TO STDOUT {opts}'.format(
  query=query, opts=opts)
  return await self._copy_out(copy_stmt, output, timeout)","Copy the results of a query to a file or file-like object. :param str query: The query to copy the results of. :param args: Query arguments. :param output: A :term:`path-like object <python:path-like object>`, or a :term:`file-like object <python:file-like object>`, or a :term:`coroutine function <python:coroutine function>` that takes a ``bytes`` instance as a sole argument. :param float timeout: Optional timeout value in seconds. The remaining keyword arguments are ``COPY`` statement options, see `COPY statement documentation`_ for details. :return: The status string of the COPY command. Example: .. code-block:: pycon >>> import asyncpg >>> import asyncio >>> async def run(): ... con = await asyncpg.connect(user='postgres') ... result = await con.copy_from_query( ... 'SELECT foo, bar FROM mytable WHERE foo > $1', 10, ... output='file.csv', format='csv') ... print(result) ... >>> asyncio.get_event_loop().run_until_complete(run()) 'COPY 10' .. _`COPY statement documentation`: https://www.postgresql.org/docs/current/static/sql-copy.html .. versionadded:: 0.11.0",0,0,0,1,1,1,0,0,1,2
"def authenticate_redirect(
  self, callback_uri=None, ax_attrs=[""name"", ""email"", ""language"",
  ""username""]):
  callback_uri = callback_uri or self.request.uri
  args = self._openid_args(callback_uri, ax_attrs=ax_attrs)
  self.redirect(self._OPENID_ENDPOINT + ""?"" + urllib.urlencode(args))","Returns the authentication URL for this service. After authentication, the service will redirect back to the given callback URI. We request the given attributes for the authenticated user by default (name, email, language, and username). If you don't need all those attributes for your app, you can request fewer with the ax_attrs keyword argument.",2,0,0,1,3,2,0,0,1,3
"def get_announcements_list(request, context):
  user = context[""user""]
  if context[""announcements_admin""] and context[""show_all""]:
  announcements = (Announcement.objects.all())
  else:
  if context[""show_expired""]:
  announcements = (Announcement.objects.visible_to_user(user))
  else:
  announcements = (Announcement.objects.visible_to_user(user).filter(expiration_date__gt=timezone.now()))
  if context[""events_admin""] and context[""show_all""]:
  events = (Event.objects.all())
  else:
  if context[""show_expired""]:
  events = (Event.objects.visible_to_user(user))
  else:
  midnight = timezone.make_aware(timezone.datetime.combine(datetime.now(), time(0, 0)))
  events = (Event.objects.visible_to_user(user).filter(time__gte=midnight, show_on_dashboard=True))
  items = sorted(chain(announcements, events), key=lambda item: (item.pinned, item.added))
  items.reverse()
  return items",An announcement will be shown if: * It is not expired * unless ?show_expired=1 * It is visible to the user * There are no groups on the announcement (so it is public) * The user's groups are in union with the groups on the announcement (at least one matches) * The user submitted the announcement directly * The user submitted the announcement through a request * The user approved the announcement through a request * ...unless ?show_all=1 An event will be shown if: * It is not expired * unless ?show_expired=1 * It is approved * unless an events admin * It is visible to the user * There are no groups * The groups are in union,2,0,1,1,4,1,0,1,1,3
"def use(broker, debug=True, **kwargs):
  if not debug:
  log.setLevel(logging.INFO)
  if broker.lower() in [""xq"", """"]:
  return XueQiuTrader(**kwargs)
  if broker.lower() in [""yh_client"", """"]:
  from .yh_clienttrader import YHClientTrader
  return YHClientTrader()
  if broker.lower() in [""ht_client"", """"]:
  from .ht_clienttrader import HTClientTrader
  return HTClientTrader()
  if broker.lower() in [""gj_client"", """"]:
  from .gj_clienttrader import GJClientTrader
  return GJClientTrader()
  if broker.lower() in [""ths"", """"]:
  from .clienttrader import ClientTrader
  return ClientTrader()
  raise NotImplementedError"," :param broker: ['yh_client', ''] ['ht_client', ''] :param debug:  debug ,  True :param initial_assets: []  :return the class of trader Usage:: >>> import easytrader >>> user = easytrader.use('xq') >>> user.prepare('xq.json')",1,0,0,0,1,1,0,0,1,2
"def count(self, with_limit_and_skip=False):
  validate_boolean(""with_limit_and_skip"", with_limit_and_skip)
  cmd = SON([(""count"", self.__collection.name),
  (""query"", self.__spec)])
  if self.__max_time_ms is not None:
  cmd[""maxTimeMS""] = self.__max_time_ms
  if self.__comment:
  cmd[""$comment""] = self.__comment
  if self.__hint is not None:
  cmd[""hint""] = self.__hint
  if with_limit_and_skip:
  if self.__limit:
  cmd[""limit""] = self.__limit
  if self.__skip:
  cmd[""skip""] = self.__skip
  return self.__collection._count(cmd, self.__collation)","Get the size of the results set for this query. Returns the number of documents in the results set for this query. Does not take :meth:`limit` and :meth:`skip` into account by default - set `with_limit_and_skip` to ``True`` if that is the desired behavior. Raises :class:`~pymongo.errors.OperationFailure` on a database error. When used with MongoDB >= 2.6, :meth:`~count` uses any :meth:`~hint` applied to the query. In the following example the hint is passed to the count command: collection.find({'field': 'value'}).hint('field_1').count() The :meth:`count` method obeys the :attr:`~pymongo.collection.Collection.read_preference` of the :class:`~pymongo.collection.Collection` instance on which :meth:`~pymongo.collection.Collection.find` was called. :Parameters: - `with_limit_and_skip` (optional): take any :meth:`limit` or :meth:`skip` that has been applied to this cursor into account when getting the count .. note:: The `with_limit_and_skip` parameter requires server version **>= 1.1.4-** .. versionchanged:: 2.8 The :meth:`~count` method now supports :meth:`~hint`.",1,0,1,1,3,1,0,1,1,3
"def run(self):
  self.running = True
  self.recordCond.acquire()
  while self.running or self.recordQueue:
  if self.running and not self.recordQueue:
  self.recordCond.wait()
  continue
  entries = list(reversed(self.recordQueue))
  self.recordQueue = []
  self.recordCond.release()
  with self.lock:
  for now, updates, source in entries:
  for pc, occupation in updates.iteritems():
  self.c.execute(, (
  self._id_for_pc(pc), now, occupation,
  self._id_for_source(source)))
  self.conn.commit()
  self.recordCond.acquire()
  self.recordCond.release()",Runs the worker thread that records occupation updates to the database.,0,1,0,0,1,1,0,0,0,1
"def mix(self, color1, color2, weight=50, *args):
  if color1 and color2:
  if isinstance(weight, string_types):
  weight = float(weight.strip('%'))
  weight = ((weight / 100.0) * 2) - 1
  rgb1 = self._hextorgb(color1)
  rgb2 = self._hextorgb(color2)
  alpha = 0
  w1 = (((weight if weight * alpha == -1 else weight + alpha) /
  (1 + weight * alpha)) + 1)
  w1 = w1 / 2.0
  w2 = 1 - w1
  rgb = [
  rgb1[0] * w1 + rgb2[0] * w2,
  rgb1[1] * w1 + rgb2[1] * w2,
  rgb1[2] * w1 + rgb2[2] * w2,
  ]
  return self._rgbatohex(rgb)
  raise ValueError('Illegal color values')","This algorithm factors in both the user-provided weight and the difference between the alpha values of the two colors to decide how to perform the weighted average of the two RGB values. It works by first normalizing both parameters to be within [-1, 1], where 1 indicates ""only use color1"", -1 indicates ""only use color 0"", and all values in between indicated a proportionately weighted average. Once we have the normalized variables w and a, we apply the formula (w + a)/(1 + w*a) to get the combined weight (in [-1, 1]) of color1. This formula has two especially nice properties: * When either w or a are -1 or 1, the combined weight is also that number (cases where w * a == -1 are undefined, and handled as a special case). * When a is 0, the combined weight is w, and vice versa Finally, the weight of color1 is renormalized to be within [0, 1] and the weight of color2 is given by 1 minus the weight of color1. Copyright (c) 2006-2009 Hampton Catlin, Nathan Weizenbaum, and Chris Eppstein http://sass-lang.com args: color1 (str): first color color2 (str): second color weight (int/str): weight raises: ValueError returns: str",1,0,0,1,2,1,0,0,1,2
"def complete_restore(
  self, location_name, operation_id, last_backup_name, custom_headers=None, raw=False, polling=True, **operation_config):
  raw_result = self._complete_restore_initial(
  location_name=location_name,
  operation_id=operation_id,
  last_backup_name=last_backup_name,
  custom_headers=custom_headers,
  raw=True,
  **operation_config
  )
  def get_long_running_output(response):
  if raw:
  client_raw_response = ClientRawResponse(None, response)
  return client_raw_response
  lro_delay = operation_config.get(
  'long_running_operation_timeout',
  self.config.long_running_operation_timeout)
  if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)
  elif polling is False: polling_method = NoPolling()
  else: polling_method = polling
  return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","Completes the restore operation on a managed database. :param location_name: The name of the region where the resource is located. :type location_name: str :param operation_id: Management operation id that this request tries to complete. :type operation_id: str :param last_backup_name: The last backup name to apply :type last_backup_name: str :param dict custom_headers: headers that will be added to the request :param bool raw: The poller return type is ClientRawResponse, the direct response alongside the deserialized response :param polling: True for ARMPolling, False for no polling, or a polling object for personal polling strategy :return: An instance of LROPoller that returns None or ClientRawResponse<None> if raw==True :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]] :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`",2,1,0,1,4,1,0,0,1,2
"def begin(self, user_url, anonymous=False):
  disco = Discovery(self.session, user_url, self.session_key_prefix)
  try:
  service = disco.getNextService(self._discover)
  except fetchers.HTTPFetchingError as why:
  raise DiscoveryFailure('Error fetching XRDS document: %s' %
  (why.why, ), None)
  if service is None:
  raise DiscoveryFailure('No usable OpenID services found for %s' %
  (user_url, ), None)
  else:
  return self.beginWithoutDiscovery(service, anonymous)","Start the OpenID authentication process. See steps 1-2 in the overview at the top of this file. @param user_url: Identity URL given by the user. This method performs a textual transformation of the URL to try and make sure it is normalized. For example, a user_url of example.com will be normalized to http://example.com/ normalizing and resolving any redirects the server might issue. @type user_url: unicode @param anonymous: Whether to make an anonymous request of the OpenID provider. Such a request does not ask for an authorization assertion for an OpenID identifier, but may be used with extensions to pass other data. e.g. ""I don't care who you are, but I'd like to know your time zone."" @type anonymous: bool @returns: An object containing the discovered information will be returned, with a method for building a redirect URL to the server, as described in step 3 of the overview. This object may also be used to add extension arguments to the request, using its L{addExtensionArg<openid.consumer.consumer.AuthRequest.addExtensionArg>} method. @returntype: L{AuthRequest<openid.consumer.consumer.AuthRequest>} @raises openid.consumer.discover.DiscoveryFailure: when I fail to find an OpenID server for this URL. If the C{yadis} package is available, L{openid.consumer.discover.DiscoveryFailure} is an alias for C{yadis.discover.DiscoveryFailure}.",2,0,0,1,3,1,0,0,1,2
"def get_render_data(self, **kwargs):
  obj = getattr(self, 'object', None)
  data = dict(self.extra_render_data)
  data.update(kwargs)
  data.update({
  'bundle': self.bundle,
  'navigation': self.get_navigation(),
  'url_params': self.kwargs,
  'user': self.request.user,
  'object_header_tmpl': self.object_header_tmpl,
  'view_tags': tag_handler.tags_to_string(self.get_tags(obj))
  })
  if not 'base' in data:
  data['base'] = self.base_template
  if not 'back_bundle' in data:
  data['back_bundle'] = self.get_back_bundle()
  return super(CMSView, self).get_render_data(**data)","Returns all data that should be passed to the renderer. By default adds the following arguments: * **bundle** - The bundle that is attached to this view instance. * **url_params** - The url keyword arguments. i.e.: self.kwargs. * **user** - The user attached to this request. * **base** - Unless base was already specified this gets set to \ 'self.base_template'. * **navigation** - The navigation bar for the page * **object_header_tmpl** - The template to use for the \ object_header. Set to `self.object_header_tmpl`. * **back_bundle** - The back_back bundle is bundle that is linked to \ from the object header as part of navigation. If there is an 'obj' \ argument in the context to render, this will be set to the bundle \ pointed to by the `main_list` attribute of this view's bundle. \ If this is not set, the template's back link will point to the \ admin_site's home page.",0,0,0,1,1,1,0,0,1,2
"def _extract_input_connections(self):
  for con in self.connections:
  ends = con.strip().split('<->')
  ends = filter(None, ends)
  if len(ends) == 0:
  continue
  if len(ends) > 0:
  host1, port1 = self._get_tuple(ends[0].split(':'))
  host2 = ''
  port2 = ''
  if len(ends) > 1:
  host2, port2 = self._get_tuple(ends[1].split(':'))
  self.input_connections.append((host1, port1, host2, port2))","Given user input of interested connections, it will extract the info and output a list of tuples. - input can be multiple values, separated by space; - either host or port is optional - it may be just one end, - e.g., ""host1<->host2 host3<-> host1:port1<->host2"" :return: None",1,0,0,0,1,1,0,0,1,2
"async def set_chat_description(self, chat_id: typing.Union[base.Integer, base.String],
  description: typing.Union[base.String, None] = None) -> base.Boolean:
  payload = generate_payload(**locals())
  result = await self.request(api.Methods.SET_CHAT_DESCRIPTION, payload)
  return result","Use this method to change the description of a supergroup or a channel. The bot must be an administrator in the chat for this to work and must have the appropriate admin rights. Source: https://core.telegram.org/bots/api#setchatdescription :param chat_id: Unique identifier for the target chat or username of the target channel :type chat_id: :obj:`typing.Union[base.Integer, base.String]` :param description: New chat description, 0-255 characters :type description: :obj:`typing.Union[base.String, None]` :return: Returns True on success :rtype: :obj:`base.Boolean`",1,0,0,2,3,1,0,0,2,3
"def read_from_user(input_type, *args, **kwargs):
  def _read_in(*args, **kwargs):
  while True:
  try: tmp = raw_input(*args, **kwargs)
  except NameError: tmp = input(*args, **kwargs)
  try: return input_type(tmp)
  except: print ('Expected type', input_type)
  return _read_in(*args, **kwargs)","Helper function to prompt user for input of a specific type e.g. float, str, int Designed to work with both python 2 and 3 Yes I know this is ugly.",1,0,0,1,2,1,0,0,1,2
"def auth_user(self, username, password):
  password_hash = hashlib.sha512(password.encode(""utf-8"")).hexdigest()
  user = self._database.users.find_one(
  {""username"": username, ""password"": password_hash, ""activate"": {""$exists"": False}})
  return user if user is not None and self.connect_user(username, user[""realname""], user[""email""], user[""language""]) else None",Authenticate the user in database :param username: Username/Login :param password: User password :return: Returns a dict represrnting the user,1,0,1,1,3,1,0,1,1,3
"def interactive_login(self, user=None, password=None, force=False,
  restrict_login=None):
  ignore = force
  log.debug('Calling interactive_login')
  if not user:
  sys.stdout.write('Bugzilla Username: ')
  sys.stdout.flush()
  user = sys.stdin.readline().strip()
  if not password:
  password = getpass.getpass('Bugzilla Password: ')
  log.info('Logging in... ')
  self.login(user, password, restrict_login)
  log.info('Authorization cookie received.')","Helper method to handle login for this bugzilla instance. :param user: bugzilla username. If not specified, prompt for it. :param password: bugzilla password. If not specified, prompt for it. :param force: Unused :param restrict_login: restricts session to IP address",1,0,0,1,2,1,0,0,1,2
"def speak(self, speech, play_behavior=None):
  ssml = ""<speak>{}</speak>"".format(self.__trim_outputspeech(
  speech_output=speech))
  self.response.output_speech = SsmlOutputSpeech(
  ssml=ssml, play_behavior=play_behavior)
  return self",Say the provided speech to the user. :param speech: the output speech sent back to the user. :type speech: str :param play_behavior: attribute to control alexa's speech interruption :type play_behavior: ask_sdk_model.ui.play_behavior.PlayBehavior :return: response factory with partial response being built and access from self.response. :rtype: ResponseFactory,1,0,0,1,2,1,0,0,1,2
"def execute(self, query):
  results = int(self.cursor.execute(query))
  if results > 0:
  result1 = self.cursor.fetchall()
  return result1
  else:
  return []","Execute an SQL query with the corresponding database. The query can be ""templated"" with {scm_db} and {sh_db}.",0,0,1,1,2,0,0,1,1,2
"def beginWithoutDiscovery(self, service, anonymous=False):
  auth_req = self.consumer.begin(service)
  self.session[self._token_key] = auth_req.endpoint
  try:
  auth_req.setAnonymous(anonymous)
  except ValueError as why:
  raise ProtocolError(str(why))
  return auth_req","Start OpenID verification without doing OpenID server discovery. This method is used internally by Consumer.begin after discovery is performed, and exists to provide an interface for library users needing to perform their own discovery. @param service: an OpenID service endpoint descriptor. This object and factories for it are found in the L{openid.consumer.discover} module. @type service: L{OpenIDServiceEndpoint<openid.consumer.discover.OpenIDServiceEndpoint>} @returns: an OpenID authentication request object. @rtype: L{AuthRequest<openid.consumer.consumer.AuthRequest>} @See: Openid.consumer.consumer.Consumer.begin @see: openid.consumer.discover",1,0,0,1,2,1,0,0,1,2
"def UpsertUser(self, database_link, user, options=None):
  if options is None:
  options = {}
  database_id, path = self._GetDatabaseIdWithPathForUser(database_link, user)
  return self.Upsert(user,
  path,
  'users',
  database_id,
  None,
  options)",Upserts a user. :param str database_link: The link to the database. :param dict user: The Azure Cosmos user to upsert. :param dict options: The request options for the request. :return: The upserted User. :rtype: dict,2,1,1,2,6,1,1,0,1,3
"def _handle_crud_like_event(
 target_cls,
 event,
 data=None,
 verb=None,
 id=None,
 customer=None,
 crud_type=None,
 crud_exact=False,
 crud_valid=False,
 ):
 data = data or event.data
 id = id or data.get(""object"", {}).get(""id"", None)
 if not id:
 logger.debug(""Ignoring %r Stripe event without object ID: %r"", event.type, event)
 return
 verb = verb or event.verb
 customer = customer or event.customer
 crud_type = crud_type or CrudType.determine(event=event, verb=verb, exact=crud_exact)
 obj = None
 if crud_valid and not crud_type.valid:
 logger.debug(
 ""Ignoring %r Stripe event without valid CRUD type: %r"", event.type, event
 )
 return
 if crud_type.deleted:
 qs = target_cls.objects.filter(id=id)
 if target_cls is models.Customer and qs.exists():
 qs.get().purge()
 else:
 obj = target_cls.objects.filter(id=id).delete()
 else:
 kwargs = {""id"": id}
 if hasattr(target_cls, ""customer""):
 kwargs[""customer""] = customer
 data = target_cls(**kwargs).api_retrieve()
 obj = target_cls.sync_from_stripe_data(data)
 return obj, crud_type","Helper to process crud_type-like events for objects. Non-deletes (creates, updates and ""anything else"" events) are treated as update_or_create events - The object will be retrieved locally, then it is synchronised with the Stripe API for parity. Deletes only occur for delete events and cause the object to be deleted from the local database, if it existed. If it doesn't exist then it is ignored (but the event processing still succeeds). :param target_cls: The djstripe model being handled. :type: ``djstripe.models.StripeObject`` :param data: The event object data (defaults to ``event.data``). :param verb: The event verb (defaults to ``event.verb``). :param id: The object Stripe ID (defaults to ``object.id``). :param customer: The customer object (defaults to ``event.customer``). :param crud_type: The CrudType object (determined by default). :param crud_exact: If True, match verb against CRUD type exactly. :param crud_valid: If True, CRUD type must match valid type. :returns: The object (if any) and the event CrudType. :rtype: ``tuple(obj, CrudType)``",1,1,1,0,3,1,1,1,0,3
"def get_user(uid):
  try:
  acl_url = urljoin(_acl_url(), 'users/{}'.format(uid))
  r = http.get(acl_url)
  return r.json()
  except DCOSHTTPException as e:
  if e.response.status_code == 400:
  return None
  else:
  raise",Returns a user from the DCOS Enterprise. It returns None if none exists. :param uid: user id :type uid: str :return: User :rtype: dict,2,0,0,1,3,2,0,0,1,3
"def get_user(self, username):
  User = get_user_model()
  try:
  user = User.objects.get(**{
  User.USERNAME_FIELD: username,
  'is_active': False
  })
  return user
  except User.DoesNotExist:
  return None","Given the verified username, look up and return the corresponding user account if it exists, or ``None`` if it doesn't.",1,0,1,1,3,1,0,1,1,3
"def reload(self):
  key = self.key()
  redis = type(self).get_redis()
  if not redis.exists(key):
  raise ModelNotFoundError('This object has been deleted')
  data = debyte_hash(redis.hgetall(key))
  for fieldname, field in self.proxy:
  value = field.recover(data, redis)
  setattr(
  self,
  fieldname,
  value
  )
  return self",reloads this object so if it was updated in the database it now contains the new values,0,0,1,1,2,0,0,1,0,1
"def user_identity_show(self, user_id, id, **kwargs):
  ""https://developer.zendesk.com/rest_api/docs/core/user_identities
  api_path = ""/api/v2/users/{user_id}/identities/{id}.json""
  api_path = api_path.format(user_id=user_id, id=id)
  return self.call(api_path, **kwargs)",https://developer.zendesk.com/rest_api/docs/core/user_identities#show-identity,2,0,0,1,3,2,0,0,1,3
"def get_mini_reviews(recid, ln=CFG_SITE_LANG):
  if CFG_WEBCOMMENT_ALLOW_SHORT_REVIEWS:
  action = 'SUBMIT'
  else:
  action = 'DISPLAY'
  reviews = query_retrieve_comments_or_remarks(recid, ranking=1)
  return webcomment_templates.tmpl_mini_review(
  recid,
  ln,
  action=action,
  avg_score=calculate_avg_score(reviews),
  nb_comments_total=len(reviews))",Returns the web controls to add reviews to a record from the detailed record pages mini-panel. :param recid: the id of the displayed record :param ln: the user's language,1,0,1,1,3,1,0,1,1,3
"def download(queries, user=None, pwd=None,
  email=None, pred_type='and'):
  user = _check_environ('GBIF_USER', user)
  pwd = _check_environ('GBIF_PWD', pwd)
  email = _check_environ('GBIF_EMAIL', email)
  if isinstance(queries, str):
  queries = [queries]
  keyval = [_parse_args(z) for z in queries]
  req = GbifDownload(user, email)
  req.main_pred_type = pred_type
  for predicate in keyval:
  req.add_predicate(predicate['key'],
  predicate['value'],
  predicate['type'])
  out = req.post_download(user, pwd)
  return out, req.payload","Spin up a download request for GBIF occurrence data. :param queries: One or more of query arguments to kick of a download job. See Details. :type queries: str or list :param pred_type: (character) One of ``equals`` (``=``), ``and`` (``&``), `or`` (``|``), ``lessThan`` (``<``), ``lessThanOrEquals`` (``<=``), ``greaterThan`` (``>``), ``greaterThanOrEquals`` (``>=``), ``in``, ``within``, ``not`` (``!``), ``like`` :param user: (character) User name within GBIF's website. Required. Set in your env vars with the option ``GBIF_USER`` :param pwd: (character) User password within GBIF's website. Required. Set in your env vars with the option ``GBIF_PWD`` :param email: (character) Email address to recieve download notice done email. Required. Set in your env vars with the option ``GBIF_EMAIL`` Argument passed have to be passed as character (e.g., ``country = US``), with a space between key (``country``), operator (``=``), and value (``US``). See the ``type`` parameter for possible options for the operator. This character string is parsed internally. Acceptable arguments to ``...`` (args) are: - taxonKey = ``TAXON_KEY`` - scientificName = ``SCIENTIFIC_NAME`` - country = ``COUNTRY`` - publishingCountry = ``PUBLISHING_COUNTRY`` - hasCoordinate = ``HAS_COORDINATE`` - hasGeospatialIssue = ``HAS_GEOSPATIAL_ISSUE`` - typeStatus = ``TYPE_STATUS`` - recordNumber = ``RECORD_NUMBER`` - lastInterpreted = ``LAST_INTERPRETED`` - continent = ``CONTINENT`` - geometry = ``GEOMETRY`` - basisOfRecord = ``BASIS_OF_RECORD`` - datasetKey = ``DATASET_KEY`` - eventDate = ``EVENT_DATE`` - catalogNumber = ``CATALOG_NUMBER`` - year = ``YEAR`` - month = ``MONTH`` - decimalLatitude = ``DECIMAL_LATITUDE`` - decimalLongitude = ``DECIMAL_LONGITUDE`` - elevation = ``ELEVATION`` - depth = ``DEPTH`` - institutionCode = ``INSTITUTION_CODE`` - collectionCode = ``COLLECTION_CODE`` - issue = ``ISSUE`` - mediatype = ``MEDIA_TYPE`` - recordedBy = ``RECORDED_BY`` - repatriated = ``REPATRIATED`` See the API docs http://www.gbif.org/developer/occurrence#download for more info, and the predicates docs http://www.gbif.org/developer/occurrence#predicates GBIF has a limit of 12,000 characters for download queries - so if you're download request is really, really long and complex, consider breaking it up into multiple requests by one factor or another. :return: A dictionary, of results Usage:: from pygbif import occurrences as occ occ.download('basisOfRecord = LITERATURE') occ.download('taxonKey = 3119195') occ.download('decimalLatitude > 50') occ.download('elevation >= 9000') occ.download('decimalLatitude >= 65') occ.download('country = US') occ.download('institutionCode = TLMF') occ.download('catalogNumber = Bird.27847588') res = occ.download(['taxonKey = 7264332', 'hasCoordinate = TRUE']) # pass output to download_meta for more information occ.download_meta(occ.download('decimalLatitude > 75')) # Multiple queries gg = occ.download(['decimalLatitude >= 65', 'decimalLatitude <= -65'], type='or') gg = occ.download(['depth = 80', 'taxonKey = 2343454'], type='or') # Repratriated data for Costa Rica occ.download(['country = CR', 'repatriated = true'])",1,0,0,1,2,1,0,0,1,2
"def check(self, user, provider, permission, **kwargs):
  try:
  social_user = self._get_social_user(user, provider)
  if not social_user:
  return False
  except SocialUserDoesNotExist:
  return False
  backend = self.get_backend(social_user, provider, context=kwargs)
  return backend.check(permission)",user - django User or UserSocialAuth instance provider - name of publisher provider permission - if backend maintains check permissions vk - binary mask in int format facebook - scope string,1,0,1,0,2,1,0,1,1,3
"def boolean(self, field, value=None, **validations):
  values = []
  for found in self._find_by_field(field):
  reality = found[""reality""]
  schema = {""type"": ""boolean""}
  if value is not None:
  schema[""enum""] = [self._input_boolean(value)]
  elif self._should_add_examples():
  schema[""examples""] = [reality]
  skip = self._input_boolean(validations.pop(""skip"", False))
  if not skip:
  self._assert_schema(schema, reality)
  values.append(reality)
  return values","*Asserts the field as JSON boolean.* The field consists of parts separated by spaces, the parts being object property names or array indices starting from 0, and the root being the instance created by the last request (see `Output` for it). For asserting deeply nested properties or multiple objects at once, [http://goessner.net/articles/JsonPath|JSONPath] can be used with [https://github.com/h2non/jsonpath-ng#jsonpath-syntax|supported JSONPath expressions], the root being the response body. *Value* If given, the property value is validated in addition to the type. *Validations* The JSON Schema validation keywords [https://json-schema.org/understanding-json-schema/reference/generic.html|common for all types] can be used. Validations are optional but update the schema of the property (more accurate) if given. `Output Schema` can be used for the current schema of the field. The keyword will fail if any of the given validations fail. Given validations can be skipped altogether by adding ``skip=true``. When skipped, the schema is updated but the validations are not ran. Skip is intented mainly for debugging the updated schema before aborting. *Examples* | `PUT` | /users/1 | { ""verified_email"": true } | | | # https://jsonplaceholder.typicode.com/users/1 | | `Boolean` | response body verified_email | | | | # value is optional | | `Boolean` | response body verified_email | true | | `Boolean` | response body verified_email | ${True} | | | # same as above | | `Boolean` | $.verified_email | true | | | # JSONPath alternative | | `Boolean` | $.verified_email | true | enum=[1, ""1""] | skip=true | # would pass |",1,0,0,1,2,1,0,0,1,2
"def friend(self, note=None, _unfriend=False):
  self.reddit_session.evict(self.reddit_session.config['friends'])
  if not self.reddit_session.is_oauth_session():
  modifier = _modify_relationship('friend', unlink=_unfriend)
  data = {'note': note} if note else {}
  return modifier(self.reddit_session.user, self, **data)
  url = self.reddit_session.config['friend_v1'].format(user=self.name)
  if _unfriend:
  data = {'id': self.name}
  else:
  data = {'note': note} if note else {}
  data = dumps(data)
  method = 'DELETE' if _unfriend else 'PUT'
  return self.reddit_session.request_json(url, data=data, method=method)",Friend the user. :param note: A personal note about the user. Requires reddit Gold. :param _unfriend: Unfriend the user. Please use :meth:`unfriend` instead of setting this parameter manually. :returns: The json response from the server.,1,0,0,2,3,2,0,0,2,4
"def get_list_retention_policies(self, database=None):
  if not (database or self._database):
  raise InfluxDBClientError(
  ""get_list_retention_policies() requires a database as a ""
  ""parameter or the client to be using a database"")
  rsp = self.query(
  ""SHOW RETENTION POLICIES ON {0}"".format(
  quote_ident(database or self._database))
  )
  return list(rsp.get_points())","Get the list of retention policies for a database. :param database: the name of the database, defaults to the client's current database :type database: str :returns: all retention policies for the database :rtype: list of dictionaries :Example: :: >> ret_policies = client.get_list_retention_policies('my_db') >> ret_policies [{u'default': True, u'duration': u'0', u'name': u'default', u'replicaN': 1}]",1,0,1,1,3,1,0,1,1,3
"def init_driver(client_id):
  profile_path = CHROME_CACHE_PATH + str(client_id)
  if not os.path.exists(profile_path):
  os.makedirs(profile_path)
  chrome_options = [
  'window-size=' + CHROME_WINDOW_SIZE,
  '--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/60.0.3112.78 Chrome/60.0.3112.78 Safari/537.36'
  ]
  if CHROME_IS_HEADLESS:
  chrome_options.append('--headless')
  if CHROME_DISABLE_GPU:
  chrome_options.append('--disable-gpu')
  d = WhatsAPIDriver(
  username=client_id,
  profile=profile_path,
  client='chrome',
  chrome_options=chrome_options
  )
  return d",Initialises a new driver via webwhatsapi module @param client_id: ID of user client @return webwhatsapi object,1,0,0,0,1,1,0,0,1,2
"def _convertPyval(self, oself, pyval):
  if pyval is None and not self.allowNone:
  raise TypeError(""attribute [%s.%s = %s()] must not be None"" % (
  self.classname, self.attrname, self.__class__.__name__))
  return self.infilter(pyval, oself, oself.store)",Convert a Python value to a value suitable for inserting into the database. @param oself: The object on which this descriptor is an attribute. @param pyval: The value to be converted. @return: A value legal for this column in the database.,0,1,0,0,1,0,0,0,0,0
"def change_settings(self, bio=None, public_images=None,
  messaging_enabled=None, album_privacy=None,
  accepted_gallery_terms=None):
  url = self._imgur._base_url + ""/3/account/{0}/settings"".format(self.name)
  resp = self._imgur._send_request(url, needs_auth=True, params=locals(),
  method='POST')
  return resp","Update the settings for the user. :param bio: A basic description filled out by the user, is displayed in the gallery profile page. :param public_images: Set the default privacy setting of the users images. If True images are public, if False private. :param messaging_enabled: Set to True to enable messaging. :param album_privacy: The default privacy level of albums created by the user. Can be public, hidden or secret. :param accepted_gallery_terms: The user agreement to Imgur Gallery terms. Necessary before the user can submit to the gallery.",1,0,0,1,2,2,0,0,1,3
"def cookiecutter(template, checkout=None, no_input=False, extra_context=None):
  config_dict = get_user_config()
  template = expand_abbreviations(template, config_dict)
  if 'git@' in template or 'https://' in template:
  repo_dir = clone(
  repo_url=template,
  checkout=checkout,
  clone_to_dir=config_dict['cookiecutters_dir'],
  no_input=no_input
  )
  else:
  repo_dir = template
  context_file = os.path.join(repo_dir, 'cookiecutter.json')
  logging.debug('context_file is {0}'.format(context_file))
  context = generate_context(
  context_file=context_file,
  default_context=config_dict['default_context'],
  extra_context=extra_context,
  )
  generate_files(
  repo_dir=repo_dir,
  context=context
  )","Replacement for cookiecutter's own cookiecutter. The difference with cookiecutter's cookiecutter function is that this one doesn't automatically str() all the values passed along to the template. :param template: A directory containing a project template directory, or a URL to a git repository. :param checkout: The branch, tag or commit ID to checkout after clone. :param no_input: Prompt the user at command line for manual configuration? :param extra_context: A dictionary of context that overrides default and user configuration.",2,0,0,0,2,1,0,0,1,2
"def find_includes(filename):
  includes = []
  with open(filename, ""r"", encoding=""utf-8"") as inp:
  for line in inp:
  line = line.strip()
  if not line or line.startswith(""//""):
  continue
  if line.startswith(""
  mm = re.match(rx_include, line)
  if mm:
  includename = os.path.join(""c"", mm.group(1))
  includes.append(includename)
  return includes","Find user includes (no system includes) requested from given source file. All .h files will be given relative to the current folder, e.g. [""c/rowindex.h"", ""c/column.h""].",1,0,0,1,2,1,0,0,1,2
"def get_s3_file_tree(s3, bucket, prefix):
  def get_some_keys(keys, marker=None):
  if marker:
  relevant_files = s3.list_objects(Bucket=bucket, Prefix=prefix,
  Marker=marker)
  else:
  relevant_files = s3.list_objects(Bucket=bucket, Prefix=prefix)
  keys.extend([entry['Key'] for entry in relevant_files['Contents']
  if entry['Key'] != marker])
  return relevant_files['IsTruncated']
  file_keys = []
  marker = None
  while get_some_keys(file_keys, marker):
  marker = file_keys[-1]
  file_tree = NestedDict()
  pref_path = prefix.split('/')[:-1]
  for key in file_keys:
  full_path = key.split('/')
  relevant_path = full_path[len(pref_path):]
  curr = file_tree
  for step in relevant_path:
  curr = curr[step]
  curr['key'] = key
  return file_tree","Overcome s3 response limit and return NestedDict tree of paths. The NestedDict object also allows the user to search by the ends of a path. The tree mimics a file directory structure, with the leave nodes being the full unbroken key. For example, 'path/to/file.txt' would be retrieved by ret['path']['to']['file.txt']['key'] The NestedDict object returned also has the capability to get paths that lead to a certain value. So if you wanted all paths that lead to something called 'file.txt', you could use ret.get_paths('file.txt') For more details, see the NestedDict docs.",1,0,0,1,2,1,0,0,1,2
"def get_database(self, name=None, codec_options=None, read_preference=None,
  write_concern=None, read_concern=None):
  if name is None:
  if self.__default_database_name is None:
  raise ConfigurationError('No default database defined')
  name = self.__default_database_name
  return database.Database(
  self, name, codec_options, read_preference,
  write_concern, read_concern)","Get a :class:`~pymongo.database.Database` with the given name and options. Useful for creating a :class:`~pymongo.database.Database` with different codec options, read preference, and/or write concern from this :class:`MongoClient`. >>> client.read_preference Primary() >>> db1 = client.test >>> db1.read_preference Primary() >>> from pymongo import ReadPreference >>> db2 = client.get_database( ... 'test', read_preference=ReadPreference.SECONDARY) >>> db2.read_preference Secondary(tag_sets=None) :Parameters: - `name` (optional): The name of the database - a string. If ``None`` (the default) the database named in the MongoDB connection URI is returned. - `codec_options` (optional): An instance of :class:`~bson.codec_options.CodecOptions`. If ``None`` (the default) the :attr:`codec_options` of this :class:`MongoClient` is used. - `read_preference` (optional): The read preference to use. If ``None`` (the default) the :attr:`read_preference` of this :class:`MongoClient` is used. See :mod:`~pymongo.read_preferences` for options. - `write_concern` (optional): An instance of :class:`~pymongo.write_concern.WriteConcern`. If ``None`` (the default) the :attr:`write_concern` of this :class:`MongoClient` is used. - `read_concern` (optional): An instance of :class:`~pymongo.read_concern.ReadConcern`. If ``None`` (the default) the :attr:`read_concern` of this :class:`MongoClient` is used. .. versionchanged:: 3.5 The `name` parameter is now optional, defaulting to the database named in the MongoDB connection URI.",1,0,0,1,2,1,0,0,1,2
"def create_user(self, projects=None, tasks=None):
  projects = projects or []
  tasks = tasks or []
  dialog = UserCreatorDialog(projects=projects, tasks=tasks, parent=self)
  dialog.exec_()
  user = dialog.user
  if user:
  userdata = djitemdata.UserItemData(user)
  treemodel.TreeItem(userdata, self.users_model.root)
  return user",Create and return a new user :param projects: the projects for the user :type projects: list of :class:`jukeboxcore.djadapter.models.Project` :param tasks: the tasks for the user :type tasks: list of :class:`jukeboxcore.djadapter.models.Task` :returns: The created user or None :rtype: None | :class:`jukeboxcore.djadapter.models.User` :raises: None,0,1,1,1,3,1,1,0,1,3
"def load_name(*names, load_order=DEFAULT_LOAD_ORDER, extension='yaml', missing=Missing.silent):
  def generate_sources():
  for source, name in product(load_order, names):
  if callable(source):
  yield source(name, extension)
  else:
  candidate = path.expanduser(source.format(name=name, extension=extension))
  yield loadf(candidate, default=NotConfigured)
  return Configuration(*generate_sources(), missing=missing)","Read a `.Configuration` instance by name, trying to read from files in increasing significance. The default load order is `.system`, `.user`, `.application`, `.environment`. Multiple names are combined with multiple loaders using names as the 'inner loop / selector', loading ``/etc/name1.yaml`` and ``/etc/name2.yaml`` before ``./name1.yaml`` and ``./name2.yaml``. :param names: application or configuration set names, in increasing significance :param load_order: ordered list of name templates or `callable`s, in increasing order of significance :param extension: file extension to be used :param missing: policy to be used when a configured key is missing, either as a `.Missing` instance or a default value :return: a `.Configuration` instances providing values loaded from *names* in *load_order* ordering",1,0,0,1,2,1,0,0,1,2
"def introspect_operation(self, operation):
  return {
  'method_name': operation.py_name,
  'api_name': operation.name,
  'docs': self.convert_docs(operation.documentation),
  'params': self.parse_params(operation.params),
  'output': operation.output,
  }","Introspects an entire operation, returning:: * the method name (to expose to the user) * the API name (used server-side) * docs * introspected information about the parameters * information about the output :param operation: The operation to introspect :type operation: A <botocore.operation.Operation> object :returns: A dict of information",1,0,0,0,1,0,0,0,1,1
"def inserir(self, protocolo):
  tipo_acesso_map = dict()
  tipo_acesso_map['protocolo'] = protocolo
  code, xml = self.submit(
  {'tipo_acesso': tipo_acesso_map}, 'POST', 'tipoacesso/')
  return self.response(code, xml)",Insert new access type and returns its identifier. :param protocolo: Protocol. :return: Dictionary with structure: {tipo_acesso: {id: < id >}} :raise ProtocoloTipoAcessoDuplicadoError: Protocol already exists. :raise InvalidParameterError: Protocol value is invalid or none. :raise DataBaseError: Networkapi failed to access the database. :raise XMLError: Networkapi failed to generate the XML response.,2,0,0,2,4,1,1,0,1,3
"def build_row_dict(cls, row, dialect, deleted=False, user_id=None, use_dirty=True):
  data = {
  'data': row.to_archivable_dict(dialect, use_dirty=use_dirty),
  'deleted': deleted,
  'updated_at': datetime.now(),
  'version_id': current_version_sql(as_is=True) if deleted else row.version_id
  }
  for col_name in row.version_columns:
  data[col_name] = utils.get_column_attribute(row, col_name, use_dirty=use_dirty)
  if user_id is not None:
  data['user_id'] = user_id
  return data","Builds a dictionary of archive data from row which is suitable for insert. NOTE: If `deleted` is False, version ID will be set to an AsIs SQL construct. :param row: instance of :class:`~SavageModelMixin` :param dialect: :py:class:`~sqlalchemy.engine.interfaces.Dialect` :param deleted: whether or not the row is deleted (defaults to False) :param user_id: ID of user that is performing the update on this row (defaults to None) :param use_dirty: whether to use the dirty fields from row or not (defaults to True) :return: a dictionary of archive table column names to values, suitable for insert :rtype: dict",0,0,1,0,1,0,0,1,0,1
"def addStream(self, streamname, schema=None, **kwargs):
  stream = self.connectordb[streamname]
  if not stream.exists():
  if schema is not None:
  stream.create(schema, **kwargs)
  else:
  raise Exception(
  ""The stream '%s' was not found"" % (streamname, ))
  self.addStream_force(streamname, stream.schema)","Adds the given stream to the logger. Requires an active connection to the ConnectorDB database. If a schema is not specified, loads the stream from the database. If a schema is specified, and the stream does not exist, creates the stream. You can also add stream properties such as description or nickname to be added during creation.",1,1,1,1,4,1,1,1,0,3
"def generate_pdf_report(self, impact_function, iface, scenario_name):
  output_dir = self.output_directory.text()
  file_path = os.path.join(output_dir, scenario_name)
  table_report_metadata = ReportMetadata(
  metadata_dict=standard_impact_report_metadata_pdf)
  impact_table_report = ImpactReport(
  iface,
  table_report_metadata,
  impact_function=impact_function)
  impact_table_report.output_folder = file_path
  impact_table_report.process_components()
  map_report_metadata = ReportMetadata(
  metadata_dict=update_template_component(map_report))
  impact_map_report = ImpactReport(
  iface,
  map_report_metadata,
  impact_function=impact_function)
  impact_map_report.qgis_composition_context.extent = \
  impact_function.impact.extent()
  impact_map_report.output_folder = file_path
  impact_map_report.process_components()",Generate and store map and impact report from impact function. Directory where the report stored is specified by user input from the dialog. This function is adapted from analysis_utilities.py :param impact_function: Impact Function. :type impact_function: ImpactFunction() :param iface: iface. :type iface: iface :param scenario_name: name of the scenario :type scenario_name: str,1,0,0,1,2,1,0,0,1,2
"def poll(connection: connection, timeout: float=1.0) -> Iterable[Event]:
  if timeout > 0.0:
  log('Polling for events (Blocking, {} seconds)...'.format(timeout), logger_name=_LOGGER_NAME)
  else:
  log('Polling for events (Non-Blocking)...', logger_name=_LOGGER_NAME)
  if select.select([connection], [], [], timeout) == ([], [], []):
  log('...No events found', logger_name=_LOGGER_NAME)
  return
  else:
  log('Events', logger_name=_LOGGER_NAME)
  log('------', logger_name=_LOGGER_NAME)
  connection.poll()
  while connection.notifies:
  event = connection.notifies.pop(0)
  log(str(event), logger_name=_LOGGER_NAME)
  yield Event.fromjson(event.payload)","Poll the connection for notification events. This method operates as an iterable. It will keep returning events until all events have been read. Parameters ---------- connection: psycopg2.extensions.connection Active connection to a PostGreSQL database. timeout: float Number of seconds to block for an event before timing out. Returns ------- event: Event or None If an event is available, an Event is returned. If no event is available, None is returned. Examples -------- >>> events = [evt for evt in connection.poll()] >>> for evt in connection.poll(): print(evt)",0,0,1,0,1,1,0,0,1,2
"def get_identity(context, prefix=None, identity_func=None, user=None):
  if prefix is not None:
  try:
  return context['%s_identity' % prefix]
  except KeyError:
  pass
  try:
  return context['analytical_identity']
  except KeyError:
  pass
  if getattr(settings, 'ANALYTICAL_AUTO_IDENTIFY', True):
  try:
  if user is None:
  user = get_user_from_context(context)
  if get_user_is_authenticated(user):
  if identity_func is not None:
  return identity_func(user)
  else:
  return user.get_username()
  except (KeyError, AttributeError):
  pass
  return None",Get the identity of a logged in user from a template context. The `prefix` argument is used to provide different identities to different analytics services. The `identity_func` argument is a function that returns the identity of the user; by default the identity is the username.,0,0,0,1,1,1,0,0,1,2
"def make_pre_authed_request(self, env, method=None, path=None, body=None,
  headers=None):
  if self.default_storage_policy:
  sp = self.default_storage_policy
  if headers:
  headers.update({'X-Storage-Policy': sp})
  else:
  headers = {'X-Storage-Policy': sp}
  subreq = swift.common.wsgi.make_pre_authed_request(
  env, method=method, path=path, body=body, headers=headers,
  agent=self.agent)
  subreq.environ['swift.source'] = self.swift_source
  return subreq","Nearly the same as swift.common.wsgi.make_pre_authed_request except that this also always sets the 'swift.source' and user agent. Newer Swift code will support swift_source as a kwarg, but we do it this way so we don't have to have a newer Swift. Since we're doing this anyway, we may as well set the user agent too since we always do that.",1,0,0,1,2,1,0,0,1,2
"def save(name, data, rc_file='~/.odoorpcrc'):
  conf = ConfigParser()
  conf.read([os.path.expanduser(rc_file)])
  if not conf.has_section(name):
  conf.add_section(name)
  for key in data:
  value = data[key]
  conf.set(name, key, str(value))
  with open(os.path.expanduser(rc_file), 'w') as file_:
  os.chmod(os.path.expanduser(rc_file), stat.S_IREAD | stat.S_IWRITE)
  conf.write(file_)","Save the `data` session configuration under the name `name` in the `rc_file` file. >>> import odoorpc >>> odoorpc.session.save( ... 'foo', ... {'type': 'ODOO', 'host': 'localhost', 'protocol': 'jsonrpc', ... 'port': 8069, 'timeout': 120, 'database': 'db_name' ... 'user': 'admin', 'passwd': 'password'}) # doctest: +SKIP .. doctest:: :hide: >>> import odoorpc >>> session = '%s_session' % DB >>> odoorpc.session.save( ... session, ... {'type': 'ODOO', 'host': HOST, 'protocol': PROTOCOL, ... 'port': PORT, 'timeout': 120, 'database': DB, ... 'user': USER, 'passwd': PWD})",1,0,0,0,1,1,0,0,0,1
"def send_message(contacts, message):
  if type(contacts) == str:
  contacts = [contacts]
  recipients = list(set(contacts))
  send_notification(
  subsystem='UNKNOWN',
  recipients=[NotificationContact('slack', x) for x in recipients],
  subject=None,
  body_html=message,
  body_text=message
  )","List of contacts the send the message to. You can send messages either to channels and private groups by using the following formats #channel-name @username-direct-message If the channel is the name of a private group / channel, you must first invite the bot to the channel to ensure it is allowed to send messages to the group. Returns true if the message was sent, else `False` Args: contacts (:obj:`list` of `str`,`str`): List of contacts message (str): Message to send Returns: `bool`",2,0,0,0,2,1,0,0,1,2
"def check_async(async_, kwargs, default):
  if async_ is not None:
  return async_
  elif 'async' in kwargs:
  warnings.warn(
  '""async"" attribute is deprecated. Use ""async_"" instead.',
  DeprecationWarning,
  )
  return kwargs.pop('async')
  else:
  return default",Return a value of 'async' in kwargs or default when async_ is None. This helper function exists for backward compatibility (See #274). It shows a warning message when 'async' in kwargs is used to note users.,0,0,0,1,1,1,0,0,1,2
"def insertTarget(self, name, path):
  sql = 'insert into {}(name, path) values (?,?);'.format(self.TABLE_ITEMS)
  try:
  _id = self.db.execute(sql, (name, path)).lastrowid
  self.db.commit()
  return _id
  except sqlite3.IntegrityError:
  return None",Inserts a new target into the vault database Returns the id of the created target,0,1,1,0,2,0,1,1,0,2
"def compose(self, sources, client=None):
  client = self._require_client(client)
  query_params = {}
  if self.user_project is not None:
  query_params[""userProject""] = self.user_project
  request = {
  ""sourceObjects"": [{""name"": source.name} for source in sources],
  ""destination"": self._properties.copy(),
  }
  api_response = client._connection.api_request(
  method=""POST"",
  path=self.path + ""/compose"",
  query_params=query_params,
  data=request,
  _target_object=self,
  )
  self._set_properties(api_response)","Concatenate source blobs into this one. If :attr:`user_project` is set on the bucket, bills the API request to that project. :type sources: list of :class:`Blob` :param sources: blobs whose contents will be composed into this blob. :type client: :class:`~google.cloud.storage.client.Client` or ``NoneType`` :param client: Optional. The client to use. If not passed, falls back to the ``client`` stored on the blob's bucket.",1,0,0,1,2,1,0,0,1,2
"def add_default_args(parser, version=None, include=None):
  include = include or ('config', 'user', 'dry-run', 'verbose', 'quiet')
  if 'config' in include:
  parser.add_argument('-c', '--config', dest='config_file', metavar='PATH', default=None,
  type=str, help='bots config file (json or yaml)')
  if 'user' in include:
  parser.add_argument('-u', '--user', dest='screen_name', type=str, help=""Twitter screen name"")
  if 'dry-run' in include:
  parser.add_argument('-n', '--dry-run', action='store_true', help=""Don't actually do anything"")
  if 'verbose' in include:
  parser.add_argument('-v', '--verbose', action='store_true', help=""Run talkatively"")
  if 'quiet' in include:
  parser.add_argument('-q', '--quiet', action='store_true', help=""Run quietly"")
  if version:
  parser.add_argument('-V', '--version', action='version', version=""%(prog)s "" + version)","Add default arguments to a parser. These are: - config: argument for specifying a configuration file. - user: argument for specifying a user. - dry-run: option for running without side effects. - verbose: option for running verbosely. - quiet: option for running quietly. - version: option for spitting out version information. Args: version (str): version to return on <cli> --version include (Sequence): default arguments to add to cli. Default: (config, user, dry-run, verbose, quiet)",1,0,0,1,2,1,0,0,1,2
"def find_and_reserve_fcp(self, assigner_id):
  fcp_list = self.db.get_from_assigner(assigner_id)
  if not fcp_list:
  new_fcp = self.db.find_and_reserve()
  if new_fcp is None:
  LOG.info(""no more fcp to be allocated"")
  return None
  LOG.debug(""allocated %s fcp for %s assigner"" %
  (new_fcp, assigner_id))
  return new_fcp
  else:
  old_fcp = fcp_list[0][0]
  self.db.reserve(fcp_list[0][0])
  return old_fcp","reserve the fcp to assigner_id The function to reserve a fcp for user 1. Check whether assigner_id has a fcp already if yes, make the reserve of that record to 1 2. No fcp, then find a fcp and reserve it fcp will be returned, or None indicate no fcp",0,1,1,0,2,1,0,1,0,2
"def update(self, friendly_name=values.unset, identity=values.unset,
  deployment_sid=values.unset, enabled=values.unset):
  return self._proxy.update(
  friendly_name=friendly_name,
  identity=identity,
  deployment_sid=deployment_sid,
  enabled=enabled,
  )",Update the DeviceInstance :param unicode friendly_name: A human readable description for this Device. :param unicode identity: An identifier of the Device user. :param unicode deployment_sid: The unique SID of the Deployment group. :param bool enabled: The enabled :returns: Updated DeviceInstance :rtype: twilio.rest.preview.deployed_devices.fleet.device.DeviceInstance,1,0,0,1,2,1,0,0,1,2
"def select_flair(self, item, flair_template_id='', flair_text=''):
  data = {'flair_template_id': flair_template_id or '',
  'text': flair_text or ''}
  if isinstance(item, objects.Submission):
  data['link'] = item.fullname
  evict = item.permalink
  else:
  data['name'] = self.user.name
  data['r'] = six.text_type(item)
  evict = self.config['flairlist'].format(
  subreddit=six.text_type(item))
  response = self.request_json(self.config['select_flair'], data=data)
  self.evict(evict)
  return response","Select user flair or link flair on subreddits. This can only be used for assigning your own name flair or link flair on your own submissions. For assigning other's flairs using moderator access, see :meth:`~praw.__init__.ModFlairMixin.set_flair`. :param item: A string, Subreddit object (for user flair), or Submission object (for link flair). If ``item`` is a string it will be treated as the name of a Subreddit. :param flair_template_id: The id for the desired flair template. Use the :meth:`~praw.objects.Subreddit.get_flair_choices` and :meth:`~praw.objects.Submission.get_flair_choices` methods to find the ids for the available user and link flair choices. :param flair_text: A string containing the custom flair text. Used on subreddits that allow it. :returns: The json response from the server.",2,0,0,2,4,1,0,0,2,3
"def subscribe(self, user):
  if self.subscription_policy == SubscriptionPolicy.OPEN:
  return self.add_member(user)
  elif self.subscription_policy == SubscriptionPolicy.APPROVAL:
  return self.add_member(user, state=MembershipState.PENDING_ADMIN)
  elif self.subscription_policy == SubscriptionPolicy.CLOSED:
  return None",Subscribe a user to a group (done by users). Wrapper around ``add_member()`` which checks subscription policy. :param user: User to subscribe. :returns: Newly created Membership or None.,1,1,0,0,2,1,0,0,1,2
"def create_document(self, name='Test Document', owner_type=0, public=True):
  payload = {
  'name': name,
  'ownerType': owner_type,
  'isPublic': public
  }
  return self._api.request('post', '/api/documents', body=payload)","Create a new document. Args: - name (str, default='Test Document'): The doc name - owner_type (int, default=0): 0 for user, 1 for company, 2 for team - public (bool, default=False): Whether or not to make doc public Returns: - requests.Response: Onshape response data",1,0,0,1,2,1,0,0,2,3
"def register(name=EopDb.DEFAULT_DBNAME):
  if isinstance(name, str):
  def wrapper(klass):
  EopDb.register(klass, name)
  return klass
  return wrapper
  else:
  klass = name
  EopDb.register(klass)
  return klass","Decorator for registering an Eop Database Example: .. code-block:: python @register class SqliteEnvDatabase: # sqlite implementation # this database will be known as 'default' @register('json') class JsonEnvDatabase: # JSON implementation EopDb.get(58090.2) # get Eop from SqliteEnvDatabase EopDb.get(58090.2, dbname='default') # same as above EopDb.get(58090.2, dbname='json') # get Eop from JsonEnvDatabase",1,0,0,0,1,0,0,0,0,0
"def _write_error_batch(batch, database, measurements):
  if not measurements:
  LOGGER.info('All %s measurements from batch %s processed',
  database, batch)
  return
  LOGGER.debug('Processing batch %s for %s by measurement, %i left',
  batch, database, len(measurements))
  url = '{}?db={}&precision=ms'.format(_base_url, database)
  measurement = measurements.pop(0)
  future = _http_client.fetch(
  url, method='POST', body=measurement.encode('utf-8'))
  ioloop.IOLoop.current().add_timeout(
  ioloop.IOLoop.current().time() + 0.025,
  _write_error_batch_wait, future, batch, database, measurement,
  measurements)","Invoked when a batch submission fails, this method will submit one measurement to InfluxDB. It then adds a timeout to the IOLoop which will invoke :meth:`_write_error_batch_wait` which will evaluate the result and then determine what to do next. :param str batch: The batch ID for correlation purposes :param str database: The database name for the measurements :param list measurements: The measurements that failed to write as a batch",0,1,0,1,2,1,0,0,1,2
"def direct_to_user_template(request, username, template_name,
  extra_context=None):
  user = get_object_or_404(get_user_model(), username__iexact=username)
  if not extra_context:
  extra_context = dict()
  extra_context['viewed_user'] = user
  extra_context['profile'] = user.get_profile()
  return ExtraContextTemplateView.as_view(template_name=template_name,
  extra_context=extra_context)(request)","Simple wrapper for Django's :func:`direct_to_template` view. This view is used when you want to show a template to a specific user. A wrapper for :func:`direct_to_template` where the template also has access to the user that is found with ``username``. For ex. used after signup, activation and confirmation of a new e-mail. :param username: String defining the username of the user that made the action. :param template_name: String defining the name of the template to use. Defaults to ``accounts/signup_complete.html``. **Keyword arguments** ``extra_context`` A dictionary containing extra variables that should be passed to the rendered template. The ``account`` key is always the ``User`` that completed the action. **Extra context** ``viewed_user`` The currently :class:`User` that is viewed.",1,0,1,2,4,1,0,1,1,3
"def climate_stats(self, startclim, endclim, type, **kwargs):
  r
  self._check_geo_param(kwargs)
  kwargs['type'] = type
  kwargs['startclim'] = startclim
  kwargs['endclim'] = endclim
  kwargs['token'] = self.token
  return self._get_response('stations/climatology', kwargs)","r"""""" Returns a dictionary of aggregated yearly climate statistics (count, standard deviation, average, median, maximum, minimum, min time, and max time depending on user specified type) of a time series for a specified range of time at user specified location. Users must specify at least one geographic search parameter ('stid', 'state', 'country', 'county', 'radius', 'bbox', 'cwa', 'nwsfirezone', 'gacc', or 'subgacc') to obtain observation data. Other parameters may also be included. See below mandatory and optional parameters. Also see the metadata() function for station IDs. Arguments: ---------- type: string, mandatory Describes what statistical values will be returned. Can be one of the following values: ""avg""/""average""/""mean"", ""max""/""maximum"", ""min""/""minimum"", ""stdev""/""standarddeviation""/""std"", ""median""/""med"", ""count"", or ""all"". ""All"" will return all of the statistics. startclim: string, mandatory Start date in form of MMDDhhmm. MUST BE USED WITH THE ENDCLIM PARAMETER. Default time is UTC e.g. startclim=06011800 Do not specify a year. endclim: string, mandatory End date in form of MMDDhhmm. MUST BE USED WITH THE STARTCLIM PARAMETER. Default time is UTC e.g. endclim=06011800 Do not specify a year. obtimezone: string, optional Set to either UTC or local. Sets timezone of obs. Default is UTC. e.g. obtimezone='local'. showemptystations: string, optional Set to '1' to show stations even if no obs exist that match the time period. Stations without obs are omitted by default. stid: string, optional Single or comma separated list of MesoWest station IDs. e.g. stid='kden,kslc,wbb' county: string, optional County/parish/borough (US/Canada only), full name e.g. county='Larimer' state: string, optional US state, 2-letter ID e.g. state='CO' country: string, optional Single or comma separated list of abbreviated 2 or 3 character countries e.g. country='us,ca,mx' radius: string, optional Distance from a lat/lon pt or stid as [lat,lon,radius (mi)] or [stid, radius (mi)]. e.g. radius=""-120,40,20"" bbox: string, optional Stations within a [lon/lat] box in the order [lonmin,latmin,lonmax,latmax] e.g. bbox=""-120,40,-119,41"" cwa: string, optional NWS county warning area. See http://www.nws.noaa.gov/organization.php for CWA list. e.g. cwa='LOX' nwsfirezone: string, optional NWS fire zones. See http://www.nws.noaa.gov/geodata/catalog/wsom/html/firezone.htm for a shapefile containing the full list of zones. e.g. nwsfirezone='LOX241' gacc: string, optional Name of Geographic Area Coordination Center e.g. gacc='EBCC' See http://gacc.nifc.gov/ for a list of GACCs. subgacc: string, optional Name of Sub GACC e.g. subgacc='EB07' vars: string, optional Single or comma separated list of sensor variables. Will return all stations that match one of provided variables. Useful for filtering all stations that sense only certain vars. Do not request vars twice in the query. e.g. vars='wind_speed,pressure' Use the variables function to see a list of sensor vars. units: string, optional String or set of strings and pipes separated by commas. Default is metric units. Set units='ENGLISH' for FREEDOM UNITS ;) Valid other combinations are as follows: temp|C, temp|F, temp|K; speed|mps, speed|mph, speed|kph, speed|kts; pres|pa, pres|mb; height|m, height|ft; precip|mm, precip|cm, precip|in; alti|pa, alti|inhg. e.g. units='temp|F,speed|kph,metric' groupby: string, optional Results can be grouped by key words: state, county, country, cwa, nwszone, mwsfirezone, gacc, subgacc e.g. groupby='state' timeformat: string, optional A python format string for returning customized date-time groups for observation times. Can include characters. e.g. timeformat='%m/%d/%Y at %H:%M' Returns: -------- Dictionary of aggregated climatology statistics. Raises: ------- None.",2,0,0,1,3,1,0,0,1,2
"def logout_listener(app, user):
  @after_this_request
  def _commit(response=None):
  if hasattr(session, 'sid_s'):
  delete_session(session.sid_s)
  session.regenerate()
  current_accounts.datastore.commit()
  return response",Connect to the user_logged_out signal. :param app: The Flask application. :param user: The :class:`invenio_accounts.models.User` instance.,0,1,0,0,1,1,1,1,0,3
"def readline(self, use_raw=None, prompt=''):
  if use_raw is None:
  use_raw = self.use_raw
  pass
  if use_raw:
  try:
  inp = input(prompt)
  return inp
  except ValueError:
  raise EOFError
  pass
  else:
  line = self.input.readline()
  if not line: raise EOFError
  return line.rstrip(""\n"")
  pass",Read a line of input. EOFError will be raised on EOF. Note: some user interfaces may decide to arrange to call DebuggerOutput.write() first with the prompt rather than pass it here.. If `use_raw' is set raw_input() will be used in that is supported by the specific input input. If this option is left None as is normally expected the value from the class initialization is used.,1,0,0,1,2,1,0,0,1,2
"def add_event(self, user_id, event_name, event_properties=None, headers=None, endpoint_url=None):
  endpoint_url = endpoint_url or self._endpoint_url
  url = endpoint_url + '/users/' + user_id + '/events'
  headers = headers or self._default_headers()
  event_properties = event_properties or {}
  payload = {
  ""event_name"": event_name,
  ""custom_properties"": event_properties
  }
  response = requests.post(url, headers=headers, json=payload)
  return response","Send an identified event. If a user doesn't exist it will create one. :param str user_id: identified user's ID :param str event_name: event name (e.g. ""visit_website"") :param dict event_properties: properties that describe the event's details :param dict headers: custom request headers (if isn't set default values are used) :param str endpoint_url: where to send the request (if isn't set default value is used) :return: Response",1,0,0,2,3,2,0,0,2,4
"def user(context, name, email):
  existing_user = context.obj['store'].user(email)
  if existing_user:
  click.echo(existing_user.to_dict())
  elif name:
  new_user = context.obj['store'].add_user(name, email)
  click.echo(click.style(f""New user added: {email} ({new_user.id})"", fg='green'))
  else:
  click.echo(click.style('User not found', fg='yellow'))",Add a new or display information about an existing user.,1,1,1,1,4,1,1,1,1,4
"def detailedInformation( cls, parent, title, info, details, buttons = None ):
  if ( buttons == None ):
  buttons = XMessageBox.Ok
  dlg = cls(parent)
  dlg.setWindowTitle(title)
  dlg.setText(info)
  dlg.setDetailedText(details)
  dlg.setStandardButtons(buttons)
  dlg.exec_()
  return dlg.clickedButton()",Creates a new information dialog with detailed information and \ presents it to the user. :param parent | <QWidget> title | <str> info | <str> details | <str> buttons | <QMessageBox.StandardButton> :return <QMessageBox.StandardButton>,1,0,0,1,2,1,0,0,1,2
"def list(self, account=None, username=None):
  username = self._resolve_username(account, username)
  uri = URITemplate(self.baseuri + '/{username}').expand(
  username=username)
  resp = self.session.get(uri)
  self.handle_http_error(resp)
  return resp","List of all uploads Returns a Response object, the json() method of which returns a list of uploads Parameters ---------- username : str Account username, defaults to the service's username. account : str, **deprecated** Alias for username. Will be removed in version 1.0. Returns ------- requests.Response",2,0,0,1,3,2,0,0,1,3
"def get_by_id(self, user_id="""", record_ids=(None,)):
  if user_id != """" and record_ids != (None,):
  return [
  vlr
  for vlr in self.vlrs
  if vlr.user_id == user_id and vlr.record_id in record_ids
  ]
  else:
  return [
  vlr
  for vlr in self.vlrs
  if vlr.user_id == user_id or vlr.record_id in record_ids
  ]","Function to get vlrs by user_id and/or record_ids. Always returns a list even if only one vlr matches the user_id and record_id >>> import pylas >>> from pylas.vlrs.known import ExtraBytesVlr, WktCoordinateSystemVlr >>> las = pylas.read(""pylastests/extrabytes.las"") >>> las.vlrs [<ExtraBytesVlr(extra bytes structs: 5)>] >>> las.vlrs.get(WktCoordinateSystemVlr.official_user_id()) [] >>> las.vlrs.get(WktCoordinateSystemVlr.official_user_id())[0] Traceback (most recent call last): IndexError: list index out of range >>> las.vlrs.get_by_id(ExtraBytesVlr.official_user_id()) [<ExtraBytesVlr(extra bytes structs: 5)>] >>> las.vlrs.get_by_id(ExtraBytesVlr.official_user_id())[0] <ExtraBytesVlr(extra bytes structs: 5)> Parameters ---------- user_id: str, optional the user id record_ids: iterable of int, optional THe record ids of the vlr(s) you wish to get Returns ------- :py:class:`list` a list of vlrs matching the user_id and records_ids",1,0,0,1,2,1,0,1,1,3
"def v1_subfolder_list(request, response, kvlclient, fid):
  fid = urllib.unquote(fid)
  try:
  return sorted(imap(attrgetter('name'),
  ifilter(lambda it: it.is_folder(),
  new_folders(kvlclient, request).list(fid))))
  except KeyError:
  response.status = 404
  return []","Retrieves a list of subfolders in a folder for the current user. The route for this endpoint is: ``GET /dossier/v1/folder/<fid>/subfolder``. (Temporarily, the ""current user"" can be set via the ``annotator_id`` query parameter.) The payload returned is a list of subfolder identifiers.",2,0,0,1,3,1,0,0,1,2
"def respond(self, message, channel=None, nick=None):
  if channel:
  if not channel.startswith('
  channel = '
  self.send('PRIVMSG %s :%s' % (channel, message))
  elif nick:
  self.send('PRIVMSG %s :%s' % (nick, message))",\ Multipurpose method for sending responses to channel or via message to a single user,1,0,0,1,2,1,0,0,1,2
"def editpermissions_anonymous_user_view(self, request, forum_id=None):
  forum = get_object_or_404(Forum, pk=forum_id) if forum_id else None
  context = self.get_forum_perms_base_context(request, forum)
  context['forum'] = forum
  context['title'] = '{} - {}'.format(_('Forum permissions'), _('Anonymous user'))
  context['form'] = self._get_permissions_form(
  request, UserForumPermission, {'forum': forum, 'anonymous_user': True},
  )
  return render(request, self.editpermissions_anonymous_user_view_template_name, context)",Allows to edit anonymous user permissions for the considered forum. The view displays a form to define which permissions are granted for the anonymous user for the considered forum.,1,0,1,1,3,1,0,1,1,3
"def send_video_note(chat_id, video_note,
  duration=None, length=None, reply_to_message_id=None, reply_markup=None, disable_notification=False,
  **kwargs):
  files = None
  if isinstance(video_note, InputFile):
  files = [video_note]
  video = None
  elif not isinstance(video_note, str):
  raise Exception('video must be instance of InputFile or str')
  params = dict(
  chat_id=chat_id,
  video_note=video_note
  )
  params.update(
  _clean_params(
  duration=duration,
  length=length,
  reply_to_message_id=reply_to_message_id,
  reply_markup=reply_markup,
  disable_notification=disable_notification,
  )
  )
  return TelegramBotRPCRequest('sendVideoNote', params=params, files=files, on_result=Message.from_result, **kwargs)","Use this method to send video files, Telegram clients support mp4 videos (other formats may be sent as Document). :param chat_id: Unique identifier for the target chat or username of the target channel (in the format @channelusername) :param video_note: Video to send. Pass a file_id as String to send a video that exists on the Telegram servers (recommended), pass an HTTP URL as a String for Telegram to get a video from the Internet, or upload a new video using multipart/form-data. :param duration: Duration of sent video in seconds :param length: Video width and height :param reply_to_message_id: If the message is a reply, ID of the original message :param reply_markup: Additional interface options. A JSON-serialized object for a custom reply keyboard, instructions to hide keyboard or to force a reply from the user. :param disable_notification: Sends the message silently. iOS users will not receive a notification, Android users will receive a notification with no sound. Other apps coming soon. :param kwargs: Args that get passed down to :class:`TelegramBotRPCRequest` :type chat_id: int or str :type video: InputFile or str :type duration: int :type caption: str :type reply_to_message_id: int :type reply_markup: ReplyKeyboardMarkup or ReplyKeyboardHide or ForceReply :returns: On success, the sent Message is returned. :rtype: TelegramBotRPCRequest",1,0,0,2,3,1,0,0,2,3
"def get_usertag(email, *tags):
  reply = _soap_client_call('get_usertag', email, *tags)
  map_el = reply('s-gensym3')
  mapping = {}
  type_attr = map_el.attributes().get('xsi:type')
  if type_attr and type_attr.value == 'apachens:Map':
  for usertag_el in map_el.children() or []:
  tag = _uc(str(usertag_el('key')))
  buglist_el = usertag_el('value')
  mapping[tag] = [int(bug) for bug in buglist_el.children() or []]
  else:
  for usertag_el in map_el.children() or []:
  tag = _uc(usertag_el.get_name())
  mapping[tag] = [int(bug) for bug in usertag_el.children() or []]
  return mapping","Get buglists by usertags. Parameters ---------- email : str tags : tuple of strings If tags are given the dictionary is limited to the matching tags, if no tags are given all available tags are returned. Returns ------- mapping : dict a mapping of usertag -> buglist",1,0,0,1,2,1,0,0,1,2
"def install(name, dst, capture_error=False):
  if dst not in sys.path:
  sys.path.insert(0, dst)
  entrypoint_type = _entry_point_type.get(dst, name)
  if entrypoint_type is _entry_point_type.PYTHON_PACKAGE:
  _modules.install(dst, capture_error)
  if entrypoint_type is _entry_point_type.COMMAND:
  os.chmod(os.path.join(dst, name), 511)","Install the user provided entry point to be executed as follow: - add the path to sys path - if the user entry point is a command, gives exec permissions to the script Args: name (str): name of the script or module. dst (str): path to directory with the script or module. capture_error (bool): Default false. If True, the running process captures the stderr, and appends it to the returned Exception message in case of errors.",1,0,0,0,1,1,0,0,0,1
"def action(self, entity, action, *, delay=4, auto_cancel=True):
  if isinstance(action, str):
  try:
  action = _ChatAction._str_mapping[action.lower()]
  except KeyError:
  raise ValueError('No such action ""{}""'.format(action)) from None
  elif not isinstance(action, types.TLObject) or action.SUBCLASS_OF_ID != 0x20b2cc21:
  if isinstance(action, type):
  raise ValueError('You must pass an instance, not the class')
  else:
  raise ValueError('Cannot use {} as action'.format(action))
  if isinstance(action, types.SendMessageCancelAction):
  return self(functions.messages.SetTypingRequest(
  entity, types.SendMessageCancelAction()))
  return _ChatAction(
  self, entity, action, delay=delay, auto_cancel=auto_cancel)","Returns a context-manager object to represent a ""chat action"". Chat actions indicate things like ""user is typing"", ""user is uploading a photo"", etc. Normal usage is as follows: .. code-block:: python async with client.action(chat, 'typing'): await asyncio.sleep(2) # type for 2 seconds await client.send_message(chat, 'Hello world! I type slow ^^') If the action is ``'cancel'``, you should just ``await`` the result, since it makes no sense to use a context-manager for it: .. code-block:: python await client.action(chat, 'cancel') Args: entity (`entity`): The entity where the action should be showed in. action (`str` | :tl:`SendMessageAction`): The action to show. You can either pass a instance of :tl:`SendMessageAction` or better, a string used while: * ``'typing'``: typing a text message. * ``'contact'``: choosing a contact. * ``'game'``: playing a game. * ``'location'``: choosing a geo location. * ``'record-audio'``: recording a voice note. You may use ``'record-voice'`` as alias. * ``'record-round'``: recording a round video. * ``'record-video'``: recording a normal video. * ``'audio'``: sending an audio file (voice note or song). You may use ``'voice'`` and ``'song'`` as aliases. * ``'round'``: uploading a round video. * ``'video'``: uploading a video file. * ``'photo'``: uploading a photo. * ``'document'``: uploading a document file. You may use ``'file'`` as alias. * ``'cancel'``: cancel any pending action in this chat. Invalid strings will raise a ``ValueError``. delay (`int` | `float`): The delay, in seconds, to wait between sending actions. For example, if the delay is 5 and it takes 7 seconds to do something, three requests will be made at 0s, 5s, and 7s to cancel the action. auto_cancel (`bool`): Whether the action should be cancelled once the context manager exists or not. The default is ``True``, since you don't want progress to be shown when it has already completed. If you are uploading a file, you may do ``progress_callback=chat.progress`` to update the progress of the action. Some clients don't care about this progress, though, so it's mostly not needed, but still available.",1,0,0,2,3,1,0,0,1,2
"def get_received(self, age=None, for_all=True):
  method, url = get_URL('received_get')
  if age:
  if not isinstance(age, int) or age < 0 or age > 90:
  raise FMBaseError('Age must be <int> between 0-90')
  past = datetime.utcnow() - timedelta(days=age)
  age = timegm(past.utctimetuple())
  payload = {
  'apikey': self.config.get('apikey'),
  'logintoken': self.session.cookies.get('logintoken'),
  'getForAllUsers': for_all,
  'from': age
  }
  res = getattr(self.session, method)(url, params=payload)
  if res.status_code == 200:
  return self._restore_transfers(res)
  hellraiser(res)",Retrieve a list of transfers sent to you or your company from other people. :param age: between 1 and 90 days. :param for_all: If ``True`` will return received files for all users in the same business. (Available for business account members only). :type age: ``int`` :type for_all: ``bool`` :rtype: ``list`` of :class:`Transfer` objects.,2,0,0,1,3,2,0,0,1,3
"def language_list(
  maintenance_db,
  user=None,
  host=None,
  port=None,
  password=None,
  runas=None):
  ret = {}
  query = 'SELECT lanname AS ""Name"" FROM pg_language'
  rows = psql_query(
  query,
  runas=runas,
  host=host,
  user=user,
  port=port,
  maintenance_db=maintenance_db,
  password=password)
  for row in rows:
  ret[row['Name']] = row['Name']
  return ret",.. versionadded:: 2016.3.0 Return a list of languages in a database. CLI Example: .. code-block:: bash salt '*' postgres.language_list dbname maintenance_db The database to check user database username if different from config or default password user password if any password for a specified user host Database host if different from config or default port Database port if different from config or default runas System user all operations should be performed on behalf of,1,0,1,1,3,1,0,1,1,3
"def sendMediaGroup(self, chat_id, media,
  disable_notification=None,
  reply_to_message_id=None):
  p = _strip(locals(), more=['media'])
  legal_media, files_to_attach = _split_input_media_array(media)
  p['media'] = legal_media
  return self._api_request('sendMediaGroup', _rectify(p), files_to_attach)","See: https://core.telegram.org/bots/api#sendmediagroup :type media: array of `InputMedia <https://core.telegram.org/bots/api#inputmedia>`_ objects :param media: To indicate media locations, each InputMedia object's ``media`` field should be one of these: - string: ``file_id`` for a file existing on Telegram servers - string: HTTP URL of a file from the Internet - file-like object: obtained by ``open(path, 'rb')`` - tuple: (form-data name, file-like object) - tuple: (form-data name, (filename, file-like object)) In case of uploading, you may supply customized multipart/form-data names for each uploaded file (as in last 2 options above). Otherwise, telepot assigns unique names to each uploaded file. Names assigned by telepot will not collide with user-supplied names, if any.",1,0,0,2,3,1,0,0,2,3
"def find_cross_contamination(databases, pair, tmpdir='tmp', log='log.txt', threads=1):
  genera_present = list()
  out, err, cmd = mash.screen('{}/refseq.msh'.format(databases), pair[0],
  pair[1], threads=threads, w='', i='0.95',
  output_file=os.path.join(tmpdir, 'screen.tab'), returncmd=True)
  write_to_logfile(log, out, err, cmd)
  screen_output = mash.read_mash_screen(os.path.join(tmpdir, 'screen.tab'))
  for item in screen_output:
  mash_genus = item.query_id.split('/')[-3]
  if mash_genus == 'Shigella':
  mash_genus = 'Escherichia'
  if mash_genus not in genera_present:
  genera_present.append(mash_genus)
  if len(genera_present) == 1:
  genera_present = genera_present[0]
  elif len(genera_present) == 0:
  genera_present = 'NA'
  else:
  tmpstr = ''
  for mash_genus in genera_present:
  tmpstr += mash_genus + ':'
  genera_present = tmpstr[:-1]
  return genera_present","Usese mash to find out whether or not a sample has more than one genus present, indicating cross-contamination. :param databases: A databases folder, which must contain refseq.msh, a mash sketch that has one representative per genus from refseq. :param tmpdir: Temporary directory to store mash result files in. :param pair: Array with path to forward reads at index 0 and path to reverse reads at index o :param log: Logfile to write to. :param threads: Number of threads to run mash wit. :return: cross_contam: a bool that is True if more than one genus is found, and False otherwise. :return: genera_present: A string. If only one genus is found, string is just genus. If more than one genus is found, the string is a list of genera present, separated by colons (i.e. for Escherichia and Salmonella found, string would be 'Escherichia:Salmonella'. If no genus found, return 'NA'",0,0,0,1,1,1,0,0,1,2
"def search(self, query=None, args=None):
  if query is None:
  if args.endpoint is None:
  bot.info('Listing shared endpoints. Add query to expand search.')
  return self._list_endpoints()
  else:
  return self._list_endpoint(args.endpoint)
  if args.endpoint is None:
  bot.info('You must specify an endpoint id to query!')
  return self._list_endpoints(query)
  return self._list_endpoint(endpoint=args.endpoint,
  query=query)","query will show images determined by the extension of img or simg. Parameters ========== query: the container name (path) or uri to search for args.endpoint: can be an endpoint id and optional path, e.g.: --endpoint 6881ae2e-db26-11e5-9772-22000b9da45e:.singularity' --endpoint 6881ae2e-db26-11e5-9772-22000b9da45e' if not defined, we show the user endpoints to choose from Usage ===== If endpoint is defined with a query, then we search the given endpoint for a container of interested (designated by ending in .img or .simg If no endpoint is provided but instead just a query, we use the query to search endpoints.",1,0,0,1,2,2,0,0,1,3
"def getACLs(self, base, searchstr):
 acls = dict()
 res = self.query(base, searchstr, ['cn', 'ipHostNumber'])
 for dn,attr in res:
 cn = attr['cn'][0]
 dests = dict()
 if attr.has_key('ipHostNumber'):
 for entry in attr['ipHostNumber']:
 dest = entry.split('
 if len(entry.split('
 desc = entry.split('
 else:
 desc = """"
 if not is_ip(dest):
 print dest, desc
 raise InputError(dest, ""Invalid IP format"")
 dests[dest] = desc
 acls[cn] = dests
 return acls","Query LDAP to obtain the network ACLs of a given user, parse the ACLs, and return the results in a dict of the form acls[group][cidr] = description",1,0,0,1,2,1,0,0,1,2
"def get_list(**kwargs):
  opts = salt.utils.namecheap.get_opts('namecheap.ssl.getList')
  for key, value in six.iteritems(kwargs):
  opts[key] = value
  response_xml = salt.utils.namecheap.get_request(opts)
  if response_xml is None:
  return []
  ssllistresult = response_xml.getElementsByTagName('SSLListResult')[0]
  result = []
  for e in ssllistresult.getElementsByTagName('SSL'):
  ssl = salt.utils.namecheap.atts_to_dict(e)
  result.append(ssl)
  return result","Returns a list of SSL certificates for a particular user ListType : All Possible values: - All - Processing - EmailSent - TechnicalProblem - InProgress - Completed - Deactivated - Active - Cancelled - NewPurchase - NewRenewal SearchTerm Keyword to look for on the SSL list Page : 1 Page number to return PageSize : 20 Total number of SSL certificates to display per page (minimum: ``10``, maximum: ``100``) SoryBy One of ``PURCHASEDATE``, ``PURCHASEDATE_DESC``, ``SSLTYPE``, ``SSLTYPE_DESC``, ``EXPIREDATETIME``, ``EXPIREDATETIME_DESC``, ``Host_Name``, or ``Host_Name_DESC`` CLI Example: .. code-block:: bash salt 'my-minion' namecheap_ssl.get_list Processing",1,0,0,1,2,1,0,0,1,2
"def identify(self, request):
  token = self.get_jwt(request)
  if token is None:
  return NO_IDENTITY
  try:
  claims_set = self.decode_jwt(token)
  except (DecodeError, ExpiredSignatureError):
  return NO_IDENTITY
  userid = self.get_userid(claims_set)
  if userid is None:
  return NO_IDENTITY
  extra_claims = self.get_extra_claims(claims_set)
  if extra_claims is not None:
  return Identity(userid=userid, **extra_claims)
  else:
  return Identity(userid=userid)",Establish what identity this user claims to have from request. :param request: Request to extract identity information from. :type request: :class:`morepath.Request`. :returns: :class:`morepath.Identity` instance or :attr:`morepath.NO_IDENTITY` if identity cannot be established.,1,0,0,1,2,1,0,0,1,2
"def parse_entry(self, name):
  alias = name
  if ':' in name:
  name, alias = name.split(':')
  if name.endswith('[]'):
  need_list = True
  name = name[:-2]
  else:
  need_list = False
  return alias, name, need_list","Parse query entry name, just like: { 'User[]:user' } 'User[]:user' is an entry name. :param name: :return:",0,0,0,1,1,1,0,0,1,2
"def GetVSSStoreIdentifiers(self, volume_system, volume_identifiers):
  print_header = True
  while True:
  if print_header:
  self._PrintVSSStoreIdentifiersOverview(
  volume_system, volume_identifiers)
  print_header = False
  self._output_writer.Write('\n')
  lines = self._textwrapper.wrap(self._USER_PROMPT_VSS)
  self._output_writer.Write('\n'.join(lines))
  self._output_writer.Write('\n\nVSS identifier(s): ')
  try:
  selected_volumes = self._ReadSelectedVolumes(
  volume_system, prefix='vss')
  if (not selected_volumes or
  not set(selected_volumes).difference(volume_identifiers)):
  break
  except ValueError:
  pass
  self._output_writer.Write('\n')
  lines = self._textwrapper.wrap(
  'Unsupported VSS identifier(s), please try again or abort with '
  'Ctrl^C.')
  self._output_writer.Write('\n'.join(lines))
  self._output_writer.Write('\n\n')
  return selected_volumes",Retrieves VSS store identifiers. This method can be used to prompt the user to provide VSS store identifiers. Args: volume_system (VShadowVolumeSystem): volume system. volume_identifiers (list[str]): volume identifiers including prefix. Returns: list[str]: selected volume identifiers including prefix or None.,1,0,0,1,2,1,0,0,1,2
"def get_booking(request):
  booking = None
  if request.user.is_authenticated():
  try:
  booking = Booking.objects.get(
  user=request.user,
  booking_status__slug='inprogress')
  except Booking.DoesNotExist:
  pass
  else:
  session = Session.objects.get(
  session_key=request.session.session_key)
  try:
  booking = Booking.objects.get(session=session)
  except Booking.DoesNotExist:
  pass
  return booking",Returns the booking that is in progress for the current user or None We assume that a user can only have one booking that is in-progress. TODO: This implementation assumes that there is a status called 'inprogress' and that there should only be one such booking for a given user. We need to see if this can be more generic for future projects. :param request: The Request object.,0,0,1,1,2,1,0,1,1,3
"def change_email(self, email):
  self.email_unconfirmed = email
  salt, hash = generate_sha1(self.username)
  self.email_confirmation_key = hash
  self.email_confirmation_key_created = get_datetime_now()
  self.save()
  self.send_confirmation_email()
  return self",Changes the email address for a user. A user needs to verify this new email address before it becomes active. By storing the new email address in a temporary field -- ``temporary_email`` -- we are able to set this email address after the user has verified it by clicking on the verification URI in the email. This email gets send out by ``send_verification_email``. :param email: The new email address that the user wants to use.,1,1,0,1,3,0,1,0,0,1
"def patch_namespaced_lease(self, name, namespace, body, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.patch_namespaced_lease_with_http_info(name, namespace, body, **kwargs)
  else:
  (data) = self.patch_namespaced_lease_with_http_info(name, namespace, body, **kwargs)
  return data","partially update the specified Lease This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.patch_namespaced_lease(name, namespace, body, async_req=True) >>> result = thread.get() :param async_req bool :param str name: name of the Lease (required) :param str namespace: object name and auth scope, such as for teams and projects (required) :param object body: (required) :param str pretty: If 'true', then the output is pretty printed. :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint. This field is required for apply requests (application/apply-patch) but optional for non-apply patch types (JsonPatch, MergePatch, StrategicMergePatch). :param bool force: Force is going to \""force\"" Apply requests. It means user will re-acquire conflicting fields owned by other people. Force flag must be unset for non-apply patch requests. :return: V1Lease If the method is called asynchronously, returns the request thread.",1,0,0,1,2,1,0,0,1,2
"def map_column(self, only_use_one=None, source_column=None, species=None, target_selection= None, verbose=False):
  PARAMS=set_param([""only_use_one"",""source_column"",""species"",""target_selection""],[only_use_one,source_column,species,target_selection])
  response=api(url=self.__url+""/map column"", PARAMS=PARAMS, method=""POST"", verbose=verbose)
  return response","Uses the BridgeDB service to look up analogous identifiers from a wide selection of other databases :param only_use_one (string, optional): When multiple identifiers can be mapped from a single term, this forces a singular result :param source_column (string): Specifies the column nmae where the source identifiers are located = [''] :param source_selection (string): Specifies the database describing the existing identifiers = [''] :param species (string, optional): The combined common or latin name of the species to which the identifiers apply = ['Human (Homo sapiens)', 'Mouse (Mus musculus)', 'Rat (Rattus norvegicus)', 'Frog (Xenopus tropicalis)', 'Zebra fish (Danio rerio)', 'Fruit fly (Drosophila melanogaster)', 'Mosquito (Anopheles gambiae)', 'Arabidopsis thaliana (Arabidopsis thaliana)', 'Yeast (Saccharomyces cerevisiae)', 'E. coli (Escherichia coli)', 'Tuberculosis (Mycobacterium tuberculosis)', 'Worm (Caenorhabditis elegans)'] :param target_selection (string): Specifies the database identifiers to be looked up = [''] :param verbose: print more :returns: eg. { ""new column"": ""SGD "" }",1,0,0,1,2,1,0,0,1,2
"def update(self, chat_id, op_user, name=None, owner=None,
  add_user_list=None, del_user_list=None):
  data = optionaldict(
  chatid=chat_id,
  op_user=op_user,
  name=name,
  owner=owner,
  add_user_list=add_user_list,
  del_user_list=del_user_list,
  )
  return self._post('chat/update', data=data)",  https://qydev.weixin.qq.com/wiki/index.php?title= :param chat_id:  ID :param op_user:  userid :param name:   :param owner: useriduserlist :param add_user_list: userid  :param del_user_list: userid  :return:  JSON ,1,0,0,0,1,1,0,0,2,3
"def validateSQL(self, sql, sqlType=""where""):
  url = self._url + ""/validateSQL""
  if not sqlType.lower() in ['where', 'expression', 'statement']:
  raise Exception(""Invalid Input for sqlType: %s"" % sqlType)
  params = {
  ""f"" : ""json"",
  ""sql"" : sql,
  ""sqlType"" : sqlType
  }
  return self._post(url=url,
  param_dict=params,
  securityHandler=self._securityHandler,
  proxy_url=self._proxy_url,
  proxy_port=self._proxy_port)","The validateSQL operation validates an SQL-92 expression or WHERE clause. The validateSQL operation ensures that an SQL-92 expression, such as one written by a user through a user interface, is correct before performing another operation that uses the expression. For example, validateSQL can be used to validate information that is subsequently passed in as part of the where parameter of the calculate operation. validateSQL also prevents SQL injection. In addition, all table and field names used in the SQL expression or WHERE clause are validated to ensure they are valid tables and fields. Inputs: sql - The SQL expression or WHERE clause to validate. sqlType - Three SQL types are supported in validateSQL: where (default) - Represents the custom WHERE clause the user can compose when querying a layer or using calculate. expression - Represents an SQL-92 expression. Currently, expression is used as a default value expression when adding a new field or using the calculate API. statement - Represents the full SQL-92 statement that can be passed directly to the database. No current ArcGIS REST API resource or operation supports using the full SQL-92 SELECT statement directly. It has been added to the validateSQL for completeness. Values: where | expression | statement",1,0,0,2,3,1,0,0,2,3
"def _check_users(users):
  messg = ''
  valid = True
  for user, user_details in six.iteritems(users):
  if not user_details:
  valid = False
  messg += 'Please provide details for username {user}.\n'.format(user=user)
  continue
  if not (isinstance(user_details.get('level'), int) or 0 <= user_details.get('level') <= 15):
  messg += 'Level must be a integer between 0 and 15 for username {user}. Will assume 0.\n'.format(user=user)
  return valid, messg",Checks if the input dictionary of users is valid.,1,0,0,0,1,1,0,0,1,2
"def read_volume_attachment(self, name, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.read_volume_attachment_with_http_info(name, **kwargs)
  else:
  (data) = self.read_volume_attachment_with_http_info(name, **kwargs)
  return data","read_volume_attachment # noqa: E501 read the specified VolumeAttachment # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.read_volume_attachment(name, async_req=True) >>> result = thread.get() :param async_req bool :param str name: name of the VolumeAttachment (required) :param str pretty: If 'true', then the output is pretty printed. :param bool exact: Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. :param bool export: Should this value be exported. Export strips fields that a user can not specify. :return: V1VolumeAttachment If the method is called asynchronously, returns the request thread.",2,0,0,1,3,2,0,0,1,3
"def GetFilename(multiple=False, sep='|', **kwargs):
  args = []
  if multiple:
  args.append('--multiple')
  if sep != '|':
  args.append('--separator=%s' % sep)
  for generic_args in kwargs_helper(kwargs):
  args.append('--%s=%s' % generic_args)
  p = run_zenity('--file-selection', *args)
  if p.wait() == 0:
  return p.stdout.read()[:-1].split('|')","Prompt the user for a filename. This will raise a Zenity File Selection Dialog. It will return a list with the selected files or None if the user hit cancel. multiple - True to allow the user to select multiple files. sep - Token to use as the path separator when parsing Zenity's return string. kwargs - Optional command line parameters for Zenity such as height, width, etc.",1,0,0,1,2,1,0,0,1,2
"def create_user(self, **kwargs):
  roles = kwargs.pop('roles', [])
  user = self.user_model(**self._prepare_create_user_args(**kwargs))
  user = self.put(user)
  for role in roles:
  self.add_role_to_user(user, role)
  self.put(user)
  return user",Creates and returns a new user from the given parameters.,1,1,1,1,4,1,1,1,1,4
"def send_chat_message(self, peer_jid: str, message: str, bot_mention_jid=None):
  if self.is_group_jid(peer_jid):
  log.info(""[+] Sending chat message '{}' to group '{}'..."".format(message, peer_jid))
  return self._send_xmpp_element(chatting.OutgoingGroupChatMessage(peer_jid, message, bot_mention_jid))
  else:
  log.info(""[+] Sending chat message '{}' to user '{}'..."".format(message, peer_jid))
  return self._send_xmpp_element(chatting.OutgoingChatMessage(peer_jid, message, False, bot_mention_jid))","Sends a text chat message to another person or a group with the given JID/username. :param peer_jid: The Jabber ID for which to send the message (looks like username_ejs@talk.kik.com) If you don't know the JID of someone, you can also specify a kik username here. :param message: The actual message body :param bot_mention_jid: If an official bot is referenced, their jid must be embedded as mention for them to respond.",1,0,0,1,2,1,0,0,1,2
"def generate_username_and_redirect(self, request, user, profile, client):
  func = self.get_username_function()
  user.username = func(user, profile, client)
  user.set_unusable_password()
  user.save()
  profile.user = user
  profile.save()
  user = profile.authenticate()
  self.send_connect_signal(request, user, profile, client)
  self.login(request, user)
  self.send_login_signal(request, user, profile, client)
  self.delete_session_data(request)
  return HttpResponseRedirect(self.get_next(request))",Generate a username and then redirect the user to the correct place. This method is called when ``SOCIALREGISTRATION_GENERATE_USERNAME`` is set. :param request: The current request object :param user: The unsaved user object :param profile: The unsaved profile object :param client: The API client,1,1,0,1,3,1,1,1,1,4
"def user_packages(
  self,
  login=None,
  platform=None,
  package_type=None,
  type_=None,
  access=None):
  if login:
  url = '{0}/packages/{1}'.format(self.domain, login)
  else:
  url = '{0}/packages'.format(self.domain)
  arguments = collections.OrderedDict()
  if platform:
  arguments['platform'] = platform
  if package_type:
  arguments['package_type'] = package_type
  if type_:
  arguments['type'] = type_
  if access:
  arguments['access'] = access
  res = self.session.get(url, params=arguments)
  self._check_response(res)
  return res.json()","Returns a list of packages for a given user and optionally filter by `platform`, `package_type` and `type_`. :param login: (optional) the login name of the user or None. If login is None this method will return the packages for the authenticated user. :param platform: only find packages that include files for this platform. (e.g. 'linux-64', 'osx-64', 'win-32') :param package_type: only find packages that have this kind of file (e.g. 'env', 'conda', 'pypi') :param type_: only find packages that have this conda `type` (i.e. 'app') :param access: only find packages that have this access level (e.g. 'private', 'authenticated', 'public')",2,0,0,1,3,2,0,0,1,3
"def user_roles_exists(name, roles, database, user=None, password=None, host=None,
  port=None, authdb=None):
  try:
  roles = _to_dict(roles)
  except Exception:
  return 'Roles provided in wrong format'
  users = user_list(user, password, host, port, database, authdb)
  if isinstance(users, six.string_types):
  return 'Failed to connect to mongo database'
  for user in users:
  if name == dict(user).get('user'):
  for role in roles:
  if not isinstance(role, dict):
  role = {'role': role, 'db': database}
  if role not in dict(user).get('roles', []):
  return False
  return True
  return False","Checks if a user of a MongoDB database has specified roles CLI Examples: .. code-block:: bash salt '*' mongodb.user_roles_exists johndoe '[""readWrite""]' dbname admin adminpwd localhost 27017 .. code-block:: bash salt '*' mongodb.user_roles_exists johndoe '[{""role"": ""readWrite"", ""db"": ""dbname"" }, {""role"": ""read"", ""db"": ""otherdb""}]' dbname admin adminpwd localhost 27017",1,0,0,1,2,1,0,1,1,3
"def get_unread_messages_in_chat(self,
  id,
  include_me=False,
  include_notifications=False):
  messages = self.wapi_functions.getUnreadMessagesInChat(
  id,
  include_me,
  include_notifications
  )
  unread = [factory_message(message, self) for message in messages]
  return unread",I fetch unread messages from an asked chat. :param id: chat id :type id: str :param include_me: if user's messages are to be included :type include_me: bool :param include_notifications: if events happening on chat are to be included :type include_notifications: bool :return: list of unread messages from asked chat :rtype: list,1,0,0,1,2,2,0,0,1,3
"async def create(self, **kwargs):
  try:
  obj = self._meta.object_class()
  self.data.update(kwargs)
  await obj.deserialize(self.data)
  await obj.insert(db=self.db)
  return await obj.serialize()
  except Exception as ex:
  logger.exception(ex)
  raise BadRequest(ex)","Corresponds to POST request without a resource identifier, inserting a document into the database",0,1,1,0,2,1,1,1,1,4
"def alterar(self, id_tipo_acesso, protocolo):
  if not is_valid_int_param(id_tipo_acesso):
  raise InvalidParameterError(
  u'Access type id is invalid or was not informed.')
  tipo_acesso_map = dict()
  tipo_acesso_map['protocolo'] = protocolo
  url = 'tipoacesso/' + str(id_tipo_acesso) + '/'
  code, xml = self.submit({'tipo_acesso': tipo_acesso_map}, 'PUT', url)
  return self.response(code, xml)",Edit access type by its identifier. :param id_tipo_acesso: Access type identifier. :param protocolo: Protocol. :return: None :raise ProtocoloTipoAcessoDuplicadoError: Protocol already exists. :raise InvalidParameterError: Protocol value is invalid or none. :raise TipoAcessoNaoExisteError: Access type doesn't exist. :raise DataBaseError: Networkapi failed to access the database. :raise XMLError: Networkapi failed to generate the XML response.,1,0,0,2,3,1,0,0,2,3
"def hl_table2canvas(self, w, res_dict):
  objlist = []
  if self.maskhltag:
  try:
  self.canvas.delete_object_by_tag(self.maskhltag, redraw=False)
  except Exception:
  pass
  for sub_dict in res_dict.values():
  for seqno in sub_dict:
  mobj = self._maskobjs[int(seqno) - 1]
  dat = self._rgbtomask(mobj)
  obj = self.dc.Image(0, 0, masktorgb(
  dat, color=self.hlcolor, alpha=self.hlalpha))
  objlist.append(obj)
  if len(objlist) > 0:
  self.maskhltag = self.canvas.add(self.dc.CompoundObject(*objlist))
  self.fitsimage.redraw()",Highlight mask on canvas when user click on table.,0,0,0,1,1,1,0,0,1,2
"def get_type_item(self, value):
  if isinstance(value, ExecCommand):
  return value
  elif isinstance(value, six.string_types + (lazy_type,)):
  return ExecCommand(value)
  elif isinstance(value, (list, tuple)):
  v_len = len(value)
  if 1 <= v_len <= 3:
  return ExecCommand(*value)
  raise ValueError(""Invalid element length; only tuples and lists of length 1-3 can be converted to a ""
  ""ExecCommand tuple. Found length {0}."".format(v_len))
  elif isinstance(value, dict):
  return ExecCommand(**value)
  raise ValueError(
  ""Invalid type; expected a list, tuple, dict, or string type, found {0}."".format(type(value).__name__))","Converts the input to a ExecCommand tuple. It can be from a single string, list, or tuple. Single values (also single-element lists or tuples) are considered a command string, whereas two-element items are read as ``(command string, user name)``. :param value: Input value for conversion. :return: ExecCommand tuple. :rtype: ExecCommand",0,0,0,1,1,1,0,0,1,2
"async def check_user(self, request, func=None, location=None, **kwargs):
  user = await self.load_user(request)
  func = func or self.cfg.default_user_checker
  if not func(user):
  location = location or self.cfg.login_url
  while callable(location):
  location = location(request)
  while asyncio.iscoroutine(location):
  location = await location
  raise HTTPFound(location, **kwargs)
  return user","Check for user is logged and pass the given func. :param func: user checker function, defaults to default_user_checker :param location: where to redirect if user is not logged in. May be either string (URL) or function which accepts request as argument and returns string URL.",1,0,0,1,2,1,0,0,1,2
"def delimiter_complete(self, text: str, line: str, begidx: int, endidx: int, match_against: Iterable,
  delimiter: str) -> List[str]:
  matches = self.basic_complete(text, line, begidx, endidx, match_against)
  if matches:
  self.matches_delimited = True
  common_prefix = os.path.commonprefix(matches)
  prefix_tokens = common_prefix.split(delimiter)
  display_token_index = 0
  if prefix_tokens:
  display_token_index = len(prefix_tokens) - 1
  for cur_match in matches:
  match_tokens = cur_match.split(delimiter)
  display_token = match_tokens[display_token_index]
  if not display_token:
  display_token = delimiter
  self.display_matches.append(display_token)
  return matches","Performs tab completion against a list but each match is split on a delimiter and only the portion of the match being tab completed is shown as the completion suggestions. This is useful if you match against strings that are hierarchical in nature and have a common delimiter. An easy way to illustrate this concept is path completion since paths are just directories/files delimited by a slash. If you are tab completing items in /home/user you don't get the following as suggestions: /home/user/file.txt /home/user/program.c /home/user/maps/ /home/user/cmd2.py Instead you are shown: file.txt program.c maps/ cmd2.py For a large set of data, this can be visually more pleasing and easier to search. Another example would be strings formatted with the following syntax: company::department::name In this case the delimiter would be :: and the user could easily narrow down what they are looking for if they were only shown suggestions in the category they are at in the string. :param text: the string prefix we are attempting to match (all returned matches must begin with it) :param line: the current input line with leading whitespace removed :param begidx: the beginning index of the prefix text :param endidx: the ending index of the prefix text :param match_against: the list being matched against :param delimiter: what delimits each portion of the matches (ex: paths are delimited by a slash) :return: a list of possible tab completions",0,0,0,1,1,1,0,0,1,2
"def verify_password(self, user, password):
  if self.use_double_hash(user.password):
  verified = self.security.pwd_context.verify(
  self.get_hmac(password), user.password)
  else:
  verified = self.security.pwd_context.verify(password, user.password)
  if verified and self.security.pwd_context.needs_update(user.password):
  user.password = password
  self.user_manager.save(user)
  return verified","Returns ``True`` if the password is valid for the specified user. Additionally, the hashed password in the database is updated if the hashing algorithm happens to have changed. :param user: The user to verify against :param password: The plaintext password to verify",1,0,0,0,1,1,1,1,0,3
"def send_login_instructions(user):
  token = generate_login_token(user)
  login_link = url_for_security('token_login', token=token, _external=True)
  _security.send_mail(config_value('EMAIL_SUBJECT_PASSWORDLESS'), user.email,
  'login_instructions', user=user, login_link=login_link)
  login_instructions_sent.send(app._get_current_object(), user=user,
  login_token=token)",Sends the login instructions email for the specified user. :param user: The user to send the instructions to :param token: The login token,1,0,0,0,1,1,0,0,0,1
"def resolve_input_references(to_resolve, inputs_to_reference):
  splitted = split_input_references(to_resolve)
  result = []
  for part in splitted:
  if is_input_reference(part):
  result.append(str(resolve_input_reference(part, inputs_to_reference)))
  else:
  result.append(part)
  return ''.join(result)","Resolves input references given in the string to_resolve by using the inputs_to_reference. See http://www.commonwl.org/user_guide/06-params/index.html for more information. Example: ""$(inputs.my_file.nameroot).md"" -> ""filename.md"" :param to_resolve: The path to match :param inputs_to_reference: Inputs which are used to resolve input references like $(inputs.my_input_file.basename). :return: A string in which the input references are replaced with actual values.",1,0,0,1,2,1,0,0,1,2
"def pull(self, images, file_name=None, save=True, force=False, **kwargs):
  if not isinstance(images,list):
  images = [images]
  bot.debug('Execution of PULL for %s images' %len(images))
  finished = []
  for image in images:
  q = parse_image_name( remove_uri(image),
  default_collection='aws' )
  image_file = self._pull(file_name=file_name,
  save=save,
  force=force,
  names=q,
  kwargs=kwargs)
  finished.append(image_file)
  if len(finished) == 1:
  finished = finished[0]
  return finished","pull an image from a docker hub. This is a (less than ideal) workaround that actually does the following: - creates a sandbox folder - adds docker layers, metadata folder, and custom metadata to it - converts to a squashfs image with build the docker manifests are stored with registry metadata. Parameters ========== images: refers to the uri given by the user to pull in the format <collection>/<namespace>. You should have an API that is able to retrieve a container based on parsing this uri. file_name: the user's requested name for the file. It can optionally be None if the user wants a default. save: if True, you should save the container to the database using self.add() Returns ======= finished: a single container path, or list of paths",2,1,0,1,4,1,0,0,1,2
"def user_cache_dir(appname):
  r
  if WINDOWS:
  path = os.path.normpath(_get_win_folder(""CSIDL_LOCAL_APPDATA""))
  if PY2 and isinstance(path, text_type):
  path = _win_path_to_bytes(path)
  path = os.path.join(path, appname, ""Cache"")
  elif sys.platform == ""darwin"":
  path = expanduser(""~/Library/Caches"")
  path = os.path.join(path, appname)
  else:
  path = os.getenv(""XDG_CACHE_HOME"", expanduser(""~/.cache""))
  path = os.path.join(path, appname)
  return path","r"""""" Return full path to the user-specific cache dir for this application. ""appname"" is the name of application. Typical user cache directories are: macOS: ~/Library/Caches/<AppName> Unix: ~/.cache/<AppName> (XDG default) Windows: C:\Users\<username>\AppData\Local\<AppName>\Cache On Windows the only suggestion in the MSDN docs is that local settings go in the `CSIDL_LOCAL_APPDATA` directory. This is identical to the non-roaming app data dir (the default returned by `user_data_dir`). Apps typically put cache data somewhere *under* the given dir here. Some examples: ...\Mozilla\Firefox\Profiles\<ProfileName>\Cache ...\Acme\SuperApp\Cache\1.0 OPINION: This function appends ""Cache"" to the `CSIDL_LOCAL_APPDATA` value.",0,0,0,0,0,1,0,0,1,2
"def as_random_variable(distribution,
  sample_shape=(),
  value=None):
  return _build_custom_rv(distribution=distribution,
  sample_shape=sample_shape,
  value=value,
  name=_simple_name(distribution))","Wrap an existing distribution as a traceable random variable. This enables the use of custom or user-provided distributions in Edward models. Unlike a bare `RandomVariable` object, this method wraps the constructor so it is included in the Edward trace and its values can be properly intercepted and overridden. Where possible, you should prefer the built-in constructors (`ed.Normal`, etc); these simultaneously construct a Distribution and a RandomVariable object so that the distribution parameters themselves may be intercepted and overridden. RVs constructed via `as_random_variable()` have a fixed distribution and may not support program transformations (e.g, conjugate marginalization) that rely on overriding distribution parameters. Args: distribution: tfd.Distribution governing the distribution of the random variable, such as sampling and log-probabilities. sample_shape: tf.TensorShape of samples to draw from the random variable. Default is `()` corresponding to a single sample. value: Fixed tf.Tensor to associate with random variable. Must have shape `sample_shape + distribution.batch_shape + distribution.event_shape`. Default is to sample from random variable according to `sample_shape`. Returns: rv: a `RandomVariable` wrapping the provided distribution. #### Example ```python from tensorflow_probability import distributions as tfd from tensorflow_probability import edward2 as ed def model(): # equivalent to ed.Normal(0., 1., name='x') return ed.as_random_variable(tfd.Normal(0., 1., name='x')) log_joint = ed.make_log_joint_fn(model) output = log_joint(x=2.) ```",1,0,0,1,2,0,0,0,1,1
"def SelectFieldPrompt(field_name, context_str, *options):
  option_format_str = '[ {} ] ""{}""'
  option_dict = {}
  print(context_str)
  print('Please select one of the following options for field ""{}""'.format(
  field_name)
  )
  for cnt, option in enumerate(options):
  option_dict['{}'.format(cnt + 1)] = option
  if not callable(option):
  print(option_format_str.format(cnt + 1, u(str(option))))
  else:
  print(option_format_str.format(cnt + 1, option.__name__))
  choice = None
  while choice not in option_dict:
  choice = input('option> ').strip()
  new_value = option_dict[choice]
  if callable(new_value):
  return new_value()
  else:
  return new_value","Prompts user to pick from provided options. It is possible to provide a function as an option although it is not yet tested. This could allow a user to be prompted to provide their own value rather than the listed options. Args: field_name (string): Name of the field. context_str (string): Printed to give the user context. options: Variable arguments, should be vobject Components in a list. As retrieved from a vCard.contents dictionary. Returns: One of the options passed in. Ideally always a list.",1,0,0,1,2,1,0,0,1,2
"def read_namespace(self, name, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.read_namespace_with_http_info(name, **kwargs)
  else:
  (data) = self.read_namespace_with_http_info(name, **kwargs)
  return data","read_namespace # noqa: E501 read the specified Namespace # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.read_namespace(name, async_req=True) >>> result = thread.get() :param async_req bool :param str name: name of the Namespace (required) :param str pretty: If 'true', then the output is pretty printed. :param bool exact: Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. :param bool export: Should this value be exported. Export strips fields that a user can not specify. :return: V1Namespace If the method is called asynchronously, returns the request thread.",2,0,0,1,3,2,0,0,1,3
"def remove_users_from_account_group(self, account_id, group_id, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('asynchronous'):
  return self.remove_users_from_account_group_with_http_info(account_id, group_id, **kwargs)
  else:
  (data) = self.remove_users_from_account_group_with_http_info(account_id, group_id, **kwargs)
  return data","Remove users from a group. # noqa: E501 An endpoint for removing users from groups. **Example usage:** `curl -X DELETE https://api.us-east-1.mbedcloud.com/v3/accounts/{accountID}/policy-groups/{groupID}/users -d '[0162056a9a1586f30242590700000000,0117056a9a1586f30242590700000000]' -H 'content-type: application/json' -H 'Authorization: Bearer API_KEY'` # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass asynchronous=True >>> thread = api.remove_users_from_account_group(account_id, group_id, asynchronous=True) >>> result = thread.get() :param asynchronous bool :param str account_id: Account ID. (required) :param str group_id: (required) :param SubjectList body: :return: UpdatedResponse If the method is called asynchronously, returns the request thread.",1,0,0,2,3,1,0,0,1,2
"def process_request(self, request):
  threadlocal.actionslog = {
  'signal_duid': (self.__class__, time.time()),
  'remote_ip': request.META.get('REMOTE_ADDR'),
  }
  if request.META.get('HTTP_X_FORWARDED_FOR'):
  threadlocal.actionslog['remote_ip'] = request.META.get('HTTP_X_FORWARDED_FOR').split(',')[0]
  if hasattr(request, 'user') and hasattr(request.user, 'is_authenticated') and request.user.is_authenticated():
  set_user = curry(self.set_user, request.user)
  pre_save.connect(set_user, sender=LogAction, dispatch_uid=threadlocal.actionslog['signal_duid'], weak=False)",Gets the current user from the request and prepares and connects a signal receiver with the user already attached to it.,1,0,0,0,1,1,0,0,0,1
"def register_with_google(full_name, email, oauth2_token,
  lang=None, timezone=None):
  response = API.login_with_google(email, oauth2_token, auto_signup=1,
  full_name=full_name, lang=lang,
  timezone=timezone)
  _fail_if_contains_errors(response)
  user_json = response.json()
  user = User(user_json)
  return user","Register a new Todoist account by linking a Google account. :param full_name: The user's full name. :type full_name: str :param email: The user's email address. :type email: str :param oauth2_token: The oauth2 token associated with the email. :type oauth2_token: str :param lang: The user's language. :type lang: str :param timezone: The user's timezone. :type timezone: str :return: The Todoist user. :rtype: :class:`pytodoist.todoist.User` .. note:: It is up to you to obtain the valid oauth2 token. >>> from pytodoist import todoist >>> oauth2_token = 'oauth2_token' >>> user = todoist.register_with_google('John Doe', 'john.doe@gmail.com', ... oauth2_token) >>> print(user.full_name) John Doe",2,0,0,2,4,2,0,0,1,3
"def get_gravatar(email, size=80, default='identicon'):
  if userena_settings.USERENA_MUGSHOT_GRAVATAR_SECURE:
  base_url = 'https://secure.gravatar.com/avatar/'
  else: base_url = '//www.gravatar.com/avatar/'
  gravatar_url = '%(base_url)s%(gravatar_id)s?' % \
  {'base_url': base_url,
  'gravatar_id': md5(email.lower().encode('utf-8')).hexdigest()}
  gravatar_url += urlencode({
  's': str(size),
  'd': default
  })
  return gravatar_url","Get's a Gravatar for a email address. :param size: The size in pixels of one side of the Gravatar's square image. Optional, if not supplied will default to ``80``. :param default: Defines what should be displayed if no image is found for this user. Optional argument which defaults to ``identicon``. The argument can be a URI to an image or one of the following options: ``404`` Do not load any image if none is associated with the email hash, instead return an HTTP 404 (File Not Found) response. ``mm`` Mystery-man, a simple, cartoon-style silhouetted outline of a person (does not vary by email hash). ``identicon`` A geometric pattern based on an email hash. ``monsterid`` A generated 'monster' with different colors, faces, etc. ``wavatar`` Generated faces with differing features and backgrounds :return: The URI pointing to the Gravatar.",2,0,0,1,3,1,0,0,1,2
"def instantiate_for_read_and_search(handle_server_url, reverselookup_username, reverselookup_password, **config):
  if handle_server_url is None and 'reverselookup_baseuri' not in config.keys():
  raise TypeError('You must specify either ""handle_server_url"" or ""reverselookup_baseuri"".' + \
  ' Searching not possible without the URL of a search servlet.')
  inst = EUDATHandleClient(
  handle_server_url,
  reverselookup_username=reverselookup_username,
  reverselookup_password=reverselookup_password,
  **config
  )
  return inst","Initialize client with read access and with search function. :param handle_server_url: The URL of the Handle Server. May be None (then, the default 'https://hdl.handle.net' is used). :param reverselookup_username: The username to authenticate at the reverse lookup servlet. :param reverselookup_password: The password to authenticate at the reverse lookup servlet. :param \**config: More key-value pairs may be passed that will be passed on to the constructor as config. Config options from the credentials object are overwritten by this. :return: An instance of the client.",1,0,0,0,1,1,0,0,1,2
"def prepare(
  self,
  config_path=None,
  user=None,
  password=None,
  exe_path=None,
  comm_password=None,
  **kwargs
  ):
  params = locals().copy()
  params.pop(""self"")
  if config_path is not None:
  account = helpers.file2dict(config_path)
  params[""user""] = account[""user""]
  params[""password""] = account[""password""]
  params[""broker""] = self._broker
  response = self._s.post(self._api + ""/prepare"", json=params)
  if response.status_code >= 300:
  raise Exception(response.json()[""error""])
  return response.json()"," :param config_path:  :param user:  :param password:   :param exe_path:  r'C:\\htzqzyb2\\xiadan.exe',  r'C:\\htzqzyb2\\xiadan.exe' :param comm_password:   :return:",1,0,0,1,2,1,0,0,2,3
"def _googleauth(key_file=None, scopes=[], user_agent=None):
  if key_file:
  if not scopes:
  scopes = DEFAULT_SCOPES
  creds = ServiceAccountCredentials.from_json_keyfile_name(key_file,
  scopes=scopes)
  else:
  creds = GoogleCredentials.get_application_default()
  http = Http()
  if user_agent:
  http = set_user_agent(http, user_agent)
  http_auth = creds.authorize(http)
  return http_auth","Google http_auth helper. If key_file is not specified, default credentials will be used. If scopes is specified (and key_file), will be used instead of DEFAULT_SCOPES :param key_file: path to key file to use. Default is None :type key_file: ``str`` :param scopes: scopes to set. Default is DEFAUL_SCOPES :type scopes: ``list`` :param user_agent: User Agent string to use in requests. Default is None. :type http_auth: ``str`` or None :return: HTTPLib2 authorized client. :rtype: :class: `HTTPLib2`",1,0,0,1,2,1,0,0,1,2
"def get_chat_members_count(
  self,
  chat_id: Union[int, str]
  ) -> int:
  peer = self.resolve_peer(chat_id)
  if isinstance(peer, types.InputPeerChat):
  return self.send(
  functions.messages.GetChats(
  id=[peer.chat_id]
  )
  ).chats[0].participants_count
  elif isinstance(peer, types.InputPeerChannel):
  return self.send(
  functions.channels.GetFullChannel(
  channel=peer
  )
  ).full_chat.participants_count
  else:
  raise ValueError(""The chat_id \""{}\"" belongs to a user"".format(chat_id))","Use this method to get the number of members in a chat. Args: chat_id (``int`` | ``str``): Unique identifier (int) or username (str) of the target chat. Returns: On success, an integer is returned. Raises: :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error. ``ValueError`` if a chat_id belongs to user.",0,0,0,1,1,2,0,0,1,3
"def pkill(pattern, user=None, signal=15, full=False):
  killed = []
  for proc in psutil.process_iter():
  name_match = pattern in ' '.join(_get_proc_cmdline(proc)) if full \
  else pattern in _get_proc_name(proc)
  user_match = True if user is None else user == _get_proc_username(proc)
  if name_match and user_match:
  try:
  proc.send_signal(signal)
  killed.append(_get_proc_pid(proc))
  except psutil.NoSuchProcess:
  pass
  if not killed:
  return None
  else:
  return {'killed': killed}",Kill processes matching a pattern. .. code-block:: bash salt '*' ps.pkill pattern [user=username] [signal=signal_number] \\ [full=(true|false)] pattern Pattern to search for in the process list. user Limit matches to the given username. Default: All users. signal Signal to send to the process(es). See manpage entry for kill for possible values. Default: 15 (SIGTERM). full A boolean value indicating whether only the name of the command or the full command line should be matched against the pattern. **Examples:** Send SIGHUP to all httpd processes on all 'www' minions: .. code-block:: bash salt 'www.*' ps.pkill httpd signal=1 Send SIGKILL to all bash processes owned by user 'tom': .. code-block:: bash salt '*' ps.pkill bash signal=9 user=tom,1,0,0,1,2,1,0,0,1,2
"def get_all_users_from_group(self, group, include_inactive_users=False, start=0, limit=50):
  url = 'rest/api/2/group/member'
  params = {}
  if group:
  params['groupname'] = group
  params['includeInactiveUsers'] = include_inactive_users
  params['startAt'] = start
  params['maxResults'] = limit
  return self.get(url, params=params)","Just wrapping method user group members :param group: :param include_inactive_users: :param start: OPTIONAL: The start point of the collection to return. Default: 0. :param limit: OPTIONAL: The limit of the number of users to return, this may be restricted by fixed system limits. Default by built-in method: 50 :return:",2,0,0,1,3,2,0,0,1,3
"def time_entry(self, prompt, message=None, formats=['%X', '%H:%M', '%I:%M', '%H.%M',
  '%I.%M'], show_example=False, rofi_args=None, **kwargs):
  def time_validator(text):
  for format in formats:
  try:
  dt = datetime.strptime(text, format)
  except ValueError:
  continue
  else:
  return (dt.time(), None)
  return (None, 'Please enter a valid time.')
  if show_example:
  message = message or """"
  message += ""Current time in the correct format: "" + datetime.now().strftime(formats[0])
  return self.generic_entry(prompt, time_validator, message, rofi_args=None, **kwargs)","Prompt the user to enter a time. Parameters ---------- prompt: string Prompt to display to the user. message: string, optional Message to display under the entry line. formats: list of strings, optional The formats that the user can enter times in. These should be format strings as accepted by the datetime.datetime.strptime() function from the standard library. They are tried in order, and the first that returns a time object without error is selected. Note that the '%X' in the default list is the current locale's time representation. show_example: Boolean If True, the current time in the first format given is appended to the message. Returns ------- datetime.time, or None if the dialog is cancelled.",1,0,0,1,2,1,0,0,1,2
"def report(self, message, http_context=None, user=None):
  stack = traceback.extract_stack()
  last_call = stack[-2]
  file_path = last_call[0]
  line_number = last_call[1]
  function_name = last_call[2]
  report_location = {
  ""filePath"": file_path,
  ""lineNumber"": line_number,
  ""functionName"": function_name,
  }
  self._send_error_report(
  message,
  http_context=http_context,
  user=user,
  report_location=report_location,
  )","Reports a message to Stackdriver Error Reporting https://cloud.google.com/error-reporting/docs/formatting-error-messages :type message: str :param message: A user-supplied message to report :type http_context: :class`google.cloud.error_reporting.HTTPContext` :param http_context: The HTTP request which was processed when the error was triggered. :type user: str :param user: The user who caused or was affected by the crash. This can be a user ID, an email address, or an arbitrary token that uniquely identifies the user. When sending an error report, leave this field empty if the user was not logged in. In this case the Error Reporting system will use other data, such as remote IP address, to distinguish affected users. Example: .. code-block:: python >>> client.report(""Something went wrong!"")",0,0,0,1,1,1,0,0,1,2
"def api_key(request, response, verify_user, context=None, **kwargs):
  api_key = request.get_header('X-Api-Key')
  if api_key:
  try:
  user = verify_user(api_key)
  except TypeError:
  user = verify_user(api_key, context)
  if user:
  return user
  else:
  return False
  else:
  return None","API Key Header Authentication The verify_user function passed in to ths authenticator shall receive an API key as input, and return a user object to store in the request context if the request was successful.",1,0,1,1,3,1,0,0,1,2
"def authenticate_with_email_and_pwd(user_email, user_password):
  if user_email is None or user_password is None:
  raise ValueError(
  'Could not authenticate user. Missing username or password')
  upload_token = uploader.get_upload_token(user_email, user_password)
  if not upload_token:
  print(""Authentication failed for user name "" +
  user_name + "", please try again."")
  sys.exit(1)
  user_key = get_user_key(user_name)
  if not user_key:
  print(""User name {} does not exist, please try again or contact Mapillary user support."".format(
  user_name))
  sys.exit(1)
  user_permission_hash, user_signature_hash = get_user_hashes(
  user_key, upload_token)
  user_items[""MAPSettingsUsername""] = section
  user_items[""MAPSettingsUserKey""] = user_key
  user_items[""user_upload_token""] = upload_token
  user_items[""user_permission_hash""] = user_permission_hash
  user_items[""user_signature_hash""] = user_signature_hash
  return user_items",Authenticate the user by passing the email and password. This function avoids prompting the command line for user credentials and is useful for calling tools programmatically,2,0,0,1,3,1,0,0,1,2
"def get_y(self, indices=None):
  if indices is None:
  indices = list(range(0, self.get_sample_size()))
  else:
  if not hasattr(indices, ""__iter__""):
  indices = [indices]
  indices = sorted(list(set(indices)))
  if len(indices) == 0:
  return []
  return self.Y[indices]","Returns the output data requested by the user @ In, indices, a list of non-negative integers specifying the row indices to return @ Out, an nparray of floating point values specifying the output data values filtered by the indices input parameter.",0,0,0,1,1,1,0,0,1,2
"def describe(cwd, rev='tip', user=None):
  cmd = [
  'hg',
  'log',
  '-r',
  '{0}'.format(rev),
  '--template',
  ""'{{latesttag}}-{{latesttagdistance}}-{{node|short}}'""
  ]
  desc = __salt__['cmd.run_stdout'](
  cmd,
  cwd=cwd,
  runas=user,
  python_shell=False)
  return desc or revision(cwd, rev, short=True)",Mimic git describe and return an identifier for the given revision cwd The path to the Mercurial repository rev: tip The path to the archive tarball user : None Run hg as a user other than what the minion runs as CLI Example: .. code-block:: bash salt '*' hg.describe /path/to/repo,1,0,0,1,2,1,0,0,1,2
"def get_cached_or_new(url, new=False):
  garbage_collection()
  old_req = DATABASE.get(url)
  if old_req and not new:
  return old_req
  if not (url.startswith(""http://"") or url.startswith(""https://"")):
  raise ValueError(""Invalid URL `%s`!"" % url)
  req = RequestInfo(url=url)
  DATABASE[url] = req
  return req","Look into the database and return :class:`RequestInfo` if the `url` was already analyzed, or create and return new instance, if not. If the `new` is set to True, always create new instance. Args: url (str): URL of the analyzed resource. new (bool, default False): Force new instance? Returns: obj: :class:`RequestInfo` instance.",1,0,1,1,3,1,0,1,0,2
"def _prep_save(self):
  log.info(""Preparing bulk save."")
  self.graph.update_bulk_saver(self.bulk_saver)
  log.info(
  ""Dropped %d unused preconditions, %d are missing"",
  sum(len(v) for v in self.summary[""precondition_entries""].values()),
  len(self.summary[""missing_preconditions""]),
  )
  log.info(
  ""Dropped %d unused postconditions, %d are missing"",
  sum(len(v) for v in self.summary[""postcondition_entries""].values()),
  len(self.summary[""missing_postconditions""]),
  )
  del self.summary[""postcondition_entries""]",Prepares the bulk saver to load the trace graph info into the database.,0,1,0,0,1,0,1,0,0,1
"def loop(self):
  while True:
  text = compat.input('ctl > ')
  command, args = self.parse_input(text)
  if not command:
  continue
  response = self.call(command, *args)
  response.show()","Enter loop, read user input then run command. Repeat",1,0,0,1,2,1,0,0,1,2
"async def get_answers(
  self,
  context: TurnContext,
  options: QnAMakerOptions = None,
  telemetry_properties: Dict[str,str] = None,
  telemetry_metrics: Dict[str,int] = None
  ) -> [QueryResult]:
  hydrated_options = self._hydrate_options(options)
  self._validate_options(hydrated_options)
  result = self._query_qna_service(context.activity, hydrated_options)
  await self._emit_trace_info(context, result, hydrated_options)
  return result","Generates answers from the knowledge base. :return: A list of answers for the user's query, sorted in decreasing order of ranking score. :rtype: [QueryResult]",1,0,0,1,2,1,0,0,1,2
"def get_published_or_draft(self):
  if self.is_published:
  return self
  elif self.publishing_linked_id:
  return self.publishing_linked
  if is_draft_request_context():
  return self.get_draft()
  return None","Return the published item, if it exists, otherwise, for privileged users, return the draft version.",1,0,1,1,3,1,0,1,1,3
"def set_tags(self, tags):
  if tags is None:
  return
  attrmap = {'color': 'color', 'emotes': 'emotes',
  'subscriber': 'subscriber',
  'turbo': 'turbo', 'user-type': 'user_type'}
  for t in tags:
  attr = attrmap.get(t.name)
  if not attr:
  continue
  else:
  setattr(self, attr, t.value)","For every known tag, set the appropriate attribute. Known tags are: :color: The user color :emotes: A list of emotes :subscriber: True, if subscriber :turbo: True, if turbo user :user_type: None, mod, staff, global_mod, admin :param tags: a list of tags :type tags: :class:`list` of :class:`Tag` | None :returns: None :rtype: None :raises: None",0,0,0,1,1,0,0,0,1,1
"def _delocalize_logging_command(self, logging_path, user_project):
  logging_prefix = os.path.splitext(logging_path.uri)[0]
  if logging_path.file_provider == job_model.P_LOCAL:
  mkdir_cmd = 'mkdir -p ""%s""\n' % os.path.dirname(logging_prefix)
  cp_cmd = 'cp'
  elif logging_path.file_provider == job_model.P_GCS:
  mkdir_cmd = ''
  if user_project:
  cp_cmd = 'gsutil -u {} -mq cp'.format(user_project)
  else:
  cp_cmd = 'gsutil -mq cp'
  else:
  assert False
  copy_logs_cmd = textwrap.dedent().format(
  cp_cmd=cp_cmd, prefix=logging_prefix)
  body = textwrap.dedent().format(
  mkdir_cmd=mkdir_cmd, copy_logs_cmd=copy_logs_cmd)
  return body",Returns a command to delocalize logs. Args: logging_path: location of log files. user_project: name of the project to be billed for the request. Returns: eg. 'gs://bucket/path/myfile' or 'gs://bucket/script-foobar-12',0,0,0,0,0,1,0,0,1,2
"def idrac_general(blade_name, command, idrac_password=None,
  host=None,
  admin_username=None, admin_password=None):
  module_network = network_info(host, admin_username,
  admin_password, blade_name)
  if idrac_password is not None:
  password = idrac_password
  else:
  password = admin_password
  idrac_ip = module_network['Network']['IP Address']
  ret = __execute_ret(command, host=idrac_ip,
  admin_username='root',
  admin_password=password)
  if ret['retcode'] == 0:
  return ret['stdout']
  else:
  return ret","Run a generic racadm command against a particular blade in a chassis. Blades are usually named things like 'server-1', 'server-2', etc. If the iDRAC has a different password than the CMC, then you can pass it with the idrac_password kwarg. :param blade_name: Name of the blade to run the command on :param command: Command like to pass to racadm :param idrac_password: Password for the iDRAC if different from the CMC :param host: Chassis hostname :param admin_username: CMC username :param admin_password: CMC password :return: stdout if the retcode is 0, otherwise a standard cmd.run_all dictionary CLI Example: .. code-block:: bash salt fx2 chassis.cmd idrac_general server-1 'get BIOS.SysProfileSettings'",2,0,0,1,3,1,0,0,1,2
"def set(self, token, request, *args, **kwargs):
  if hasattr(request, 'user') and request.user:
  user = request.user
  elif self.current_user:
  user = self.current_user()
  client = request.client
  tokens = self.query.filter_by(
  client_id=client.client_id,
  user_id=user.id).all()
  if tokens:
  for tk in tokens:
  self.session.delete(tk)
  self.session.commit()
  expires_in = token.get('expires_in')
  expires = datetime.utcnow() + timedelta(seconds=expires_in)
  tok = self.model(**token)
  tok.expires = expires
  tok.client_id = client.client_id
  tok.user_id = user.id
  self.session.add(tok)
  self.session.commit()
  return tok",Creates a Token object and removes all expired tokens that belong to the user :param token: token object :param request: OAuthlib request object,1,1,1,1,4,1,1,1,0,3
"def login(cls, email, password):
  try:
  doc = yield cls.view.first(key=email, include_docs=True)
  except couch.NotFound:
  raise exceptions.Unauthorized('Unknown email address')
  user = cls(**doc['doc'])
  verified = user.verify_password(password)
  if not verified:
  raise exceptions.Unauthorized('Invalid password')
  token = yield Token.create(user)
  raise Return((user, token.id))","Log in a user :param email: the user's email address :param password: the user's password :returns: (User, token) :raises: SocketError, CouchException",1,1,1,1,4,1,0,1,1,3
"def host_create(host, groups, interfaces, **kwargs):
  conn_args = _login(**kwargs)
  ret = {}
  try:
  if conn_args:
  method = 'host.create'
  params = {""host"": host}
  if not isinstance(groups, list):
  groups = [groups]
  grps = []
  for group in groups:
  grps.append({""groupid"": group})
  params['groups'] = grps
  if not isinstance(interfaces, list):
  interfaces = [interfaces]
  params['interfaces'] = interfaces
  params = _params_extend(params, _ignore_name=True, **kwargs)
  ret = _query(method, params, conn_args['url'], conn_args['auth'])
  return ret['result']['hostids']
  else:
  raise KeyError
  except KeyError:
  return ret",".. versionadded:: 2016.3.0 Create new host .. note:: This function accepts all standard host properties: keyword argument names differ depending on your zabbix version, see here__. .. __: https://www.zabbix.com/documentation/2.4/manual/api/reference/host/object#host :param host: technical name of the host :param groups: groupids of host groups to add the host to :param interfaces: interfaces to be created for the host :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring) :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring) :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring) :param visible_name: string with visible name of the host, use 'visible_name' instead of 'name' parameter to not mess with value supplied from Salt sls file. return: ID of the created host. CLI Example: .. code-block:: bash salt '*' zabbix.host_create technicalname 4 interfaces='{type: 1, main: 1, useip: 1, ip: ""192.168.3.1"", dns: """", port: 10050}' visible_name='Host Visible Name' inventory_mode=0 inventory='{""alias"": ""something""}'",1,1,0,2,4,1,0,0,1,2
"def sigint(self) -> None:
  if self.stop_attempts < 1:
  Log.info('gracefully stopping tasks')
  self.stop_attempts += 1
  self.terminate()
  elif self.stop_attempts < 2:
  Log.info('forcefully cancelling tasks')
  self.stop_attempts += 1
  self.terminate(force=True)
  else:
  Log.info('forcefully stopping event loop')
  self.loop.stop()","Handle the user pressing Ctrl-C by stopping tasks nicely at first, then forcibly upon further presses.",1,0,0,1,2,1,0,0,0,1
"def user_exists(alias, **kwargs):
  conn_args = _login(**kwargs)
  ret = {}
  try:
  if conn_args:
  method = 'user.get'
  params = {""output"": ""extend"", ""filter"": {""alias"": alias}}
  ret = _query(method, params, conn_args['url'], conn_args['auth'])
  return True if ret['result'] else False
  else:
  raise KeyError
  except KeyError:
  return ret","Checks if user with given alias exists. .. versionadded:: 2016.3.0 :param alias: user alias :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring) :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring) :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring) :return: True if user exists, else False. CLI Example: .. code-block:: bash salt '*' zabbix.user_exists james",2,0,0,1,3,1,0,0,1,2
"def breadcrumb(crumb, coerce_to=None):
  def wrapper(view):
  @wraps(view)
  def inner(request, *args, **kwargs):
  return view(request, *args, **kwargs)
  if coerce_to is not None:
  inner.breadcrumb = (
  lambda *args, **kwargs: coerce_to(crumb(*args, **kwargs)))
  else:
  inner.breadcrumb = crumb
  return inner
  return wrapper","Usage:: from navigation.decorators import breadcrumb @breadcrumb('greeting') def some_view(request): return 'Hello world!' @breadcrumb(lambda request: u'greeting for %s' % request.user.username) def some_view(request): return 'Hello %s!' % request.user.username .. note:: By default the value returned by a callable is required to be a ``unicode`` object. If the function returns a model instance, its ``__unicode__`` method is *not* called. Use ``coerce_to=unicode``. :param crumb: A ``unicode`` string or a callable that returns it. :param coerce_to: Coerces *crumb* to given type. The value can be ``unicode`` or any callable that returns a ``unicode`` object.",1,0,0,0,1,1,0,0,1,2
"def create_admin(cls, email, password, **kwargs):
  data = {
  'email': email,
  'password': cls.hash_password(password),
  'has_agreed_to_terms': True,
  'state': State.approved,
  'role': cls.roles.administrator.value,
  'organisations': {}
  }
  data.update(**kwargs)
  user = cls(**data)
  yield user._save()
  raise Return(user)",Create an approved 'global' administrator :param email: the user's email address :param password: the user's plain text password :returns: a User,1,0,1,1,3,1,1,1,0,3
"def create_finance_metrics(metrics: list, pronacs: list):
  missing = missing_metrics(metrics, pronacs)
  print(f""There are {len(missing)} missing metrics!"")
  processors = mp.cpu_count()
  print(f""Using {processors} processors to calculate metrics!"")
  indicators_qs = FinancialIndicator.objects.filter(
  project_id__in=[p for p, _ in missing]
  )
  indicators = {i.project_id: i for i in indicators_qs}
  pool = mp.Pool(processors)
  results = [
  pool.apply_async(create_metric, args=(indicators, metric_name, pronac))
  for pronac, metric_name in missing
  ]
  calculated_metrics = [p.get() for p in results]
  if calculated_metrics:
  Metric.objects.bulk_create(calculated_metrics)
  print(""Bulk completed"")
  for indicator in indicators.values():
  indicator.fetch_weighted_complexity()
  print(""Finished update indicators!"")
  pool.close()
  print(""Finished metrics calculation!"")","Creates metrics, creating an Indicator if it doesn't already exists Metrics are created for projects that are in pronacs and saved in database. args: metrics: list of names of metrics that will be calculated pronacs: pronacs in dataset that is used to calculate those metrics",0,1,1,1,3,1,1,1,1,4
"def is_subscribed_list(self, list_id, user_id):
  try:
  return bool(self._client.show_list_subscriber(list_id=list_id, user_id=user_id))
  except TweepError as e:
  if e.api_code == TWITTER_USER_IS_NOT_LIST_MEMBER_SUBSCRIBER:
  return False
  raise","Check if user is a subscribed of specified list :param list_id: list ID number :param user_id: user ID number :return: :code:`True` if user is subscribed of list, :code:`False` otherwise",1,0,0,1,2,2,0,0,1,3
"def capture_insert_from_model(cls, table_name, record_id, *, exclude_fields=()):
  exclude_cols = ()
  if exclude_fields:
  model_cls = get_connected_model_for_table_name(table_name)
  exclude_cols = cls._fieldnames_to_colnames(model_cls, exclude_fields)
  raw_query = sql.SQL().format(
  schema=sql.Identifier(settings.HEROKU_CONNECT_SCHEMA),
  table_name=sql.Identifier(table_name),
  exclude_cols=sql.SQL(', ').join(sql.Identifier(col) for col in exclude_cols),
  )
  params = {'record_id': record_id, 'table_name': table_name}
  result_qs = TriggerLog.objects.raw(raw_query, params)
  return list(result_qs)","Create a fresh insert record from the current model state in the database. For read-write connected models, this will lead to the attempted creation of a corresponding object in Salesforce. Args: table_name (str): The name of the table backing the connected model (without schema) record_id (int): The primary id of the connected model exclude_fields (Iterable[str]): The names of fields that will not be included in the write record Returns: A list of the created TriggerLog entries (usually one). Raises: LookupError: if ``table_name`` does not belong to a connected model",0,1,1,1,3,0,1,1,1,3
"def update_entity_alias(self, alias_id, name, canonical_id, mount_accessor, mount_point=DEFAULT_MOUNT_POINT):
  params = {
  'name': name,
  'canonical_id': canonical_id,
  'mount_accessor': mount_accessor,
  }
  api_path = '/v1/{mount_point}/entity-alias/id/{id}'.format(
  mount_point=mount_point,
  id=alias_id,
  )
  response = self._adapter.post(
  url=api_path,
  json=params,
  )
  if response.status_code == 204:
  return response
  else:
  return response.json()","Update an existing entity alias. Supported methods: POST: /{mount_point}/entity-alias/id/{id}. Produces: 200 application/json :param alias_id: Identifier of the entity alias. :type alias_id: str | unicode :param name: Name of the alias. Name should be the identifier of the client in the authentication source. For example, if the alias belongs to userpass backend, the name should be a valid username within userpass backend. If alias belongs to GitHub, it should be the GitHub username. :type name: str | unicode :param canonical_id: Entity ID to which this alias belongs to. :type canonical_id: str | unicode :param mount_accessor: Accessor of the mount to which the alias should belong to. :type mount_accessor: str | unicode :param mount_point: The ""path"" the method/backend was mounted on. :type mount_point: str | unicode :return: The JSON response where available, otherwise the generic response object, of the request. :rtype: dict | requests.Response",2,0,0,2,4,1,0,0,2,3
"def login(self, email=None, password=None, user=None):
  if user is not None:
  data = {'login': user, 'password': password}
  elif email is not None:
  data = {'email': email, 'password': password}
  else:
  raise ValueError('Neither username nor email provided to login')
  self.headers = {'connection': 'close'}
  response = self.post('/session', **data)
  self.token = response['private_token']
  self.headers = {'PRIVATE-TOKEN': self.token,
  'connection': 'close'}
  return response",Logs the user in and setups the header with the private token :param email: Gitlab user Email :param user: Gitlab username :param password: Gitlab user password :return: True if login successful :raise: HttpError :raise: ValueError,1,0,0,1,2,2,0,0,2,4
"def login(self, username, password):
  data = dict(user=username, passwd=password, api_type='json')
  r = requests.post(LOGIN_URL, data=data)
  if r.status_code == 200:
  try:
  j = json.loads(r.content)
  self._cookies = r.cookies
  self._modhash = j['json']['data']['modhash']
  self._username = username
  return r
  except Exception:
  raise LoginFail()
  else:
  raise BadResponse(r)","Logs into reddit with supplied credentials using SSL. Returns :class:`requests.Response` object, or raises :class:`exceptions.LoginFail` or :class:`exceptions.BadResponse` if not a 200 response. URL: ``https://ssl.reddit.com/api/login`` :param username: reddit username :param password: corresponding reddit password",2,0,0,2,4,1,0,0,2,3
"def readInputFile(self, card_name, directory, session, spatial=False,
  spatialReferenceID=None, **kwargs):
  self.project_directory = directory
  with tmp_chdir(directory):
  replaceParamFile = self._readReplacementFiles(directory, session, spatial, spatialReferenceID)
  return self._readXputFile(self.INPUT_FILES, card_name, directory,
  session, spatial, spatialReferenceID,
  replaceParamFile, **kwargs)","Read specific input file for a GSSHA project to the database. Args: card_name(str): Name of GSSHA project card. directory (str): Directory containing all GSSHA model files. This method assumes that all files are located in the same directory. session (:mod:`sqlalchemy.orm.session.Session`): SQLAlchemy session object bound to PostGIS enabled database spatial (bool, optional): If True, spatially enabled objects will be read in as PostGIS spatial objects. Defaults to False. spatialReferenceID (int, optional): Integer id of spatial reference system for the model. If no id is provided GsshaPy will attempt to automatically lookup the spatial reference ID. If this process fails, default srid will be used (4326 for WGS 84). Returns: file object",0,1,1,0,2,1,0,0,1,2
"def _get_sorted_section(self, nts_section):
  if self.section_sortby is True:
  return sorted(nts_section, key=lambda nt: self.sortgos.usrgo_sortby(nt))
  if self.section_sortby is False or self.section_sortby is None:
  return nts_section
  return sorted(nts_section, key=lambda nt: self.section_sortby(nt))","Sort GO IDs in each section, if requested by user.",1,0,0,1,2,1,0,0,1,2
"def update_user(self, id, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('async_req'):
  return self.update_user_with_http_info(id, **kwargs)
  else:
  (data) = self.update_user_with_http_info(id, **kwargs)
  return data","Update user with given user groups and permissions. # noqa: E501 # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.update_user(id, async_req=True) >>> result = thread.get() :param async_req bool :param str id: (required) :param UserRequestDTO body: Example Body: <pre>{ \""identifier\"": \""user@example.com\"", \""groups\"": [ \""user_management\"" ], \""userGroups\"": [ \""8b23136b-ecd2-4cb5-8c92-62477dcc4090\"" ] }</pre> :return: UserModel If the method is called asynchronously, returns the request thread.",2,0,0,1,3,2,1,0,1,4
"def reference_handler(self,iobject, fact, attr_info, add_fact_kargs):
  (namespace_uri,uid) = (self.identifier_ns_uri,attr_info['idref'])
  timestamp = attr_info['@timestamp']
  (target_mantis_obj, existed) = MantisImporter.create_iobject(
  uid=uid,
  identifier_ns_uri=namespace_uri,
  timestamp=timestamp)
  logger.debug(""Creation of Placeholder for %s %s returned %s"" % (namespace_uri,uid,existed))
  add_fact_kargs['value_iobject_id'] = Identifier.objects.get(uid=uid,namespace__uri=namespace_uri)
  return True","Handler for facts that contain a reference to a fact. See below in the comment regarding the fact_handler_list for a description of the signature of handler functions. As shown below in the handler list, this handler is called when a attribute with key '@idref' on the fact's node is detected -- this attribute signifies that this fact does not contain a value but points to another object. Thus, we try to retrieve that object from the database. If it exists, fine -- if not, then the call to 'create_iobject' returns a PLACEHOLDER object. We further create/refer to the fitting fact data type: we want the fact data type to express that the fact is a reference to an object.",1,1,1,0,3,0,0,1,0,1
"def _check_permission(self, name, obj=None):
  def redirect_or_exception(ex):
  if not self.request.user or not self.request.user.is_authenticated:
  if self.auto_login_redirect:
  redirect_to_login(self.request.get_full_path())
  else:
  raise HTTPUnauthorizedResponseException
  else:
  raise ex
  try:
  if not self._has_permission(name, obj):
  redirect_or_exception(HTTPForbiddenResponseException)
  except Http404 as ex:
  redirect_or_exception(ex)",If customer is not authorized he should not get information that object is exists. Therefore 403 is returned if object was not found or is redirected to the login page. If custmer is authorized and object was not found is returned 404. If object was found and user is not authorized is returned 403 or redirect to login page. If object was found and user is authorized is returned 403 or 200 according of result of _has_permission method.,1,0,0,1,2,1,0,0,1,2
"def trimExtend(self,
  sr,
  polylines,
  trimExtendTo,
  extendHow=0):
  allowedHow = [0,1,2,4,8,16]
  if extendHow not in allowedHow:
  raise AttributeError(""Invalid extend How value."")
  url = self._url + ""/trimExtend""
  params = {
  ""f"" : ""json"",
  ""sr"" : sr,
  ""polylines"" : self.__geomToStringArray(geometries=polylines, returnType=""list""),
  ""extendHow"": extendHow,
  ""trimExtendTo"" : trimExtendTo.asDictionary
  }
  return self._get(url=url, param_dict=params,
  proxy_url=self._proxy_url,
  securityHandler=self._securityHandler,
  proxy_port=self._proxy_port)","The trimExtend operation is performed on a geometry service resource. This operation trims or extends each polyline specified in the input array, using the user-specified guide polylines. When trimming features, the part to the left of the oriented cutting line is preserved in the output, and the other part is discarded. An empty polyline is added to the output array if the corresponding input polyline is neither cut nor extended. Inputs: polylines - array of polylines to be trimmed or extended (structured as JSON polyline objects returned by the ArcGIS REST API). trimExtendTo - a polyline that is used as a guide for trimming or extending input polylines (structured as JSON polyline objects returned by the ArcGIS REST API). sr - spatial reference of the input polylines. extendHow - a flag that is used along with the trimExtend operation.",2,0,0,1,3,1,0,0,1,2
"def exit_on_keyboard_interrupt(f):
  @wraps(f)
  def wrapper(*args, **kwargs):
  raise_exception = kwargs.pop('raise_exception', False)
  try:
  return f(*args, **kwargs)
  except KeyboardInterrupt:
  if not raise_exception:
  sys.exit()
  raise KeyboardInterrupt
  return wrapper",Decorator that allows user to exit script by sending a keyboard interrupt (ctrl + c) without raising an exception.,1,0,0,0,1,1,0,0,1,2
"def info(cwd,
  targets=None,
  user=None,
  username=None,
  password=None,
  fmt='str'):
  opts = list()
  if fmt == 'xml':
  opts.append('--xml')
  if targets:
  opts += salt.utils.args.shlex_split(targets)
  infos = _run_svn('info', cwd, user, username, password, opts)
  if fmt in ('str', 'xml'):
  return infos
  info_list = []
  for infosplit in infos.split('\n\n'):
  info_list.append(_INI_RE.findall(infosplit))
  if fmt == 'list':
  return info_list
  if fmt == 'dict':
  return [dict(tmp) for tmp in info_list]","Display the Subversion information from the checkout. cwd The path to the Subversion repository targets : None files, directories, and URLs to pass to the command as arguments svn uses '.' by default user : None Run svn as a user other than what the minion runs as username : None Connect to the Subversion server as another user password : None Connect to the Subversion server with this password .. versionadded:: 0.17.0 fmt : str How to fmt the output from info. (str, xml, list, dict) CLI Example: .. code-block:: bash salt '*' svn.info /path/to/svn/repo",1,0,0,1,2,1,0,0,1,2
"def set_chat_photo(chat_id, photo, **kwargs):
  files = None
  if isinstance(photo, InputFile):
  files = [photo]
  photo = None
  elif not isinstance(photo, str):
  raise Exception('photo must be instance of InputFile or str')
  params = dict(
  chat_id=chat_id,
  photo=photo
  )
  return TelegramBotRPCRequest('setChatPhoto', params=params, files=files, on_result=lambda result: result, **kwargs)","Use this method to set a new profile photo for the chat. Photos can't be changed for private chats. The bot must be an administrator in the chat for this to work and must have the appropriate admin rights. :param chat_id: Unique identifier for the target chat or username of the target channel (in the format @channelusername) :param photo: New chat photo, uploaded using multipart/form-data :param kwargs: Args that get passed down to :class:`TelegramBotRPCRequest` :return: Returns True on success. :rtype: bool",1,0,0,2,3,1,0,0,2,3
"def get_url_filemeta(url):
  parsed_url = try_parse_url(url)
  if parsed_url is None:
  return None
  if parsed_url.scheme.startswith('ftp'):
  return get_ftp_filemeta(parsed_url)
  url = parsed_url.geturl()
  try:
  r = requests.get(url, stream=True, allow_redirects=True, timeout=5)
  remote_size = r.headers.get('Content-Length', -1)
  return dict(url=url, hostname=parsed_url.hostname, path=parsed_url.path,
  username=parsed_url.username, remote_size=remote_size,
  filename=os.path.basename(parsed_url.path))
  except ConnectionError:
  return None
  except (InvalidURL, InvalidSchema, InvalidHeader, MissingSchema):
  return None
  return None","Request HTML for the page at the URL indicated and return the url, filename, and remote size TODO: just add remote_size and basename and filename attributes to the urlparse object instead of returning a dict >>> sorted(get_url_filemeta('mozilla.com').items()) [('filename', ''), ('hostname', 'mozilla.com'), ('path', ''), ('remote_size', -1), ('url', 'http://mozilla.com'), ('username', None)] >>> sorted(get_url_filemeta('https://duckduckgo.com/about?q=nlp').items()) [('filename', 'about'), ('hostname', 'duckduckgo.com'), ('path', '/about'), ('remote_size', -1), ('url', 'https://duckduckgo.com/about?q=nlp'), ('username', None)] >>> 1000 <= int(get_url_filemeta('en.wikipedia.org')['remote_size']) <= 200000 True",2,0,0,1,3,1,0,0,1,2
"def get_vm_list(self):
  action = ""list all guests in database""
  with zvmutils.log_and_reraise_sdkbase_error(action):
  guests_in_db = self._GuestDbOperator.get_guest_list()
  guests_migrated = self._GuestDbOperator.get_migrated_guest_list()
  userids_in_db = [g[1].upper() for g in guests_in_db]
  userids_migrated = [g[1].upper() for g in guests_migrated]
  userid_list = list(set(userids_in_db) - set(userids_migrated))
  return userid_list",Get the list of guests that are created by SDK return userid list,1,0,1,1,3,1,0,1,1,3
"def _email(name, *, allow_unverified=False):
  def inner(fn):
  @functools.wraps(fn)
  def wrapper(request, user_or_users, **kwargs):
  if isinstance(user_or_users, (list, set)):
  recipients = user_or_users
  else:
  recipients = [user_or_users]
  context = fn(request, user_or_users, **kwargs)
  msg = EmailMessage.from_template(name, context, request=request)
  for recipient in recipients:
  if isinstance(recipient, tuple):
  user, email = recipient
  else:
  user, email = recipient, None
  _send_email_to_user(
  request, user, msg, email=email, allow_unverified=allow_unverified
  )
  return context
  return wrapper
  return inner","This decorator is used to turn an e function into an email sending function! The name parameter is the name of the email we're going to be sending (used to locate the templates on the file system). The allow_unverified kwarg flags whether we will send this email to an unverified email or not. We generally do not want to do this, but some emails are important enough or have special requirements that require it. Functions that are decorated by this need to accept two positional arguments, the first argument is the Pyramid request object, and the second argument is either a single User, or a list of Users. These users represent the recipients of this email. Additional keyword arguments are supported, but are not otherwise restricted. Functions decorated by this must return a mapping of context variables that will ultimately be returned, but which will also be used to render the templates for the emails. Thus this function can decorate functions with a signature like so: def foo( request: Request, user_or_users: Union[User, List[User]] ) -> Mapping[str, Any]: ... Finally, if the email needs to be sent to an address *other* than the user's primary email address, instead of a User object, a tuple of (User, Email) objects may be used in place of a User object.",1,0,0,1,2,1,0,0,1,2
"def start_entry(self, target, var_id):
  self.in_progress = ConfigEntry(target, var_id, b'')
  if self.data_size - self.data_index < self.in_progress.data_space():
  return Error.DESTINATION_BUFFER_TOO_SMALL
  self.in_progress.data += struct.pack(""<H"", var_id)
  self.data_index += self.in_progress.data_space()
  return Error.NO_ERROR","Begin a new config database entry. If there is a current entry in progress, it is aborted but the data was already committed to persistent storage so that space is wasted. Args: target (SlotIdentifer): The target slot for this config variable. var_id (int): The config variable ID Returns: int: An error code from the global Errors enum.",0,1,0,0,1,1,0,0,1,2
"def get_friends(self, query):
  api = self._connectToAPI()
  self._rate_limit_status(api=api, mode=""get_friends"")
  try:
  friends_ids = api.friends_ids(query)
  except:
  return []
  return friends_ids",Method to get the friends of a user. :param query: Query to be performed. :return: List of users.,2,0,0,1,3,2,0,0,1,3
"def get(name, download=False, install=False):
  wua = salt.utils.win_update.WindowsUpdateAgent()
  updates = wua.search(name)
  ret = {}
  if download or install:
  ret['Download'] = wua.download(updates)
  if install:
  ret['Install'] = wua.install(updates)
  return ret if ret else updates.list()",".. versionadded:: 2017.7.0 Returns details for the named update Args: name (str): The name of the update you're searching for. This can be the GUID, a KB number, or any part of the name of the update. GUIDs and KBs are preferred. Run ``list`` to get the GUID for the update you're looking for. download (bool): Download the update returned by this function. Run this function first to see if the update exists, then set ``download=True`` to download the update. install (bool): Install the update returned by this function. Run this function first to see if the update exists, then set ``install=True`` to install the update. Returns: dict: Returns a dict containing a list of updates that match the name if download and install are both set to False. Should usually be a single update, but can return multiple if a partial name is given. If download or install is set to true it will return the results of the operation. .. code-block:: cfg List of Updates: {'<GUID>': {'Title': <title>, 'KB': <KB>, 'GUID': <the globally unique identifier for the update> 'Description': <description>, 'Downloaded': <has the update been downloaded>, 'Installed': <has the update been installed>, 'Mandatory': <is the update mandatory>, 'UserInput': <is user input required>, 'EULAAccepted': <has the EULA been accepted>, 'Severity': <update severity>, 'NeedsReboot': <is the update installed and awaiting reboot>, 'RebootBehavior': <will the update require a reboot>, 'Categories': [ '<category 1>', '<category 2>', ...] } } CLI Examples: .. code-block:: bash # Recommended Usage using GUID without braces # Use this to find the status of a specific update salt '*' win_wua.get 12345678-abcd-1234-abcd-1234567890ab # Use the following if you don't know the GUID: # Using a KB number # Not all updates have an associated KB salt '*' win_wua.get KB3030298 # Using part or all of the name of the update # Could possibly return multiple results # Not all updates have an associated KB salt '*' win_wua.get 'Microsoft Camera Codec Pack'",1,0,0,1,2,1,0,0,1,2
"def schema_import(conn, dbpath):
  conn.execute(
  ""ATTACH DATABASE ? AS source"", (str(dbpath),))
  conn.execute(
  ""INSERT OR IGNORE INTO profiles (name, data)""
  "" SELECT name, data FROM source.profiles""
  "" WHERE data IS NOT NULL"")
  conn.commit()
  conn.execute(
  ""DETACH DATABASE source"")",Import profiles from another database. This does not overwrite existing profiles in the target database. Profiles in the source database that share names with those in the target database are ignored. :param conn: A connection to an SQLite3 database into which to copy profiles. :param dbpath: The filesystem path to the source SQLite3 database.,1,1,1,0,3,0,1,1,0,2
"def get_ftp(ftp_conf, debug=0):
  server = ftp_conf.get('server')
  user = ftp_conf.get('user')
  password = ftp_conf.get('password')
  start_path = ftp_conf.get('start_path')
  slog.info(""Connecting FTP server %s ......"", server)
  ftpStr = 'ftp://%s/'%server
  if start_path:
  ftpStr = ftpStr+start_path
  ftp = ftplib.FTP(server, user, password)
  ftp.set_debuglevel(debug)
  if start_path:
  ftp.cwd(start_path)
  serverFiles = ftp.nlst()
  slog.info('There are some files in %s:\n[%s]'%(ftpStr, ', '.join(serverFiles)))
  return ftp, ftpStr"," FTP  ftp  :param dict ftp_conf: ftp  >>> { >>> 'server':'127.0.0.1', >>> 'start_path':None, >>> 'user':'admin', >>> 'password':'123456', >>> } :returns: ftp, ftpserverstr :rtype: :class:`ftplib.FTP` , str",1,0,0,0,1,1,0,0,1,2
"def parse_friends(self, friends_page):
  user_info = self.parse_sidebar(friends_page)
  second_col = friends_page.find(u'div', {u'id': u'content'}).find(u'table').find(u'tr').find_all(u'td', recursive=False)[1]
  try:
  user_info[u'friends'] = {}
  friends = second_col.find_all(u'div', {u'class': u'friendHolder'})
  if friends:
  for row in friends:
  block = row.find(u'div', {u'class': u'friendBlock'})
  cols = block.find_all(u'div')
  friend_link = cols[1].find(u'a')
  friend = self.session.user(friend_link.text)
  friend_info = {}
  if len(cols) > 2 and cols[2].text != u'':
  friend_info[u'last_active'] = utilities.parse_profile_date(cols[2].text.strip())
  if len(cols) > 3 and cols[3].text != u'':
  friend_info[u'since'] = utilities.parse_profile_date(cols[3].text.replace(u'Friends since', '').strip())
  user_info[u'friends'][friend] = friend_info
  except:
  if not self.session.suppress_parse_exceptions:
  raise
  return user_info",Parses the DOM and returns user friends attributes. :type friends_page: :class:`bs4.BeautifulSoup` :param friends_page: MAL user friends page's DOM :rtype: dict :return: User friends attributes.,1,0,0,1,2,1,0,0,1,2
"def run_sql_query(self, sql, required=False, query_params=[]):
  try:
  cursor = self.connection.execute(sql, query_params)
  except sqlite3.OperationalError as e:
  error_message = e.message if hasattr(e, 'message') else str(e)
  logger.warn(
  ""Encountered error \""%s\"" from query \""%s\"" with parameters %s"",
  error_message,
  sql,
  query_params)
  raise
  results = cursor.fetchall()
  if required and not results:
  raise ValueError(
  ""No results found for query:\n%s\nwith parameters: %s"" % (
  sql, query_params))
  return results","Given an arbitrary SQL query, run it against the database and return the results. Parameters ---------- sql : str SQL query required : bool Raise an error if no results found in the database query_params : list For each '?' in the query there must be a corresponding value in this list.",0,0,1,0,1,0,0,1,0,1
"def usermacro_create(macro, value, hostid, **kwargs):
  conn_args = _login(**kwargs)
  ret = {}
  try:
  if conn_args:
  params = {}
  method = 'usermacro.create'
  if macro:
  if isinstance(macro, dict):
  macro = ""{"" + six.text_type(macro.keys()[0]) +""}""
  if not macro.startswith('{') and not macro.endswith('}'):
  macro = ""{"" + macro + ""}""
  params['macro'] = macro
  params['value'] = value
  params['hostid'] = hostid
  params = _params_extend(params, _ignore_name=True, **kwargs)
  ret = _query(method, params, conn_args['url'], conn_args['auth'])
  return ret['result']['hostmacroids'][0]
  else:
  raise KeyError
  except KeyError:
  return ret","Create new host usermacro. :param macro: name of the host usermacro :param value: value of the host usermacro :param hostid: hostid or templateid :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring) :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring) :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring) return: ID of the created host usermacro. CLI Example: .. code-block:: bash salt '*' zabbix.usermacro_create '{$SNMP_COMMUNITY}' 'public' 1",1,0,0,2,3,1,0,0,1,2
"def get_active_for(self, user, user_agent=_MARK, ip_address=_MARK):
  conditions = [LoginSession.user == user]
  if user_agent is not _MARK:
  if user_agent is None:
  user_agent = request.environ.get(""HTTP_USER_AGENT"", """")
  conditions.append(LoginSession.user_agent == user_agent)
  if ip_address is not _MARK:
  if ip_address is None:
  ip_addresses = request.headers.getlist(""X-Forwarded-For"")
  ip_address = ip_addresses[0] if ip_addresses else request.remote_addr
  conditions.append(LoginSession.ip_address == ip_address)
  session = (
  LoginSession.query.filter(*conditions)
  .order_by(LoginSession.id.desc())
  .first()
  )
  return session","Return last known session for given user. :param user: user session :type user: `abilian.core.models.subjects.User` :param user_agent: *exact* user agent string to lookup, or `None` to have user_agent extracted from request object. If not provided at all, no filtering on user_agent. :type user_agent: string or None, or absent :param ip_address: client IP, or `None` to have ip_address extracted from request object (requires header 'X-Forwarded-For'). If not provided at all, no filtering on ip_address. :type ip_address: string or None, or absent :rtype: `LoginSession` or `None` if no session is found.",1,0,0,1,2,1,0,1,1,3
"def from_connection_string(s):
 cred = KerberosCredential()
 cred.domain, t = s.split('/', 1)
 cred.username, t = t.split('/', 1)
 secret_type, t = t.split(':', 1)
 secret, target = t.rsplit('@', 1)
 st = KerberosSecretType(secret_type.upper())
 if st == KerberosSecretType.PASSWORD or st == KerberosSecretType.PW or st == KerberosSecretType.PASS:
 cred.password = secret
 elif st == KerberosSecretType.NT or st == KerberosSecretType.RC4:
 cred.nt_hash = secret
 cred.kerberos_key_rc4 = secret
 elif st == KerberosSecretType.AES:
 cred.kerberos_key_aes_256 = secret
 cred.kerberos_key_aes_128 = secret
 elif st == KerberosSecretType.DES:
 cred.kerberos_key_des = secret
 elif st == KerberosSecretType.DES3 or st == KerberosSecretType.TDES:
 cred.kerberos_key_des3 = secret
 elif st == KerberosSecretType.CCACHE:
 cred.ccache = CCACHE.from_file(secret)
 return cred",Credential input format: <domain>/<username>/<secret_type>:<secret>@<dc_ip_or_hostname>,1,0,0,0,1,1,0,0,1,2
"def from_ini(ini_file, section=None):
  if not os.path.exists(ini_file):
  raise RuntimeError('Could not find ini file %s' % ini_file)
  cp = configparser.ConfigParser()
  cp.read(ini_file)
  if not cp.sections():
  raise RuntimeError('Could not find any section in ini file %s' % ini_file)
  if section:
  sec_list = [section]
  elif len(cp.sections()) == 1:
  sec_list = cp.sections()
  else:
  sec_list = ['hana', 'pytest']
  for sec in sec_list:
  try:
  param_values = cp.items(sec)
  except configparser.NoSectionError:
  continue
  params = dict(param_values)
  break
  else:
  raise RuntimeError('Could not guess which section to use for hana credentials from %s' % ini_file)
  def rm_prefix(param):
  return param[5:] if param.startswith('hana_') else param
  valid_keys = ('host', 'port', 'user', 'password')
  clean_params = {'%s' % rm_prefix(key): val for key, val in params.items() if rm_prefix(key) in valid_keys}
  return connect(**clean_params)","Make connection to database by reading connection parameters from an ini file. :param ini_file: Name of ini file, e.g. 'pytest.ini' :param section: specify alternative section in ini file. Section 'hana' and 'pytest' will be searched by default :return: connection object Example: [pytest] hana_host = 10.97.76.24 hana_hostname = mo-2384d0f48.mo.sap.corp hana_port = 30015 hana_user = D037732 hana_password = Abcd1234 For historical reasons a 'hana_' prefix is allowed, but will be removed automatically.",1,0,1,0,2,1,0,0,1,2
"def click(self):
  if self.is_checkable():
  if self.is_checked(): self.set_checked(False)
  else: self.set_checked(True)
  self.signal_clicked.emit(self.is_checked())
  else:
  self.signal_clicked.emit(True)
  return self","Pretends to user clicked it, sending the signal and everything.",1,0,0,1,2,1,0,0,1,2
"def getControllerStateWithPose(self, eOrigin, unControllerDeviceIndex, unControllerStateSize=sizeof(VRControllerState_t)):
  fn = self.function_table.getControllerStateWithPose
  pControllerState = VRControllerState_t()
  pTrackedDevicePose = TrackedDevicePose_t()
  result = fn(eOrigin, unControllerDeviceIndex, byref(pControllerState), unControllerStateSize, byref(pTrackedDevicePose))
  return result, pControllerState, pTrackedDevicePose",fills the supplied struct with the current state of the controller and the provided pose with the pose of the controller when the controller state was updated most recently. Use this form if you need a precise controller pose as input to your application when the user presses or releases a button. This function is deprecated in favor of the new IVRInput system.,1,0,0,1,2,1,0,0,1,2
"def get_session_key(limedict):
  url = limedict['url']
  user = limedict['username']
  password = limedict['password']
  params = {'username': user, 'password': password}
  data = set_params('get_session_key', params)
  req = requests.post(url, data=data, headers=headers)
  return {'token': req.json()['result'], 'user': user, 'url': url}","This function receive a dictionary with connection parameters. { ""url"": ""full path for remote control"", ""username: ""account name to be used"" ""password"" ""password for account""}",2,0,0,2,4,1,0,0,1,2
"def confirm_or_abort(prompt, exitcode=os.EX_TEMPFAIL, msg=None, **extra_args):
  if click.confirm(prom1pt, **extra_args):
  return True
  else:
  if msg:
  sys.stderr.write(msg)
  sys.stderr.write('\n')
  sys.exit(exitcode)",Prompt user for confirmation and exit on negative reply. Arguments `prompt` and `extra_args` will be passed unchanged to `click.confirm`:func: (which is used for actual prompting). :param str prompt: Prompt string to display. :param int exitcode: Program exit code if negative reply given. :param str msg: Message to display before exiting.,1,0,0,1,2,1,0,0,1,2
"def to_pickle(self, path, compression='infer',
  protocol=pickle.HIGHEST_PROTOCOL):
  from pandas.io.pickle import to_pickle
  return to_pickle(self, path, compression=compression,
  protocol=protocol)","Pickle (serialize) object to file. Parameters ---------- path : str File path where the pickled object will be stored. compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, \ default 'infer' A string representing the compression to use in the output file. By default, infers from the file extension in specified path. .. versionadded:: 0.20.0 protocol : int Int which indicates which protocol should be used by the pickler, default HIGHEST_PROTOCOL (see [1]_ paragraph 12.1.2). The possible values are 0, 1, 2, 3, 4. A negative value for the protocol parameter is equivalent to setting its value to HIGHEST_PROTOCOL. .. [1] https://docs.python.org/3/library/pickle.html .. versionadded:: 0.21.0 See Also -------- read_pickle : Load pickled pandas object (or any object) from file. DataFrame.to_hdf : Write DataFrame to an HDF5 file. DataFrame.to_sql : Write DataFrame to a SQL database. DataFrame.to_parquet : Write a DataFrame to the binary parquet format. Examples -------- >>> original_df = pd.DataFrame({""foo"": range(5), ""bar"": range(5, 10)}) >>> original_df foo bar 0 0 5 1 1 6 2 2 7 3 3 8 4 4 9 >>> original_df.to_pickle(""./dummy.pkl"") >>> unpickled_df = pd.read_pickle(""./dummy.pkl"") >>> unpickled_df foo bar 0 0 5 1 1 6 2 2 7 3 3 8 4 4 9 >>> import os >>> os.remove(""./dummy.pkl"")",1,0,0,0,1,0,0,0,0,0
"def create(self, attributes, silent=False, hook=True):
  attributes = json.dumps(attributes)
  return self.transport.POST(url='/task/%s' % self.get_options(silent=silent, hook=hook),
  body=attributes,
  type='application/json')","https://developers.podio.com/doc/tasks/create-task-22419 Creates the task using the supplied attributes. If 'silent' is true, Podio will send no notifications to subscribed users and not post updates to the stream. If 'hook' is false webhooks will not be called.",1,0,0,2,3,1,0,0,1,2
"def get_working_login(self, database, username=None, password=None):
  login_user = None
  self.get_db(database, username=username, password=password,
  never_auth_with_admin=True)
  login_user = self.get_login_user(database)
  if login_user:
  username = login_user[""username""]
  password = (login_user[""password""] if ""password"" in login_user
  else None)
  return username, password","authenticate to the specified database starting with specified username/password (if present), try to return a successful login within 3 attempts",1,0,1,1,3,1,0,1,1,3
"def selected_mapping(self):
  value_map = {}
  tree_clone = self.treeClasses.invisibleRootItem().clone()
  for tree_branch in tree_clone.takeChildren():
  value_list = []
  for tree_leaf in tree_branch.takeChildren():
  value_list += [tree_leaf.data(0, Qt.UserRole)]
  if value_list:
  value_map[tree_branch.data(0, Qt.UserRole)] = value_list
  return value_map",Obtain the value-to-class mapping set by user. :returns: The complete mapping as a dict of lists. :rtype: dict,1,0,0,1,2,1,0,0,1,2
"def authenticate(self, username, password):
  r = requests.post(self.apiurl + ""/token"",
  params={""grant_type"": ""password"", ""username"": username, ""password"": password,
  ""client_id"": self.cid, ""client_secret"": self.csecret})
  if r.status_code != 200:
  raise ServerError
  jsd = r.json()
  if self.remember:
  self.token_storage[username] = {'token': jsd['access_token'], 'refresh': jsd['refresh_token'],
  'expiration': int(jsd['created_at']) + int(jsd['expires_in'])}
  return jsd['access_token'], int(jsd['expires_in']) + int(jsd['created_at']), jsd['refresh_token']","Obtain an oauth token. Pass username and password. Get a token back. If KitsuAuth is set to remember your tokens for this session, it will store the token under the username given. :param username: username :param password: password :param alias: A list of alternative names for a person if using the KitsuAuth token storage :return: A tuple of (token, expiration time in unix time stamp, refresh_token) or ServerError",2,0,0,2,4,2,0,0,1,3
"def get_fno_lot_sizes(self, cached=True, as_json=False):
  url = self.fno_lot_size_url
  req = Request(url, None, self.headers)
  res_dict = {}
  if cached is not True or self.__CODECACHE__ is None:
  res = self.opener.open(req)
  if res is not None:
  res = byte_adaptor(res)
  for line in res.read().split('\n'):
  if line != '' and re.search(',', line) and (line.casefold().find('symbol') == -1):
  (code, name) = [x.strip() for x in line.split(',')[1:3]]
  res_dict[code] = int(name)
  else:
  raise Exception('no response received')
  self.__CODECACHE__ = res_dict
  return self.render_response(self.__CODECACHE__, as_json)",returns a dictionary with key as stock code and value as stock name. It also implements cache functionality and hits the server only if user insists or cache is empty :return: dict,2,0,0,1,3,1,0,0,1,2
"def explainParam(self, param):
  param = self._resolveParam(param)
  values = []
  if self.isDefined(param):
  if param in self._defaultParamMap:
  values.append(""default: %s"" % self._defaultParamMap[param])
  if param in self._paramMap:
  values.append(""current: %s"" % self._paramMap[param])
  else:
  values.append(""undefined"")
  valueStr = ""("" + "", "".join(values) + "")""
  return ""%s: %s %s"" % (param.name, param.doc, valueStr)","Explains a single param and returns its name, doc, and optional default value and user-supplied value in a string.",0,0,0,1,1,1,0,0,1,2
"def get_paths_goobjs(go_objs, go_top=None, go2obj=None):
  go_paths = []
  go_all = set()
  pathobj = GoPaths()
  for go_obj in go_objs:
  if go_obj.id not in go_all:
  paths_curr = pathobj.get_paths_from_to(go_obj, go_top, True)
  if paths_curr:
  for path_goobjs in paths_curr:
  for path_goobj in path_goobjs:
  goid = path_goobj.id
  if goid not in go_all:
  go_all.add(goid)
  go2obj[goid] = path_goobj
  go_paths.extend(path for path in paths_curr)
  return go_paths, go_all","Given a list of GO objects, return: paths, user GOs as ints, all GO terms paths.",0,0,0,0,0,1,0,1,1,3
"def query_entry():
  allowed_str_args = ['name', 'symbol', 'status', 'uuid', 'locus_group', 'locus_type',
  'date_name_changed', 'date_modified', 'date_symbol_changed', 'date_approved_reserved',
  'ensembl_gene', 'vega', 'lncrnadb', 'horde', 'mirbase', 'iuphar', 'ucsc', 'snornabase',
  'pseudogeneorg', 'bioparadigmsslc', 'locationsortable', 'merops', 'location', 'cosmic', 'imgt'
  ]
  allowed_int_args = ['limit', 'identifier', 'orphanet', 'entrez', ]
  args = get_args(
  request_args=request.args,
  allowed_int_args=allowed_int_args,
  allowed_str_args=allowed_str_args
  )
  return jsonify(query.hgnc(**args))","Returns list of HGNC entries by query paramaters --- tags: - Query functions parameters: - name: name in: query type: string required: false description: 'HGNC approved name for the gene' default: 'lysine demethylase 1A' - name: symbol in: query type: string required: false description: 'HGNC symbol' default: KDM1A - name: identifier in: query type: integer required: false description: 'HGNC ID. A unique ID created by the HGNC for every approved symbol' default: 29079 - name: status in: query type: string required: false description: 'Status of the symbol report, which can be either ""Approved"" or ""Entry Withdrawn""' default: Approved - name: uuid in: query type: string required: false description: 'universally unique identifier' default: '032998f3-5339-4c36-a521-1960c2f86cee' - name: orphanet in: query type: integer required: false description: 'Orphanet database identifier' default: 478263 - name: locus_group in: query type: string required: false description: 'group name for a set of related locus types as defined by the HGNC' default: 'protein-coding gene' - name: locus_type in: query type: string required: false description: 'locus type as defined by the HGNC' default: 'gene with protein product' - name: date_name_changed in: query type: string required: false description: 'date the gene name was last changed' default: '2016-02-12' - name: date_modified in: query type: string required: false description: 'date the entry was last modified' default: '2017-07-07' - name: date_symbol_changed in: query type: string required: false description: 'date the gene symbol was last changed' default: '2009-09-29' - name: date_approved_reserved in: query type: string required: false description: 'date the entry was first approved' default: '2004-02-26' - name: ensembl_gene in: query type: string required: false description: 'Ensembl gene ID. Found within the ""GENE RESOURCES"" section of the gene symbol report' default: 'ENSG00000004487' - name: vega in: query type: string required: false description: 'Vega gene ID. Found within the ""GENE RESOURCES"" section of the gene symbol report' default: 'OTTHUMG00000003220' - name: lncrnadb in: query type: string required: false description: 'Noncoding RNA Database identifier' default: - name: horde in: query type: string required: false description: 'symbol used within HORDE for the gene (not available in JSON)' default: - name: entrez in: query type: string required: false description: 'Entrez gene ID. Found within the ""GENE RESOURCES"" section of the gene symbol report' default: 23028 - name: mirbase in: query type: string required: false description: 'miRBase ID' default: - name: iuphar in: query type: string required: false description: 'The objectId used to link to the IUPHAR/BPS Guide to PHARMACOLOGY database' default: 'objectId:2669' - name: ucsc in: query type: string required: false description: 'UCSC gene ID. Found within the ""GENE RESOURCES"" section of the gene symbol report' default: 'uc001bgi.3' - name: snornabase in: query type: string required: false description: 'snoRNABase ID' default: - name: pseudogeneorg in: query type: string required: false description: 'Pseudogene.org ID' default: - name: bioparadigmsslc in: query type: string required: false description: 'Symbol used to link to the SLC tables database at bioparadigms.org for the gene' default: - name: locationsortable in: query type: string required: false description: 'locations sortable' default: 01p36.12 - name: merops in: query type: string required: false description: 'ID used to link to the MEROPS peptidase database' default: - name: location in: query type: string required: false description: 'Cytogenetic location of the gene (e.g. 2q34)' default: 1p36.12 - name: cosmic in: query type: string required: false description: 'Symbol used within the Catalogue of somatic mutations in cancer for the gene' default: 'KDM1A' - name: imgt in: query type: string required: false description: 'Symbol used within international ImMunoGeneTics information system' default: - name: limit in: query type: integer required: false default: 1",0,0,0,1,1,1,0,0,1,2
"def invite(self, users, role, expiration=1440):
  params = {
  ""f"" : ""json"",
  ""users"" : users,
  ""role"" : role,
  ""expiration"" : expiration
  }
  return self._post(url=self._url + ""/invite"",
  securityHandler=self._securityHandler,
  param_dict=params,
  proxy_url=self._proxy_url,
  proxy_port=self._proxy_port)","A group administrator can invite users to join their group using the Invite to Group operation. This creates a new user invitation, which the users accept or decline. The role of the user and the invitation expiration date can be set in the invitation. A notification is created for the user indicating that they were invited to join the group. Available only to authenticated users. Inputs: users - A comma separated list of usernames to be invited to the group. If a user is already a member of the group or an invitation has already been sent, the call still returns a success. Example: users=regularusername1,regularusername2 role - Allows administrators to set the user's role in the group Roles are: group_member: Ability to view and share items with group. group_admin: In addition to viewing and sharing items, the group_admin has the same capabilities as the group owner-invite users to the group, accept or decline group applications, delete content, and remove users. expiration - Expiration date on the invitation can be set for one day, three days, one week, or two weeks, in minutes. Default is 1440",1,0,0,2,3,2,0,0,2,4
"def validate_account_user_email(self, account_id, user_id, **kwargs):
  kwargs['_return_http_data_only'] = True
  if kwargs.get('asynchronous'):
  return self.validate_account_user_email_with_http_info(account_id, user_id, **kwargs)
  else:
  (data) = self.validate_account_user_email_with_http_info(account_id, user_id, **kwargs)
  return data","Validate the user email. # noqa: E501 An endpoint for validating the user email. **Example usage:** `curl -X POST https://api.us-east-1.mbedcloud.com/v3/accounts/{accountID}/users/{user-id}/validate-email -H 'Authorization: Bearer API_KEY'` # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass asynchronous=True >>> thread = api.validate_account_user_email(account_id, user_id, asynchronous=True) >>> result = thread.get() :param asynchronous bool :param str account_id: Account ID. (required) :param str user_id: The ID of the user whose email is validated. (required) :return: None If the method is called asynchronously, returns the request thread.",1,0,0,2,3,1,0,0,1,2
"def extract_to(self, *, stream=None, fileprefix=''):
  if bool(stream) == bool(fileprefix):
  raise ValueError(""Cannot set both stream and fileprefix"")
  if stream:
  return self._extract_to_stream(stream=stream)
  bio = BytesIO()
  extension = self._extract_to_stream(stream=bio)
  bio.seek(0)
  filepath = Path(Path(fileprefix).name + extension)
  with filepath.open('wb') as target:
  copyfileobj(bio, target)
  return str(filepath)","Attempt to extract the image directly to a usable image file If possible, the compressed data is extracted and inserted into a compressed image file format without transcoding the compressed content. If this is not possible, the data will be decompressed and extracted to an appropriate format. Because it is not known until attempted what image format will be extracted, users should not assume what format they are getting back. When saving the image to a file, use a temporary filename, and then rename the file to its final name based on the returned file extension. Examples: >>> im.extract_to(stream=bytes_io) '.png' >>> im.extract_to(fileprefix='/tmp/image00') '/tmp/image00.jpg' Args: stream: Writable stream to write data to. fileprefix (str or Path): The path to write the extracted image to, without the file extension. Returns: str: If *fileprefix* was provided, then the fileprefix with the appropriate extension. If no *fileprefix*, then an extension indicating the file type.",1,0,0,1,2,1,0,0,1,2
"def sendMessage(self, to, message, extra={}):
  to = to if isinstance(to, list) else [to]
  to = [str(num) for num in to]
  data = {'to': to, 'text': message}
  data = self.merge(data, {'callback': 7, 'mo': 1}, extra)
  content = self.parseRest(self.request('rest/message', data, {}, 'POST'));
  result = []
  for entry in content['message']:
  entry = self.merge({'apiMessageId': False, 'to': data['to'][0], 'error': False, 'errorCode': False}, entry)
  result.append({
  'id': entry['apiMessageId'].encode('utf-8'),
  'destination': entry['to'].encode('utf-8'),
  'error': entry['error']['description'].encode('utf-8') if entry['error'] != False else False,
  'errorCode': entry['error']['code'].encode('utf-8') if entry['error'] != False else False
  });
  return result","If the 'to' parameter is a single entry, we will parse it into a list. We will merge default values into the request data and the extra parameters provided by the user.",2,0,0,2,4,1,0,0,2,3
"def send_sticker(chat_id, sticker,
  reply_to_message_id=None, reply_markup=None,
  **kwargs):
  files = None
  if isinstance(sticker, InputFile):
  files = [sticker]
  sticker = None
  elif not isinstance(sticker, str):
  raise Exception('sticker must be instance of InputFile or str')
  params = dict(
  chat_id=chat_id,
  sticker=sticker
  )
  params.update(
  _clean_params(
  reply_to_message_id=reply_to_message_id,
  reply_markup=reply_markup
  )
  )
  return TelegramBotRPCRequest('sendSticker', params=params, files=files, on_result=Message.from_result, **kwargs)",":param chat_id: Unique identifier for the message recipient  User or GroupChat id :param sticker: Sticker to send. You can either pass a file_id as String to resend a sticker that is already on the Telegram servers, or upload a new sticker using multipart/form-data. :param reply_to_message_id: If the message is a reply, ID of the original message :param reply_markup: Additional interface options. A JSON-serialized object for a custom reply keyboard, instructions to hide keyboard or to force a reply from the user. :param \*\*kwargs: Args that get passed down to :class:`TelegramBotRPCRequest` :type chat_id: int :type sticker: InputFile or str :type reply_to_message_id: int :type reply_markup: ReplyKeyboardMarkup or ReplyKeyboardHide or ForceReply :returns: On success, the sent Message is returned. :rtype: TelegramBotRPCRequest",1,0,0,2,3,1,0,0,2,3
"def remover(self, id_user_group):
  if not is_valid_int_param(id_user_group):
  raise InvalidParameterError(
  u'Invalid or inexistent user group id.')
  url = 'ugroup/' + str(id_user_group) + '/'
  code, xml = self.submit(None, 'DELETE', url)
  return self.response(code, xml)",Removes a user group by its id. :param id_user_group: User Group's identifier. Valid integer greater than zero. :return: None :raise GrupoUsuarioNaoExisteError: User Group not found. :raise InvalidParameterError: User Group id is invalid or none. :raise DataBaseError: Networkapi failed to access database. :raise XMLError: Networkapi fails generating response XML.,1,0,0,2,3,2,0,0,1,3
"def admin_authenticate(self, password):
  auth_params = {
  'USERNAME': self.username,
  'PASSWORD': password
  }
  self._add_secret_hash(auth_params, 'SECRET_HASH')
  tokens = self.client.admin_initiate_auth(
  UserPoolId=self.user_pool_id,
  ClientId=self.client_id,
  AuthFlow='ADMIN_NO_SRP_AUTH',
  AuthParameters=auth_params,
  )
  self.verify_token(tokens['AuthenticationResult']['IdToken'], 'id_token','id')
  self.refresh_token = tokens['AuthenticationResult']['RefreshToken']
  self.verify_token(tokens['AuthenticationResult']['AccessToken'], 'access_token','access')
  self.token_type = tokens['AuthenticationResult']['TokenType']",Authenticate the user using admin super privileges :param password: User's password :return:,2,0,0,2,4,1,0,0,1,2
"def set_actor(user, sender, instance, signal_duid, **kwargs):
  if hasattr(threadlocal, 'auditlog'):
  if signal_duid != threadlocal.auditlog['signal_duid']:
  return
  try:
  app_label, model_name = settings.AUTH_USER_MODEL.split('.')
  auth_user_model = apps.get_model(app_label, model_name)
  except ValueError:
  auth_user_model = apps.get_model('auth', 'user')
  if sender == LogEntry and isinstance(user, auth_user_model) and instance.actor is None:
  instance.actor = user
  instance.remote_addr = threadlocal.auditlog['remote_addr']","Signal receiver with an extra, required 'user' kwarg. This method becomes a real (valid) signal receiver when it is curried with the actor.",0,1,1,0,2,1,0,1,0,2
"def create_user(self, user, account_id=None):
  if account_id is None:
  account_id = self._canvas_account_id
  if account_id is None:
  raise MissingAccountID()
  url = ACCOUNTS_API.format(account_id) + ""/users""
  data = self._post_resource(url, user.post_data())
  return CanvasUser(data=data)",Create and return a new user and pseudonym for an account. https://canvas.instructure.com/doc/api/users.html#method.users.create,1,0,0,1,2,2,0,0,2,4
"def set_users(users, test=False, commit=True, **kwargs):
  return __salt__['net.load_template']('set_users',
  users=users,
  test=test,
  commit=commit,
  inherit_napalm_device=napalm_device)","Configures users on network devices. :param users: Dictionary formatted as the output of the function config() :param test: Dry run? If set as True, will apply the config, discard and return the changes. Default: False :param commit: Commit? (default: True) Sometimes it is not needed to commit the config immediately after loading the changes. E.g.: a state loads a couple of parts (add / remove / update) and would not be optimal to commit after each operation. Also, from the CLI when the user needs to apply the similar changes before committing, can specify commit=False and will not discard the config. :raise MergeConfigException: If there is an error on the configuration sent. :return a dictionary having the following keys: - result (bool): if the config was applied successfully. It is `False` only in case of failure. In case there are no changes to be applied and successfully performs all operations it is still `True` and so will be the `already_configured` flag (example below) - comment (str): a message for the user - already_configured (bool): flag to check if there were no changes applied - diff (str): returns the config changes applied CLI Example: .. code-block:: bash salt '*' users.set_users ""{'mircea': {}}""",1,0,0,1,2,1,1,0,1,3
"def dimensions(self):
  try:
  call = fcntl.ioctl(self.termfd, termios.TIOCGWINSZ, ""\000"" * 8)
  except IOError:
  return (79, 40)
  else:
  height, width = struct.unpack(""hhhh"", call)[:2]
  return (width, height)","Returns terminal dimensions Don't save this information for long periods of time because the user might resize their terminal. :return: Returns ``(width, height)``. If there's no terminal to be found, we'll just return ``(79, 40)``.",0,0,0,1,1,1,0,0,1,2
"def load(self, **kwargs):
  if isinstance(self._data, dask_array_type):
  self._data = as_compatible_data(self._data.compute(**kwargs))
  elif not isinstance(self._data, np.ndarray):
  self._data = np.asarray(self._data)
  return self","Manually trigger loading of this variable's data from disk or a remote source into memory and return this variable. Normally, it should not be necessary to call this method in user code, because all xarray functions should either work on deferred data or load data automatically. Parameters ---------- **kwargs : dict Additional keyword arguments passed on to ``dask.array.compute``. See Also -------- dask.array.compute",0,0,0,0,0,0,0,0,1,1
"def create_heroku_connect_schema(using=DEFAULT_DB_ALIAS):
  connection = connections[using]
  with connection.cursor() as cursor:
  cursor.execute(_SCHEMA_EXISTS_QUERY, [settings.HEROKU_CONNECT_SCHEMA])
  schema_exists = cursor.fetchone()[0]
  if schema_exists:
  return False
  cursor.execute(""CREATE SCHEMA %s;"", [AsIs(settings.HEROKU_CONNECT_SCHEMA)])
  with connection.schema_editor() as editor:
  for model in get_heroku_connect_models():
  editor.create_model(model)
  editor.execute('CREATE EXTENSION IF NOT EXISTS ""hstore"";')
  from heroku_connect.models import (TriggerLog, TriggerLogArchive)
  for cls in [TriggerLog, TriggerLogArchive]:
  editor.create_model(cls)
  return True","Create Heroku Connect schema. Note: This function is only meant to be used for local development. In a production environment the schema will be created by Heroku Connect. Args: using (str): Alias for database connection. Returns: bool: ``True`` if the schema was created, ``False`` if the schema already exists.",1,1,0,0,2,0,1,1,0,2
"def save(self, db):
  assert self.collection
  self._ensure_indexes(db)
  outgoing = dict(dict_to_db(self, self.structure))
  object_id = db[self.collection].save(outgoing)
  if self.get('_id') is None:
  self['_id'] = object_id
  else:
  pass
  return object_id",Saves the object to given database. Usage:: item = Item(title=u'Hello') item.save(db) Collection name is taken from :attr:`MongoBoundDictMixin.collection`.,0,1,0,0,1,0,1,1,0,2
"def complete(request, provider):
  data = request.GET.copy()
  data.update(request.POST)
  if 'next_url' not in request.session:
  request.session['next_url'] = request.GET.get(""next"") or settings.LOGIN_REDIRECT_URL
  backend = get_backend(provider)
  response = backend.validate(request, data)
  if isinstance(response, HttpResponseRedirect):
  return response
  if request.user.is_authenticated():
  success = backend.login_user(request)
  backend.merge_accounts(request)
  else:
  success = backend.login_user(request)
  if not success and not settings.REGISTRATION_ALLOWED:
  messages.warning(request, lang.REGISTRATION_DISABLED)
  return redirect(settings.REGISTRATION_DISABLED_REDIRECT)
  if success:
  return redirect(request.session.pop('next_url', settings.LOGIN_REDIRECT_URL))
  return backend.complete(request, response)","After first step of net authentication, we must validate the response. If everything is ok, we must do the following: 1. If user is already authenticated: a. Try to login him again (strange variation but we must take it to account). b. Create new netID record in database. c. Merge authenticated account with newly created netID record. d. Redirect user to 'next' url stored in session. 2. If user is anonymouse: a. Try to log him by identity and redirect to 'next' url. b. Create new netID record in database. c. Try to automaticaly fill all extra fields with information returned form server. If successfull, login the user and redirect to 'next' url. d. Redirect user to extra page where he can fill all extra fields by hand.",2,2,2,1,7,1,0,0,1,2
"def __make_security_group_api_request(server_context, api, user_ids, group_id, container_path):
  url = server_context.build_url(security_controller, api, container_path)
  if not hasattr(user_ids, ""__iter__""):
  user_ids = [user_ids]
  return server_context.make_request(url, {
  'groupId': group_id,
  'principalIds': user_ids
  })",Execute a request against the LabKey Security Controller Group Membership apis :param server_context: A LabKey server context. See utils.create_server_context. :param api: Action to execute :param user_ids: user ids to apply action to :param group_id: group id to apply action to :param container_path: Additional container context path :return: Request json object,0,1,0,1,2,1,0,0,1,2
"def delete(self, *, auto_commit=False):
  try:
  db.session.delete(self.resource)
  if auto_commit:
  db.session.commit()
  except SQLAlchemyError:
  self.log.exception('Failed deleting resource: {}'.format(self.id))
  db.session.rollback()",Removes a resource from the database Args: auto_commit (bool): Automatically commit the transaction. Default: `False` Returns: `None`,0,1,0,0,1,0,1,1,0,2
"def VerifierMiddleware(verifier):
  @wraps(verifier.verify)
  def wrapper(environ, start_response):
  data = get_post(environ)
  kwargs = dict(urlparse.parse_qsl(data))
  kwargs[""state""] = json.loads(urllib.unquote(kwargs[""state""]))
  val, completed = verifier.verify(**kwargs)
  if not completed:
  return val(environ, start_response)
  if val:
  set_cookie, cookie_value = verifier.create_cookie(val, ""auth"")
  cookie_value += ""; path=/""
  url = ""{base_url}?{query_string}"".format(
  base_url=""/authorization"",
  query_string=kwargs[""state""][""query""])
  response = SeeOther(url, headers=[(set_cookie, cookie_value)])
  return response(environ, start_response)
  else:
  url = ""{base_url}?{query_string}"".format(
  base_url=""/authorization"",
  query_string=kwargs[""state""][""query""])
  response = SeeOther(url)
  return response(environ, start_response)
  return wrapper",Common wrapper for the authentication modules. * Parses the request before passing it on to the authentication module. * Sets 'pyoidc' cookie if authentication succeeds. * Redirects the user to complete the authentication. * Allows the user to retry authentication if it fails. :param verifier: authentication module,2,0,0,1,3,1,0,0,1,2
"def get(self, id_vlan):
  if not is_valid_int_param(id_vlan):
  raise InvalidParameterError(
  u'Parameter id_vlan is invalid. Value: ' +
  id_vlan)
  url = 'vlan/' + str(id_vlan) + '/network/'
  code, xml = self.submit(None, 'GET', url)
  return get_list_map(
  self.response(
  code, xml, [
  'redeipv4', 'redeipv6']), 'vlan')","Get a VLAN by your primary key. Network IPv4/IPv6 related will also be fetched. :param id_vlan: ID for VLAN. :return: Following dictionary: :: {'vlan': {'id': < id_vlan >, 'nome': < nome_vlan >, 'num_vlan': < num_vlan >, 'id_ambiente': < id_ambiente >, 'descricao': < descricao >, 'acl_file_name': < acl_file_name >, 'acl_valida': < acl_valida >, 'acl_file_name_v6': < acl_file_name_v6 >, 'acl_valida_v6': < acl_valida_v6 >, 'ativada': < ativada >, 'redeipv4': [ { all networkipv4 related } ], 'redeipv6': [ { all networkipv6 related } ] } } :raise InvalidParameterError: Invalid ID for VLAN. :raise VlanNaoExisteError: VLAN not found. :raise DataBaseError: Networkapi failed to access the database. :raise XMLError: Networkapi failed to generate the XML response.",1,0,1,1,3,2,0,0,1,3
"def has_read_permission(self, request, path):
  user = request.user
  if not user.is_authenticated():
  return False
  elif user.is_superuser:
  return True
  elif user.is_staff:
  return True
  else:
  return False",Just return True if the user is an authenticated staff member. Extensions could base the permissions on the path too.,1,0,0,1,2,1,0,0,1,2
"def update(self, attributes=values.unset, assignment_status=values.unset,
  reason=values.unset, priority=values.unset,
  task_channel=values.unset):
  return self._proxy.update(
  attributes=attributes,
  assignment_status=assignment_status,
  reason=reason,
  priority=priority,
  task_channel=task_channel,
  )",Update the TaskInstance :param unicode attributes: The user-defined JSON data describing the custom attributes of this task. :param TaskInstance.Status assignment_status: A 'pending' or 'reserved' Task may be canceled by posting AssignmentStatus='canceled'. :param unicode reason: This is only required if the Task is canceled or completed. :param unicode priority: Override priority for the Task. :param unicode task_channel: The task_channel :returns: Updated TaskInstance :rtype: twilio.rest.taskrouter.v1.workspace.task.TaskInstance,1,0,0,1,2,1,0,0,1,2
"def query(self, query, param=None):
  with self.conn.cursor() as curs:
  print 'XXX QUERY', curs.mogrify(query, param)
  try:
  curs.execute(query, param)
  except BaseException as exc:
  msg = 'query: {}, param: {}, exc: {}'.format(query, param, exc)
  if hasattr(exc, 'pgcode'):
  msg = '{}, exc code: {}'.format(msg, exc.pgcode)
  print msg
  handle_exc(exc)
  result = curs.fetchall()
  return result",Perform a SQL based query This will abort on a failure to communicate with the database. :query: string query :params: parameters for the query :return: RecordList from psycopg2,0,0,1,1,2,0,0,1,1,2
"def _default_transform_fn(self, model, content, content_type, accept):
  try:
  data = self._input_fn(content, content_type)
  except _errors.UnsupportedFormatError as e:
  return self._error_response(e, http_client.UNSUPPORTED_MEDIA_TYPE)
  prediction = self._predict_fn(data, model)
  try:
  result = self._output_fn(prediction, accept)
  except _errors.UnsupportedFormatError as e:
  return self._error_response(e, http_client.NOT_ACCEPTABLE)
  return result","Make predictions against the model and return a serialized response. This serves as the default implementation of transform_fn, used when the user has not implemented one themselves. Args: model (obj): model loaded by model_fn. content: request content. content_type (str): the request Content-Type. accept (str): accept content-type expected by the client. Returns: sagemaker_containers.beta.framework.worker.Response or tuple: the serialized response data and its content type, either as a Response object or a tuple of the form (response_data, content_type)",0,0,0,1,1,1,0,0,1,2
"def respond(self, text, sessionID = ""general""):
  text = self.__normalize(text)
  previousText = self.__normalize(self.conversation[sessionID][-2])
  text_correction = self.__correction(text)
  current_topic = self.topic[sessionID]
  current_topic_order = current_topic.split(""."")
  while current_topic_order:
  try:return self.__response_on_topic(text, previousText, text_correction, current_topic, sessionID)
  except ValueError as e:pass
  current_topic_order.pop()
  current_topic = ""."".join(current_topic_order)
  try:return self.__response_on_topic(text, previousText, text_correction, current_topic, sessionID)
  except ValueError as e:return ""Sorry I couldn't find anything relevant""",Generate a response to the user input. :type text: str :param text: The string to be mapped :rtype: str,1,0,0,1,2,1,0,0,1,2
"def oauth_get_user(client_id, account_info=None, access_token=None):
  if access_token:
  token = RemoteToken.get_by_token(client_id, access_token)
  if token:
  return token.remote_account.user
  if account_info:
  external_id = _get_external_id(account_info)
  if external_id:
  user_identity = UserIdentity.query.filter_by(
  id=external_id['id'], method=external_id['method']).first()
  if user_identity:
  return user_identity.user
  email = account_info.get('user', {}).get('email')
  if email:
  return User.query.filter_by(email=email).one_or_none()
  return None",Retrieve user object for the given request. Uses either the access token or extracted account information to retrieve the user object. :param client_id: The client id. :param account_info: The dictionary with the account info. (Default: ``None``) :param access_token: The access token. (Default: ``None``) :returns: A :class:`invenio_accounts.models.User` instance or ``None``.,1,0,1,1,3,2,0,1,1,4
