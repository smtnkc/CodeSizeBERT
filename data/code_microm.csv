code,docstring,interaction,communication,process,microm
"async def _bind_key_to_queue(self, routing_key: AnyStr, queue_name: AnyStr) -> None:
        
        logger.info(""Binding key='%s'"", routing_key)

        result = await self._channel.queue_bind(
            exchange_name=self._exchange_name,
            queue_name=queue_name,
            routing_key=routing_key,
        )
        return result","Bind to queue with specified routing key.

        :param routing_key: Routing key to bind with.
        :param queue_name: Name of the queue
        :return: Does not return anything",0,2,0,2
"async def _defaultExpectHandler(request):  
    
    expect = request.headers.get(hdrs.EXPECT)
    if request.version == HttpVersion11:
        if expect.lower() == ""100-continue"":
            request.transport.write(b""HTTP/1.1 100 Continue\r\n\r\n"")
        else:
            raise HTTPExpectationFailed(text=""Unknown Expect: %s"" % expect)","Default handler for Expect header.

    Just send ""100 Continue"" to client.
    raise HTTPExpectationFailed if value of header is not ""100-continue""",1,0,2,3
"async def _handle_container_hard_timeout(self, container_id, hard_timeout):
        
        if container_id in self._watching:
            self._logger.info(""Killing container %s as it used its %i wall time seconds"",
                              container_id, hard_timeout)
            await self._kill_it_with_fire(container_id)","Kills a container (should be called with loop.call_later(hard_timeout, ...)) and displays a message on the log
        :param container_id:
        :param hard_timeout:
        :return:",0,1,1,2
"async def _next_event(self):
        
        while True:
            for event in self._connection.events():
                if isinstance(event, Message):
                    
                    if event.message_finished:
                        return self._wrap_data(self._gather_buffers(event))
                    self._buffer(event)
                    break  
                else:
                    return event

            data = await self._sock.receive_some(4096)
            if not data:
                return CloseConnection(code=500, reason=""Socket closed"")
            self._connection.receive_data(data)",Gets the next event.,0,2,1,3
"async def _process_access_form(self, html: str) -> (str, str):
        
        
        p = AccessPageParser()
        p.feed(html)
        p.close()

        form_url = p.url
        form_data = dict(p.inputs)

        
        url, html = await self.driver.post_text(form_url, form_data)
        return url, html","Parsing page with access rights

        :param html: html page
        :return: url and  html from redirected page",0,1,1,2
"async def _send_email(email_, config, loop=asyncio.get_event_loop()):
    
    smtp_server = get_attribute_from_config(config, EMAIL_SECTION_KEY, SMTP_SERVER_KEY)
    smtp_port = int(get_attribute_from_config(config, EMAIL_SECTION_KEY, SMTP_PORT_KEY))
    user = get_attribute_from_config(config, EMAIL_SECTION_KEY, USER_KEY)
    password = get_attribute_from_config(config, EMAIL_SECTION_KEY, PASSWORD_KEY)
    server = aiosmtplib.SMTP(hostname=smtp_server, port=smtp_port, loop=loop, use_tls=False)
    await server.connect()
    await server.starttls()
    await server.login(user, password)
    await server.send_message(email_)
    await server.quit()","Send an email.

    Args:
        email_ (email.MIMEMultipart): The email to send.
        config (defaultdict): A defaultdict.",0,5,1,6
"async def _string_data(self, data):
        
        reply = ''
        data = data[1:-1]
        for x in data:
            reply_data = x
            if reply_data:
                reply += chr(reply_data)
        if self.log_output:
            logging.info(reply)
        else:
            print(reply)","This is a private message handler method.
        It is the message handler for String data messages that will be
        printed to the console.
        :param data:  message

        :returns: None - message is sent to console",0,1,1,2
"async def add_unknown_id(self, unknown_id, timeout=OTGW_DEFAULT_TIMEOUT):
        
        cmd = OTGW_CMD_UNKNOWN_ID
        unknown_id = int(unknown_id)
        if unknown_id < 1 or unknown_id > 255:
            return None
        ret = await self._wait_for_cmd(cmd, unknown_id, timeout)
        if ret is not None:
            return int(ret)","Inform the gateway that the boiler doesn't support the
        specified Data-ID, even if the boiler doesn't indicate that
        by returning an Unknown-DataId response. Using this command
        allows the gateway to send an alternative Data-ID to the boiler
        instead.
        Return the added ID, or None on failure.

        This method is a coroutine",0,1,2,3
"async def async_poller(client, initial_response, deserialization_callback, polling_method):
    

    try:
        client = client if isinstance(client, ServiceClientAsync) else client._client
    except AttributeError:
        raise ValueError(""Poller client parameter must be a low-level msrest Service Client or a SDK client."")
    response = initial_response.response if isinstance(initial_response, ClientRawResponse) else initial_response

    if isinstance(deserialization_callback, type) and issubclass(deserialization_callback, Model):
        deserialization_callback = deserialization_callback.deserialize

    
    polling_method.initialize(client, response, deserialization_callback)

    await polling_method.run()
    return polling_method.resource()","Async Poller for long running operations.

    :param client: A msrest service client. Can be a SDK client and it will be casted to a ServiceClient.
    :type client: msrest.service_client.ServiceClient
    :param initial_response: The initial call response
    :type initial_response: msrest.universal_http.ClientResponse or msrest.pipeline.ClientRawResponse
    :param deserialization_callback: A callback that takes a Response and return a deserialized object. If a subclass of Model is given, this passes ""deserialize"" as callback.
    :type deserialization_callback: callable or msrest.serialization.Model
    :param polling_method: The polling strategy to adopt
    :type polling_method: msrest.polling.PollingMethod",1,1,1,3
"async def async_send(self, request, **kwargs):
        
        kwargs.setdefault('stream', True)
        
        
        
        pipeline_response = await self.config.pipeline.run(request, **kwargs)
        response = pipeline_response.http_response
        response.context = pipeline_response.context
        return response","Prepare and send request object according to configuration.

        :param ClientRequest request: The request object to be sent.
        :param dict headers: Any headers to add to the request.
        :param content: Any body data to add to the request.
        :param config: Any specific config overrides",0,1,1,2
"async def claim_work(context):
    
    log.debug(""Calling claimWork..."")
    payload = {
        'workerGroup': context.config['worker_group'],
        'workerId': context.config['worker_id'],
        
        
        'tasks': 1,
    }
    try:
        return await context.queue.claimWork(
            context.config['provisioner_id'],
            context.config['worker_type'],
            payload
        )
    except (taskcluster.exceptions.TaskclusterFailure, aiohttp.ClientError) as exc:
        log.warning(""{} {}"".format(exc.__class__, exc))","Find and claim the next pending task in the queue, if any.

    Args:
        context (scriptworker.context.Context): the scriptworker context.

    Returns:
        dict: a dict containing a list of the task definitions of the tasks claimed.",0,3,1,4
"async def connect(
    host,
    port=22223,
    version=""1.19"",
    on_event=None,
    on_disconnect=None,
    timeout=5,
    loop=None,
) -> QRTConnection:
    
    loop = loop or asyncio.get_event_loop()

    try:
        _, protocol = await loop.create_connection(
            lambda: QTMProtocol(
                loop=loop, on_event=on_event, on_disconnect=on_disconnect
            ),
            host,
            port,
        )
    except (ConnectionRefusedError, TimeoutError, OSError) as exception:
        LOG.error(exception)
        return None

    try:
        await protocol.set_version(version)
    except QRTCommandException as exception:
        LOG.error(Exception)
        return None
    except TypeError as exception:  
        LOG.error(exception)
        return None

    return QRTConnection(protocol, timeout=timeout)","Async function to connect to QTM

    :param host: Address of the computer running QTM.
    :param port: Port number to connect to, should be the port configured for little endian.
    :param version: What version of the protocol to use, tested for 1.17 and above but could
        work with lower versions as well.
    :param on_disconnect: Function to be called when a disconnect from QTM occurs.
    :param on_event: Function to be called when there's an event from QTM.
    :param timeout: The default timeout time for calls to QTM.
    :param loop: Alternative event loop, will use asyncio default if None.

    :rtype: A :class:`.QRTConnection`",0,4,0,4
"async def create_link_secret(self, link_secret: str) -> None:
        

        LOGGER.debug('HolderProver.create_link_secret >>> link_secret: %s', link_secret)

        try:
            await anoncreds.prover_create_master_secret(self.wallet.handle, link_secret)
        except IndyError as x_indy:
            if x_indy.error_code == ErrorCode.AnoncredsMasterSecretDuplicateNameError:
                LOGGER.info('HolderProver did not create link secret - it already exists')
            else:
                LOGGER.debug(
                    'HolderProver.create_link_secret: <!< cannot create link secret %s, indy error code %s',
                    self.wallet.name,
                    x_indy.error_code)
                raise

        self._link_secret = link_secret
        LOGGER.debug('HolderProver.create_link_secret <<<')","Create link secret (a.k.a. master secret) used in proofs by HolderProver.

        Raise any IndyError causing failure to set link secret in wallet.

        :param link_secret: label for link secret; indy-sdk uses label to generate link secret",1,4,2,7
"async def data(self, email_message):
        
        code, message = await self.do_cmd(""DATA"", success=(354,))

        email_message = SMTP.prepare_message(email_message)

        self.writer.write(email_message)  
        await self.writer.drain()  

        code, message = await self.reader.read_reply()

        return code, message","Sends a SMTP 'DATA' command. - Transmits the message to the server.

        If ``email_message`` is a bytes object, sends it as it is. Else,
        makes all the required changes so it can be safely trasmitted to the
        SMTP server.`

        For further details, please check out `RFC 5321 § 4.1.1.4`_.

        Args:
            email_message (str or bytes): Message to be sent.

        Raises:
            ConnectionError subclass: If the connection to the server is
                unexpectedely lost.
            SMTPCommandFailedError: If the DATA command fails.

         Returns:
            (int, str): A (code, message) 2-tuple containing the server last
                response (the one the server sent after all data were sent by
                the client).

        .. seealso: :meth:`SMTP.prepare_message`

        .. _`RFC 5321 § 4.1.1.4`: https://tools.ietf.org/html/rfc5321#section-4.1.1.4",0,2,1,3
"async def deserialize(data: dict):
        
        issuer_credential = await IssuerCredential._deserialize(""vcx_issuer_credential_deserialize"",
                                                      json.dumps(data),
                                                      data.get('data').get('source_id'),
                                                      data.get('data').get('price'),
                                                      data.get('data').get('credential_attributes'),
                                                      data.get('data').get('schema_seq_no'),
                                                      data.get('data').get('credential_request'))
        return issuer_credential","Creates IssuerCredential object from a dict.
            :param data: dict representing a serialized IssuerCredential Object
            :return: IssuerCredential object

            Example:
            source_id = '1'
            cred_def_id = 'cred_def_id1'
            attrs = {'key': 'value', 'key2': 'value2', 'key3': 'value3'}
            name = 'Credential Name'
            issuer_did = '8XFh8yBzrpJQmNyZzgoTqB'
            phone_number = '8019119191'
            price = 1
            issuer_credential = await IssuerCredential.create(source_id, attrs, cred_def_id, name, price)
            data = await issuer_credential.serialize()
            issuer_credential2 = await IssuerCredential.deserialize(data)",0,0,1,1
"async def emit_event(self, event):
        
        self.log.info(""publishing event on %s"", self.publish_topic)
        if self.config.extra['config']['pub_options']['retain']:
            try:
                await persist_event(
                    self.publish_topic,
                    event,
                    self.pool
                )
            except SystemError as error:
                self.log.error(error)
                return

        loop = asyncio.get_event_loop()
        producer = AIOKafkaProducer(
            loop=loop,
            bootstrap_servers=self.transport_host
        )
        await producer.start()

        try:
            event = json.dumps(event.__dict__).encode()
            await producer.send_and_wait(
                self.publish_topic,
                event
            )
        finally:
            await producer.stop()","Publish an event
        :param event: Event object",0,4,0,4
"async def encoder_config(self, pin_a, pin_b, cb=None, cb_type=None,
                             hall_encoder=False):
        
        
        self.hall_encoder = hall_encoder
        data = [pin_a, pin_b]
        if cb:
            self.digital_pins[pin_a].cb = cb
        if cb_type:
            self.digital_pins[pin_a].cb_type = cb_type

        await self._send_sysex(PrivateConstants.ENCODER_CONFIG, data)","This command enables the rotary encoder support and will
        enable encoder reporting.

        This command is not part of StandardFirmata. For 2 pin + ground
        encoders, FirmataPlus is required to be used for 2 pin rotary encoder,
        and for hell effect wheel encoder support, FirmataPlusRB is required.

        Encoder data is retrieved by performing a digital_read from pin a
        (encoder pin_a).

        When using 2 hall effect sensors (e.g. 2 wheel robot)
        specify pin_a for 1st encoder and pin_b for 2nd encoder.

        :param pin_a: Encoder pin 1.

        :param pin_b: Encoder pin 2.

        :param cb: callback function to report encoder changes

        :param cb_type: Constants.CB_TYPE_DIRECT = direct call or
                        Constants.CB_TYPE_ASYNCIO = asyncio coroutine

        :param hall_encoder: wheel hall_encoder - set to
                             True to select hall encoder support support.

        :returns: No return value",0,1,1,2
"async def fetch_token(self):
        
        url = '{}/login'.format(API_URL)
        payload = 'email={}&password={}'.format(self._email, self._password)

        reg = await self.api_post(url, None, payload)
        if reg is None:
            _LOGGER.error('Unable to authenticate and fetch eight token.')
        else:
            self._userid = reg['session']['userId']
            self._token = reg['session']['token']
            self._expdate = reg['session']['expirationDate']
            _LOGGER.debug('UserID: %s, Token: %s', self._userid, self.token)",Fetch new session token from api.,0,3,1,4
"async def get_files_to_delete(self) -> List[str]:
        
        dir_name, base_name = os.path.split(self.absolute_file_path)
        file_names = await self.loop.run_in_executor(
            None, lambda: os.listdir(dir_name)
        )
        result = []
        prefix = base_name + "".""
        plen = len(prefix)
        for file_name in file_names:
            if file_name[:plen] == prefix:
                suffix = file_name[plen:]
                if self.ext_match.match(suffix):
                    result.append(os.path.join(dir_name, file_name))
        if len(result) < self.backup_count:
            return []
        else:
            return result[: len(result) - self.backup_count]",Determine the files to delete when rolling over.,0,0,1,1
"async def get_matches(self, force_update=False) -> list:
        
        if force_update or self.matches is None:
            res = await self.connection('GET',
                                        'tournaments/{}/matches'.format(self._id),
                                        include_attachments=1)
            self._refresh_matches_from_json(res)
        return self.matches or []","get all matches (once the tournament is started)

        |methcoro|

        Args:
            force_update (default=False): True to force an update to the Challonge API

        Returns:
            list[Match]:

        Raises:
            APIException",0,1,1,2
"async def get_neighbourhood(postcode_like: PostCodeLike) -> Optional[Neighbourhood]:
    
    try:
        postcode = await get_postcode(postcode_like)
    except CachingError as e:
        raise e
    else:
        if postcode is None:
            return None
        elif postcode.neighbourhood is not None:
            return postcode.neighbourhood

    try:
        data = await fetch_neighbourhood(postcode.lat, postcode.long)
    except ApiError as e:
        raise CachingError(f""Neighbourhood not in cache, and could not reach API: {e.status}"")

    if data is not None:
        neighbourhood = Neighbourhood.from_dict(data)
        locations = [Location.from_dict(neighbourhood, postcode, location) for location in data[""locations""]]
        links = [Link.from_dict(neighbourhood, link) for link in data[""links""]]

        with Neighbourhood._meta.database.atomic():
            neighbourhood.save()
            postcode.neighbourhood = neighbourhood
            postcode.save()
            for location in locations:
                location.save()
            for link in links:
                link.save()
    else:
        neighbourhood = None
    return neighbourhood","Gets a police neighbourhood from the database.
    Acts as a middleware between us and the API, caching results.
    :param postcode_like: The UK postcode to look up.
    :return: The Neighbourhood or None if the postcode does not exist.
    :raises CachingError: If the needed neighbourhood is not in cache, and the fetch isn't responding.

    todo save locations/links",2,2,3,7
"async def has_commit_landed_on_repository(self, context, revision):
        
        
        if not _is_git_full_hash(revision):
            revision = self.get_tag_hash(tag_name=revision)

        repo = self._github_repository.html_url

        url = '/'.join([repo.rstrip('/'), 'branch_commits', revision])
        html_data = await retry_request(context, url)
        html_text = html_data.strip()
        
        
        
        return html_text != ''","Tell if a commit was landed on the repository or if it just comes from a pull request.

        Args:
            context (scriptworker.context.Context): the scriptworker context.
            revision (str): the commit hash or the tag name.

        Returns:
            bool: True if the commit is present in one of the branches of the main repository",0,1,1,2
"async def list_transactions(self, request):
        
        paging_controls = self._get_paging_controls(request)
        validator_query = client_transaction_pb2.ClientTransactionListRequest(
            head_id=self._get_head_id(request),
            transaction_ids=self._get_filter_ids(request),
            sorting=self._get_sorting_message(request, ""default""),
            paging=self._make_paging_message(paging_controls))

        response = await self._query_validator(
            Message.CLIENT_TRANSACTION_LIST_REQUEST,
            client_transaction_pb2.ClientTransactionListResponse,
            validator_query)

        data = [self._expand_transaction(t) for t in response['transactions']]

        return self._wrap_paginated_response(
            request=request,
            response=response,
            controls=paging_controls,
            data=data)","Fetches list of txns from validator, optionally filtered by id.

        Request:
            query:
                - head: The id of the block to use as the head of the chain
                - id: Comma separated list of txn ids to include in results

        Response:
            data: JSON array of Transaction objects with expanded headers
            head: The head used for this query (most recent if unspecified)
            link: The link to this exact query, including head block
            paging: Paging info and nav, like total resources and a next link",1,1,2,4
"async def password_dialog(key, title, message, options):
    
    with PasswordDialog.create(key, title, message, options) as dialog:
        response = await dialog
        if response == Gtk.ResponseType.OK:
            return PasswordResult(dialog.get_text(),
                                  dialog.use_cache.get_active())
        return None","Show a Gtk password dialog.

    :returns: the password or ``None`` if the user aborted the operation
    :raises RuntimeError: if Gtk can not be properly initialized",0,1,1,2
"async def profile(self):
        

        state = self._state
        data = await state.http.get_user_profile(self.id)

        def transform(d):
            return state._get_guild(int(d['id']))

        since = data.get('premium_since')
        mutual_guilds = list(filter(None, map(transform, data.get('mutual_guilds', []))))
        return Profile(flags=data['user'].get('flags', 0),
                       premium_since=parse_time(since),
                       mutual_guilds=mutual_guilds,
                       user=self,
                       connected_accounts=data['connected_accounts'])","|coro|

        Gets the user's profile.

        .. note::

            This only applies to non-bot accounts.

        Raises
        -------
        Forbidden
            Not allowed to fetch profiles.
        HTTPException
            Fetching the profile failed.

        Returns
        --------
        :class:`Profile`
            The profile of the user.",0,1,2,3
"async def prover_search_credentials(wallet_handle: int,
                                    query_json: str) -> (int, int):
    

    logger = logging.getLogger(__name__)
    logger.debug(""prover_search_credentials: >>> wallet_handle: %r, query_json: %r"",
                 wallet_handle,
                 query_json)

    if not hasattr(prover_search_credentials, ""cb""):
        logger.debug(""prover_search_credentials: Creating callback"")
        prover_search_credentials.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_int32, c_uint))

    c_wallet_handle = c_int32(wallet_handle)
    c_query_json = c_char_p(query_json.encode('utf-8'))

    res = await do_call('indy_prover_search_credentials',
                        c_wallet_handle,
                        c_query_json,
                        prover_search_credentials.cb)

    logger.debug(""prover_search_credentials: <<< res: %r"", res)
    return res","Search for credentials stored in wallet.
    Credentials can be filtered by tags created during saving of credential.

    Instead of immediately returning of fetched credentials this call returns search_handle that can be used later
    to fetch records by small batches (with prover_credentials_search_fetch_records).

    :param wallet_handle: wallet handler (created by open_wallet).
    :param query_json: wql style filter for credentials searching based on tags.
        where wql query: indy-sdk/docs/design/011-wallet-query-language/README.md
    :return:
        search_handle: Search handle that can be used later to fetch records by small batches
            (with prover_credentials_search_fetch_records)
        total_count: Total count of records",0,5,0,5
"async def register(self, node, *,
                       check=None, service=None, write_token=None):
        
        node_id, node = prepare_node(node)
        service_id, service = prepare_service(service)
        _, check = prepare_check(check)
        if check and node_id:
            check[""Node""] = node_id
        if check and service_id:
            check[""ServiceID""] = service_id
        token = extract_attr(write_token, keys=[""ID""])
        entry = node
        if service:
            entry[""Service""] = service
        if check:
            entry[""Check""] = check
        if token:
            entry[""WriteRequest""] = {
                ""Token"": token
            }
        response = await self._api.put(""/v1/catalog/register"", data=entry)
        return response.status == 200","Registers a new node, service or check

        Parameters:
            node (Object): Node definition
            check (Object): Check definition
            service (Object or ObjectID): Service
            write_token (ObjectID): Token ID
        Returns:
            bool: ``True`` on success

        .. note:: it is usually preferrable instead to use the agent
                  endpoints for registration as they are simpler and
                  perform anti-entropy.

        **Node** body must look something like::

            {
                ""Datacenter"": ""dc1"",
                ""Node"": ""foobar"",
                ""Address"": ""192.168.10.10"",
                ""TaggedAddresses"": {
                    ""lan"": ""192.168.10.10"",
                    ""wan"": ""10.0.10.10""
                }
            }

        **Service** body must look something like::

            {
                ""ID"": ""redis1"",
                ""Service"": ""redis"",
                ""Tags"": [
                    ""master"",
                    ""v1""
                ],
                ""Address"": ""127.0.0.1"",
                ""Port"": 8000
            }

        **Check** body must look something like::

            {
                ""Node"": ""foobar"",
                ""CheckID"": ""service:redis1"",
                ""Name"": ""Redis health check"",
                ""Notes"": ""Script based health check"",
                ""Status"": ""passing"",
                ""ServiceID"": ""redis1""
            }

        The behavior of the endpoint depends on what parameters are provided.

        **Node** and **Address** fields are required, while
        **Datacenter** will be defaulted to match that of the agent.
        If only those are provided, the endpoint will register the
        node with the catalog.
        **TaggedAddresses** can be used in conjunction with the
        translate_wan_addrs configuration option and the ""wan"" address.

        If the **Service** key is provided, the service will also be
        registered. If **ID** is not provided, it will be defaulted to the
        value of the **Service.Service** property. Only one service with a
        given **ID** may be present per node. The service **Tags**,
        **Address**, and **Port** fields are all optional.

        If the **Check** key is provided, a health check will also be
        registered.

        .. note:: This register API manipulates the health check entry in
                  the Catalog, but it does not setup the script, **TTL**,
                  or **HTTP** check to monitor the node's health. To truly
                  enable a new health check, the check must either be provided
                  in agent configuration or set via the agent endpoint.

        The **CheckID** can be omitted and will default to the value of
        **Name**. As with **Service.ID**, the **CheckID** must be unique on
        this node. **Notes** is an opaque field that is meant to hold
        human-readable text. If a **ServiceID** is provided that matches the
        **ID** of a service on that node, the check is treated as a service
        level health check, instead of a node level health check. The
        **Status** must be one of ``passing``, ``warning``, or ``critical``.

        Multiple checks can be provided by replacing **Check** with **Checks**
        and sending a list of Check objects.

        It is important to note that **Check** does not have to be provided
        with **Service** and vice versa. A catalog entry can have either,
        neither, or both.

        An optional ACL token may be provided to perform the registration by
        including a **WriteRequest** block in the query, like this::

            {
                ""WriteRequest"": {
                    ""Token"": ""foo""
                }
            }",0,1,1,2
"async def send_media_group(self, chat_id: typing.Union[base.Integer, base.String],
                               media: typing.Union[types.MediaGroup, typing.List],
                               disable_notification: typing.Union[base.Boolean, None] = None,
                               reply_to_message_id: typing.Union[base.Integer,
                                                                 None] = None) -> typing.List[types.Message]:
        
        
        if isinstance(media, list):
            media = types.MediaGroup(media)

        files = dict(media.get_files())

        media = prepare_arg(media)
        payload = generate_payload(**locals(), exclude=['files'])

        result = await self.request(api.Methods.SEND_MEDIA_GROUP, payload, files)
        return [types.Message(**message) for message in result]","Use this method to send a group of photos or videos as an album.

        Source: https://core.telegram.org/bots/api#sendmediagroup

        :param chat_id: Unique identifier for the target chat or username of the target channel
        :type chat_id: :obj:`typing.Union[base.Integer, base.String]`
        :param media: A JSON-serialized array describing photos and videos to be sent
        :type media: :obj:`typing.Union[types.MediaGroup, typing.List]`
        :param disable_notification: Sends the message silently. Users will receive a notification with no sound
        :type disable_notification: :obj:`typing.Union[base.Boolean, None]`
        :param reply_to_message_id: If the message is a reply, ID of the original message
        :type reply_to_message_id: :obj:`typing.Union[base.Integer, None]`
        :return: On success, an array of the sent Messages is returned
        :rtype: typing.List[types.Message]",0,1,1,2
"async def send_message(
        self, request: str, response_expected: bool, **kwargs: Any
    ) -> Response:
        
        with async_timeout.timeout(self.timeout):
            async with self.session.post(
                self.endpoint, data=request, ssl=self.ssl
            ) as response:
                response_text = await response.text()
                return Response(response_text, raw=response)","Transport the message to the server and return the response.

        Args:
            request: The JSON-RPC request string.
            response_expected: Whether the request expects a response.

        Returns:
            A Response object.",0,1,1,2
"async def send_message(  
        self, request: str, response_expected: bool, **kwargs: Any
    ) -> Response:
        
        headers = dict(self.DEFAULT_HEADERS)
        headers.update(kwargs.pop(""headers"", {}))

        response = await self.client.fetch(
            self.endpoint, method=""POST"", body=request, headers=headers, **kwargs
        )

        return Response(response.body.decode(), raw=response)","Transport the message to the server and return the response.

        Args:
            request: The JSON-RPC request string.
            response_expected: Whether the request expects a response.

        Returns:
            A Response object.",0,1,1,2
"async def set_game_score(self, user_id: base.Integer, score: base.Integer,
                             force: typing.Union[base.Boolean, None] = None,
                             disable_edit_message: typing.Union[base.Boolean, None] = None,
                             chat_id: typing.Union[base.Integer, None] = None,
                             message_id: typing.Union[base.Integer, None] = None,
                             inline_message_id: typing.Union[base.String,
                                                             None] = None) -> types.Message or base.Boolean:
        
        payload = generate_payload(**locals())

        result = await self.request(api.Methods.SET_GAME_SCORE, payload)
        if isinstance(result, bool):
            return result
        return types.Message(**result)","Use this method to set the score of the specified user in a game.

        Source: https://core.telegram.org/bots/api#setgamescore

        :param user_id: User identifier
        :type user_id: :obj:`base.Integer`
        :param score: New score, must be non-negative
        :type score: :obj:`base.Integer`
        :param force: Pass True, if the high score is allowed to decrease
            This can be useful when fixing mistakes or banning cheaters
        :type force: :obj:`typing.Union[base.Boolean, None]`
        :param disable_edit_message: Pass True, if the game message should not be automatically
            edited to include the current scoreboard
        :type disable_edit_message: :obj:`typing.Union[base.Boolean, None]`
        :param chat_id: Required if inline_message_id is not specified. Unique identifier for the target chat
        :type chat_id: :obj:`typing.Union[base.Integer, None]`
        :param message_id: Required if inline_message_id is not specified. Identifier of the sent message
        :type message_id: :obj:`typing.Union[base.Integer, None]`
        :param inline_message_id: Required if chat_id and message_id are not specified. Identifier of the inline message
        :type inline_message_id: :obj:`typing.Union[base.String, None]`
        :return: On success, if the message was sent by the bot, returns the edited Message, otherwise returns True
            Returns an error, if the new score is not greater than the user's
            current score in the chat and force is False.
        :rtype: :obj:`typing.Union[types.Message, base.Boolean]`",0,1,1,2
"async def set_volume(self, vol: int):
        
        if self._lavalink._server_version <= 2:
            self.volume = max(min(vol, 150), 0)
        else:
            self.volume = max(min(vol, 1000), 0)
        await self._lavalink.ws.send(op='volume', guildId=self.guild_id, volume=self.volume)",Sets the player's volume (150% or 1000% limit imposed by lavalink depending on the version).,0,1,2,3
"async def stat(self, path):
        
        path = pathlib.PurePosixPath(path)
        try:
            code, info = await self.command(""MLST "" + str(path), ""2xx"")
            name, info = self.parse_mlsx_line(info[1].lstrip())
            return info
        except errors.StatusCodeError as e:
            if not e.received_codes[-1].matches(""50x""):
                raise

        for p, info in await self.list(path.parent):
            if p.name == path.name:
                return info
        else:
            raise errors.StatusCodeError(
                Code(""2xx""),
                Code(""550""),
                ""path does not exists"",
            )",":py:func:`asyncio.coroutine`

        Getting path stats.

        :param path: path for getting info
        :type path: :py:class:`str` or :py:class:`pathlib.PurePosixPath`

        :return: path info
        :rtype: :py:class:`dict`",2,1,2,5
"async def stepper_config(self, command):
        
        steps_per_revs = int(command[0])
        pins = command[1]
        pin1 = int(pins[0])
        pin2 = int(pins[1])
        pin3 = int(pins[2])
        pin4 = int(pins[3])
        await self.core.stepper_config(steps_per_revs, [pin1, pin2, pin3, pin4])","This method configures 4 pins for stepper motor operation.
        This is a FirmataPlus feature.
        :param command: {""method"": ""stepper_config"", ""params"": [STEPS_PER_REVOLUTION, [PIN1, PIN2, PIN3, PIN4]]}
        :returns:No message returned.",0,0,2,2
"async def stop_live_location(self, reply_markup=None) -> typing.Union[Message, base.Boolean]:
        
        return await self.bot.stop_message_live_location(chat_id=self.chat.id, message_id=self.message_id,
                                                         reply_markup=reply_markup)","Use this method to stop updating a live location message sent by the bot or via the bot
        (for inline bots) before live_period expires.

        Source: https://core.telegram.org/bots/api#stopmessagelivelocation

        :param reply_markup: A JSON-serialized object for a new inline keyboard.
        :type reply_markup: :obj:`typing.Union[types.InlineKeyboardMarkup, None]`
        :return: On success, if the message was sent by the bot, the sent Message is returned,
            otherwise True is returned.
        :rtype: :obj:`typing.Union[types.Message, base.Boolean]`",0,1,0,1
"async def stream(
        self, version=""1.1"", keep_alive=False, keep_alive_timeout=None
    ):
        
        headers = self.get_headers(
            version,
            keep_alive=keep_alive,
            keep_alive_timeout=keep_alive_timeout,
        )
        self.protocol.push_data(headers)
        await self.protocol.drain()
        await self.streaming_fn(self)
        self.protocol.push_data(b""0\r\n\r\n"")","Streams headers, runs the `streaming_fn` callback that writes
        content to the response body, then finalizes the response body.",0,3,0,3
"async def sync_recent_conversations(
            self, sync_recent_conversations_request
    ):
        
        response = hangouts_pb2.SyncRecentConversationsResponse()
        await self._pb_request('conversations/syncrecentconversations',
                               sync_recent_conversations_request,
                               response)
        return response",Return info on recent conversations and their events.,0,2,0,2
"async def unsubscribe(self, topic):
        
        if self.socket_type not in {SUB, XSUB}:
            raise AssertionError(
                ""A %s socket cannot unsubscribe."" % self.socket_type.decode(),
            )

        
        
        self._subscriptions.remove(topic)
        tasks = [
            asyncio.ensure_future(
                peer.connection.local_unsubscribe(topic),
                loop=self.loop,
            )
            for peer in self._peers
            if peer.connection
        ]

        if tasks:
            try:
                await asyncio.wait(tasks, loop=self.loop)
            finally:
                for task in tasks:
                    task.cancel()","Unsubscribe the socket from the specified topic.

        :param topic: The topic to unsubscribe from.",1,1,2,4
"def AddFileEntry(
      self, path, file_entry_type=definitions.FILE_ENTRY_TYPE_FILE,
      file_data=None, link_data=None):
    
    if path in self._paths:
      raise KeyError('File entry already set for path: {0:s}.'.format(path))

    if file_data and file_entry_type != definitions.FILE_ENTRY_TYPE_FILE:
      raise ValueError('File data set for non-file file entry type.')

    if link_data and file_entry_type != definitions.FILE_ENTRY_TYPE_LINK:
      raise ValueError('Link data set for non-link file entry type.')

    if file_data is not None:
      path_data = file_data
    elif link_data is not None:
      path_data = link_data
    else:
      path_data = None

    self._paths[path] = (file_entry_type, path_data)","Adds a fake file entry.

    Args:
      path (str): path of the file entry.
      file_entry_type (Optional[str]): type of the file entry object.
      file_data (Optional[bytes]): data of the fake file-like object.
      link_data (Optional[bytes]): link data of the fake file entry object.

    Raises:
      KeyError: if the path already exists.
      ValueError: if the file data is set but the file entry type is not a file
          or if the link data is set but the file entry type is not a link.",3,0,4,7
"def Analyze(self, hashes):
    
    logger.debug(
        'Opening connection to {0:s}:{1:d}'.format(self._host, self._port))

    nsrl_socket = self._GetSocket()
    if not nsrl_socket:
      self.SignalAbort()
      return []

    hash_analyses = []
    for digest in hashes:
      response = self._QueryHash(nsrl_socket, digest)
      if response is None:
        continue

      hash_analysis = interface.HashAnalysis(digest, response)
      hash_analyses.append(hash_analysis)

    nsrl_socket.close()

    logger.debug(
        'Closed connection to {0:s}:{1:d}'.format(self._host, self._port))

    return hash_analyses","Looks up hashes in nsrlsvr.

    Args:
      hashes (list[str]): hash values to look up.

    Returns:
      list[HashAnalysis]: analysis results, or an empty list on error.",0,3,2,5
"def Bankoff(m, x, rhol, rhog, mul, mug, D, roughness=0, L=1):
    r
    
    v_lo = m/rhol/(pi/4*D**2)
    Re_lo = Reynolds(V=v_lo, rho=rhol, mu=mul, D=D)
    fd_lo = friction_factor(Re=Re_lo, eD=roughness/D)
    dP_lo = fd_lo*L/D*(0.5*rhol*v_lo**2)

    gamma = (0.71 + 2.35*rhog/rhol)/(1. + (1.-x)/x*rhog/rhol)
    phi_Bf = 1./(1.-x)*(1 - gamma*(1 - rhog/rhol))**(3/7.)*(1. + x*(rhol/rhog -1.))
    return dP_lo*phi_Bf**(7/4.)","r'''Calculates two-phase pressure drop with the Bankoff (1960) correlation,
    as shown in [2]_, [3]_, and [4]_.

    .. math::
        \Delta P_{tp} = \phi_{l}^{7/4} \Delta P_{l}

    .. math::
        \phi_l = \frac{1}{1-x}\left[1 - \gamma\left(1 - \frac{\rho_g}{\rho_l}
        \right)\right]^{3/7}\left[1 + x\left(\frac{\rho_l}{\rho_g} - 1\right)
        \right]

    .. math::
        \gamma = \frac{0.71 + 2.35\left(\frac{\rho_g}{\rho_l}\right)}
        {1 + \frac{1-x}{x} \cdot \frac{\rho_g}{\rho_l}}

    Parameters
    ----------
    m : float
        Mass flow rate of fluid, [kg/s]
    x : float
        Quality of fluid, [-]
    rhol : float
        Liquid density, [kg/m^3]
    rhog : float
        Gas density, [kg/m^3]
    mul : float
        Viscosity of liquid, [Pa*s]
    mug : float
        Viscosity of gas, [Pa*s]
    D : float
        Diameter of pipe, [m]
    roughness : float, optional
        Roughness of pipe for use in calculating friction factor, [m]
    L : float, optional
        Length of pipe, [m]

    Returns
    -------
    dP : float
        Pressure drop of the two-phase flow, [Pa]

    Notes
    -----
    This correlation is not actually shown in [1]_. Its origin is unknown.
    The author recommends against using this.

    Examples
    --------
    >>> Bankoff(m=0.6, x=0.1, rhol=915., rhog=2.67, mul=180E-6, mug=14E-6,
    ... D=0.05, roughness=0, L=1)
    4746.059442453399

    References
    ----------
    .. [1] Bankoff, S. G. ""A Variable Density Single-Fluid Model for Two-Phase
       Flow With Particular Reference to Steam-Water Flow."" Journal of Heat
       Transfer 82, no. 4 (November 1, 1960): 265-72. doi:10.1115/1.3679930.
    .. [2] Thome, John R. ""Engineering Data Book III."" Wolverine Tube Inc
       (2004). http://www.wlv.com/heat-transfer-databook/
    .. [3] Moreno Quibén, Jesús. ""Experimental and Analytical Study of Two-
       Phase Pressure Drops during Evaporation in Horizontal Tubes,"" 2005.
       doi:10.5075/epfl-thesis-3337.
    .. [4] Mekisso, Henock Mateos. ""Comparison of Frictional Pressure Drop
       Correlations for Isothermal Two-Phase Horizontal Flow."" Thesis, Oklahoma
       State University, 2013. https://shareok.org/handle/11244/11109.",0,0,1,1
"def CheckApproversForLabel(self, token, client_urn, requester, approvers,
                             label):
    
    auth = self.reader.GetAuthorizationForSubject(label)
    if not auth:
      
      return True

    if auth.requester_must_be_authorized:
      if not self.CheckPermissions(requester, label):
        raise access_control.UnauthorizedAccess(
            ""User %s not in %s or groups:%s for %s"" % (requester, auth.users,
                                                       auth.groups, label),
            subject=client_urn,
            requested_access=token.requested_access)

    approved_count = 0
    for approver in approvers:
      if self.CheckPermissions(approver, label) and approver != requester:
        approved_count += 1

    if approved_count < auth.num_approvers_required:
      raise access_control.UnauthorizedAccess(
          ""Found %s approvers for %s, needed %s"" %
          (approved_count, label, auth.num_approvers_required),
          subject=client_urn,
          requested_access=token.requested_access)
    return True","Checks if requester and approvers have approval privileges for labels.

    Checks against list of approvers for each label defined in approvers.yaml to
    determine if the list of approvers is sufficient.

    Args:
      token: user token
      client_urn: ClientURN object of the client
      requester: username string of person requesting approval.
      approvers: list of username strings that have approved this client.
      label: label strings to check approval privs for.
    Returns:
      True if access is allowed, raises otherwise.",2,0,4,6
"def Chueh_Prausnitz_Tc(zs, Tcs, Vcs, taus):
    r
    if not none_and_length_check([zs, Tcs, Vcs]):
        raise Exception('Function inputs are incorrect format')

    denominator = sum(zs[i]*Vcs[i]**(2/3.) for i in range(len(zs)))
    Tcm = 0
    for i in range(len(zs)):
        Tcm += zs[i]*Vcs[i]**(2/3.)*Tcs[i]/denominator
        for j in range(len(zs)):
            Tcm += (zs[i]*Vcs[i]**(2/3.)/denominator)*(zs[j]*Vcs[j]**(2/3.)/denominator)*taus[i][j]
    return Tcm","r'''Calculates critical temperature of a mixture according to
    mixing rules in [1]_.

    .. math::
        T_{cm} = \sum_i^n \theta_i Tc_i + \sum_i^n\sum_j^n(\theta_i \theta_j
        \tau_{ij})T_{ref}

        \theta = \frac{x_i V_{ci}^{2/3}}{\sum_{j=1}^n x_j V_{cj}^{2/3}}

    For a binary mxiture, this simplifies to:

    .. math::
        T_{cm} = \theta_1T_{c1} + \theta_2T_{c2}  + 2\theta_1\theta_2\tau_{12}

    Parameters
    ----------
    zs : array-like
        Mole fractions of all components
    Tcs : array-like
        Critical temperatures of all components, [K]
    Vcs : array-like
        Critical volumes of all components, [m^3/mol]
    taus : array-like of shape `zs` by `zs`
        Interaction parameters

    Returns
    -------
    Tcm : float
        Critical temperatures of the mixture, [K]

    Notes
    -----
    All parameters, even if zero, must be given to this function.

    Examples
    --------
    butane/pentane/hexane 0.6449/0.2359/0.1192 mixture, exp: 450.22 K.

    >>> Chueh_Prausnitz_Tc([0.6449, 0.2359, 0.1192], [425.12, 469.7, 507.6],
    ... [0.000255, 0.000313, 0.000371], [[0, 1.92681, 6.80358],
    ... [1.92681, 0, 1.89312], [ 6.80358, 1.89312, 0]])
    450.1225764723492

    References
    ----------
    .. [1] Chueh, P. L., and J. M. Prausnitz. ""Vapor-Liquid Equilibria at High
       Pressures: Calculation of Critical Temperatures, Volumes, and Pressures
       of Nonpolar Mixtures."" AIChE Journal 13, no. 6 (November 1, 1967):
       1107-13. doi:10.1002/aic.690130613.
    .. [2] Najafi, Hamidreza, Babak Maghbooli, and Mohammad Amin Sobati.
       ""Prediction of True Critical Temperature of Multi-Component Mixtures:
       Extending Fast Estimation Methods."" Fluid Phase Equilibria 392
       (April 25, 2015): 104-26. doi:10.1016/j.fluid.2015.02.001.",1,0,2,3
"def ClaimRecords(self,
                   limit=10000,
                   timeout=""30m"",
                   start_time=None,
                   record_filter=lambda x: False,
                   max_filtered=1000):
    
    if not self.locked:
      raise aff4.LockError(""Queue must be locked to claim records."")

    with data_store.DB.GetMutationPool() as mutation_pool:
      return mutation_pool.QueueClaimRecords(
          self.urn,
          self.rdf_type,
          limit=limit,
          timeout=timeout,
          start_time=start_time,
          record_filter=record_filter,
          max_filtered=max_filtered)","Returns and claims up to limit unclaimed records for timeout seconds.

    Returns a list of records which are now ""claimed"", a claimed record will
    generally be unavailable to be claimed until the claim times out. Note
    however that in case of an unexpected timeout or other error a record might
    be claimed twice at the same time. For this reason it should be considered
    weaker than a true lock.

    Args:
      limit: The number of records to claim.

      timeout: The duration of the claim.

      start_time: The time to start claiming records at. Only records with a
        timestamp after this point will be claimed.

      record_filter: A filter method to determine if the record should be
        returned. It will be called serially on each record and the record will
        be filtered (not returned or locked) if it returns True.

      max_filtered: If non-zero, limits the number of results read when
        filtered. Specifically, if max_filtered filtered results are read
        sequentially without any unfiltered results, we stop looking for
        results.

    Returns:
      A list (id, record) where record is a self.rdf_type and id is a record
      identifier which can be used to delete or release the record.

    Raises:
      LockError: If the queue is not locked.",3,0,1,4
"def Close(self):
    
    super(TimesketchOutputModule, self).Close()

    with self._timesketch.app_context():
      search_index = timesketch_sketch.SearchIndex.query.filter_by(
          index_name=self._index_name).first()
      search_index.status.remove(search_index.status[0])
      timesketch_db_session.add(search_index)
      timesketch_db_session.commit()","Closes the connection to TimeSketch Elasticsearch database.

    Sends the remaining events for indexing and removes the processing status on
    the Timesketch search index object.",2,0,0,2
"def Collect(self, knowledge_base):
    
    environment_variable = knowledge_base.GetEnvironmentVariable('programdata')
    allusersappdata = getattr(environment_variable, 'value', None)

    if not allusersappdata:
      environment_variable = knowledge_base.GetEnvironmentVariable(
          'allusersprofile')
      allusersdata = getattr(environment_variable, 'value', None)

      if allusersdata:
        allusersappdata = '\\'.join([allusersdata, 'Application Data'])

    if allusersappdata:
      environment_variable = artifacts.EnvironmentVariableArtifact(
          case_sensitive=False, name='allusersappdata', value=allusersappdata)

      try:
        logger.debug('setting environment variable: {0:s} to: ""{1:s}""'.format(
            'allusersappdata', allusersappdata))
        knowledge_base.AddEnvironmentVariable(environment_variable)
      except KeyError:
        
        pass","Collects values from the knowledge base.

    Args:
      knowledge_base (KnowledgeBase): to fill with preprocessing information.

    Raises:
      PreProcessFail: if the preprocessing fails.",0,1,2,3
"def Connect(self, Username, WaitConnected=False):
        
        if WaitConnected:
            self._Connect_Event = threading.Event()
            self._Connect_Stream = [None]
            self._Connect_Username = Username
            self._Connect_ApplicationStreams(self, self.Streams)
            self._Owner.RegisterEventHandler('ApplicationStreams', self._Connect_ApplicationStreams)
            self._Alter('CONNECT', Username)
            self._Connect_Event.wait()
            self._Owner.UnregisterEventHandler('ApplicationStreams', self._Connect_ApplicationStreams)
            try:
                return self._Connect_Stream[0]
            finally:
                del self._Connect_Stream, self._Connect_Event, self._Connect_Username
        else:
            self._Alter('CONNECT', Username)","Connects application to user.

        :Parameters:
          Username : str
            Name of the user to connect to.
          WaitConnected : bool
            If True, causes the method to wait until the connection is established.

        :return: If ``WaitConnected`` is True, returns the stream which can be used to send the
                 data. Otherwise returns None.
        :rtype: `ApplicationStream` or None",0,3,1,4
"def ContinuousXor(n):
    ""2 inputs are chosen uniformly from (0.0 .. 2.0]; output is xor of ints.""
    examples = []
    for i in range(n):
        x, y = [random.uniform(0.0, 2.0) for i in '12']
        examples.append([x, y, int(x) != int(y)])
    return DataSet(name=""continuous xor"", examples=examples)",2 inputs are chosen uniformly from (0.0 .. 2.0]; output is xor of ints.,0,0,1,1
"def Create(name,alias=None,location=None,session=None):
                

                if not alias:  alias = clc.v2.Account.GetAlias(session=session)
                if not location:  location = clc.v2.Account.GetLocation(session=session)

                r = clc.v2.API.Call('POST','antiAffinityPolicies/%s' % alias,
                                                        json.dumps({'name': name, 'location': location}),
                                                        session=session)
                return(AntiAffinity(id=r['id'],name=r['name'],location=r['location'],servers=[],session=session))","Creates a new anti-affinity policy within a given account.

		https://t3n.zendesk.com/entries/45042770-Create-Anti-Affinity-Policy

		*TODO* Currently returning 400 error:
		clc.APIFailedResponse: Response code 400. . POST https://api.tier3.com/v2/antiAffinityPolicies/BTDI",0,1,1,2
"def CreateTaskStorage(self, task):
    
    if task.identifier in self._task_storage_writers:
      raise IOError('Storage writer for task: {0:s} already exists.'.format(
          task.identifier))

    storage_writer = FakeStorageWriter(
        self._session, storage_type=definitions.STORAGE_TYPE_TASK, task=task)
    self._task_storage_writers[task.identifier] = storage_writer
    return storage_writer","Creates a task storage.

    Args:
      task (Task): task.

    Returns:
      FakeStorageWriter: storage writer.

    Raises:
      IOError: if the task storage already exists.
      OSError: if the task storage already exists.",2,0,1,3
"def DKL(arrays):
    
    samples, prior_samples = arrays
    samples = samples[~numpy.isnan(samples)]
    prior_samples = prior_samples[~numpy.isnan(prior_samples)]
    return (
            gaussian_kde(samples).logpdf(samples)
            - gaussian_kde(prior_samples).logpdf(samples)
            ).mean()","Compute the Kullback-Leibler divergence from one distribution Q to another
    P, where Q and P are represented by a set of samples.

    Parameters
    ----------
    arrays: tuple(1D numpy.array,1D numpy.array)
        samples defining distributions P & Q respectively

    Returns
    -------
    float:
        Kullback Leibler divergence.",0,0,1,1
"def DateField(formatter=types.DEFAULT_DATE_FORMAT, default=NOTHING,
              required=True, repr=True, cmp=True, key=None):
    
    default = _init_fields.init_default(required, default, None)
    validator = _init_fields.init_validator(required, date)
    converter = converters.to_date_field(formatter)
    return attrib(default=default, converter=converter, validator=validator,
                  repr=repr, cmp=cmp,
                  metadata=dict(formatter=formatter, key=key))","Create new date field on a model.

    :param formatter: date formatter string (default: ""%Y-%m-%d"")
    :param default: any date or string that can be converted to a date value
    :param bool required: whether or not the object is invalid if not provided.
    :param bool repr: include this field should appear in object's repr.
    :param bool cmp: include this field in generated comparison.
    :param string key: override name of the value when converted to dict.",0,0,1,1
"def DbDeleteAllDeviceAttributeProperty(self, argin):
        
        self._log.debug(""In DbDeleteAllDeviceAttributeProperty()"")

        if len(argin) < 2:
            self.warn_stream(""DataBase::DbDeleteAllDeviceAttributeProperty(): insufficient number of arguments "")
            th_exc(DB_IncorrectArguments,
                   ""insufficient number of arguments to delete all device attribute(s) property"",
                   ""DataBase::DbDeleteAllDeviceAttributeProperty()"")

        dev_name = argin[0]

        ret, d_name, dfm = check_device_name(dev_name)

        if not ret:
            th_exc(DB_IncorrectDeviceName,
                  ""device name ("" + argin + "") syntax error (should be [tango:][//instance/]domain/family/member)"",
                  ""DataBase::DbDeleteAllDeviceAttributeProperty()"")

        self.db.delete_all_device_attribute_property(dev_name, argin[1:])","Delete all attribute properties for the specified device attribute(s)

        :param argin: str[0] = device name
        Str[1]...str[n] = attribute name(s)
        :type: tango.DevVarStringArray
        :return:
        :rtype: tango.DevVoid",1,1,2,4
"def FileEntryExistsByPathSpec(self, path_spec):
    
    location = getattr(path_spec, 'location', None)

    if location is None:
      return False

    is_device = False
    if platform.system() == 'Windows':
      
      
      try:
        is_device = pysmdev.check_device(location)
      except IOError as exception:
        
        
        

        
        exception_string = str(exception)
        if not isinstance(exception_string, py2to3.UNICODE_TYPE):
          exception_string = py2to3.UNICODE_TYPE(
              exception_string, errors='replace')

        if ' access denied ' in exception_string:
          is_device = True

    
    
    return is_device or os.path.exists(location) or os.path.islink(location)","Determines if a file entry for a path specification exists.

    Args:
      path_spec (PathSpec): a path specification.

    Returns:
      bool: True if the file entry exists, false otherwise.",0,0,1,1
"def FileEntryExistsByPathSpec(self, path_spec):
    
    volume_index = lvm.LVMPathSpecGetVolumeIndex(path_spec)

    
    
    if volume_index is None:
      location = getattr(path_spec, 'location', None)
      return location is not None and location == self.LOCATION_ROOT

    return (
        0 <= volume_index < self._vslvm_volume_group.number_of_logical_volumes)","Determines if a file entry for a path specification exists.

    Args:
      path_spec (PathSpec): path specification.

    Returns:
      bool: True if the file entry exists.",0,0,1,1
"def GET(self, mid=None):
        
        if mid:
            lowstate = [{
                'client': 'wheel',
                'fun': 'key.finger',
                'match': mid,
            }]
        else:
            lowstate = [{
                'client': 'wheel',
                'fun': 'key.list_all',
            }]

        cherrypy.request.lowstate = lowstate
        result = self.exec_lowstate(token=cherrypy.session.get('token'))

        return {'return': next(result, {}).get('data', {}).get('return', {})}","Show the list of minion keys or detail on a specific key

        .. versionadded:: 2014.7.0

        .. http:get:: /keys/(mid)

            List all keys or show a specific key

            :reqheader X-Auth-Token: |req_token|
            :reqheader Accept: |req_accept|

            :status 200: |200|
            :status 401: |401|
            :status 406: |406|

        **Example request:**

        .. code-block:: bash

            curl -i localhost:8000/keys

        .. code-block:: text

            GET /keys HTTP/1.1
            Host: localhost:8000
            Accept: application/x-yaml

        **Example response:**

        .. code-block:: text

            HTTP/1.1 200 OK
            Content-Length: 165
            Content-Type: application/x-yaml

            return:
              local:
              - master.pem
              - master.pub
              minions:
              - jerry
              minions_pre: []
              minions_rejected: []

        **Example request:**

        .. code-block:: bash

            curl -i localhost:8000/keys/jerry

        .. code-block:: text

            GET /keys/jerry HTTP/1.1
            Host: localhost:8000
            Accept: application/x-yaml

        **Example response:**

        .. code-block:: text

            HTTP/1.1 200 OK
            Content-Length: 73
            Content-Type: application/x-yaml

            return:
              minions:
                jerry: 51:93:b3:d0:9f:3a:6d:e5:28:67:c2:4b:27:d6:cd:2b",0,1,1,2
"def GET(self, url):
        
        r = requests.get(url)
        if self.verbose:
            sys.stdout.write(""%s %s\n"" % (r.status_code, r.encoding))
            sys.stdout.write(str(r.headers) + ""\n"")
        self.encoding = r.encoding
        return r.text",returns text content of HTTP GET response.,0,1,1,2
"def GaussianBlur(X, ksize_width, ksize_height, sigma_x, sigma_y):
    
    return image_transform(
        X,
        cv2.GaussianBlur,
        ksize=(ksize_width, ksize_height),
        sigmaX=sigma_x,
        sigmaY=sigma_y
    )","Apply Gaussian blur to the given data.

    Args:
        X: data to blur
        kernel_size: Gaussian kernel size
        stddev: Gaussian kernel standard deviation (in both X and Y directions)",0,0,1,1
"def GetFilename(multiple=False, sep='|', **kwargs):
    

    args = []
    if multiple:
        args.append('--multiple')
    if sep != '|':
        args.append('--separator=%s' % sep)
    
    for generic_args in kwargs_helper(kwargs):
        args.append('--%s=%s' % generic_args)

    p = run_zenity('--file-selection', *args)

    if p.wait() == 0:
        return p.stdout.read()[:-1].split('|')","Prompt the user for a filename.
    
    This will raise a Zenity File Selection Dialog. It will return a list with 
    the selected files or None if the user hit cancel.
    
    multiple - True to allow the user to select multiple files.
    sep - Token to use as the path separator when parsing Zenity's return 
          string.
    kwargs - Optional command line parameters for Zenity such as height,
             width, etc.",1,0,1,2
"def GetFormattedField(self, event, field_name):
    
    callback_name = self._FIELD_FORMAT_CALLBACKS.get(field_name, None)
    callback_function = None
    if callback_name:
      callback_function = getattr(self, callback_name, None)

    if callback_function:
      output_value = callback_function(event)
    else:
      output_value = getattr(event, field_name, '-')

    if output_value is None:
      output_value = '-'

    elif not isinstance(output_value, py2to3.STRING_TYPES):
      output_value = '{0!s}'.format(output_value)

    return output_value","Formats the specified field.

    Args:
      event (EventObject): event.
      field_name (str): name of the field.

    Returns:
      str: value of the field.",0,0,1,1
"def GetInfo(self):
        
        return {
            'vendor': self.vendor,
            'product': self.product,
            'version': self.version,
            'url': self.url,
            'interfaces': list(self.interfaces.keys())
        }",The standardized org.varlink.service.GetInfo() varlink method.,0,0,1,1
"def GetItemContainerInfo(self_link, alt_content_path, id_from_response):
     

    self_link = TrimBeginningAndEndingSlashes(self_link) + '/'

    index = IndexOfNth(self_link, '/', 4)

    if index != -1:
        collection_id = self_link[0:index]

        if 'colls' in self_link:
            
            index_second_slash = IndexOfNth(alt_content_path, '/', 2)
            if index_second_slash == -1:
                collection_name = alt_content_path + '/colls/' + urllib_quote(id_from_response)
                return collection_id, collection_name
            else:
                collection_name = alt_content_path
                return collection_id, collection_name
        else:
            raise ValueError('Response Not from Server Partition, self_link: {0}, alt_content_path: {1}, id: {2}'
                .format(self_link, alt_content_path, id_from_response))
    else:
        raise ValueError('Unable to parse document collection link from ' + self_link)","Given the self link and alt_content_path from the reponse header and result
        extract the collection name and collection id

        Ever response header has alt-content-path that is the 
        owner's path in ascii. For document create / update requests, this can be used
        to get the collection name, but for collection create response, we can't use it.
        So we also rely on  

    :param str self_link:
        Self link of the resource, as obtained from response result.
    :param str alt_content_path:
        Owner path of the resource, as obtained from response header.
    :param str resource_id:
        'id' as returned from the response result. This is only used if it is deduced that the
         request was to create a collection.

    :return:
        tuple of (collection rid, collection name)
    :rtype: tuple",2,0,3,5
"def GetLocalPath(self, inode, cache, database):
    
    local_path = cache.GetResults('local_path')
    if not local_path:
      results = database.Query(self.LOCAL_PATH_CACHE_QUERY)

      cache.CacheQueryResults(
          results, 'local_path', 'child_inode_number',
          ('parent_inode_number', 'filename'))
      local_path = cache.GetResults('local_path')

    parent, path = local_path.get(inode, [None, None])

    
    
    root_value = '%local_sync_root%/'

    if not path:
      return root_value

    paths = []
    while path:
      paths.append(path)
      parent, path = local_path.get(parent, [None, None])

    if not paths:
      return root_value

    
    
    paths.reverse()
    return root_value + '/'.join(paths)","Return local path for a given inode.

    Args:
      inode (int): inode number for the file.
      cache (SQLiteCache): cache.
      database (SQLiteDatabase): database.

    Returns:
      str: full path, including the filename of the given inode value.",2,0,1,3
"def GetMessages(self, formatter_mediator, event):
    
    if self.DATA_TYPE != event.data_type:
      raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format(
          event.data_type))

    event_values = event.CopyToDict()

    visit_type = event_values.get('visit_type', 0)
    transition = self._URL_TRANSITIONS.get(visit_type, None)
    if transition:
      transition_str = 'Transition: {0!s}'.format(transition)

    extra = event_values.get('extra', None)
    if extra:
      if transition:
        extra.append(transition_str)
      event_values['extra_string'] = ' '.join(extra)

    elif transition:
      event_values['extra_string'] = transition_str

    return self._ConditionalFormatMessages(event_values)","Determines the formatted message strings for an event object.

    Args:
      formatter_mediator (FormatterMediator): mediates the interactions
          between formatters and other components, such as storage and Windows
          EventLog resources.
      event (EventObject): event.

    Returns:
      tuple(str, str): formatted message string and short message string.

    Raises:
      WrongFormatter: if the event object cannot be formatted by the formatter.",1,0,2,3
"def GetRadioButtonSelect(selectList, title=""Select"", msg=""""):
    
    root = tkinter.Tk()
    root.title(title)
    val = tkinter.IntVar()
    val.set(0)

    if msg != """":
        tkinter.Label(root, text=msg).pack()

    index = 0
    for item in selectList:
        tkinter.Radiobutton(root, text=item, variable=val,
                            value=index).pack(anchor=tkinter.W)
        index += 1

    tkinter.Button(root, text=""OK"", fg=""black"", command=root.quit).pack()
    root.mainloop()
    root.destroy()

    print(selectList[val.get()] + "" is selected"")
    return (selectList[val.get()], val.get())","Create radio button window for option selection

    title: Window name
    mag: Label of the radio button

    return (seldctedItem, selectedindex)",1,0,0,1
"def Hash(self):
        
        if not self.__hash:
            hashdata = self.RawData()
            ba = bytearray(binascii.unhexlify(hashdata))
            hash = bin_dbl_sha256(ba)
            self.__hash = UInt256(data=hash)

        return self.__hash","Get the hash value of the Blockbase.

        Returns:
            UInt256: containing the hash of the data.",0,0,1,1
"def Hf(CASRN, AvailableMethods=False, Method=None):
    r
    def list_methods():
        methods = []
        if CASRN in API_TDB_data.index:
            methods.append(API_TDB)
        methods.append(NONE)
        return methods
    if AvailableMethods:
        return list_methods()
    if not Method:
        Method = list_methods()[0]

    if Method == API_TDB:
        _Hf = float(API_TDB_data.at[CASRN, 'Hf'])
    elif Method == NONE:
        _Hf = None
    else:
        raise Exception('Failure in in function')
    return _Hf","r'''This function handles the retrieval of a chemical's standard-phase
    heat of formation. The lookup is based on CASRNs. Selects the only
    data source available ('API TDB') if the chemical is in it.
    Returns None if the data is not available.

    Function has data for 571 chemicals.

    Parameters
    ----------
    CASRN : string
        CASRN [-]

    Returns
    -------
    Hf : float
        Standard-state heat of formation, [J/mol]
    methods : list, only returned if AvailableMethods == True
        List of methods which can be used to obtain Hf with the given inputs

    Other Parameters
    ----------------
    Method : string, optional
        A string for the method name to use, as defined by constants in
        Hf_methods
    AvailableMethods : bool, optional
        If True, function will determine which methods can be used to obtain
        Hf for the desired chemical, and will return methods instead of Hf

    Notes
    -----
    Only one source of information is available to this function. it is:

        * 'API_TDB', a compilation of heats of formation of unspecified phase.
          Not the original data, but as reproduced in [1]_. Some chemicals with
          duplicated CAS numbers were removed.

    Examples
    --------
    >>> Hf(CASRN='7732-18-5')
    -241820.0

    References
    ----------
    .. [1] Albahri, Tareq A., and Abdulla F. Aljasmi. ""SGC Method for
       Predicting the Standard Enthalpy of Formation of Pure Compounds from
       Their Molecular Structures."" Thermochimica Acta 568
       (September 20, 2013): 46-60. doi:10.1016/j.tca.2013.06.020.",1,0,3,4
"def Log(self, frame):
    
    
    if not self._log_message:
      return {'isError': True,
              'description': {'format': LOG_ACTION_NOT_SUPPORTED}}

    if self._quota_recovery_start_time:
      ms_elapsed = (time.time() - self._quota_recovery_start_time) * 1000
      if ms_elapsed > self.quota_recovery_ms:
        
        self._quota_recovery_start_time = None
      else:
        
        return

    
    message = 'LOGPOINT: ' + _FormatMessage(
        self._definition.get('logMessageFormat', ''),
        self._EvaluateExpressions(frame))

    line = self._definition['location']['line']
    cdbg_logging_location = (NormalizePath(frame.f_code.co_filename), line,
                             _GetFrameCodeObjectName(frame))

    if native.ApplyDynamicLogsQuota(len(message)):
      self._log_message(message)
    else:
      self._quota_recovery_start_time = time.time()
      self._log_message(DYNAMIC_LOG_OUT_OF_QUOTA)
    del cdbg_logging_location
    return None","Captures the minimal application states, formats it and logs the message.

    Args:
      frame: Python stack frame of breakpoint hit.

    Returns:
      None on success or status message on error.",0,0,5,5
"def MessageEncoder(field_number, is_repeated, is_packed):
  

  tag = TagBytes(field_number, wire_format.WIRETYPE_LENGTH_DELIMITED)
  local_EncodeVarint = _EncodeVarint
  assert not is_packed
  if is_repeated:
    def EncodeRepeatedField(write, value):
      for element in value:
        write(tag)
        local_EncodeVarint(write, element.ByteSize())
        element._InternalSerialize(write)
    return EncodeRepeatedField
  else:
    def EncodeField(write, value):
      write(tag)
      local_EncodeVarint(write, value.ByteSize())
      return value._InternalSerialize(write)
    return EncodeField",Returns an encoder for a message field.,0,0,4,4
"def Nu_Bishop(Re, Pr, rho_w=None, rho_b=None, D=None, x=None):
    r
    Nu = 0.0069*Re**0.9*Pr**0.66
    if rho_w and rho_b:
        Nu *= (rho_w/rho_b)**0.43
    if D and x:
        Nu *= (1 + 2.4*D/x)
    return Nu","r'''Calculates internal convection Nusselt number for turbulent vertical
    upward flow in a pipe under supercritical conditions according to [1]_.
    Correlation includes an adjustment for the thermal entry length.
    One of the most common correlations for supercritical convection.
        
    .. math::
        Nu_b = 0.0069 Re_b^{0.9} \bar Pr_b^{0.66}
        \left(\frac{\rho_w}{\rho_b}\right)^{0.43}(1+2.4D/x)

        \bar{Cp} = \frac{H_w-H_b}{T_w-T_b}
        
    Parameters
    ----------
    Re : float
        Reynolds number with bulk fluid properties, [-]
    Pr : float
        Prandtl number with bulk fluid properties and an average heat capacity
        between the wall and bulk temperatures [-]
    rho_w : float, optional
        Density at the wall temperature, [kg/m^3]
    rho_b : float, optional
        Density at the bulk temperature, [kg/m^3]
    D : float, optional
        Diameter of tube, [m]
    x : float, optional
        Axial distance along the tube, [m]

    Returns
    -------
    Nu : float
        Nusselt number with wall fluid properties, [-]

    Notes
    -----
    For the data used to develop the correlation, P varied from 22.8 to 27.6 
    MPa, and D was x/D varied from 30-365. G varied from 651-3662 kg/m^2/s and 
    q varied from 310 to 3460 kW/m^2. T_b varied from 282 to 527 degrees 
    Celsius.
    
    Cp used in the calculation of Prandtl number should be the average value
    of those at the wall and the bulk temperatures.

    For enhanced heat transfer, this was the 11th most accurate correlation in 
    [2]_ with a MAD of 19.0%. On the overall database in [3]_, it was the 
    most accurate correlation however.
    
    If the extra density information is not provided, it will not be used.
    If both diameter and axial distance are not provided, the entrance 
    correction is not used.

    Examples
    --------
    >>> Nu_Bishop(1E5, 1.2, 330, 290., .01, 1.2)
    265.3620050072533

    References
    ----------
    .. [1] Bishop A.A., Sandberg R.O., Tong L.S. (1965) Forced convection heat 
       transfer to water at near-critical temperature and supercritical 
       pressures. In: AIChE J. Chemical engineering symposium series, no. 2.
       Institute of Chemical Engineers, London
    .. [2] Chen, Weiwei, Xiande Fang, Yu Xu, and Xianghui Su. ""An Assessment of
       Correlations of Forced Convection Heat Transfer to Water at 
       Supercritical Pressure."" Annals of Nuclear Energy 76 (February 2015): 
       451-60. doi:10.1016/j.anucene.2014.10.027.
    .. [3] Yu, Jiyang, Baoshan Jia, Dan Wu, and Daling Wang. ""Optimization of 
       Heat Transfer Coefficient Correlation at Supercritical Pressure Using 
       Genetic Algorithms."" Heat and Mass Transfer 45, no. 6 (January 8, 2009): 
       757-66. doi:10.1007/s00231-008-0475-4.
    .. [4] Jäger, Wadim, Victor Hugo Sánchez Espinoza, and Antonio Hurtado. 
       ""Review and Proposal for Heat Transfer Predictions at Supercritical 
       Water Conditions Using Existing Correlations and Experiments."" Nuclear 
       Engineering and Design, (W3MDM) University of Leeds International 
       Symposium: What Where When? Multi-dimensional Advances for Industrial 
       Process Monitoring, 241, no. 6 (June 2011): 2184-2203. 
       doi:10.1016/j.nucengdes.2011.03.022.",0,0,1,1
"def ParseLeakFilesTable(
      self, parser_mediator, database=None, table=None, **unused_kwargs):
    
    if database is None:
      raise ValueError('Missing database value.')

    if table is None:
      raise ValueError('Missing table value.')

    for esedb_record in table.records:
      if parser_mediator.abort:
        break

      record_values = self._GetRecordValues(
          parser_mediator, table.name, esedb_record)

      event_data = MsieWebCacheLeakFilesEventData()
      event_data.cached_filename = record_values.get('Filename', None)
      event_data.leak_identifier = record_values.get('LeakId', None)

      timestamp = record_values.get('CreationTime', None)
      if timestamp:
        date_time = dfdatetime_filetime.Filetime(timestamp=timestamp)
        event = time_events.DateTimeValuesEvent(
            date_time, definitions.TIME_DESCRIPTION_CREATION)
        parser_mediator.ProduceEventWithEventData(event, event_data)","Parses the LeakFiles table.

    Args:
      parser_mediator (ParserMediator): mediates interactions between parsers
          and other components, such as storage and dfvfs.
      database (Optional[pyesedb.file]): ESE database.
      table (Optional[pyesedb.table]): table.

    Raises:
      ValueError: if the database or table value is missing.",3,0,5,8
"def ParseOptions(cls, options, output_module):
    
    if not isinstance(output_module, timesketch_out.TimesketchOutputModule):
      raise errors.BadConfigObject(
          'Output module is not an instance of TimesketchOutputModule')

    document_type = cls._ParseStringOption(
        options, 'document_type', default_value=cls._DEFAULT_DOCUMENT_TYPE)
    output_module.SetDocumentType(document_type)

    flush_interval = cls._ParseNumericOption(
        options, 'flush_interval', default_value=cls._DEFAULT_FLUSH_INTERVAL)
    output_module.SetFlushInterval(flush_interval)

    index = cls._ParseStringOption(
        options, 'index', default_value=cls._DEFAULT_UUID)
    output_module.SetIndexName(index)

    name = cls._ParseStringOption(
        options, 'timeline_name', default_value=cls._DEFAULT_NAME)
    output_module.SetTimelineName(name)

    username = cls._ParseStringOption(
        options, 'username', default_value=cls._DEFAULT_USERNAME)
    output_module.SetTimelineOwner(username)","Parses and validates options.

    Args:
      options (argparse.Namespace): parser options.
      output_module (TimesketchOutputModule): output module to configure.

    Raises:
      BadConfigObject: when the output module object is of the wrong type.
      BadConfigOption: when a configuration parameter fails validation.",1,0,3,4
"def ParseRow(self, parser_mediator, row_offset, row):
    
    try:
      timestamp = self._ConvertToTimestamp(
          row['date'], row['time'], parser_mediator.timezone)
    except errors.TimestampError as exception:
      parser_mediator.ProduceExtractionWarning(
          'Unable to parse time string: [{0:s} {1:s}] with error {2:s}'.format(
              repr(row['date']), repr(row['time']), exception))
      return

    if timestamp is None:
      return

    event_data = McafeeAVEventData()
    event_data.action = row['action']
    event_data.filename = row['filename']
    event_data.offset = row_offset
    event_data.rule = row['rule']
    event_data.status = row['status']
    event_data.trigger_location = row['trigger_location']
    event_data.username = row['username']

    event = time_events.TimestampEvent(
        timestamp, definitions.TIME_DESCRIPTION_WRITTEN)
    parser_mediator.ProduceEventWithEventData(event, event_data)","Parses a line of the log file and produces events.

    Args:
      parser_mediator (ParserMediator): mediates interactions between parsers
          and other components, such as storage and dfvfs.
      row_offset (int): line number of the row.
      row (dict[str, str]): fields of a single row, as specified in COLUMNS.",0,1,2,3
"def QueryService(svc_name):
  
  hscm = win32service.OpenSCManager(None, None,
                                    win32service.SC_MANAGER_ALL_ACCESS)
  result = None
  try:
    hs = win32serviceutil.SmartOpenService(hscm, svc_name,
                                           win32service.SERVICE_ALL_ACCESS)
    result = win32service.QueryServiceConfig(hs)
    win32service.CloseServiceHandle(hs)
  finally:
    win32service.CloseServiceHandle(hscm)

  return result",Query service and get its config.,0,4,0,4
"def S_mag(self, T):
        

        tau = T / self.Tc_mag

        if tau <= 1.0:
            s = 1 - (self._B_mag*(2*tau**3/3 + 2*tau**9/27 + 2*tau**15/75)) / \
                self._D_mag
        else:
            s = (2*tau**-5/5 + 2*tau**-15/45 + 2*tau**-25/125)/self._D_mag

        return -R*math.log(self.beta0_mag + 1)*s","Calculate the phase's magnetic contribution to entropy at the
        specified temperature.

        :param T: [K] temperature

        :returns: [J/mol/K] The magnetic entropy of the compound phase.

        Dinsdale, A. T. (1991). SGTE data for pure elements. Calphad, 15(4),
        317–425. http://doi.org/10.1016/0364-5916(91)90030-N",0,0,1,1
"def Scan(self, after_timestamp=None, include_suffix=False, max_records=None):
    
    suffix = None
    if isinstance(after_timestamp, tuple):
      suffix = after_timestamp[1]
      after_timestamp = after_timestamp[0]

    for item, timestamp, suffix in data_store.DB.CollectionScanItems(
        self.collection_id,
        self.RDF_TYPE,
        after_timestamp=after_timestamp,
        after_suffix=suffix,
        limit=max_records):
      if include_suffix:
        yield ((timestamp, suffix), item)
      else:
        yield (timestamp, item)","Scans for stored records.

    Scans through the collection, returning stored values ordered by timestamp.

    Args:
      after_timestamp: If set, only returns values recorded after timestamp.
      include_suffix: If true, the timestamps returned are pairs of the form
        (micros_since_epoc, suffix) where suffix is a 24 bit random refinement
        to avoid collisions. Otherwise only micros_since_epoc is returned.
      max_records: The maximum number of records to return. Defaults to
        unlimited.

    Yields:
      Pairs (timestamp, rdf_value), indicating that rdf_value was stored at
      timestamp.",1,0,2,3
"def Search(path):
  
  def SearchCandidates(p):
    
    while p:
      yield p
      (_, _, p) = p.partition(os.sep)

  
  assert not path.startswith(os.sep)

  
  src_root, src_ext = os.path.splitext(path)
  assert src_ext == '.py'

  
  
  for src_part in SearchCandidates(src_root):
    
    
    for sys_path in sys.path:
      f = os.path.join(sys_path, src_part)
      
      for ext in ('.pyo', '.pyc', '.py'):
        
        
        fext = f + ext
        if os.path.exists(fext):
          
          
          
          
          
          return fext

  
  return path","Search sys.path to find a source file that matches path.

  The provided input path may have an unknown number of irrelevant outer
  directories (e.g., /garbage1/garbage2/real1/real2/x.py').  This function
  does multiple search iterations until an actual Python module file that
  matches the input path is found. At each iteration, it strips one leading
  directory from the path and searches the directories at sys.path
  for a match.

  Examples:
    sys.path: ['/x1/x2', '/y1/y2']
    Search order: [.pyo|.pyc|.py]
      /x1/x2/a/b/c
      /x1/x2/b/c
      /x1/x2/c
      /y1/y2/a/b/c
      /y1/y2/b/c
      /y1/y2/c
    Filesystem: ['/y1/y2/a/b/c.pyc']

    1) Search('a/b/c.py')
         Returns '/y1/y2/a/b/c.pyc'
    2) Search('q/w/a/b/c.py')
         Returns '/y1/y2/a/b/c.pyc'
    3) Search('q/w/c.py')
         Returns 'q/w/c.py'

    The provided input path may also be relative to an unknown directory.
    The path may include some or all outer package names.

  Examples (continued):

    4) Search('c.py')
         Returns 'c.py'
    5) Search('b/c.py')
         Returns 'b/c.py'

  Args:
    path: Path that describes a source file. Must contain .py file extension.
          Must not contain any leading os.sep character.

  Returns:
    Full path to the matched source file, if a match is found. Otherwise,
    returns the input path.

  Raises:
    AssertionError: if the provided path is an absolute path, or if it does not
      have a .py extension.",0,0,4,4
"def SendTextMessage(self, Text):
        
        if self.Type == cctReliable:
            self.Stream.Write(Text)
        elif self.Type == cctDatagram:
            self.Stream.SendDatagram(Text)
        else:
            raise SkypeError(0, 'Cannot send using %s channel type' & repr(self.Type))","Sends a text message over channel.

        :Parameters:
          Text : unicode
            Text to send.",1,2,1,4
"def ServicesGetMetod (self, sensor_id, service_id, method):
        
        if self.__SenseApiCall__('/sensors/{0}/services/{1}/{2}.json'.format(sensor_id, service_id, method), 'GET'):
            return True
        else:
            self.__error__ = ""api call unsuccessful""
            return False","Set expression for the math service.
            
            @param sensor_id (int) - Sensor id of the sensor the service is connected to.
            @param service_id (int) - Service id of the service for which to set the expression.
            @param method (string) - The get method name.
                    
            @return (bool) - Boolean indicating whether ServicesSetExpression was successful.",0,1,1,2
"def StubOutWithMock(self, obj, attr_name, use_mock_anything=False):
    

    attr_to_replace = getattr(obj, attr_name)
    if type(attr_to_replace) in self._USE_MOCK_OBJECT and not use_mock_anything:
      stub = self.CreateMock(attr_to_replace)
    else:
      stub = self.CreateMockAnything()

    self.stubs.Set(obj, attr_name, stub)","Replace a method, attribute, etc. with a Mock.

    This will replace a class or module with a MockObject, and everything else
    (method, function, etc) with a MockAnything.  This can be overridden to
    always use a MockAnything by setting use_mock_anything to True.

    Args:
      obj: A Python object (class, module, instance, callable).
      attr_name: str.  The name of the attribute to replace with a mock.
      use_mock_anything: bool. True if a MockAnything should be used regardless
        of the type of attribute.",0,0,2,2
"def Tc_mixture(Tcs=None, zs=None, CASRNs=None, AvailableMethods=False, Method=None):  
    
    def list_methods():
        methods = []
        if none_and_length_check([Tcs]):
            methods.append('Simple')
        methods.append('None')
        return methods
    if AvailableMethods:
        return list_methods()
    if not Method:
        Method = list_methods()[0]
    
    if Method == 'Simple':
        return mixing_simple(zs, Tcs)
    elif Method == 'None':
        return None
    else:
        raise Exception('Failure in in function')","This function handles the retrival of a mixture's critical temperature.

    This API is considered experimental, and is expected to be removed in a
    future release in favor of a more complete object-oriented interface.

    >>> Tc_mixture([400, 550], [0.3, 0.7])
    505.0",1,0,3,4
"def UpdateIncludeState(filename, include_dict, io=codecs):
  
  headerfile = None
  try:
    headerfile = io.open(filename, 'r', 'utf8', 'replace')
  except IOError:
    return False
  linenum = 0
  for line in headerfile:
    linenum += 1
    clean_line = CleanseComments(line)
    match = _RE_PATTERN_INCLUDE.search(clean_line)
    if match:
      include = match.group(2)
      include_dict.setdefault(include, linenum)
  return True","Fill up the include_dict with new includes found from the file.

  Args:
    filename: the name of the header to read.
    include_dict: a dictionary in which the headers are inserted.
    io: The io factory to use to read the file. Provided for testability.

  Returns:
    True if a header was successfully added. False otherwise.",1,0,2,3
"def UpdateNumberOfEventTags(
      self, number_of_consumed_event_tags, number_of_produced_event_tags):
    
    consumed_event_tags_delta = 0
    if number_of_consumed_event_tags is not None:
      if number_of_consumed_event_tags < self.number_of_consumed_event_tags:
        raise ValueError(
            'Number of consumed event tags smaller than previous update.')

      consumed_event_tags_delta = (
          number_of_consumed_event_tags - self.number_of_consumed_event_tags)

      self.number_of_consumed_event_tags = number_of_consumed_event_tags
      self.number_of_consumed_event_tags_delta = consumed_event_tags_delta

    produced_event_tags_delta = 0
    if number_of_produced_event_tags is not None:
      if number_of_produced_event_tags < self.number_of_produced_event_tags:
        raise ValueError(
            'Number of produced event tags smaller than previous update.')

      produced_event_tags_delta = (
          number_of_produced_event_tags - self.number_of_produced_event_tags)

      self.number_of_produced_event_tags = number_of_produced_event_tags
      self.number_of_produced_event_tags_delta = produced_event_tags_delta

    return consumed_event_tags_delta > 0 or produced_event_tags_delta > 0","Updates the number of event tags.

    Args:
      number_of_consumed_event_tags (int): total number of event tags consumed
          by the process.
      number_of_produced_event_tags (int): total number of event tags produced
          by the process.

    Returns:
      bool: True if either number of event tags has increased.

    Raises:
      ValueError: if the consumed or produced number of event tags is smaller
          than the value of the previous update.",2,0,4,6
"def Vfgs(self, T=None, P=None):
        r
        if (T is None or T == self.T) and (P is None or P == self.P):
            Vmgs = self.Vmgs
        else:
            if T is None: T = self.T
            if P is None: P = self.P
            Vmgs = [i(T, P) for i in self.VolumeGases]
        if none_and_length_check([Vmgs]):
            return zs_to_Vfs(self.zs, Vmgs)
        return None","r'''Volume fractions of all species in a hypothetical pure-gas phase 
        at the current or specified temperature and pressure. If temperature 
        or pressure are specified, the non-specified property is assumed to be 
        that of the mixture. Note this is a method, not a property. Volume 
        fractions are calculated based on **pure species volumes only**.

        Examples
        --------
        >>> Mixture(['sulfur hexafluoride', 'methane'], zs=[.2, .9], T=315).Vfgs()
        [0.18062059238682632, 0.8193794076131737]
        
        >>> S = Mixture(['sulfur hexafluoride', 'methane'], zs=[.1, .9])
        >>> S.Vfgs(P=1E2)
        [0.0999987466608421, 0.9000012533391578]",0,0,2,2
"def _AcceptResponses(self, expected_header, info_cb, timeout_ms=None):
        
        while True:
            response = self.usb.BulkRead(64, timeout_ms=timeout_ms)
            header = bytes(response[:4])
            remaining = bytes(response[4:])

            if header == b'INFO':
                info_cb(FastbootMessage(remaining, header))
            elif header in self.FINAL_HEADERS:
                if header != expected_header:
                    raise FastbootStateMismatch(
                        'Expected %s, got %s', expected_header, header)
                if header == b'OKAY':
                    info_cb(FastbootMessage(remaining, header))
                return remaining
            elif header == b'FAIL':
                info_cb(FastbootMessage(remaining, header))
                raise FastbootRemoteFailure('FAIL: %s', remaining)
            else:
                raise FastbootInvalidResponse(
                    'Got unknown header %s and response %s', header, remaining)","Accepts responses until the expected header or a FAIL.

        Args:
          expected_header: OKAY or DATA
          info_cb: Optional callback for text sent from the bootloader.
          timeout_ms: Timeout in milliseconds to wait for each response.

        Raises:
          FastbootStateMismatch: Fastboot responded with the wrong packet type.
          FastbootRemoteFailure: Fastboot reported failure.
          FastbootInvalidResponse: Fastboot responded with an unknown packet type.

        Returns:
          OKAY packet's message.",3,1,4,8
"def _D2O_Melting_Pressure(T, ice=""Ih""):
    
    if ice == ""Ih"" and 254.415 <= T <= 276.969:
        
        Tita = T/276.969
        ai = [-0.30153e5, 0.692503e6]
        ti = [5.5, 8.2]
        suma = 1
        for a, t in zip(ai, ti):
            suma += a*(1-Tita**t)
        P = suma*0.00066159
    elif ice == ""III"" and 254.415 < T <= 258.661:
        
        Tita = T/254.415
        P = 222.41*(1-0.802871*(1-Tita**33))
    elif ice == ""V"" and 258.661 < T <= 275.748:
        
        Tita = T/258.661
        P = 352.19*(1-1.280388*(1-Tita**7.6))
    elif (ice == ""VI"" and 275.748 < T <= 276.969) or 276.969 < T <= 315:
        
        Tita = T/275.748
        P = 634.53*(1-1.276026*(1-Tita**4))
    else:
        raise NotImplementedError(""Incoming out of bound"")
    return P","Melting Pressure correlation for heavy water

    Parameters
    ----------
    T : float
        Temperature, [K]
    ice: string
        Type of ice: Ih, III, V, VI, VII.
        Below 276.969 is a mandatory input, the ice Ih is the default value.
        Above 276.969, the ice type is unnecesary.

    Returns
    -------
    P : float
        Pressure at melting line, [MPa]

    Notes
    ------
    Raise :class:`NotImplementedError` if input isn't in limit:

        * 254.415 ≤ T ≤ 315

    Examples
    --------
    >>> _D2O__Melting_Pressure(260)
    8.947352740189152e-06
    >>> _D2O__Melting_Pressure(254, ""III"")
    268.6846466336108

    References
    ----------
    IAPWS, Revised Release on the Pressure along the Melting and Sublimation
    Curves of Ordinary Water Substance, http://iapws.org/relguide/MeltSub.html.",1,0,2,3
"def _GetPurgeMessage(most_recent_step, most_recent_wall_time, event_step,
                     event_wall_time, num_expired):
  
  return ('Detected out of order event.step likely caused by a TensorFlow '
          'restart. Purging {} expired tensor events from Tensorboard display '
          'between the previous step: {} (timestamp: {}) and current step: {} '
          '(timestamp: {}).'
         ).format(num_expired, most_recent_step, most_recent_wall_time,
                  event_step, event_wall_time)",Return the string message associated with TensorBoard purges.,0,0,1,1
"def _GetTSKPartitionIdentifiers(self, scan_node):
    
    if not scan_node or not scan_node.path_spec:
      raise errors.ScannerError('Invalid scan node.')

    volume_system = tsk_volume_system.TSKVolumeSystem()
    volume_system.Open(scan_node.path_spec)

    volume_identifiers = self._source_scanner.GetVolumeIdentifiers(
        volume_system)
    if not volume_identifiers:
      return []

    if len(volume_identifiers) == 1:
      return volume_identifiers

    if not self._mediator:
      raise errors.ScannerError(
          'Unable to proceed. Partitions found but no mediator to determine '
          'how they should be used.')

    try:
      volume_identifiers = self._mediator.GetPartitionIdentifiers(
          volume_system, volume_identifiers)

    except KeyboardInterrupt:
      raise errors.UserAbort('File system scan aborted.')

    return self._NormalizedVolumeIdentifiers(
        volume_system, volume_identifiers, prefix='p')","Determines the TSK partition identifiers.

    Args:
      scan_node (SourceScanNode): scan node.

    Returns:
      list[str]: TSK partition identifiers.

    Raises:
      ScannerError: if the format of or within the source is not supported or
          the scan node is invalid or if the volume for a specific identifier
          cannot be retrieved.
      UserAbort: if the user requested to abort.",3,0,5,8
"def _Kw(rho, T):
    
    
    if rho < 0 or rho > 1250 or T < 273.15 or T > 1073.15:
        raise NotImplementedError(""Incoming out of bound"")

    
    d = rho/1000.

    
    Mw = 18.015268

    gamma = [6.1415e-1, 4.825133e4, -6.770793e4, 1.01021e7]
    pKg = 0
    for i, g in enumerate(gamma):
        pKg += g/T**i

    Q = d*exp(-0.864671+8659.19/T-22786.2/T**2*d**(2./3))
    pKw = -12*(log10(1+Q)-Q/(Q+1)*d*(0.642044-56.8534/T-0.375754*d)) + \
        pKg+2*log10(Mw/1000)
    return pKw","Equation for the ionization constant of ordinary water

    Parameters
    ----------
    rho : float
        Density, [kg/m³]
    T : float
        Temperature, [K]

    Returns
    -------
    pKw : float
        Ionization constant in -log10(kw), [-]

    Notes
    ------
    Raise :class:`NotImplementedError` if input isn't in limit:

        * 0 ≤ ρ ≤ 1250
        * 273.15 ≤ T ≤ 1073.15

    Examples
    --------
    >>> _Kw(1000, 300)
    13.906565

    References
    ----------
    IAPWS, Release on the Ionization Constant of H2O,
    http://www.iapws.org/relguide/Ionization.pdf",1,0,2,3
"def _MultiStream(cls, fds):
    

    missing_chunks_by_fd = {}
    for chunk_fd_pairs in collection.Batch(
        cls._GenerateChunkPaths(fds), cls.MULTI_STREAM_CHUNKS_READ_AHEAD):

      chunks_map = dict(chunk_fd_pairs)
      contents_map = {}
      for chunk_fd in FACTORY.MultiOpen(
          chunks_map, mode=""r"", token=fds[0].token):
        if isinstance(chunk_fd, AFF4Stream):
          fd = chunks_map[chunk_fd.urn]
          contents_map[chunk_fd.urn] = chunk_fd.read()

      for chunk_urn, fd in chunk_fd_pairs:
        if chunk_urn not in contents_map or not contents_map[chunk_urn]:
          missing_chunks_by_fd.setdefault(fd, []).append(chunk_urn)

      for chunk_urn, fd in chunk_fd_pairs:
        if fd in missing_chunks_by_fd:
          continue

        yield fd, contents_map[chunk_urn], None

    for fd, missing_chunks in iteritems(missing_chunks_by_fd):
      e = MissingChunksError(
          ""%d missing chunks (multi-stream)."" % len(missing_chunks),
          missing_chunks=missing_chunks)
      yield fd, None, e","Effectively streams data from multiple opened AFF4ImageBase objects.

    Args:
      fds: A list of opened AFF4Stream (or AFF4Stream descendants) objects.

    Yields:
      Tuples (chunk, fd, exception) where chunk is a binary blob of data and fd
      is an object from the fds argument.

      If one or more chunks are missing, exception will be a MissingChunksError
      while chunk will be None. _MultiStream does its best to skip the file
      entirely if one of its chunks is missing, but in case of very large files
      it's still possible to yield a truncated file.",0,1,4,5
"def _MultipleModulesFoundError(path, candidates):
  
  assert len(candidates) > 1
  params = [path] + _StripCommonPathPrefix(candidates[:2])
  if len(candidates) == 2:
    fmt = ERROR_LOCATION_MULTIPLE_MODULES_3
  else:
    fmt = ERROR_LOCATION_MULTIPLE_MODULES_4
    params.append(str(len(candidates) - 2))
  return fmt, params","Generates an error message to be used when multiple matches are found.

  Args:
    path: The breakpoint location path that the user provided.
    candidates: List of paths that match the user provided path. Must
        contain at least 2 entries (throws AssertionError otherwise).

  Returns:
    A (format, parameters) tuple that should be used in the description
    field of the breakpoint error status.",0,0,3,3
"def _PSat_h(h):
    
    
    hmin_Ps3 = _Region1(623.15, Ps_623)[""h""]
    hmax_Ps3 = _Region2(623.15, Ps_623)[""h""]
    if h < hmin_Ps3 or h > hmax_Ps3:
        raise NotImplementedError(""Incoming out of bound"")

    nu = h/2600
    I = [0, 1, 1, 1, 1, 5, 7, 8, 14, 20, 22, 24, 28, 36]
    J = [0, 1, 3, 4, 36, 3, 0, 24, 16, 16, 3, 18, 8, 24]
    n = [0.600073641753024, -0.936203654849857e1, 0.246590798594147e2,
         -0.107014222858224e3, -0.915821315805768e14, -0.862332011700662e4,
         -0.235837344740032e2, 0.252304969384128e18, -0.389718771997719e19,
         -0.333775713645296e23, 0.356499469636328e11, -0.148547544720641e27,
         0.330611514838798e19, 0.813641294467829e38]

    suma = 0
    for i, j, ni in zip(I, J, n):
        suma += ni * (nu-1.02)**i * (nu-0.608)**j
    return 22*suma","Define the saturated line, P=f(h) for region 3

    Parameters
    ----------
    h : float
        Specific enthalpy, [kJ/kg]

    Returns
    -------
    P : float
        Pressure, [MPa]

    Notes
    ------
    Raise :class:`NotImplementedError` if input isn't in limit:

        * h'(623.15K) ≤ h ≤ h''(623.15K)

    References
    ----------
    IAPWS, Revised Supplementary Release on Backward Equations for the
    Functions T(p,h), v(p,h) and T(p,s), v(p,s) for Region 3 of the IAPWS
    Industrial Formulation 1997 for the Thermodynamic Properties of Water and
    Steam, http://www.iapws.org/relguide/Supp-Tv%28ph,ps%293-2014.pdf, Eq 10

    Examples
    --------
    >>> _PSat_h(1700)
    17.24175718
    >>> _PSat_h(2400)
    20.18090839",1,0,2,3
"def _ParseContainerLogJSON(self, parser_mediator, file_object):
    
    container_id = self._GetIdentifierFromPath(parser_mediator)

    text_file_object = text_file.TextFile(file_object)
    for log_line in text_file_object:
      json_log_line = json.loads(log_line)

      time = json_log_line.get('time', None)
      if not time:
        continue

      event_data = DockerJSONContainerLogEventData()
      event_data.container_id = container_id
      event_data.log_line = json_log_line.get('log', None)
      event_data.log_source = json_log_line.get('stream', None)
      
      event_data.offset = 0

      timestamp = timelib.Timestamp.FromTimeString(time)

      event = time_events.TimestampEvent(
          timestamp, definitions.TIME_DESCRIPTION_WRITTEN)
      parser_mediator.ProduceEventWithEventData(event, event_data)","Extract events from a Docker container log files.

    The format is one JSON formatted log message per line.

    The path of each container log file (which logs the container stdout and
    stderr) is:
    DOCKER_DIR/containers/<container_id>/<container_id>-json.log

    Args:
      parser_mediator (ParserMediator): mediates interactions between parsers
          and other components, such as storage and dfvfs.
      file_object (dfvfs.FileIO): a file-like object.",1,0,1,2
"def _ParseCredentialOptions(self, options):
    
    credentials = getattr(options, 'credentials', [])
    if not isinstance(credentials, list):
      raise errors.BadConfigOption('Unsupported credentials value.')

    for credential_string in credentials:
      credential_type, _, credential_data = credential_string.partition(':')
      if not credential_type or not credential_data:
        raise errors.BadConfigOption(
            'Badly formatted credential: {0:s}.'.format(credential_string))

      if credential_type not in self._SUPPORTED_CREDENTIAL_TYPES:
        raise errors.BadConfigOption(
            'Unsupported credential type for: {0:s}.'.format(
                credential_string))

      if credential_type in self._BINARY_DATA_CREDENTIAL_TYPES:
        try:
          credential_data = credential_data.decode('hex')
        except TypeError:
          raise errors.BadConfigOption(
              'Unsupported credential data for: {0:s}.'.format(
                  credential_string))

      self._credentials.append((credential_type, credential_data))","Parses the credential options.

    Args:
      options (argparse.Namespace): command line arguments.

    Raises:
      BadConfigOption: if the options are invalid.",4,0,5,9
"def _ParseDLSPageHeader(self, file_object, page_offset):
    
    page_header_map = self._GetDataTypeMap('dls_page_header')

    try:
      page_header, page_size = self._ReadStructureFromFileObject(
          file_object, page_offset, page_header_map)
    except (ValueError, errors.ParseError) as exception:
      raise errors.ParseError(
          'Unable to parse page header at offset: 0x{0:08x} '
          'with error: {1!s}'.format(page_offset, exception))

    if page_header.signature not in self._DLS_SIGNATURES:
      raise errors.UnableToParseFile(
          'Unsupported page header signature at offset: 0x{0:08x}'.format(
              page_offset))

    return page_header, page_size","Parses a DLS page header from a file-like object.

    Args:
      file_object (file): file-like object to read the header from.
      page_offset (int): offset of the start of the page header, relative
          to the start of the file.

    Returns:
      tuple: containing:

        dls_page_header: parsed record structure.
        int: header size.

    Raises:
      ParseError: when the header cannot be parsed.",3,0,3,6
"def _ReadAttributeValueDateTime(
      self, attribute_values_data, record_offset, attribute_values_data_offset,
      attribute_value_offset):
    
    if attribute_value_offset == 0:
      return None

    data_type_map = self._GetDataTypeMap('keychain_date_time')

    file_offset = (
        record_offset + attribute_values_data_offset + attribute_value_offset)

    attribute_value_offset -= attribute_values_data_offset + 1
    attribute_value_data = attribute_values_data[attribute_value_offset:]

    try:
      date_time_attribute_value = self._ReadStructureFromByteStream(
          attribute_value_data, file_offset, data_type_map)
    except (ValueError, errors.ParseError) as exception:
      raise errors.ParseError((
          'Unable to map date time attribute value data at offset: 0x{0:08x} '
          'with error: {1!s}').format(file_offset, exception))

    return date_time_attribute_value.date_time.rstrip('\x00')","Reads a date time attribute value.

    Args:
      attribute_values_data (bytes): attribute values data.
      record_offset (int): offset of the record relative to the start of
          the file.
      attribute_values_data_offset (int): offset of the attribute values data
          relative to the start of the record.
      attribute_value_offset (int): offset of the attribute relative to
          the start of the record.

    Returns:
      str: date and time values.

    Raises:
      ParseError: if the attribute value cannot be read.",2,0,1,3
"def _Rforce(self,R,z,phi=0.,t=0.):
        
        m= 4.*R*self.a/((R+self.a)**2+z**2)
        return -2.*self.a/R/nu.sqrt((R+self.a)**2+z**2)\
            *(m*(R**2-self.a2-z**2)/4./(1.-m)/self.a/R*special.ellipe(m)
              +special.ellipk(m))","NAME:
           _Rforce
        PURPOSE:
           evaluate the radial force for this potential
        INPUT:
           R - Galactocentric cylindrical radius
           z - vertical height
           phi - azimuth
           t - time
        OUTPUT:
           the radial force
        HISTORY:
           2018-08-04 - Written - Bovy (UofT)",0,0,1,1
"def _SchemedMerge(self, scheme, a, b):
    
    migrated = self._Migrate(b, self.feed_merger.b_schedule, False)
    for attr, merger in scheme.items():
      a_attr = getattr(a, attr, None)
      b_attr = getattr(b, attr, None)
      try:
        merged_attr = merger(a_attr, b_attr)
      except MergeError as merge_error:
        raise MergeError(""Attribute '%s' could not be merged: %s."" % (
            attr, merge_error))
      setattr(migrated, attr, merged_attr)
    return migrated","Tries to merge two entities according to a merge scheme.

    A scheme is specified by a map where the keys are entity attributes and the
    values are merge functions like Merger._MergeIdentical or
    Merger._MergeOptional. The entity is first migrated to the merged schedule.
    Then the attributes are individually merged as specified by the scheme.

    Args:
      scheme: The merge scheme, a map from entity attributes to merge
              functions.
      a: The entity from the old schedule.
      b: The entity from the new schedule.

    Returns:
      The migrated and merged entity.

    Raises:
      MergeError: One of the attributes was not able to be merged.",1,0,2,3
"def _SetCredentials(self, **kwds):
        
        args = {
            'api_key': self._API_KEY,
            'client': self,
            'client_id': self._CLIENT_ID,
            'client_secret': self._CLIENT_SECRET,
            'package_name': self._PACKAGE,
            'scopes': self._SCOPES,
            'user_agent': self._USER_AGENT,
        }
        args.update(kwds)
        
        from apitools.base.py import credentials_lib
        
        
        
        
        
        self._credentials = credentials_lib.GetCredentials(**args)","Fetch credentials, and set them for this client.

        Note that we can't simply return credentials, since creating them
        may involve side-effecting self.

        Args:
          **kwds: Additional keyword arguments are passed on to GetCredentials.

        Returns:
          None. Sets self._credentials.",0,1,2,3
"def _SkipField(tokenizer):
  
  if tokenizer.TryConsume('['):
    
    tokenizer.ConsumeIdentifier()
    while tokenizer.TryConsume('.'):
      tokenizer.ConsumeIdentifier()
    tokenizer.Consume(']')
  else:
    tokenizer.ConsumeIdentifier()

  _SkipFieldContents(tokenizer)

  
  
  if not tokenizer.TryConsume(','):
    tokenizer.TryConsume(';')","Skips over a complete field (name and value/message).

  Args:
    tokenizer: A tokenizer to parse the field name and values.",0,0,2,2
"def __add_introjs_tour_step(self, message, selector=None, name=None,
                                title=None, alignment=None):
        
        if selector != ""html"":
            element_row = ""element: '%s',"" % selector
        else:
            element_row = """"

        if title:
            message = ""<center><b>"" + title + ""</b></center><hr>"" + message

        message = '<font size=\""3\"" color=\""

        step = ( % (element_row, message, alignment))

        self._tour_steps[name].append(step)","Allows the user to add tour steps for a website.
            @Params
            message - The message to display.
            selector - The CSS Selector of the Element to attach to.
            name - If creating multiple tours at the same time,
                   use this to select the tour you wish to add steps to.
            title - Additional header text that appears above the message.
            alignment - Choose from ""top"", ""bottom"", ""left"", and ""right"".
                        (""top"" is the default alignment).",0,0,2,2
"def __at_om_to_im(self, om):
        

        original_om = om
        
        if om[0] == 'U':
            om = om[1:]
            is_um = True
        else:
            is_um = False

        if om == 'r':
            return (original_om, O_RDONLY, False, is_um)
        elif om == 'w':
            return (original_om, O_WRONLY | O_CREAT | O_TRUNC, False, is_um)
        elif om == 'a':
            return (original_om, O_WRONLY | O_CREAT, False, is_um)
        elif om == 'r+':
            return (original_om, O_RDWR | O_CREAT, False, is_um)
        elif om == 'w+':
            return (original_om, O_RDWR | O_CREAT | O_TRUNC, False, is_um)
        elif om == 'a+':
            return (original_om, O_RDWR | O_CREAT, True, is_um)
        else:
            raise Exception(""Outer access mode [%s] is invalid."" % 
                            (original_om))","Convert an ""outer"" access mode to an ""inner"" access mode.
        Returns a tuple of:

            (<system access mode>, <is append>, <is universal newlines>).",1,0,2,3
"def __construct_lda_model(self):
        
        
        repos_of_interest = self.__get_interests()

        
        cleaned_tokens = self.__clean_and_tokenize(repos_of_interest)

        
        
        
        
        
        if not cleaned_tokens:
            cleaned_tokens = [[""zkfgzkfgzkfgzkfgzkfgzkfg""]]

        
        dictionary = corpora.Dictionary(cleaned_tokens)
        corpus = [dictionary.doc2bow(text) for text in cleaned_tokens]

        
        self.lda_model = models.ldamodel.LdaModel(
            corpus, num_topics=1, id2word=dictionary, passes=10
        )","Method to create LDA model to procure list of topics from.

        We do that by first fetching the descriptions of repositories user has
        shown interest in. We tokenize the hence fetched descriptions to
        procure list of cleaned tokens by dropping all the stop words and
        language names from it.

        We use the cleaned and sanitized token list to train LDA model from
        which we hope to procure topics of interests to the authenticated user.",0,1,3,4
"def __driver_helper(self, line):
        
        if line.strip() == '?':
            self.stdout.write('\n')
            self.stdout.write(self.doc_string())
        else:
            toks = shlex.split(line[:-1])
            try:
                msg = self.__get_help_message(toks)
            except Exception as e:
                self.stderr.write('\n')
                self.stderr.write(traceback.format_exc())
                self.stderr.flush()
            self.stdout.write('\n')
            self.stdout.write(msg)
        
        self.stdout.write('\n')
        self.stdout.write(self.prompt)
        self.stdout.write(line)
        self.stdout.flush()","Driver level helper method.

        1.  Display help message for the given input. Internally calls
            self.__get_help_message() to obtain the help message.
        2.  Re-display the prompt and the input line.

        Arguments:
            line: The input line.

        Raises:
            Errors from helper methods print stack trace without terminating
            this shell. Other exceptions will terminate this shell.",4,0,1,5
"def __encoded_params_for_signature(cls, params):
        
        def encoded_pairs(params):
            for k, v in six.iteritems(params):
                if k == 'hmac':
                    continue

                if k.endswith('[]'):
                    
                    k = k.rstrip('[]')
                    v = json.dumps(list(map(str, v)))

                
                k = str(k).replace(""%"", ""%25"").replace(""="", ""%3D"")
                v = str(v).replace(""%"", ""%25"")
                yield '{0}={1}'.format(k, v).replace(""&"", ""%26"")

        return ""&"".join(sorted(encoded_pairs(params)))","Sort and combine query parameters into a single string, excluding those that should be removed and joining with '&'",0,0,3,3
"def __get_or_create_database(self, doc_client, id) -> str:
        
        
        dbs = list(doc_client.QueryDatabases({
            ""query"": ""SELECT * FROM r WHERE r.id=@id"",
            ""parameters"": [
                {""name"": ""@id"", ""value"": id}
            ]
        }))
        
        if len(dbs) > 0:
            return dbs[0]['id']
        else:
            
            res = doc_client.CreateDatabase({'id': id})
            return res['id']","Return the database link.

        Check if the database exists or create the db.

        :param doc_client:
        :param id:
        :return str:",2,0,1,3
"def __get_response(self, uri, params=None, method=""get"", stream=False):
        
        if not hasattr(self, ""session"") or not self.session:
            self.session = requests.Session()
            if self.access_token:
                self.session.headers.update(
                    {'Authorization': 'Bearer {}'.format(self.access_token)}
                )

        
        if params:
            params = {k: v for k, v in params.items() if v is not None}

        kwargs = {
            ""url"": uri,
            ""verify"": True,
            ""stream"": stream
        }

        kwargs[""params"" if method == ""get"" else ""data""] = params

        return getattr(self.session, method)(**kwargs)","Creates a response object with the given params and option

            Parameters
            ----------
            url : string
                The full URL to request.
            params: dict
                A list of parameters to send with the request.  This
                will be sent as data for methods that accept a request
                body and will otherwise be sent as query parameters.
            method : str
                The HTTP method to use.
            stream : bool
                Whether to stream the response.

            Returns a requests.Response object.",0,1,1,2
"def __ginibre_matrix(nrow, ncol=None, seed=None):
    
    if ncol is None:
        ncol = nrow
    if seed is not None:
        np.random.seed(seed)
    G = np.random.normal(size=(nrow, ncol)) + \
        np.random.normal(size=(nrow, ncol)) * 1j
    return G","Return a normally distributed complex random matrix.

    Args:
        nrow (int): number of rows in output matrix.
        ncol (int): number of columns in output matrix.
        seed (int): Optional. To set a random seed.
    Returns:
        ndarray: A complex rectangular matrix where each real and imaginary
            entry is sampled from the normal distribution.",0,0,2,2
"def __insert_internal_blob(self, key, blob, compressed=True):
        
        with self.get_conn() as conn:
            conn.isolation_level = None
            c = conn.cursor()
            try:
                compressed_flag = 1 if compressed else 0
                if compressed:
                    blob = zlib.compress(blob)
                c.execute(""BEGIN"")
                c.execute(""INSERT INTO cache_entries (key, value) VALUES (?,?)"", (key, JiCache.INTERNAL_BLOB))
                c.execute(""INSERT INTO blob_entries (key, compressed, blob_data) VALUES (?,?,?)"", (key, compressed_flag, sqlite3.Binary(blob),))
                c.execute(""COMMIT"")
                return True
            except:
                getLogger().debug(""Cannot insert"")
                return False",This method will insert blob data to blob table,4,1,1,6
"def __load_list(self, config_filename, file_names):
        
        self._read_configuration_file(config_filename)
        self.connect()
        self.find_source_files_from_list(file_names)
        self._get_column_type()
        self.__read_stored_routine_metadata()
        self.__get_constants()
        self._get_old_stored_routine_info()
        self._get_correct_sql_mode()
        self.__load_stored_routines()
        self.__write_stored_routine_metadata()
        self.disconnect()","Loads all stored routines in a list into the RDBMS instance.

        :param str config_filename: The filename of the configuration file.
        :param list[str] file_names: The list of files to be loaded.",4,0,1,5
"def __parse_hits(self, hit_raw):
        

        
        bs_result = bs4.BeautifulSoup(hit_raw, 'html.parser')
        hit_string = bs_result.find(""div"", id=""resultStats"").text

        
        hit_string = hit_string.replace(',', u'')
        hit_string = hit_string.replace('.', u'')

        fetched_on = datetime_utcnow().timestamp()
        id_args = self.keywords[:]
        id_args.append(str(fetched_on))

        hits_json = {
            'fetched_on': fetched_on,
            'id': uuid(*id_args),
            'keywords': self.keywords,
            'type': 'googleSearchHits'
        }

        if not hit_string:
            logger.warning(""No hits for %s"", self.keywords)
            hits_json['hits'] = 0

            return hits_json

        str_hits = re.search(r'\d+', hit_string).group(0)
        hits = int(str_hits)
        hits_json['hits'] = hits

        return hits_json",Parse the hits returned by the Google Search API,0,1,1,2
"def __proxy_to_real_parser(value):
    
    if isinstance(value, ProxyArgumentParser):
        return __parsers[value]
    elif any(isinstance(value, t) for t in [list, tuple]):
        new_value = []
        for subvalue in iter(value):
            new_value.append(__proxy_to_real_parser(subvalue))
        return new_value
    return value","This recursively converts ProxyArgumentParser instances to actual parsers.

    Use case: defining subparsers with a parent
      >>> [...]
      >>> parser.add_argument(...)  # argument common to all subparsers
      >>> subparsers = parser.add_subparsers()
      >>> subparsers.add_parser(..., parents=[parent])
                                                ^
                              this is an instance of ProxyArgumentParser
                              and must be converted to an actual parser instance

    :param value: a value coming from args or kwargs aimed to an actual parser",0,0,1,1
"def __request_except(self, requestId, exc, set_and_forget=True):
        
        try:
            with self.__requests:
                if set_and_forget:
                    req = self.__requests.pop(requestId)
                else:
                    req = self.__requests[requestId]
        except KeyError:
            logger.error('Unknown request %s - cannot set exception', requestId)
        else:
            if exc is not None:
                req.exception = exc
            if set_and_forget:
                req._set()","Set exception (if not None) for the given request and (optionally) remove from internal cache & setting its
           event",0,2,2,4
"def __run_sql_file(self, filepath):
        

        with open(filepath, 'r') as delta_file:
            sql = delta_file.read()
            if self.variables:
                self.cursor.execute(sql, self.variables)
            else:
                self.cursor.execute(sql)
            self.connection.commit()","Execute the sql file at the passed path

        Parameters
        ----------
        filepath: str
            the path of the file to execute",3,1,1,5
"def __set_baudrate(self, baud):
        
        log.info('Changing communication to %s baud', baud)
        self.__writeln(UART_SETUP.format(baud=baud))
        
        time.sleep(0.1)
        try:
            self._port.setBaudrate(baud)
        except AttributeError:
            
            self._port.baudrate = baud",setting baudrate if supported,0,1,1,2
"def _access_token_endpoint(self, grantType, extraParams={}):
        
        response = requests.post(
            self._format_url(OAUTH2_ROOT + 'access_token'),
            data = _extend({
                'grant_type': grantType,
                'client_id': self.client.get('client_id', ''),
                'client_secret': self.client.get('client_secret', ''),
                'redirect_uri': self.client.get('redirect_uri', '')
            }, extraParams))
        
        data = response.json()
        if 'error' in data or 'error_description' in data:
            raise _token_error_from_data(data)
        else:
            return self.set_creds(data)",Base exchange of data for an access_token.,1,1,2,4
"def _add(self, other):
        
        if isinstance(other, self.__class__):
            sum_ = self._ip_dec + other._ip_dec
        elif isinstance(other, int):
            sum_ = self._ip_dec + other
        else:
            other = self.__class__(other)
            sum_ = self._ip_dec + other._ip_dec
        return sum_",Sum two IP addresses.,0,0,2,2
"def _addPeptide(self, sequence, proteinId, digestInfo):
        
        stdSequence = self.getStdSequence(sequence)

        if stdSequence not in self.peptides:
            self.peptides[stdSequence] = PeptideEntry(
                stdSequence, mc=digestInfo['missedCleavage']
            )
        if sequence not in self.peptides:
            self.peptides[sequence] = self.peptides[stdSequence]

        if proteinId not in self.peptides[stdSequence].proteins:
            
            
            self.peptides[stdSequence].proteins.add(proteinId)
            self.peptides[stdSequence].proteinPositions[proteinId] = (
                                    digestInfo['startPos'], digestInfo['endPos']
                                    )
            self.proteins[proteinId].peptides.add(sequence)","Add a peptide to the protein database.

        :param sequence: str, amino acid sequence
        :param proteinId: str, proteinId
        :param digestInfo: dict, contains information about the in silico digest
            must contain the keys 'missedCleavage', 'startPos' and 'endPos'",1,0,1,2
"def _addRecordToKNN(self, record):
    
    classifier = self.htm_prediction_model._getAnomalyClassifier()
    knn = classifier.getSelf()._knn

    prototype_idx = classifier.getSelf().getParameter('categoryRecencyList')
    category = self._labelListToCategoryNumber(record.anomalyLabel)

    
    if record.ROWID in prototype_idx:
      knn.prototypeSetCategory(record.ROWID, category)
      return

    
    pattern = self._getStateAnomalyVector(record)
    rowID = record.ROWID
    knn.learn(pattern, category, rowID=rowID)",This method will add the record to the KNN classifier.,0,0,1,1
"def _add_ce_record(self, curr_dr_len, thislen):
        
        
        if self.dr_entries.ce_record is None:
            self.dr_entries.ce_record = RRCERecord()
            self.dr_entries.ce_record.new()
            curr_dr_len += RRCERecord.length()
        self.dr_entries.ce_record.add_record(thislen)
        return curr_dr_len","An internal method to add a new length to a Continuation Entry.  If the
        Continuation Entry does not yet exist, this method creates it.

        Parameters:
         curr_dr_len - The current Directory Record length.
         thislen - The new length to add to the Continuation Entry.
        Returns:
         An integer representing the current directory record length after
         adding the Continuation Entry.",0,0,3,3
"def _add_conntrack_stats_metrics(self, conntrack_path, tags):
        
        try:
            output, _, _ = get_subprocess_output([""sudo"", conntrack_path, ""-S""], self.log)
            
            
            
            
            

            lines = output.splitlines()

            for line in lines:
                cols = line.split()
                cpu_num = cols[0].split('=')[-1]
                cpu_tag = ['cpu:{}'.format(cpu_num)]
                cols = cols[1:]

                for cell in cols:
                    metric, value = cell.split('=')
                    self.monotonic_count('system.net.conntrack.{}'.format(metric), int(value), tags=tags + cpu_tag)
        except SubprocessOutputEmptyError:
            self.log.debug(""Couldn't use {} to get conntrack stats"".format(conntrack_path))","Parse the output of conntrack -S
        Add the parsed metrics",0,1,2,3
"def _add_current_usage(self, value, maximum=None, resource_id=None,
                           aws_type=None):
        
        self._current_usage.append(
            AwsLimitUsage(
                self,
                value,
                maximum=maximum,
                resource_id=resource_id,
                aws_type=aws_type
            )
        )","Add a new current usage value for this limit.

        Creates a new :py:class:`~.AwsLimitUsage` instance and
        appends it to the internal list. If more than one usage value
        is given to this service, they should have ``id`` and
        ``aws_type`` set.

        This method should only be called from the :py:class:`~._AwsService`
        instance that created and manages this Limit.

        :param value: the numeric usage value
        :type value: :py:obj:`int` or :py:obj:`float`
        :param resource_id: If there can be multiple usage values for one limit,
          an AWS ID for the resource this instance describes
        :type resource_id: str
        :param aws_type: if ``id`` is not None, the AWS resource type
          that ID represents. As a convention, we use the AWS Resource
          Type names used by
          `CloudFormation <http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-template-resource-type-ref.html>`_  # noqa
        :type aws_type: str",0,0,1,1
"def _add_logger_by_name(self, name):
        
        data = dict(request.forms)
        loc = data.pop('loc', '')
        port = data.pop('port', None)
        conn_type = data.pop('conn_type', None)

        if not port or not conn_type:
            e = 'Port and/or conn_type not set'
            raise ValueError(e)
        address = [loc, int(port)]

        if 'rotate_log' in data:
            data['rotate_log'] = True if data == 'true' else False

        if 'rotate_log_delta' in data:
            data['rotate_log_delta'] = int(data['rotate_log_delta'])

        self._logger_manager.add_logger(name, address, conn_type, **data)","Handles POST requests for adding a new logger.

        Expects logger configuration to be passed in the request's query string.
        The logger name is included in the URL and the address components and
        connection type should be included as well. The loc attribute is
        defaulted to ""localhost"" when making the socket connection if not
        defined.

        loc = IP / interface
        port = port / protocol
        conn_type = udp or ethernet

        Raises:
            ValueError:
                if the port or connection type are not supplied.",1,0,3,4
"def _add_to_batch(self, partition_key, row_key, request):
        
        
        if self._partition_key:
            if self._partition_key != partition_key:
                raise AzureBatchValidationError(_ERROR_INCORRECT_PARTITION_KEY_IN_BATCH)
        else:
            self._partition_key = partition_key

        
        if row_key in self._row_keys:
            raise AzureBatchValidationError(_ERROR_DUPLICATE_ROW_KEY_IN_BATCH)
        else:
            self._row_keys.append(row_key)

        
        if len(self._requests) >= 100:
            raise AzureBatchValidationError(_ERROR_TOO_MANY_ENTITIES_IN_BATCH)

        
        self._requests.append((row_key, request))","Validates batch-specific rules.
        
        :param str partition_key:
            PartitionKey of the entity.
        :param str row_key:
            RowKey of the entity.
        :param request:
            the request to insert, update or delete entity",3,0,4,7
"def _api_call(self, path, data={}, http_method=requests.get):
        
        log.info('performing api request', path=path)
        response = http_method('/'.join([self.api_url, path]),
                               params={'auth_token': self.api_key},
                               data=data)
        log.debug('{} remaining calls'.format(
            response.headers['x-ratelimit-remaining']))
        return response.json()",Process an http call against the hipchat api,0,3,0,3
"def _api_delete(path, data, server=None):
    
    server = _get_server(server)
    response = requests.delete(
            url=_get_url(server['ssl'], server['url'], server['port'], path),
            auth=_get_auth(server['user'], server['password']),
            headers=_get_headers(),
            params=data,
            verify=False
    )
    return _api_response(response)",Do a DELETE request to the API,0,1,1,2
"def _authenticate(self):
        
        opts = {'domain': self._domain}
        opts.update(self._auth)
        response = self._api.domain.info(opts)
        self._validate_response(
            response=response, message='Failed to authenticate')

        
        
        self.domain_id = 1

        return True","run any request against the API just to make sure the credentials
        are valid

        :return bool: success status
        :raises Exception: on error",0,1,2,3
"def _authorization_code_flow(self):
        
        options = {
            'scope': getattr(self, 'scope', 'non-expiring'),
            'client_id': self.options.get('client_id'),
            'response_type': 'code',
            'redirect_uri': self._redirect_uri()
        }
        url = '%s%s/connect' % (self.scheme, self.host)
        self._authorize_url = '%s?%s' % (url, urlencode(options))",Build the the auth URL so the user can authorize the app.,0,0,1,1
"def _begin(self, client=None, retry=DEFAULT_RETRY):
        
        if self.state is not None:
            raise ValueError(""Job already begun."")

        client = self._require_client(client)
        path = ""/projects/%s/jobs"" % (self.project,)

        
        
        api_response = client._call_api(
            retry, method=""POST"", path=path, data=self.to_api_repr()
        )
        self._set_properties(api_response)","API call:  begin the job via a POST request

        See
        https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs/insert

        :type client: :class:`~google.cloud.bigquery.client.Client` or
                      ``NoneType``
        :param client: the client to use.  If not passed, falls back to the
                       ``client`` stored on the current dataset.

        :type retry: :class:`google.api_core.retry.Retry`
        :param retry: (Optional) How to retry the RPC.

        :raises: :exc:`ValueError` if the job has already begin.",1,1,2,4
"def _bfd_rx(self, **kwargs):
        
        int_type = kwargs['int_type']
        method_name = 'interface_%s_bfd_interval_min_rx' % int_type
        bfd_rx = getattr(self._interface, method_name)
        config = bfd_rx(**kwargs)
        if kwargs['delete']:
            tag = 'min-rx'
            config.find('.//*%s' % tag).set('operation', 'delete')
            pass
        return config","Return the BFD minimum receive interval XML.

        You should not use this method.
        You probably want `BGP.bfd`.

        Args:
            min_rx (str): BFD receive interval in milliseconds (300, 500, etc)
            delete (bool): Remove the configuration if ``True``.

        Returns:
            XML to be passed to the switch.

        Raises:
            None",0,0,2,2
"def _build_purchase_item(course_id, course_url, cost_in_cents, mode, course_data, sku):
    

    
    item = {
        'id': ""{}-{}"".format(course_id, mode),
        'url': course_url,
        'price': cost_in_cents,
        'qty': 1,
    }

    
    if 'title' in course_data:
        item['title'] = course_data['title']
    else:
        
        item['title'] = 'Course {} mode: {}'.format(course_id, mode)

    if 'tags' in course_data:
        item['tags'] = course_data['tags']

    
    item['vars'] = dict(course_data.get('vars', {}), mode=mode, course_run_id=course_id)

    item['vars']['purchase_sku'] = sku

    return item",Build and return Sailthru purchase item object,0,0,2,2
"def _build_time(time, kwargs):
    
    tz = kwargs.pop('tz', 'UTC')
    if time:
        if kwargs:
            raise ValueError('Cannot pass kwargs and a time')
        else:
            return ensure_utc(time, tz)
    elif not kwargs:
        raise ValueError('Must pass a time or kwargs')
    else:
        return datetime.time(**kwargs)",Builds the time argument for event rules.,2,0,4,6
"def _calcSpecificity(self):
        
        hashCount = 0
        qualifierCount = 0
        elementCount = int(self.name != '*')
        for q in self.qualifiers:
            if q.isHash():
                hashCount += 1
            elif q.isClass():
                qualifierCount += 1
            elif q.isAttr():
                qualifierCount += 1
            elif q.isPseudo():
                elementCount += 1
            elif q.isCombiner():
                i, h, q, e = q.selector.specificity()
                hashCount += h
                qualifierCount += q
                elementCount += e
        return self.inline, hashCount, qualifierCount, elementCount",from http://www.w3.org/TR/CSS21/cascade.html#specificity,0,0,3,3
"def _cartesian(arraySizes, out=None):
    
    arrays = []
    for i in range(len(arraySizes)):
        arrays.append(nu.arange(0, arraySizes[i]))

    arrays = [nu.asarray(x) for x in arrays]
    dtype = arrays[0].dtype
   
    n = nu.prod([x.size for x in arrays])
    if out is None:
        out = nu.zeros([n, len(arrays)], dtype=dtype)
    
    m = n // arrays[0].size
    out[:,0] = nu.repeat(arrays[0], m)
    if arrays[1:]:
        _cartesian(arraySizes[1:], out=out[0:m,1:])
        for j in range(1, arrays[0].size):
            out[j*m:(j+1)*m,1:] = out[0:m,1:]
    return out","NAME:
        cartesian
    PURPOSE:
        Generate a cartesian product of input arrays.
    INPUT: 
        arraySizes - list of size of arrays
        out - Array to place the cartesian product in.
    OUTPUT: 
        2-D array of shape (product(arraySizes), len(arraySizes)) containing cartesian products
        formed of input arrays.
    HISTORY:
        2016-06-02 - Obtained from
        http://stackoverflow.com/questions/1208118/using-numpy-to-build-an-array-of-all-combinations-of-two-arrays",0,0,1,1
"def _chattrib(name, key, value, param, persist=False, root=None):
    
    pre_info = info(name, root=root)
    if not pre_info:
        raise CommandExecutionError('User \'{0}\' does not exist'.format(name))

    if value == pre_info[key]:
        return True

    cmd = ['usermod']

    if root is not None and __grains__['kernel'] != 'AIX':
        cmd.extend(('-R', root))

    if persist and __grains__['kernel'] != 'OpenBSD':
        cmd.append('-m')

    cmd.extend((param, value, name))

    __salt__['cmd.run'](cmd, python_shell=False)
    return info(name, root=root).get(key) == value",Change an attribute for a named user,1,1,3,5
"def _check_appliances():
    
    filenames = os.listdir(os.getcwd())
    missing = []
    for dirname in ['nodes', 'environments', 'roles', 'cookbooks', 'data_bags']:
        if (dirname not in filenames) or (not os.path.isdir(dirname)):
            missing.append(dirname)
    return (not bool(missing)), missing","Looks around and return True or False based on whether we are in a
    kitchen",0,0,2,2
"def _check_for_life_signs(self):
        
        if not self._running.is_set():
            return False
        if self._writes_since_check == 0:
            self.send_heartbeat_impl()
        self._lock.acquire()
        try:
            if self._reads_since_check == 0:
                self._threshold += 1
                if self._threshold >= 2:
                    self._running.clear()
                    self._raise_or_append_exception()
                    return False
            else:
                self._threshold = 0
        finally:
            self._reads_since_check = 0
            self._writes_since_check = 0
            self._lock.release()

        return self._start_new_timer()","Check Connection for life signs.

            First check if any data has been sent, if not send a heartbeat
            to the remote server.

            If we have not received any data what so ever within two
            intervals, we need to raise an exception so that we can
            close the connection.

        :rtype: bool",2,2,3,7
"def _check_hetcaller(item):
    
    svs = _get_as_list(item, ""svcaller"")
    hets = _get_as_list(item, ""hetcaller"")
    if hets or any([x in svs for x in [""titancna"", ""purecn""]]):
        if not any([x in svs for x in [""cnvkit"", ""gatk-cnv""]]):
            raise ValueError(""Heterogeneity caller used but need CNV calls. Add `gatk4-cnv` ""
                             ""or `cnvkit` to `svcaller` in sample: %s"" % item[""description""])",Ensure upstream SV callers requires to heterogeneity analysis are available.,1,0,2,3
"def _check_hla_alleles(
            alleles,
            valid_alleles=None):
        
        require_iterable_of(alleles, string_types, ""HLA alleles"")

        
        
        alleles = {
            normalize_allele_name(allele.strip().upper())
            for allele in alleles
        }
        if valid_alleles:
            
            
            missing_alleles = [
                allele
                for allele in alleles
                if allele not in valid_alleles
            ]
            if len(missing_alleles) > 0:
                raise UnsupportedAllele(
                    ""Unsupported HLA alleles: %s"" % missing_alleles)

        return list(alleles)","Given a list of HLA alleles and an optional list of valid
        HLA alleles, return a set of alleles that we will pass into
        the MHC binding predictor.",1,0,2,3
"def _check_if_downloaded(self):
        
        if not os.path.isfile(self.path + self.file_name):
            print("""")
            self.msg.template(78)
            print(""| Download '{0}' file [ {1}FAILED{2} ]"".format(
                self.file_name, self.meta.color[""RED""],
                self.meta.color[""ENDC""]))
            self.msg.template(78)
            print("""")
            if not self.msg.answer() in [""y"", ""Y""]:
                raise SystemExit()",Check if file downloaded,1,0,2,3
"def _check_nan(self, epoch_data: EpochData) -> None:
        
        for stream_name in epoch_data.keys():
            stream_data = epoch_data[stream_name]
            variables = self._variables if self._variables is not None else stream_data.keys()
            for variable in variables:
                if variable not in stream_data:
                    raise KeyError('Variable `{}` to be nan-checked was not found in the batch data for stream `{}`. '
                                   'Available variables are `{}`.'.format(variable, stream_name, stream_data.keys()))

                value = stream_data[variable]
                if self._is_nan(variable, value):
                    raise TrainingTerminated('Variable `{}` is NaN.'.format(variable))","Raise an exception when some of the monitored data is NaN.

        :param epoch_data: epoch data checked
        :raise KeyError: if the specified variable is not found in the stream
        :raise ValueError: if the variable value is of unsupported type and ``self._on_unknown_type`` is set to ``error``",2,0,3,5
"def _check_result(self, result):
    
    if not isinstance(result, tuple):
      return (FAIL, APIViolationError(
        'Result must be a tuple but '
        'it is {}.'.format(type(result)), result))

    if len(result) != 2:
      return (FAIL, APIViolationError(
        'Result must have 2 items, but it '
        'has {}.'.format(len(result)), result))

    status, message = result
    
    if isinstance(status, bool):
      
      status = PASS if status else FAIL
      result = (status, message)

    if not isinstance(status, Status):
      return (FAIL, APIViolationError(
        'Result item `status` must be an instance of '
        'Status, but it is {} a {}.'.format(status, type(status)), result))

    return result","Check that the check returned a well formed result:
          a tuple (<Status>, message)

        A boolean Status is allowd and will be transformed to:
        True <Status: PASS>, False <Status: FAIL>

       Checks will be implemented by other parties. This is to
       help implementors creating good checks, to spot erroneous
       implementations early and to make it easier to handle
       the results tuple.",0,0,4,4
"def _clean_filepath(self, filepath):
        
        if (os.path.isdir(filepath) and
                os.path.isfile(os.path.join(filepath, '__init__.py'))):

            filepath = os.path.join(filepath, '__init__.py')

        if (not filepath.endswith('.py') and
                os.path.isfile(filepath + '.py')):
            filepath += '.py'
        return filepath","processes the filepath by checking if it is a directory or not
        and adding `.py` if not present.",0,0,3,3
"def _close_cursor_now(self, cursor_id, address=None, session=None):
        
        if not isinstance(cursor_id, integer_types):
            raise TypeError(""cursor_id must be an instance of (int, long)"")

        if self.__cursor_manager is not None:
            self.__cursor_manager.close(cursor_id, address)
        else:
            try:
                self._kill_cursors(
                    [cursor_id], address, self._get_topology(), session)
            except PyMongoError:
                
                self.__kill_cursors_queue.append((address, [cursor_id]))","Send a kill cursors message with the given id.

        What closing the cursor actually means depends on this client's
        cursor manager. If there is none, the cursor is closed synchronously
        on the current thread.",1,2,2,5
"def _collect_data(directory):
  
  
  data_files = []
  transcripts = [
      filename for filename in os.listdir(directory)
      if filename.endswith("".csv"")
  ]
  for transcript in transcripts:
    transcript_path = os.path.join(directory, transcript)
    with open(transcript_path, ""r"") as transcript_file:
      transcript_reader = csv.reader(transcript_file)
      
      _ = next(transcript_reader)
      for transcript_line in transcript_reader:
        media_name, label = transcript_line[0:2]
        filename = os.path.join(directory, media_name)
        data_files.append((media_name, filename, label))
  return data_files","Traverses directory collecting input and target files.

  Args:
   directory: base path to extracted audio and transcripts.
  Returns:
   list of (media_base, media_filepath, label) tuples",1,0,2,3
"def _compute_bbox(self, fig, kw):
        
        fig_id = id(fig)
        if kw['bbox_inches'] == 'tight':
            if not fig_id in MPLRenderer.drawn:
                fig.set_dpi(self.dpi)
                fig.canvas.draw()
                extra_artists = kw.pop(""bbox_extra_artists"", [])
                pad = mpl.rcParams['savefig.pad_inches']
                bbox_inches = get_tight_bbox(fig, extra_artists, pad=pad)
                MPLRenderer.drawn[fig_id] = bbox_inches
                kw['bbox_inches'] = bbox_inches
            else:
                kw['bbox_inches'] = MPLRenderer.drawn[fig_id]
        return kw","Compute the tight bounding box for each figure once, reducing
        number of required canvas draw calls from N*2 to N+1 as a
        function of the number of frames.

        Tight bounding box computing code here mirrors:
        matplotlib.backend_bases.FigureCanvasBase.print_figure
        as it hasn't been factored out as a function.",3,0,2,5
"def _compute_mean(self, C, mag, rrup):
        
        mean = (C['c1'] +
                self._compute_term1(C, mag) +
                self._compute_term2(C, mag, rrup) +
                self._compute_term3(C, rrup))
        return mean","Compute mean value according to equation 30, page 1021.",0,0,1,1
"def _config2indy(self, config: dict) -> dict:
        

        assert {'name', 'id'} & {k for k in config}
        return {
            'id': config.get('name', config.get('id')),
            'storage_type': config.get('storage_type', self.default_storage_type),
            'freshness_time': config.get('freshness_time', self.default_freshness_time)
        }","Given a configuration dict with indy and possibly more configuration values, return the
        corresponding indy wallet configuration dict from current default and input values.

        :param config: input configuration
        :return: configuration dict for indy wallet",0,0,2,2
"def _connect(self, endpoint: str):
        
        if self._socket:
            raise RuntimeError('Cannot run multiple Servers on the same socket')

        context = zmq.asyncio.Context()
        self._socket = context.socket(zmq.ROUTER)
        self._socket.bind(endpoint)

        _log.info(""Starting server, listening on endpoint {}"".format(endpoint))","Connect the server to an endpoint. Creates a ZMQ ROUTER socket for the given endpoint.

        :param endpoint: Socket endpoint, e.g. ""tcp://*:1234""",1,2,1,4
"def _construct_SAX_chunk_dict(self, sax):
        
        frequency = defaultdict(int)
        chunk_size = self.chunk_size
        length = len(sax)
        for i in range(length):
            if i + chunk_size <= length:
                chunk = sax[i: i + chunk_size]
                frequency[chunk] += 1
        return frequency","Form a chunk frequency dictionary from a SAX representation.
        :param str sax: a SAX representation.
        :return dict: frequency dictionary for chunks in the SAX representation.",0,0,2,2
"def _contains(expr, pat, case=True, flags=0, regex=True):
    

    if regex and isinstance(pat, six.string_types):
        import re
        try:
            re.compile(pat, flags=flags)
        except:
            raise ValueError('Failed to compile regular expression, '
                             'please check re.compile(""{0}"")'.format(pat))

    return _string_op(expr, Contains, output_type=types.boolean,
                      _pat=pat, _case=case, _flags=flags, _regex=regex)","Return boolean sequence whether given pattern/regex is contained in each string in the sequence

    :param expr: sequence or scalar
    :param pat: Character sequence or regular expression
    :param case: If True, case sensitive
    :type case: bool
    :param flags: re module flags, e.g. re.IGNORECASE
    :param regex: If True use regex, otherwise use string finder
    :return: sequence or scalar",1,0,2,3
"def _convert_hexstr_base(hexstr, base):
    r
    if base is _ALPHABET_16:
        
        return hexstr
    baselen = len(base)
    x = int(hexstr, 16)  
    if x == 0:
        return '0'
    sign = 1 if x > 0 else -1
    x *= sign
    digits = []
    while x:
        digits.append(base[x % baselen])
        x //= baselen
    if sign < 0:
        digits.append('-')
    digits.reverse()
    newbase_str = ''.join(digits)
    return newbase_str","r""""""
    Packs a long hexstr into a shorter length string with a larger base.

    Args:
        hexstr (str): string of hexidecimal symbols to convert
        base (list): symbols of the conversion base

    Example:
        >>> print(_convert_hexstr_base('ffffffff', _ALPHABET_26))
        nxmrlxv
        >>> print(_convert_hexstr_base('0', _ALPHABET_26))
        0
        >>> print(_convert_hexstr_base('-ffffffff', _ALPHABET_26))
        -nxmrlxv
        >>> print(_convert_hexstr_base('aafffff1', _ALPHABET_16))
        aafffff1

    Sympy:
        >>> import sympy as sy
        >>> # Determine the length savings with lossless conversion
        >>> consts = dict(hexbase=16, hexlen=256, baselen=27)
        >>> symbols = sy.symbols('hexbase, hexlen, baselen, newlen')
        >>> haexbase, hexlen, baselen, newlen = symbols
        >>> eqn = sy.Eq(16 ** hexlen,  baselen ** newlen)
        >>> newlen_ans = sy.solve(eqn, newlen)[0].subs(consts).evalf()
        >>> print('newlen_ans = %r' % (newlen_ans,))
        >>> # for a 26 char base we can get 216
        >>> print('Required length for lossless conversion len2 = %r' % (len2,))
        >>> def info(base, len):
        ...     bits = base ** len
        ...     print('base = %r' % (base,))
        ...     print('len = %r' % (len,))
        ...     print('bits = %r' % (bits,))
        >>> info(16, 256)
        >>> info(27, 16)
        >>> info(27, 64)
        >>> info(27, 216)",0,0,1,1
"def _convert_md_type(self, type_to_convert: str):
        
        if type_to_convert in FILTER_TYPES:
            return FILTER_TYPES.get(type_to_convert)
        elif type_to_convert in FILTER_TYPES.values():
            return [k for k, v in FILTER_TYPES.items() if v == type_to_convert][0]
        else:
            raise ValueError(
                ""Incorrect metadata type to convert: {}"".format(type_to_convert)
            )","Metadata types are not consistent in Isogeo API. A vector dataset is
         defined as vector-dataset in query filter but as vectorDataset in
         resource (metadata) details.

        see: https://github.com/isogeo/isogeo-api-py-minsdk/issues/29",1,0,3,4
"def _convert_to_style(cls, style_dict, num_format_str=None):
        
        import xlwt

        if style_dict:
            xlwt_stylestr = cls._style_to_xlwt(style_dict)
            style = xlwt.easyxf(xlwt_stylestr, field_sep=',', line_sep=';')
        else:
            style = xlwt.XFStyle()
        if num_format_str is not None:
            style.num_format_str = num_format_str

        return style","converts a style_dict to an xlwt style object
        Parameters
        ----------
        style_dict : style dictionary to convert
        num_format_str : optional number format string",0,0,2,2
"def _create_affine(x_axis, y_axis, z_axis, image_pos, voxel_sizes):
    

    

    affine = numpy.array(
        [[x_axis[0] * voxel_sizes[0], y_axis[0] * voxel_sizes[1], z_axis[0] * voxel_sizes[2], image_pos[0]],
         [x_axis[1] * voxel_sizes[0], y_axis[1] * voxel_sizes[1], z_axis[1] * voxel_sizes[2], image_pos[1]],
         [x_axis[2] * voxel_sizes[0], y_axis[2] * voxel_sizes[1], z_axis[2] * voxel_sizes[2], image_pos[2]],
         [0, 0, 0, 1]])
    return affine","Function to generate the affine matrix for a dicom series
    This method was based on (http://nipy.org/nibabel/dicom/dicom_orientation.html)

    :param sorted_dicoms: list with sorted dicom files",0,0,1,1
"def _create_genome_regions(data):
    
    work_dir = utils.safe_makedir(os.path.join(dd.get_work_dir(data), ""coverage"", dd.get_sample_name(data)))
    variant_regions = os.path.join(work_dir, ""target-genome.bed"")
    with file_transaction(data, variant_regions) as tx_variant_regions:
        with open(tx_variant_regions, ""w"") as out_handle:
            for c in shared.get_noalt_contigs(data):
                out_handle.write(""%s\t%s\t%s\n"" % (c.name, 0, c.size))
    return variant_regions","Create whole genome contigs we want to process, only non-alts.

    Skips problem contigs like HLAs for downstream analysis.",2,0,1,3
"def _create_http_client():
    
    global _http_client

    defaults = {'user_agent': USER_AGENT}
    auth_username, auth_password = _credentials
    if auth_username and auth_password:
        defaults['auth_username'] = auth_username
        defaults['auth_password'] = auth_password

    _http_client = httpclient.AsyncHTTPClient(
        force_instance=True, defaults=defaults,
        max_clients=_max_clients)",Create the HTTP client with authentication credentials if required.,0,1,1,2
"def _create_network_doscript(self, network_file_path):
        
        
        LOG.debug('Creating network doscript in the folder %s'
                  % network_file_path)
        network_doscript = os.path.join(network_file_path, 'network.doscript')
        tar = tarfile.open(network_doscript, ""w"")
        for file in os.listdir(network_file_path):
            file_name = os.path.join(network_file_path, file)
            tar.add(file_name, arcname=file)
        tar.close()
        return network_doscript","doscript: contains a invokeScript.sh which will do the special work

        The network.doscript contains network configuration files and it will
        be used by zvmguestconfigure to configure zLinux os network when it
        starts up",2,1,0,3
"def _create_tags(ctx):
    ""create all classes and put them in ctx""

    for (tag, info) in _TAGS.items():
        class_name = tag.title()
        quote_, compact, self_closing, docs = info

        def __init__(self, *childs, **attrs):
            TagBase.__init__(self, childs, attrs)

        cls = type(class_name, (TagBase,), {
            ""__doc__"": docs,
            ""__init__"": __init__
        })

        cls.QUOTE = quote_
        cls.COMPACT = compact
        cls.SELF_CLOSING = self_closing

        ctx[class_name] = cls",create all classes and put them in ctx,0,0,2,2
"def _create_user_profiles(self, profiles, file, valid_records,
                              ip_user=False, year=None, week=None):
        
        for record in file.get_records():
            recid = record[2]
            if not valid_records.get(recid, None):
                
                continue

            if ip_user:
                ip = record[4]
                user_agent = record[5]
                
                user_id = ""{0}-{1}_{2}_{3}"".format(year, week, ip, user_agent)
                try:
                    uid = hashlib.md5(user_id.encode('utf-8')).hexdigest()
                except UnicodeDecodeError:
                    logger.info(""UnicodeDecodeError {}"".format(user_id))
            else:
                uid = record[1]

            profiles[uid].append(recid)

        return profiles","Create user profiles with all the records visited or downloaded.

        Returns: Dictionary with the user id and a record list.
        {'2323': [1, 2, 4]}",0,1,1,2
"def _cut_range(self, line, start, current_position):
        
        result = []
        try:
            for j in range(start, len(line)):
                index = _setup_index(j)
                try:
                    result.append(line[index])
                except IndexError:
                    result.append(self.invalid_pos)
                finally:
                    result.append(self.separator)
            result.append(line[-1])
        except IndexError:
            pass

        try:
            int(self.positions[current_position+1])
            result.append(self.separator)
        except (ValueError, IndexError):
            pass

        return result","Performs cut for range from start position to end

        Arguments:
            line -              input to cut
            start -             start of range
            current_position -  current position in main cut function",0,0,2,2
"def _cutoff(table, frequency_cutoff):
    
    new_table = {}
    
    for amino_acid, codons in table.iteritems():
        average_cutoff = frequency_cutoff * sum(codons.values()) / len(codons)
        new_table[amino_acid] = {}
        for codon, frequency in codons.iteritems():
            if frequency > average_cutoff:
                new_table[amino_acid][codon] = frequency
    return new_table","Generate new codon frequency table given a mean cutoff.

    :param table: codon frequency table of form {amino acid: codon: frequency}
    :type table: dict
    :param frequency_cutoff: value between 0 and 1.0 for mean frequency cutoff
    :type frequency_cutoff: float
    :returns: A codon frequency table with some codons removed.
    :rtype: dict",0,0,2,2
"def _debug(self):
        

        class Bunch(object):
            
            pass

        debug_tree = Bunch()

        if not self.v_annotations.f_is_empty():
            debug_tree.v_annotations = self.v_annotations
        if not self.v_comment == '':
            debug_tree.v_comment = self.v_comment

        for leaf_name in self._leaves:
            leaf = self._leaves[leaf_name]
            setattr(debug_tree, leaf_name, leaf)

        for link_name in self._links:
            linked_node = self._links[link_name]
            setattr(debug_tree, link_name, 'Link to `%s`' % linked_node.v_full_name)

        for group_name in self._groups:
            group = self._groups[group_name]
            setattr(debug_tree, group_name, group._debug())

        return debug_tree","Creates a dummy object containing the whole tree to make unfolding easier.

        This method is only useful for debugging purposes.
        If you use an IDE and want to unfold the trajectory tree, you always need to
        open the private attribute `_children`. Use to this function to create a new
        object that contains the tree structure in its attributes.

        Manipulating the returned object does not change the original tree!",0,0,2,2
"def _debug(self, out, print_prefix=True):
        
        if self.debug:
            if print_prefix:
                pre = self.__class__.__name__
                if hasattr(self, 'debug_prefix'):
                    pre = getattr(self, 'debug_prefix')
                sys.stderr.write(""%s: "" % pre)
            sys.stderr.write(out)","Print out to stderr, if debugging is enabled.",1,0,1,2
"def _decrypt_verify_with_context(ctx, encrypted):
    
    try:
        (plaintext, _, verify_result) = ctx.decrypt(
                encrypted, verify=True)
        sigs = verify_result.signatures
    except gpg.errors.GPGMEError as e:
        raise GPGProblem(str(e), code=e.getcode())
    except gpg.errors.BadSignatures as e:
        (plaintext, _, _) = ctx.decrypt(encrypted, verify=False)
        sigs = e.result.signatures
    return sigs, plaintext","Decrypts the given ciphertext string using the gpg context
    and returns both the signatures (if any) and the plaintext.

    :param gpg.Context ctx: the gpg context
    :param bytes encrypted: the mail to decrypt
    :returns: the signatures and decrypted plaintext data
    :rtype: tuple[list[gpg.resuit.Signature], str]
    :raises alot.errors.GPGProblem: if the decryption fails",1,0,2,3
"def _digi_nan(fmt):
    
    if isinstance(fmt, list):
        return [_digi_nan(f) for f in fmt]

    if fmt == '80':
        return -128
    if fmt == '310':
        return -512
    if fmt == '311':
        return -512
    elif fmt == '212':
        return -2048
    elif fmt == '16':
        return -32768
    elif fmt == '61':
        return -32768
    elif fmt == '160':
        return -32768
    elif fmt == '24':
        return -8388608
    elif fmt == '32':
        return -2147483648","Return the wfdb digital value used to store nan for the format type.

    Parmeters
    ---------
    fmt : str, or list
        The wfdb dat format, or a list of them.",0,0,1,1
"def _download_image(self, imageURL):
        

        
        
        if(self._imageCounter >= self._imageCount):
            return

        try:
            imageResponse = requests.get(imageURL)

            
            imageType, imageEncoding = mimetypes.guess_type(imageURL)

            if imageType is not None:
                imageExtension = mimetypes.guess_extension(imageType)
            else:
                imageExtension = mimetypes.guess_extension(
                    imageResponse.headers['Content-Type'])

            imageFileName = self._imageQuery.replace(
                ' ', '_') + '_' + str(self._imageCounter) + imageExtension

            imageFileName = os.path.join(self._storageFolder, imageFileName)

            image = Image.open(BytesIO(imageResponse.content))
            image.save(imageFileName)

            self._imageCounter += 1
            self._downloadProgressBar.update(self._imageCounter)

        except Exception as exception:
            pass","Downloads an image file from the given image URL

        Arguments:
            imageURL {[str]} -- [Image URL]",1,1,1,3
"def _emit_twisted(signal, *args, **kwargs):
    
    errback = kwargs.pop('errback', lambda f: f)

    dl = []
    for callback in set(receivers[signal]):  
        d = _call(callback, args=args, kwargs=kwargs)
        if d is not None:
            dl.append(d.addErrback(errback))

    def simplify(results):
        return [x[1] for x in results]

    from twisted.internet.defer import DeferredList
    return DeferredList(dl).addCallback(simplify)","Emits a single signal to call callbacks registered to respond to that signal.
    Optionally accepts args and kwargs that are passed directly to callbacks.

    :param signal: Signal to send",0,0,2,2
"def _equal_values(self, val1, val2):
        
        if self.f_supports(val1) != self.f_supports(val2):
            return False

        if not self.f_supports(val1) and not self.f_supports(val2):
            raise TypeError('I do not support the types of both inputs (`%s` and `%s`), '
                            'therefore I cannot judge whether '
                            'the two are equal.' % (str(type(val1)), str(type(val2))))

        if not self._values_of_same_type(val1, val2):
            return False

        return comparisons.nested_equal(val1, val2)","Checks if the parameter considers two values as equal.

        This is important for the trajectory in case of merging. In case you want to delete
        duplicate parameter points, the trajectory needs to know when two parameters
        are equal. Since equality is not always implemented by values handled by
        parameters in the same way, the parameters need to judge whether their values are equal.

        The straightforward example here is a numpy array.
        Checking for equality of two numpy arrays yields
        a third numpy array containing truth values of a piecewise comparison.
        Accordingly, the parameter could judge two numpy arrays equal if ALL of the numpy
        array elements are equal.

        In this BaseParameter class values are considered to be equal if they obey
        the function :func:`~pypet.utils.comparisons.nested_equal`.
        You might consider implementing a different equality comparison in your subclass.

        :raises: TypeError: If both values are not supported by the parameter.",1,0,2,3
"def _equivalent(self, char, prev, next, implicitA):
        
        result = []
        if char.isVowel == False:
            result.append(char.chr)
            if char.isConsonant \
            and ((next is not None and next.isConsonant) \
            or next is None): 
                result.append(DevanagariCharacter._VIRAMA)
        else:
            if prev is None or prev.isConsonant == False:
                result.append(char.chr)
            else:
                if char._dependentVowel is not None:
                    result.append(char._dependentVowel)
        return result","Transliterate a Latin character equivalent to Devanagari.
        
        Add VIRAMA for ligatures.
        Convert standalone to dependent vowels.",0,0,2,2
"def _estimate_friedrich_coefficients(x, m, r):
    
    assert m > 0, ""Order of polynomial need to be positive integer, found {}"".format(m)

    df = pd.DataFrame({'signal': x[:-1], 'delta': np.diff(x)})
    try:
        df['quantiles'] = pd.qcut(df.signal, r)
    except ValueError:
        return [np.NaN] * (m + 1)

    quantiles = df.groupby('quantiles')

    result = pd.DataFrame({'x_mean': quantiles.signal.mean(), 'y_mean': quantiles.delta.mean()})
    result.dropna(inplace=True)

    try:
        return np.polyfit(result.x_mean, result.y_mean, deg=m)
    except (np.linalg.LinAlgError, ValueError):
        return [np.NaN] * (m + 1)","Coefficients of polynomial :math:`h(x)`, which has been fitted to
    the deterministic dynamics of Langevin model
    .. math::
        \dot{x}(t) = h(x(t)) + \mathcal{N}(0,R)

    As described by

        Friedrich et al. (2000): Physics Letters A 271, p. 217-222
        *Extracting model equations from experimental data*

    For short time-series this method is highly dependent on the parameters.

    :param x: the time series to calculate the feature of
    :type x: numpy.ndarray
    :param m: order of polynom to fit for estimating fixed points of dynamics
    :type m: int
    :param r: number of quantils to use for averaging
    :type r: float

    :return: coefficients of polynomial of deterministic dynamics
    :return type: ndarray",0,0,3,3
"def _extract_header_value(line):
    

    
    if not line:
        return None

    
    halves = line.split('=')
    if len(halves) > 1:
        key = halves[0].strip()
        value = halves[1].strip()
        return {key: value}

    
    else:
        halves = line.split(':')
        key = halves[0].strip()
        value = halves[1].strip()
        return {key: value}",Extracts a key / value pair from a header line in an ODF file,0,0,2,2
"def _extract_recipients(
    message: Message, resent_dates: List[Union[str, Header]] = None
) -> List[str]:
    
    recipients = []  

    if resent_dates:
        recipient_headers = (""Resent-To"", ""Resent-Cc"", ""Resent-Bcc"")
    else:
        recipient_headers = (""To"", ""Cc"", ""Bcc"")

    for header in recipient_headers:
        recipients.extend(message.get_all(header, []))  

    parsed_recipients = [
        str(email.utils.formataddr(address))
        for address in email.utils.getaddresses(recipients)
    ]

    return parsed_recipients",Extract the recipients from the message object given.,0,0,2,2
"def _fact_to_tuple(self, fact):
        
        
        if fact.category:
            category = fact.category.name
        else:
            category = ''

        description = fact.description or ''

        return FactTuple(
            start=fact.start.strftime(self.datetime_format),
            end=fact.end.strftime(self.datetime_format),
            activity=text_type(fact.activity.name),
            duration=fact.get_string_delta(format='%M'),
            category=text_type(category),
            description=text_type(description),
        )","Convert a ``Fact`` to its normalized tuple.

        This is where all type conversion for ``Fact`` attributes to strings as
        well as any normalization happens.

        Note:
            Because different writers may require different types, we need to
            do this individually.

        Args:
            fact (hamster_lib.Fact): Fact to be converted.

        Returns:
            FactTuple: Tuple representing the original ``Fact``.",0,0,1,1
"def _feature_most_common(self, results):
        
        try:
            country_count = Counter([i['country_code3'] for i in results['hits']['hits']])
            most_common = country_count.most_common()[0][0]
            return most_common
        except IndexError:
            return """"
        except TypeError:
            return """"","Find the most common country name in ES/Geonames results

        Paramaters
        ----------
        results: dict
            output of `query_geonames`

        Returns
        -------
        most_common: str
            ISO code of most common country, or empty string if none",0,0,1,1
"def _fetch(self, resource, params):
        
        url = urijoin(self.base_url, 'v3', resource)

        do_fetch = True

        while do_fetch:
            logger.debug(""Puppet forge client calls resource: %s params: %s"",
                         resource, str(params))

            r = self.fetch(url, payload=params)
            yield r.text

            json_data = r.json()

            if 'pagination' in json_data:
                next_url = json_data['pagination']['next']

                if next_url:
                    
                    url = urijoin(self.base_url, next_url)
                    params = {}
                else:
                    do_fetch = False
            else:
                do_fetch = False","Fetch a resource.

        Method to fetch and to iterate over the contents of a
        type of resource. The method returns a generator of
        pages for that resource and parameters.

        :param resource: type of the resource
        :param params: parameters to filter

        :returns: a generator of pages for the requested resource",0,2,1,3
"def _fetch(self, url, params):
        
        if not self.from_archive:
            self.sleep_for_rate_limit()

        headers = {'Authorization': 'Bearer ' + self.api_key}
        r = self.fetch(url, payload=params, headers=headers)

        if not self.from_archive:
            self.update_rate_limit(r)

        return r.text","Fetch a resource.

        Method to fetch and to iterate over the contents of a
        type of resource. The method returns a generator of
        pages for that resource and parameters.

        :param url: the endpoint of the API
        :param params: parameters to filter

        :returns: the text of the response",0,1,1,2
"def _fetch_from_archive(self, method, args):
        
        if not self.archive:
            raise ArchiveError(cause=""Archive not provided"")

        data = self.archive.retrieve(method, args, None)

        if isinstance(data, nntplib.NNTPTemporaryError):
            raise data

        return data","Fetch data from the archive

        :param method: the name of the command to execute
        :param args: the arguments required by the command",2,1,2,5
"def _fftshift_single(d_g, res_g, ax = 0):
    

    dtype_kernel_name = {np.float32:""fftshift_1_f"",
                   np.complex64:""fftshift_1_c""
                   }

    N = d_g.shape[ax]
    N1 = 1 if ax==0 else np.prod(d_g.shape[:ax])
    N2 = 1 if ax == len(d_g.shape)-1 else np.prod(d_g.shape[ax+1:])

    dtype = d_g.dtype.type

    prog = OCLProgram(abspath(""kernels/fftshift.cl""))
    prog.run_kernel(dtype_kernel_name[dtype],(N2,N//2,N1),None,
                    d_g.data, res_g.data,
                    np.int32(N),
                    np.int32(N2))


    return res_g","basic fftshift of an OCLArray


    shape(d_g) =  [N_0,N_1...., N, .... N_{k-1, N_k]
    = [N1, N, N2]

    the we can address each element in the flat buffer by

     index = i + N2*j + N2*N*k

    where   i = 1 .. N2
            j = 1 .. N
            k = 1 .. N1

    and the swap of elements is performed on the index j",0,1,1,2
"def _file_name(self, dtype_out_time, extension='nc'):
        
        if dtype_out_time is None:
            dtype_out_time = ''
        out_lbl = utils.io.data_out_label(self.intvl_out, dtype_out_time,
                                          dtype_vert=self.dtype_out_vert)
        in_lbl = utils.io.data_in_label(self.intvl_in, self.dtype_in_time,
                                        self.dtype_in_vert)
        start_year = utils.times.infer_year(self.start_date)
        end_year = utils.times.infer_year(self.end_date)
        yr_lbl = utils.io.yr_label((start_year, end_year))
        return '.'.join(
            [self.name, out_lbl, in_lbl, self.model.name,
             self.run.name, yr_lbl, extension]
        ).replace('..', '.')",Create the name of the aospy file.,0,0,1,1
"def _fill_function(func, globals, defaults, dict, module, closure_values):
    
    func.__globals__.update(globals)
    func.__defaults__ = defaults
    func.__dict__ = dict
    func.__module__ = module

    cells = func.__closure__
    if cells is not None:
        for cell, value in zip(cells, closure_values):
            if value is not _empty_cell_value:
                cell_set(cell, value)

    return func","Fills in the rest of function data into the skeleton function object
        that were created via _make_skel_func().",0,0,1,1
"def _filter_contains(self, term, field_name, field_type, is_not):
        
        if field_type == 'text':
            term_list = term.split()
        else:
            term_list = [term]

        query = self._or_query(term_list, field_name, field_type)
        if is_not:
            return xapian.Query(xapian.Query.OP_AND_NOT, self._all_query(), query)
        else:
            return query","Splits the sentence in terms and join them with OR,
        using stemmed and un-stemmed.

        Assumes term is not a list.",1,0,2,3
"def _filter_queues(self, queues):
        

        def match(queue):
            
            for part in reversed_dotted_parts(queue):
                if part in self.exclude_queues:
                    return False
                if part in self.only_queues:
                    return True
            return not self.only_queues

        return [q for q in queues if match(q)]","Applies the queue filter to the given list of queues and returns the
        queues that match. Note that a queue name matches any subqueues
        starting with the name, followed by a date. For example, ""foo"" will
        match both ""foo"" and ""foo.bar"".",0,0,2,2
"def _fix_example_namespace(self):
        
        example_prefix = 'example'  
        idgen_prefix = idgen.get_id_namespace_prefix()

        
        if idgen_prefix != example_prefix:
            return

        
        
        if example_prefix not in self._input_namespaces:
            return

        self._input_namespaces[example_prefix] = idgen.EXAMPLE_NAMESPACE.name","Attempts to resolve issues where our samples use
        'http://example.com/' for our example namespace but python-stix uses
        'http://example.com' by removing the former.",0,0,3,3
"def _fix_gaussian_width(gaussian_samples, amp: float, center: float, sigma: float,
                        zeroed_width: Union[None, float] = None, rescale_amp: bool = False,
                        ret_scale_factor: bool = False) -> np.ndarray:
    r
    if zeroed_width is None:
        zeroed_width = 2*(center+1)

    zero_offset = gaussian(np.array([-zeroed_width/2]), amp, center, sigma)
    gaussian_samples -= zero_offset
    amp_scale_factor = 1.
    if rescale_amp:
        amp_scale_factor = amp/(amp-zero_offset)
        gaussian_samples *= amp_scale_factor

    if ret_scale_factor:
        return gaussian_samples, amp_scale_factor
    return gaussian_samples","r""""""Enforce that the supplied gaussian pulse is zeroed at a specific width.

    This is acheived by subtracting $\Omega_g(center \pm zeroed_width/2)$ from all samples.

    amp: Pulse amplitude at `2\times center+1`.
    center: Center (mean) of pulse.
    sigma: Width (standard deviation) of pulse.
    zeroed_width: Subtract baseline to gaussian pulses to make sure
             $\Omega_g(center \pm zeroed_width/2)=0$ is satisfied. This is used to avoid
             large discontinuities at the start of a gaussian pulse. If unsupplied,
             defaults to $2*(center+1)$ such that the samples are zero at $\Omega_g(-1)$.
    rescale_amp: If `zeroed_width` is not `None` and `rescale_amp=True` the pulse will
                 be rescaled so that $\Omega_g(center)-\Omega_g(center\pm zeroed_width/2)=amp$.
    ret_scale_factor: Return amplitude scale factor.",0,0,2,2
"def _flush(self):
        
        d = os.path.dirname(self.path)
        if not os.path.isdir(d):
            os.makedirs(d)
        with io.open(self.path, 'w', encoding='utf8') as f:
            yaml.safe_dump(self._data, f, default_flow_style=False, encoding=None)",Save the contents of data to the file on disk. You should not need to call this manually.,1,0,0,1
"def _format_linedata(linedata, indent, indent_width):
    
    lines = []
    WIDTH = 78 - indent_width
    SPACING = 2
    NAME_WIDTH_LOWER_BOUND = 13
    NAME_WIDTH_UPPER_BOUND = 30
    NAME_WIDTH = max([len(s) for s, d in linedata])
    if NAME_WIDTH < NAME_WIDTH_LOWER_BOUND:
        NAME_WIDTH = NAME_WIDTH_LOWER_BOUND
    elif NAME_WIDTH > NAME_WIDTH_UPPER_BOUND:
        NAME_WIDTH = NAME_WIDTH_UPPER_BOUND

    DOC_WIDTH = WIDTH - NAME_WIDTH - SPACING
    for namestr, doc in linedata:
        line = indent + namestr
        if len(namestr) <= NAME_WIDTH:
            line += ' ' * (NAME_WIDTH + SPACING - len(namestr))
        else:
            lines.append(line)
            line = indent + ' ' * (NAME_WIDTH + SPACING)
        line += _summarize_doc(doc, DOC_WIDTH)
        lines.append(line.rstrip())
    return lines","Format specific linedata into a pleasant layout.

        ""linedata"" is a list of 2-tuples of the form:
            (<item-display-string>, <item-docstring>)
        ""indent"" is a string to use for one level of indentation
        ""indent_width"" is a number of columns by which the
            formatted data will be indented when printed.

    The <item-display-string> column is held to 30 columns.",0,0,2,2
"def _format_regular_value(self, str_in):
        
        
        try:
            dt = datetime.strptime(str_in, self.format)
            return strftime(dt, DateSpec.OUTPUT_FORMAT)
        except ValueError as e:
            msg = ""Unable to format date value '{}'. Reason: {}"".format(str_in,
                                                                        e)
            e_new = InvalidEntryError(msg)
            e_new.field_spec = self
            raise_from(e_new, e)","we overwrite default behaviour as we want to hash the numbers
        only, no fillers like '-', or '/'

        :param str str_in: date string
        :return: str date string with format DateSpec.OUTPUT_FORMAT",1,0,2,3
"def _format_response(self, request, response):
        

        res = datamapper.format(request, response, self)
        
        if res.status_code is 0:
            res.status_code = 200
        
        self._add_resposne_headers(res, response)
        return res","Format response using appropriate datamapper.

        Take the devil response and turn it into django response, ready to
        be returned to the client.",0,0,1,1
"def _from_pointer(pointer, incref):
        
        if pointer == ffi.NULL:
            raise ValueError('Null pointer')
        if incref:
            cairo.cairo_font_face_reference(pointer)
        self = object.__new__(FONT_TYPE_TO_CLASS.get(
            cairo.cairo_font_face_get_type(pointer), FontFace))
        FontFace.__init__(self, pointer)  
        return self","Wrap an existing :c:type:`cairo_font_face_t *` cdata pointer.

        :type incref: bool
        :param incref:
            Whether increase the :ref:`reference count <refcounting>` now.
        :return:
            A new instance of :class:`FontFace` or one of its sub-classes,
            depending on the face’s type.",1,0,2,3
"def _get(self, api_call, params=None, method='GET', auth=False,
             file_=None):
        
        url = ""{0}/{1}"".format(self.site_url, api_call)

        if method == 'GET':
            request_args = {'params': params}
        else:
            request_args = {'data': params, 'files': file_}

        
        
        if auth or (self.username and self.api_key):
            if self.username and self.api_key:
                request_args['auth'] = (self.username, self.api_key)
            else:
                raise PybooruError(""'username' and 'api_key' attribute of ""
                                   ""Danbooru are required."")

        
        return self._request(url, api_call, request_args, method)","Function to preapre API call.

        Parameters:
            api_call (str): API function to be called.
            params (str): API function parameters.
            method (str): (Defauld: GET) HTTP method (GET, POST, PUT or
                           DELETE)
            file_ (file): File to upload (only uploads).

        Raise:
            PybooruError: When 'username' or 'api_key' are not set.",1,1,2,4
"def _get(url, headers={}, params=None):
    
    param_string = _foursquare_urlencode(params)
    for i in xrange(NUM_REQUEST_RETRIES):
        try:
            try:
                response = requests.get(url, headers=headers, params=param_string, verify=VERIFY_SSL)
                return _process_response(response)
            except requests.exceptions.RequestException as e:
                _log_and_raise_exception('Error connecting with foursquare API', e)
        except FoursquareException as e:
            
            if e.__class__ in [InvalidAuth, ParamError, EndpointError, NotAuthorized, Deprecated]: raise
            
            if ((i + 1) == NUM_REQUEST_RETRIES): raise
        time.sleep(1)",Tries to GET data from an endpoint using retries,1,1,2,4
"def _getAssociation(self, endpoint):
        
        assoc = self.store.getAssociation(endpoint.server_url)

        if assoc is None or assoc.expiresIn <= 0:
            assoc = self._negotiateAssociation(endpoint)
            if assoc is not None:
                self.store.storeAssociation(endpoint.server_url, assoc)

        return assoc","Get an association for the endpoint's server_url.

        First try seeing if we have a good association in the
        store. If we do not, then attempt to negotiate an association
        with the server.

        If we negotiate a good association, it will get stored.

        @returns: A valid association for the endpoint's server_url or None
        @rtype: openid.association.Association or NoneType",0,1,2,3
"def _getOpenID1SessionType(self, assoc_response):
        
        
        
        session_type = assoc_response.getArg(OPENID1_NS, 'session_type')

        
        

        
        
        
        if session_type == 'no-encryption':
            logging.warning('OpenID server sent ""no-encryption""'
                            'for OpenID 1.X')

        
        
        
        
        elif session_type == '' or session_type is None:
            session_type = 'no-encryption'

        return session_type","Given an association response message, extract the OpenID
        1.X session type.

        This function mostly takes care of the 'no-encryption' default
        behavior in OpenID 1.

        If the association type is plain-text, this function will
        return 'no-encryption'

        @returns: The association type for this message
        @rtype: str

        @raises KeyError: when the session_type field is absent.",0,1,2,3
"def _get_action(self, action_meta):
        
        conf = {
            'fun': list(action_meta.keys())[0],
            'arg': [],
            'kwargs': {},
        }
        if not len(conf['fun'].split('.')) - 1:
            conf['salt.int.intfunc'] = True

        action_meta = action_meta[conf['fun']]
        info = action_meta.get('info', 'Action for {}'.format(conf['fun']))
        for arg in action_meta.get('args') or []:
            if not isinstance(arg, dict):
                conf['arg'].append(arg)
            else:
                conf['kwargs'].update(arg)

        return info, action_meta.get('output'), conf","Parse action and turn into a calling point.
        :param action_meta:
        :return:",0,0,3,3
"def _get_api_call(self, function_name, *args):
    
    api_call = dedent() % {
        'api_call': function_name,
        'args': ', '.join(args)
    }
    script = '\n'.join((api.API_SCRIPT, api_call))
    try:
      return self._browser.execute_async_script(script)
    except TimeoutException:
      
      raise APIError","Runs an api call with javascript-formatted arguments.

    Args:
      function_name: The name of the KindleAPI call to run.
      *args: Javascript-formatted arguments to pass to the API call.

    Returns:
      The result of the API call.

    Raises:
      APIError: If the API call fails or times out.",1,1,2,4
"def _get_api_content(self):
        

        if GITHUB_TOKEN is not None:
            self.add_params_to_url({
                ""access_token"": GITHUB_TOKEN
            })

        api_content_response = requests.get(self.api_url)
        self.api_content = json.loads(
            api_content_response.text
        )",Updates class api content by calling Github api and storing result,0,1,2,3
"def _get_authorization_headers(self, context):
        
        headers = {}
        self._credentials.before_request(
            self._request,
            context.method_name,
            context.service_url,
            headers)

        return list(six.iteritems(headers))","Gets the authorization headers for a request.

        Returns:
            Sequence[Tuple[str, str]]: A list of request headers (key, value)
                to add to the request.",0,0,1,1
"def _get_available_engine_upgrades(client, major=False):
    
    results = {}
    engine_versions = client.describe_db_engine_versions()['DBEngineVersions']
    for v in engine_versions:
        if not v['Engine'] in results:
            results[v['Engine']] = {}
        if 'ValidUpgradeTarget' not in v or len(v['ValidUpgradeTarget']) == 0:
            continue
        for t in v['ValidUpgradeTarget']:
            if not major and t['IsMajorVersionUpgrade']:
                continue
            if LooseVersion(t['EngineVersion']) > LooseVersion(
                    results[v['Engine']].get(v['EngineVersion'], '0.0.0')):
                results[v['Engine']][v['EngineVersion']] = t['EngineVersion']
    return results","Returns all extant rds engine upgrades.

    As a nested mapping of engine type to known versions
    and their upgrades.

    Defaults to minor upgrades, but configurable to major.

    Example::

      >>> _get_engine_upgrades(client)
      {
         'oracle-se2': {'12.1.0.2.v2': '12.1.0.2.v5',
                        '12.1.0.2.v3': '12.1.0.2.v5'},
         'postgres': {'9.3.1': '9.3.14',
                      '9.3.10': '9.3.14',
                      '9.3.12': '9.3.14',
                      '9.3.2': '9.3.14'}
      }",0,1,1,2
"def _get_bios_mappings_resource(self, data):
        
        try:
            map_uri = data['links']['Mappings']['href']
        except KeyError:
            msg = ('Mappings resource not found.')
            raise exception.IloCommandNotSupportedError(msg)

        status, headers, map_settings = self._rest_get(map_uri)
        if status != 200:
            msg = self._get_extended_error(map_settings)
            raise exception.IloError(msg)

        return map_settings","Get the Mappings resource.

        :param data: Existing Bios settings of the server.
        :returns: mappings settings.
        :raises: IloCommandNotSupportedError, if resource is not found.
        :raises: IloError, on an error from iLO.",2,1,3,6
"def _get_code_dir(self, code_path):
        

        decompressed_dir = None

        try:
            if os.path.isfile(code_path) and code_path.endswith(self.SUPPORTED_ARCHIVE_EXTENSIONS):

                decompressed_dir = _unzip_file(code_path)
                yield decompressed_dir

            else:
                LOG.debug(""Code %s is not a zip/jar file"", code_path)
                yield code_path
        finally:
            if decompressed_dir:
                shutil.rmtree(decompressed_dir)","Method to get a path to a directory where the Lambda function code is available. This directory will
        be mounted directly inside the Docker container.

        This method handles a few different cases for ``code_path``:
            - ``code_path``is a existent zip/jar file: Unzip in a temp directory and return the temp directory
            - ``code_path`` is a existent directory: Return this immediately
            - ``code_path`` is a file/dir that does not exist: Return it as is. May be this method is not clever to
                detect the existence of the path

        :param string code_path: Path to the code. This could be pointing at a file or folder either on a local
            disk or in some network file system
        :return string: Directory containing Lambda function code. It can be mounted directly in container",1,1,3,5
"def _get_conn(self, host, port):
        
        host_key = (host, port)
        if host_key not in self.conns:
            self.conns[host_key] = KafkaConnection(
                host,
                port,
                timeout=self.timeout
            )

        return self.conns[host_key]",Get or create a connection to a broker using host and port,0,1,0,1
"def _get_dummy_request(base_url, user):
    
    split_url = urlsplit(base_url)
    is_secure = split_url[0] == 'https'
    dummy_request = RequestFactory(HTTP_HOST=split_url[1]).get('/', secure=is_secure)
    dummy_request.is_secure = lambda: is_secure
    dummy_request.user = user or AnonymousUser()
    dummy_request.site = None  
    return dummy_request","Create a dummy request.
    Use the ``base_url``, so code can use ``request.build_absolute_uri()`` to create absolute URLs.",0,0,1,1
"def _get_el_to_normative(parent_elem, normative_parent_elem):
        
        el_to_normative = OrderedDict()
        if normative_parent_elem is None:
            for el in parent_elem:
                el_to_normative[el] = None
        else:
            for norm_el in normative_parent_elem:
                matches = [
                    el
                    for el in parent_elem
                    if el.get(""TYPE"") == norm_el.get(""TYPE"")
                    and el.get(""LABEL"") == norm_el.get(""LABEL"")
                ]
                if matches:
                    el_to_normative[matches[0]] = norm_el
                else:
                    el_to_normative[norm_el] = None
        return el_to_normative","Return ordered dict ``el_to_normative``, which maps children of
        ``parent_elem`` to their normative counterparts in the children of
        ``normative_parent_elem`` or to ``None`` if there is no normative
        parent. If there is a normative div element with no non-normative
        counterpart, that element is treated as a key with value ``None``.
        This allows us to create ``FSEntry`` instances for empty directory div
        elements, which are only documented in a normative logical structmap.",0,0,4,4
"def _get_exponential_spaced_values(mmin, mmax, number_samples):
    
    lhs = np.exp(mmin) + np.arange(0., number_samples - 1., 1.) *\
        ((np.exp(mmax) - np.exp(mmin)) / (number_samples - 1.))
    magval = np.hstack([lhs, np.exp(mmax)])

    return np.log(magval)","Function to return a set of exponentially spaced values between mmin and
    mmax

    :param float mmin:
        Minimum value
    :param float mmax:
        Maximum value
    :param float number_samples:
        Number of exponentially spaced samples
    :return np.ndarray:
        Set of 'number_samples' exponentially spaced values",0,0,1,1
"def _get_extra_args(extra_args, arg_keys):
    
    
    single_keys = set([""sam_ref"", ""config""])
    out = []
    for i, arg_key in enumerate(arg_keys):
        vals = [xs[i] for xs in extra_args]
        if arg_key in single_keys:
            out.append(vals[-1])
        else:
            out.append(vals)
    return out","Retrieve extra arguments to pass along to combine function.

    Special cases like reference files and configuration information
    are passed as single items, the rest as lists mapping to each data
    item combined.",0,0,1,1
"def _get_field_by_name(model_class, field_name):
    
    field = model_class._meta.get_field(field_name)
    return (
        field,                                       
        field.model,                                 
        not field.auto_created or field.concrete,    
        field.many_to_many                           
    )",Compatible with old API of model_class._meta.get_field_by_name(field_name),0,0,1,1
"def _get_file_by_alias(part, files):
    
    
    if _is_output(part):
        return Output.from_string(part.pop())

    
    else:
        inputs = [[]]

        if part.magic_or:
            and_or = 'or'
        else:
            and_or = 'and'

        for cut in part.asList():
            if cut == OR_TOKEN:
                inputs.append([])
                continue
            if cut == AND_TOKEN:
                continue

            input = Input(cut, filename=cut, and_or=and_or)
            for file in files:
                if file.alias == cut:
                    
                    input.filename = file.filename
                    inputs[-1].append(input)
                    break
            else:
                inputs[-1].append(input)


        return [input for input in inputs if input]","Given a command part, find the file it represents. If not found,
    then returns a new token representing that file.
    :throws ValueError: if the value is not a command file alias.",0,0,3,3
"def _get_file_size(self):
        
        file_size = retry(self._retry_count)(_get_content_length)(
            self._session, self.url, self._timeout
        )
        file_size = int(file_size)
        if file_size == 0:
            with io.open(self._file_path, 'a', encoding='utf-8'):
                pass
        return file_size","Fetches file size by reading the Content-Length header
        for the resource.
        :return: File size.",0,1,0,1
"def _get_firmware_file_in_new_path(searching_path):
    
    firmware_file_path = _get_firmware_file(searching_path)
    if not firmware_file_path:
        return None

    
    
    
    
    
    file_name, file_ext_with_dot = common.get_filename_and_extension_of(
        firmware_file_path)
    new_firmware_file_path = os.path.join(
        tempfile.gettempdir(), str(uuid.uuid4()) + '_' +
        file_name + file_ext_with_dot)

    
    os.link(firmware_file_path, new_firmware_file_path)
    return new_firmware_file_path","Gets the raw firmware file in a new path

    Gets the raw firmware file from the extracted directory structure
    and creates a hard link to that in a file path and cleans up the
    lookup extract path.
    :param searching_path: the directory structure to search for
    :returns: the raw firmware file with the complete new path",0,0,2,2
"def _get_group_randomstate(rs, seed, group):
        
        if rs is None:
            rs = np.random.RandomState(seed=seed)
            
            
            if 'last_random_state' in group._v_attrs:
                rs.set_state(group._v_attrs['last_random_state'])
                print(""INFO: Random state set to last saved state in '%s'."" %
                      group._v_name)
            else:
                print(""INFO: Random state initialized from seed (%d)."" % seed)
        return rs","Return a RandomState, equal to the input unless rs is None.

        When rs is None, try to get the random state from the
        'last_random_state' attribute in `group`. When not available,
        use `seed` to generate a random state. When seed is None the returned
        random state will have a random seed.",0,0,5,5
"def _get_header(self):
        

        try:
            header_lines = [int(e) for e in str(self.get_value('headerlines', 0)).split(',')]
        except ValueError as e:
            header_lines = [0]

        
        header_rows = islice(self.row_generator, min(header_lines), max(header_lines) + 1)

        from tableintuit import RowIntuiter
        headers = RowIntuiter.coalesce_headers(header_rows)

        return headers","Get the header from the deinfed header rows, for use  on references or resources where the schema
        has not been run",0,0,1,1
"def _get_html_response(url, session):
    
    
    if _is_url_like_archive(url):
        _ensure_html_response(url, session=session)

    logger.debug('Getting page %s', url)

    resp = session.get(
        url,
        headers={
            ""Accept"": ""text/html"",
            
            
            
            
            
            
            
            
            
            
            
            
            
            ""Cache-Control"": ""max-age=0"",
        },
    )
    resp.raise_for_status()

    
    
    
    
    
    _ensure_html_header(resp)

    return resp","Access an HTML page with GET, and return the response.

    This consists of three parts:

    1. If the URL looks suspiciously like an archive, send a HEAD first to
       check the Content-Type is HTML, to avoid downloading a large file.
       Raise `_NotHTTP` if the content type cannot be determined, or
       `_NotHTML` if it is not HTML.
    2. Actually perform the request. Raise HTTP exceptions on network failures.
    3. Check the Content-Type header to make sure we got HTML, and raise
       `_NotHTML` otherwise.",1,2,0,3
"def _get_initial_request(self):
        
        
        
        if self._leaser is not None:
            
            
            lease_ids = list(self._leaser.ack_ids)
        else:
            lease_ids = []

        
        request = types.StreamingPullRequest(
            modify_deadline_ack_ids=list(lease_ids),
            modify_deadline_seconds=[self.ack_deadline] * len(lease_ids),
            stream_ack_deadline_seconds=self.ack_histogram.percentile(99),
            subscription=self._subscription,
        )

        
        return request","Return the initial request for the RPC.

        This defines the initial request that must always be sent to Pub/Sub
        immediately upon opening the subscription.

        Returns:
            google.cloud.pubsub_v1.types.StreamingPullRequest: A request
            suitable for being the first request on the stream (and not
            suitable for any other purpose).",0,1,0,1
"def _get_input_name(self, input_str, region=None, describe_output=None):
        
        if '.' in input_str:
            stage_identifier, input_name = input_str.split('.', 1)
            
            return self._get_stage_id(stage_identifier) + '.' + input_name

        return input_str",":param input_str: A string of one of the forms: ""<exported input field name>"", ""<explicit workflow input field name>"", ""<stage ID>.<input field name>"", ""<stage index>.<input field name>"", ""<stage name>.<input field name>""
        :type input_str: string
        :returns: If the given form was one of those which uses the stage index or stage name, it is translated to the stage ID for use in the API call (stage name takes precedence)",0,0,1,1
"def _get_ip_public(self, queue_target, url, json=False, key=None):
        
        try:
            response = urlopen(url, timeout=self.timeout).read().decode('utf-8')
        except Exception as e:
            logger.debug(""IP plugin - Cannot open URL {} ({})"".format(url, e))
            queue_target.put(None)
        else:
            
            try:
                if not json:
                    queue_target.put(response)
                else:
                    queue_target.put(loads(response)[key])
            except ValueError:
                queue_target.put(None)",Request the url service and put the result in the queue_target.,0,2,1,3
"def _get_job(self, project_id, job_id):
        
        job_name = 'projects/{}/jobs/{}'.format(project_id, job_id)
        request = self._mlengine.projects().jobs().get(name=job_name)
        while True:
            try:
                return request.execute()
            except HttpError as e:
                if e.resp.status == 429:
                    
                    time.sleep(30)
                else:
                    self.log.error('Failed to get MLEngine job: {}'.format(e))
                    raise","Gets a MLEngine job based on the job name.

        :return: MLEngine job object if succeed.
        :rtype: dict

        Raises:
            googleapiclient.errors.HttpError: if HTTP error is returned from server",0,2,1,3
"def _get_mixed_actions(tableaux, bases):
    
    nums_actions = tableaux[1].shape[0], tableaux[0].shape[0]
    num = nums_actions[0] + nums_actions[1]
    out = np.zeros(num)

    for pl, (start, stop) in enumerate(zip((0, nums_actions[0]),
                                           (nums_actions[0], num))):
        sum_ = 0.
        for i in range(nums_actions[1-pl]):
            k = bases[pl][i]
            if start <= k < stop:
                out[k] = tableaux[pl][i, -1]
                sum_ += tableaux[pl][i, -1]
        if sum_ != 0:
            out[start:stop] /= sum_

    return out[:nums_actions[0]], out[nums_actions[0]:]","From `tableaux` and `bases`, extract non-slack basic variables and
    return a tuple of the corresponding, normalized mixed actions.

    Parameters
    ----------
    tableaux : tuple(ndarray(float, ndim=2))
        Tuple of two arrays containing the tableaux, of shape (n, m+n+1)
        and (m, m+n+1), respectively.

    bases : tuple(ndarray(int, ndim=1))
        Tuple of two arrays containing the bases, of shape (n,) and
        (m,), respectively.

    Returns
    -------
    tuple(ndarray(float, ndim=1))
        Tuple of mixed actions as given by the non-slack basic variables
        in the tableaux.",0,0,2,2
"def _get_next_child_node(self, parent):
        
        children_keys = list(parent.children)
        sorted_keys = sorted(children_keys)
        for key in sorted_keys:
            node = parent.children[key]
            if node.job_record is None:
                self.timetable.assign_job_record(node)
                return node
            elif self.should_skip_tree_node(node):
                continue
            elif node.job_record.is_active:
                return node

        
        new_parent = self._get_next_parent_node(parent)
        if new_parent is not None:
            
            return self._get_next_child_node(new_parent)
        else:
            
            process_name = parent.children[sorted_keys[0]].process_name
            time_qualifier = parent.children[sorted_keys[0]].time_qualifier
            actual_timeperiod = time_helper.actual_timeperiod(time_qualifier)
            return self.get_node(process_name, actual_timeperiod)","Iterates among children of the given parent and looks for a suitable node to process
            In case given parent has no suitable nodes, a younger parent will be found
            and the logic will be repeated for him",0,0,4,4
"def _get_num_of_documents(self):
        
        parser = etree.XMLParser(target=XMLElementCountTarget('document'))
        
        
        num_of_documents = etree.parse(self.urml_file, parser)
        self._num_of_documents = num_of_documents
        return num_of_documents","counts the number of documents in an URML file.
        adapted from Listing 2 on
        http://www.ibm.com/developerworks/library/x-hiperfparse/",0,0,1,1
"def _get_obs_array(self, k, use_raw=False, layer='X'):
        
        in_raw_var_names = k in self.raw.var_names if self.raw is not None else False

        if use_raw and self.raw is None:
            raise ValueError('.raw doesn\'t exist')

        if k in self.obs.keys():
            x = self._obs[k]
        elif in_raw_var_names and use_raw and layer == 'X':
            x = self.raw[:, k].X
        elif k in self.var_names and not use_raw and (layer == 'X' or layer in self.layers.keys()):
            x = self[:, k].X if layer=='X' else self[:, k].layers[layer]
        elif use_raw and layer != 'X':
            raise ValueError('No layers in .raw')
        elif layer != 'X' and layer not in self.layers.keys():
            raise ValueError('Did not find {} in layers.keys.'
                             .format(layer))
        else:
            raise ValueError('Did not find {} in obs.keys or var_names.'
                             .format(k))
        return x","Get an array from the layer (default layer='X') along the observation dimension by first looking up
        obs.keys and then var.index.",3,0,4,7
"def _get_or_fetch_id(self, zobj, fetch_func):
        

        try:
            return zobj.id
        except AttributeError:
            try:
                return fetch_func(zobj).id
            except AttributeError:
                raise ValueError('Unqualified Resource')","Returns the ID of a Zobject wether it's already known or not

        If zobj.id is not known (frequent if zobj is a selector), fetches first
        the object and then returns its ID.

        :type zobj:       a zobject subclass
        :type fetch_func: the function to fetch the zobj from server if its id
                          is undefined.
        :returns:         the object id",0,0,2,2
"def _get_paged_resource(self, url, params=None, data_key=None):
        
        if not params:
            params = {}

        self._set_as_user(params)

        auto_page = not ('page' in params or 'per_page' in params)

        if 'per_page' not in params and self._per_page != DEFAULT_PAGINATION:
            params[""per_page""] = self._per_page

        full_url = url + self._params(params)
        return self._get_resource_url(full_url, auto_page, data_key)","Canvas GET method. Return representation of the requested paged
        resource, either the requested page, or chase pagination links to
        coalesce resources.",0,1,0,1
"def _get_philny(self, C, mag):
        
        if mag <= 4.5:
            return C[""phi1""]
        elif mag >= 5.5:
            return C[""phi2""]
        else:
            return C[""phi2""] + (C[""phi1""] - C[""phi2""]) * (5.5 - mag)","Returns the intra-event random effects coefficient (phi)
        Equation 28.",0,0,1,1
"def _get_range(self, endpoint_name):
        

        url = self.build_url(self._endpoints.get(endpoint_name))
        response = self.session.get(url)
        if not response:
            return None
        data = response.json()
        return self.range_constructor(parent=self, **{self._cloud_data_key: data})",Returns a Range based on the endpoint name,0,1,1,2
"def _get_record(self, name):
        
        request = self._session.get(self._baseurl, params={'name': name,
                                                           'type': 'A'})
        if not request.ok:
            raise RuntimeError('Failed to search record: %s - %s' %
                               (self._format_hostname(name), request.json()))
        records = request.json()
        if len(records) == 0:
            return
        record = records[0]
        if 'record' not in record or 'id' not in record['record']:
            raise RuntimeError('Invalid record JSON format: %s - %s' %
                               (self._format_hostname(name), request.json()))
        return int(record['record']['id'])","Returns the id of a record, if it exists.",2,1,2,5
"def _get_registry(self, registry_path_or_url):
        
        if os.path.isfile(registry_path_or_url):
            with open(registry_path_or_url, 'r') as f:
                reader = compat.csv_dict_reader(f.readlines())
        else:
            res = requests.get(registry_path_or_url)
            res.raise_for_status()

            reader = compat.csv_dict_reader(StringIO(res.text))

        return dict([(o['id'], o) for o in reader])",Return a dict with objects mapped by their id from a CSV endpoint,1,1,1,3
"def _get_signature_block(message: str, signing_key: SigningKey, close_block: bool = True,
                             comment: Optional[str] = None) -> str:
        
        base64_signature = base64.b64encode(signing_key.signature(message))

        block = .format(begin_signature_header=BEGIN_SIGNATURE_HEADER, version_field=AsciiArmor._get_version_field())

        
        if comment:
            block += .format(comment_field=AsciiArmor._get_comment_field(comment))

        
        block += '\n'

        block += .format(base64_signature=base64_signature.decode('utf-8'))

        if close_block:
            block += END_SIGNATURE_HEADER

        return block","Return a signature block

        :param message: Message (not encrypted!) to sign
        :param signing_key: The libnacl SigningKey instance of the keypair
        :param close_block: Optional flag to close the signature block with the signature tail header
        :param comment: Optional comment field content
        :return:",0,0,5,5
"def _get_snmpv2c(self, oid):
        
        snmp_target = (self.hostname, self.snmp_port)
        cmd_gen = cmdgen.CommandGenerator()

        (error_detected, error_status, error_index, snmp_data) = cmd_gen.getCmd(
            cmdgen.CommunityData(self.community),
            cmdgen.UdpTransportTarget(snmp_target, timeout=1.5, retries=2),
            oid,
            lookupNames=True,
            lookupValues=True,
        )

        if not error_detected and snmp_data[0][1]:
            return text_type(snmp_data[0][1])
        return """"","Try to send an SNMP GET operation using SNMPv2 for the specified OID.

        Parameters
        ----------
        oid : str
            The SNMP OID that you want to get.

        Returns
        -------
        string : str
            The string as part of the value from the OID you are trying to retrieve.",0,1,0,1
"def _get_token_rate_limit(self, token):
        

        rate_url = urijoin(self.base_url, ""rate_limit"")
        self.session.headers.update({'Authorization': 'token ' + token})
        remaining = 0
        try:
            headers = super().fetch(rate_url).headers
            if self.rate_limit_header in headers:
                remaining = int(headers[self.rate_limit_header])
        except requests.exceptions.HTTPError as error:
            logger.warning(""Rate limit not initialized: %s"", error)
        return remaining",Return token's remaining API points,0,2,0,2
"def _get_type_id_options(name, configuration):
    
    
    if '.' in name:
        type_, sep, id_ = name.partition('.')
        options = configuration
    else:
        type_ = next(six.iterkeys(configuration))
        id_ = name
        options = configuration[type_]

    return type_, id_, options","Returns the type, id and option of a configuration object.",0,0,1,1
"def _get_weight_param_summary(wp):
    
    summary_str = ''
    if wp.HasField('quantization'):
        nbits = wp.quantization.numberOfBits
        quant_type = 'linearly' if wp.quantization.HasField('linearQuantization') else 'lookup-table'
        summary_str += '{}-bit {} quantized'.format(nbits, quant_type)

    if len(wp.floatValue) > 0:
        summary_str += '({} floatValues)'.format(len(wp.floatValue))
    if len(wp.float16Value) > 0:
        summary_str += '({} bytes float16Values)'.format(len(wp.float16Value))
    if len(wp.rawValue) > 0:
        summary_str += '({} bytes rawValues)'.format(len(wp.rawValue))

    return summary_str","Get a summary of _NeuralNetwork_pb2.WeightParams
    Args:
    wp : _NeuralNetwork_pb2.WeightParams - the _NeuralNetwork_pb2.WeightParams message to display
    Returns:
    a str summary for wp",0,0,1,1
"def _grad_neg_log_likelihood_and_fim(model_matrix, linear_response, response,
                                     model):
  
  
  
  mean, variance, grad_mean = model(linear_response)

  is_valid = (
      tf.math.is_finite(grad_mean) & tf.not_equal(grad_mean, 0.)
      & tf.math.is_finite(variance) & (variance > 0.))

  def _mask_if_invalid(x, mask):
    mask = tf.fill(
        tf.shape(input=x), value=np.array(mask, x.dtype.as_numpy_dtype))
    return tf.where(is_valid, x, mask)

  
  v = (response - mean) * _mask_if_invalid(grad_mean, 1) / _mask_if_invalid(
      variance, np.inf)
  grad_log_likelihood = sparse_or_dense_matvecmul(
      model_matrix, v, adjoint_a=True)
  fim_middle = _mask_if_invalid(grad_mean, 0.)**2 / _mask_if_invalid(
      variance, np.inf)
  return -grad_log_likelihood, fim_middle","Computes the neg-log-likelihood gradient and Fisher information for a GLM.

  Note that Fisher information is related to the Hessian of the log-likelihood
  by the equation

  ```none
  FisherInfo = E[Hessian with respect to model_coefficients of -LogLikelihood(
      Y | model_matrix, model_coefficients)]
  ```

  where `LogLikelihood` is the log-likelihood of a generalized linear model
  parameterized by `model_matrix` and `model_coefficients`, and the expectation
  is taken over Y, distributed according to the same GLM with the same parameter
  values.

  Args:
    model_matrix: (Batch of) matrix-shaped, `float` `Tensor` or `SparseTensor`
      where each row represents a sample's features.  Has shape `[N, n]` where
      `N` is the number of data samples and `n` is the number of features per
      sample.
    linear_response: (Batch of) vector-shaped `Tensor` with the same dtype as
      `model_matrix`, equal to `model_matix @ model_coefficients` where
      `model_coefficients` are the coefficients of the linear component of the
      GLM.
    response: (Batch of) vector-shaped `Tensor` with the same dtype as
      `model_matrix` where each element represents a sample's observed response
      (to the corresponding row of features).
    model: `tfp.glm.ExponentialFamily`-like instance, which specifies the link
      function and distribution of the GLM, and thus characterizes the negative
      log-likelihood. Must have sufficient statistic equal to the response, that
      is, `T(y) = y`.

  Returns:
    grad_neg_log_likelihood: (Batch of) vector-shaped `Tensor` with the same
      shape and dtype as a single row of `model_matrix`, representing the
      gradient of the negative log likelihood of `response` given linear
      response `linear_response`.
    fim_middle: (Batch of) vector-shaped `Tensor` with the same shape and dtype
      as a single column of `model_matrix`, satisfying the equation
      `Fisher information =
      Transpose(model_matrix)
      @ diag(fim_middle)
      @ model_matrix`.",0,0,2,2
"def _granule_identifier_to_xml_name(granule_identifier):
    
    
    changed_item_type = re.sub(""_MSI_"", ""_MTD_"", granule_identifier)
    
    split_by_underscores = changed_item_type.split(""_"")
    del split_by_underscores[-1]
    cleaned = str()
    
    for i in split_by_underscores:
        cleaned += (i + ""_"")
    
    out_xml = cleaned[:-1] + "".xml""

    return out_xml","Very ugly way to convert the granule identifier.

    e.g.
    From
    Granule Identifier:
    S2A_OPER_MSI_L1C_TL_SGS__20150817T131818_A000792_T28QBG_N01.03
    To
    Granule Metadata XML name:
    S2A_OPER_MTD_L1C_TL_SGS__20150817T131818_A000792_T28QBG.xml",0,0,1,1
"def _handle_ignores(testtag):
    
    if ""ignores"" in testtag.attrib:
        from xml.etree.ElementTree import Element
        for varname in re.split(""[\s,]+"", testtag.attrib[""ignores""]):
            
            
            if varname[0] == '^':
                varname = varname[1::]
            e = Element(""global"", {""name"": varname, ""ignore"": ""true""})
            testtag.append(e)","Checks if the specified test tag has attribute ""ignores""; if it
    does, a <global ignore=""true"" /> tag is created for each variable
    name in the list.",0,0,3,3
"def _handle_ticker(self, dtype, data, ts):
        
        self.log.debug(""_handle_ticker: %s - %s - %s"", dtype, data, ts)
        channel_id, *data = data
        channel_identifier = self.channel_directory[channel_id]

        entry = (data, ts)
        self.tickers[channel_identifier].put(entry)","Adds received ticker data to self.tickers dict, filed under its channel
        id.

        :param dtype:
        :param data:
        :param ts:
        :return:",1,0,1,2
"def _import_astorb_to_database(
            self,
            astorbDictList):
        
        self.log.info('starting the ``_import_astorb_to_database`` method')

        print ""Refreshing the orbital elements database table""

        dbSettings = self.settings[""database settings""][""atlasMovers""]

        insert_list_of_dictionaries_into_database_tables(
            dbConn=self.atlasMoversDBConn,
            log=self.log,
            dictList=astorbDictList,
            dbTableName=""orbital_elements"",
            uniqueKeyList=[""name""],
            dateModified=True,
            batchSize=10000,
            replace=True,
            dbSettings=dbSettings
        )

        print ""Finished refreshing the orbital elements database table""

        self.log.info('completed the ``_import_astorb_to_database`` method')
        return None","*import the astorb orbital elements to database*

        **Key Arguments:**
            - ``astorbDictList`` -- the astorb database parsed as a list of dictionaries

        **Return:**
            - None",1,2,0,3
"def _in_gae_environment():
    
    if SETTINGS.env_name is not None:
        return SETTINGS.env_name in ('GAE_PRODUCTION', 'GAE_LOCAL')

    try:
        import google.appengine  
    except ImportError:
        pass
    else:
        server_software = os.environ.get(_SERVER_SOFTWARE, '')
        if server_software.startswith('Google App Engine/'):
            SETTINGS.env_name = 'GAE_PRODUCTION'
            return True
        elif server_software.startswith('Development/'):
            SETTINGS.env_name = 'GAE_LOCAL'
            return True

    return False","Detects if the code is running in the App Engine environment.

    Returns:
        True if running in the GAE environment, False otherwise.",1,0,1,2
"def _index_keys_for(self, idx_name, *ids_and_fcs):
        
        idx = self._index(idx_name)
        icreate, itrans = idx['create'], idx['transform']
        if isinstance(idx_name, unicode):
            idx_name = idx_name.encode('utf-8')

        for cid_fc in ids_and_fcs:
            content_id = cid_fc[0]

            
            
            seen_values = set()
            for index_value in icreate(itrans, cid_fc):
                if index_value and index_value not in seen_values:
                    yield (index_value, idx_name, content_id)
                    seen_values.add(index_value)","Returns a generator of index triples.

        Returns a generator of index keys for the ``ids_and_fcs`` pairs
        given. The index keys have the form ``(idx_name, idx_val,
        content_id)``.

        :type idx_name: unicode
        :type ids_and_fcs: ``[(content_id, FeatureCollection)]``
        :rtype: generator of ``(str, str, str)``",0,0,1,1
"def _inhibitColumns(self, overlaps):
    
    
    
    
    if (self._localAreaDensity > 0):
      density = self._localAreaDensity
    else:
      inhibitionArea = ((2*self._inhibitionRadius + 1)
                                    ** self._columnDimensions.size)
      inhibitionArea = min(self._numColumns, inhibitionArea)
      density = float(self._numActiveColumnsPerInhArea) / inhibitionArea
      density = min(density, 0.5)

    if self._globalInhibition or \
      self._inhibitionRadius > max(self._columnDimensions):
      return self._inhibitColumnsGlobal(overlaps, density)
    else:
      return self._inhibitColumnsLocal(overlaps, density)","Performs inhibition. This method calculates the necessary values needed to
    actually perform inhibition and then delegates the task of picking the
    active columns to helper functions.

    Parameters:
    ----------------------------
    :param overlaps: an array containing the overlap score for each  column.
                    The overlap score for a column is defined as the number
                    of synapses in a ""connected state"" (connected synapses)
                    that are connected to input bits which are turned on.",0,0,2,2
"def _initModuleList(self):
        
        if self.checkVersion('1.4'):
            cmd = ""module show""
        else:
            cmd = ""show modules""
        cmdresp = self.executeCommand(cmd)
        self._modules = set()
        for line in cmdresp.splitlines()[1:-1]:
            mobj = re.match('\s*(\S+)\s', line)
            if mobj:
                self._modules.add(mobj.group(1).lower())","Query Asterisk Manager Interface to initialize internal list of 
        loaded modules.
        
        CLI Command - core show modules",0,2,2,4
"def _init_map(self, record_types=None, **kwargs):
        
        osid_objects.OsidRelationshipForm._init_map(self, record_types=record_types)
        self._my_map['assignedObjectiveBankIds'] = [str(kwargs['objective_bank_id'])]
        self._my_map['completion'] = self._completion_default
        self._my_map['objectiveId'] = str(kwargs['objective_id'])
        self._my_map['resourceId'] = str(kwargs['resource_id'])
        self._my_map['levelId'] = self._level_default",Initialize form map,0,0,1,1
"def _initialize_from_dict(self, data):
        
        self._json = data
        self._validate()

        for name, value in self._json.items():
            if name in self._properties:
                if '$ref' in self._properties[name]:
                    if 'decimal' in self._properties[name]['$ref']:
                        value = Decimal(value)

                
                if 'format' in self._properties[name]:
                    format = self._properties[name]['format']

                    if 'date-time' == format:
                        value = utils.string_to_datetime(value)
                    elif 'date' == format:
                        value = utils.string_to_date(value)
                setattr(self, name, value)",Loads serializer from a request object,0,0,1,1
"def _inline_image(image, mime_type=None):
    
    file = StringValue(image).value
    mime_type = StringValue(mime_type).value or mimetypes.guess_type(file)[0]
    path = None
    if callable(STATIC_ROOT):
        try:
            _file, _storage = list(STATIC_ROOT(file))[0]
            path = _storage.open(_file)
        except:
            pass
    else:
        _path = os.path.join(STATIC_ROOT, file)
        if os.path.exists(_path):
            path = open(_path, 'rb')
    if path:
        url = 'data:' + mime_type + ';base64,' + base64.b64encode(path.read())
    url = url = '%s%s?_=%s' % (STATIC_URL, file, 'NA')
    inline = 'url(""%s"")' % escape(url)
    return StringValue(inline)","Embeds the contents of a file directly inside your stylesheet, eliminating
    the need for another HTTP request. For small files such images or fonts,
    this can be a performance benefit at the cost of a larger generated CSS
    file.",1,0,1,2
"def _input_as_lines(self, data):
        
        filename = self._input_filename = \
            FilePath(self.getTmpFilename(self.TmpDir))
        filename = FilePath(filename)
        data_file = open(filename, 'w')
        data_to_file = '\n'.join([str(d).strip('\n') for d in data])
        data_file.write(data_to_file)
        data_file.close()
        return filename","Write a seq of lines to a temp file and return the filename string

            data: a sequence to be written to a file, each element of the
                sequence will compose a line in the file
           * Note: the result will be the filename as a FilePath object
            (which is a string subclass).

           * Note: '\n' will be stripped off the end of each sequence element
                before writing to a file in order to avoid multiple new lines
                accidentally be written to a file",3,0,1,4
"def _iq_request_coro_done_send_reply(self, request, task):
        
        try:
            payload = task.result()
        except errors.XMPPError as err:
            self._send_iq_reply(request, err)
        except Exception:
            response = self._compose_undefined_condition(request)
            self._enqueue(response)
            self._logger.exception(""IQ request coroutine failed"")
        else:
            self._send_iq_reply(request, payload)","Called when an IQ request handler coroutine returns. `request` holds
        the IQ request which triggered the excecution of the coroutine and
        `task` is the :class:`asyncio.Task` which tracks the running coroutine.

        Compose a response and send that response.",0,2,1,3
"def _is_packed_binary(self, data):
        
        packed = False
        if isinstance(data, bytes) and len(data) == 16 and b':' not in data:
            try:
                packed = bool(int(binascii.hexlify(data), 16))
            except (ValueError, TypeError):
                pass

        return packed","Check if data is hexadecimal packed

        :param data:
        :return:",0,0,1,1
"def _iter_valid_subtotal_dicts(self):
        
        for insertion_dict in self._insertion_dicts:
            
            if not isinstance(insertion_dict, dict):
                continue

            
            if insertion_dict.get(""function"") != ""subtotal"":
                continue

            
            if not {""anchor"", ""args"", ""name""}.issubset(insertion_dict.keys()):
                continue

            
            if not self._element_ids.intersection(insertion_dict[""args""]):
                continue

            
            
            yield insertion_dict",Generate each insertion dict that represents a valid subtotal.,0,0,2,2
"def _iterator(self, plugins, context):
        

        test = pyblish.logic.registered_test()

        for plug, instance in pyblish.logic.Iterator(plugins, context):
            if not plug.active:
                continue

            if instance is not None and instance.data.get(""publish"") is False:
                continue

            self.processing[""nextOrder""] = plug.order

            if not self.is_running:
                raise StopIteration(""Stopped"")

            if test(**self.processing):
                raise StopIteration(""Stopped due to %s"" % test(
                    **self.processing))

            yield plug, instance","Yield next plug-in and instance to process.

        Arguments:
            plugins (list): Plug-ins to process
            context (pyblish.api.Context): Context to process",2,0,3,5
"def _learn(
             permanences, rng,

             
             activeCells, activeInput, growthCandidateInput,

             
             sampleSize, initialPermanence, permanenceIncrement,
             permanenceDecrement, connectedPermanence):
    

    permanences.incrementNonZerosOnOuter(
      activeCells, activeInput, permanenceIncrement)
    permanences.incrementNonZerosOnRowsExcludingCols(
      activeCells, activeInput, -permanenceDecrement)
    permanences.clipRowsBelowAndAbove(
      activeCells, 0.0, 1.0)
    if sampleSize == -1:
      permanences.setZerosOnOuter(
        activeCells, activeInput, initialPermanence)
    else:
      existingSynapseCounts = permanences.nNonZerosPerRowOnCols(
        activeCells, activeInput)

      maxNewByCell = numpy.empty(len(activeCells), dtype=""int32"")
      numpy.subtract(sampleSize, existingSynapseCounts, out=maxNewByCell)

      permanences.setRandomZerosOnOuter(
        activeCells, growthCandidateInput, maxNewByCell, initialPermanence, rng)","For each active cell, reinforce active synapses, punish inactive synapses,
    and grow new synapses to a subset of the active input bits that the cell
    isn't already connected to.

    Parameters:
    ----------------------------
    @param  permanences (SparseMatrix)
            Matrix of permanences, with cells as rows and inputs as columns

    @param  rng (Random)
            Random number generator

    @param  activeCells (sorted sequence)
            Sorted list of the cells that are learning

    @param  activeInput (sorted sequence)
            Sorted list of active bits in the input

    @param  growthCandidateInput (sorted sequence)
            Sorted list of active bits in the input that the activeCells may
            grow new synapses to

    For remaining parameters, see the __init__ docstring.",0,0,3,3
"def _list_releases():
    
    if is_linux():
        base_path = '/usr/local/MATLAB/R%d%s/bin/matlab'
    else:
        
        base_path = '/Applications/MATLAB_R%d%s.app/bin/matlab'
    years = range(2050,1990,-1)
    release_letters = ('h', 'g', 'f', 'e', 'd', 'c', 'b', 'a')
    for year in years:
        for letter in release_letters:
            release = 'R%d%s' % (year, letter)
            matlab_path = base_path % (year, letter)
            if os.path.exists(matlab_path):
                yield (release, matlab_path)","Tries to guess matlab process release version and location path on
    osx machines.

    The paths we will search are in the format:
    /Applications/MATLAB_R[YEAR][VERSION].app/bin/matlab
    We will try the latest version first. If no path is found, None is reutrned.",0,0,2,2
"def _loadCSVDataFrame(self):
        
        if self._filename and os.path.exists(self._filename):
            
            encoding = self._encodingKey or 'UTF_8'

            try:
                dataFrame = superReadFile(self._filename,
                    sep=self._delimiter, first_codec=encoding,
                    header=self._header)
                dataFrame = dataFrame.apply(fillNoneValues)
                dataFrame = dataFrame.apply(convertTimestamps)
            except Exception as err:
                self.updateStatusBar(str(err))
                print(err)
                return pandas.DataFrame()
            self.updateStatusBar('Preview generated.')
            return dataFrame
        self.updateStatusBar('File could not be read.')
        return pandas.DataFrame()","Loads the given csv file with pandas and generate a new dataframe.

        The file will be loaded with the configured encoding, delimiter
        and header.git
        If any execptions will occur, an empty Dataframe is generated
        and a message will appear in the status bar.

        Returns:
            pandas.DataFrame: A dataframe containing all the available
                information of the csv file.",1,0,1,2
"def _loadData(self, data):
        
        self._data = data
        self.listType = 'video'
        self.addedAt = utils.toDatetime(data.attrib.get('addedAt'))
        self.key = data.attrib.get('key', '')
        self.lastViewedAt = utils.toDatetime(data.attrib.get('lastViewedAt'))
        self.librarySectionID = data.attrib.get('librarySectionID')
        self.ratingKey = utils.cast(int, data.attrib.get('ratingKey'))
        self.summary = data.attrib.get('summary')
        self.thumb = data.attrib.get('thumb')
        self.title = data.attrib.get('title')
        self.titleSort = data.attrib.get('titleSort', self.title)
        self.type = data.attrib.get('type')
        self.updatedAt = utils.toDatetime(data.attrib.get('updatedAt'))
        self.viewCount = utils.cast(int, data.attrib.get('viewCount', 0))",Load attribute values from Plex XML response.,0,0,1,1
"def _load_data(path):
    
    path = os.path.abspath(path)
    if not os.path.isdir(path):
        raise InitializationError('Directory not found: {0}'.format(path))
    wordlists = {}
    for file_name in os.listdir(path):
        if os.path.splitext(file_name)[1] != '.txt':
            continue
        file_path = os.path.join(path, file_name)
        name = os.path.splitext(os.path.split(file_path)[1])[0]
        try:
            with codecs.open(file_path, encoding='utf-8') as file:
                wordlists[name] = _load_wordlist(name, file)
        except OSError as ex:
            raise InitializationError('Failed to read {}: {}'.format(file_path, ex))
    config = _load_config(os.path.join(path, 'config.json'))
    return (config, wordlists)","Loads data from a directory.
    Returns tuple (config_dict, wordlists).
    Raises Exception on failure (e.g. if data is corrupted).",3,0,2,5
"def _load_data(self, band):
        
        
        df = bandpass_data_frame('filter_mko_' + band + '.dat', 'wlen resp')
        
        df = df[::-1]
        df.index = np.arange(df.shape[0])
        df.wlen *= 1e4 
        df.resp *= df.wlen 
        return df","Filter responses for MKO NIR filters as specified in Tokunaga+ 2002 (see
        also Tokunaga+ 2005). I downloaded the L' profile from
        http://irtfweb.ifa.hawaii.edu/~nsfcam/hist/filters.2006.html.

        Pivot wavelengths from Tokunaga+ 2005 (Table 2) confirm that the
        profile is in QE convention, although my calculation of the pivot
        wavelength for L' is actually closer if I assume otherwise. M' and K_s
        are substantially better in QE convention, though, and based on the
        paper and nomenclature it seems more appropriate.",0,0,1,1
"def _load_plugins(self):
        
        plugins = ait.config.get('server.plugins')

        if plugins is None:
            log.warn('No plugins specified in config.')
        else:
            for index, p in enumerate(plugins):
                try:
                    plugin = self._create_plugin(p['plugin'])
                    self.plugins.append(plugin)
                    log.info('Added plugin {}'.format(plugin))

                except Exception:
                    exc_type, value, tb = sys.exc_info()
                    log.error('{} creating plugin {}: {}'.format(exc_type,
                                                                 index,
                                                                 value))
            if not self.plugins:
                log.warn('No valid plugin configurations found. No plugins will be added.')","Reads, parses and creates plugins specified in config.yaml.",0,4,3,7
"def _load_script(self):
        
        script_text = filesystem.read_file(self.path, self.filename)

        if not script_text:
            raise IOError(""Script file could not be opened or was empty: {0}""
                          """".format(os.path.join(self.path, self.filename)))
        self.script = script_text","Loads the script from the filesystem

        :raises exceptions.IOError: if the script file could not be opened",2,0,0,2
"def _loc_vec(self, loc):
        
        gxy = self.scale_xy*(loc[:, :2, :] - self.dboxes[:, :2, :])/self.dboxes[:, 2:, ]
        gwh = self.scale_wh*(loc[:, 2:, :]/self.dboxes[:, 2:, :]).log()

        return torch.cat((gxy, gwh), dim=1).contiguous()",Generate Location Vectors,0,0,1,1
"def _locate_schemas(self, schemas, skip_missing_schemas):
        
        for schema in schemas:
            if schema == os.path.abspath(schema):
                schema_file = schema
            else:
                schema_file = os.path.join(self.paths.schemas, schema)

            if os.path.isfile(schema_file):
                
                yield schema_file
            elif skip_missing_schemas:
                logger.warning(""Unable to locate schema %s at %s"", schema, schema_file)
            else:
                raise PathError(""Unable to locate schema %s at %s"" % (schema, schema_file))","Locate all schemas (look in openldap store).

        If skip_missing_schemas is True, ignore missing schemas;
        otherwise, raise.",2,1,1,4
"def _log(self, name, element):  
        
        from bs4 import BeautifulSoup, Tag
        if isinstance(element, Response):
            LOGGER.debug('%s response: URL=%s Code=%s', name, element.url, element.status_code)
        elif isinstance(element, (BeautifulSoup, Tag)):
            LOGGER.debug('%s HTML:\n%s', name, element)",Log Response and Tag elements. Do nothing if elements is none of them.,0,2,2,4
"def _main(self):
        
        probes = self.config.get('probes', None)
        if not probes:
            raise ValueError('no probes specified')

        for probe_config in self.config['probes']:
            probe = plugin.get_probe(probe_config, self.plugin_context)
            
            if 'output' not in probe_config:
                raise ValueError(""no output specified"")

            
            for output_name in probe_config['output']:
                output = plugin.get_output(output_name, self.plugin_context)
                if not output.started:
                    output.start()
                    self.joins.append(output)
                probe._emit.append(output)

            probe.start()
            self.joins.append(probe)

        vaping.io.joinall(self.joins)
        return 0",process,3,0,3,6
"def _make_list_or_1d_tensor(values):
  
  values = tf.convert_to_tensor(value=values, name='values')
  values_ = tf.get_static_value(values)

  
  if values_ is None:
    
    return values + tf.zeros([1], dtype=values.dtype)

  
  if values_.ndim > 1:
    raise ValueError('values had > 1 dim: {}'.format(values_.shape))
  
  values_ = values_ + np.zeros([1], dtype=values_.dtype)
  return list(values_)","Return a list (preferred) or 1d Tensor from values, if values.ndims < 2.",1,0,3,4
"def _make_sql_compatible(ll):
    

    new_ll = []
    for l in ll:
        new_l = ()
        for i in l:
            if not i:
                new_l = new_l + (None,)
            else:

                if isinstance(i, str):
                    if sys.version_info < (3, 0):

                        val = i.decode('utf8').encode('ascii', errors='ignore')
                    else:
                        
                        val = i
                else:
                    val = i
                new_l = new_l + (val,)
        new_ll.append(new_l)

    return new_ll","Convert any python list of lists (or tuples) so that the strings are formatted correctly for insertion into

    Args:
        ll (list): List of lists (or tuples)",0,0,1,1
"def _map_segmentation_mask_to_stft_domain(mask, times, frequencies, stft_times, stft_frequencies):
    
    assert mask.shape == (frequencies.shape[0], times.shape[0]), ""Times is shape {} and frequencies is shape {}, but mask is shaped {}"".format(
        times.shape, frequencies.shape, mask.shape
    )
    result = np.zeros((stft_frequencies.shape[0], stft_times.shape[0]))

    if len(stft_times) > len(times):
        all_j = [j for j in range(len(stft_times))]
        idxs  = [int(i) for i in np.linspace(0, len(times) - 1, num=len(stft_times))]
        all_i = [all_j[idx] for idx in idxs]
    else:
        all_i = [i for i in range(len(times))]
        idxs  = [int(i) for i in np.linspace(0, len(stft_times) - 1, num=len(times))]
        all_j = [all_i[idx] for idx in idxs]

    for i, j in zip(all_i, all_j):
        result[:, j] = np.interp(stft_frequencies, frequencies, mask[:, i])

    return result","Maps the given `mask`, which is in domain (`frequencies`, `times`) to the new domain (`stft_frequencies`, `stft_times`)
    and returns the result.",0,0,2,2
"def _match_filenames_w_dfs(filenames, lo_dfs):
    
    logger_dataframes.info(""enter match_filenames_w_dfs"")
    dfs = {}

    for filename in filenames:
        try:
            if filename in lo_dfs[""chronData""]:
                dfs[filename] = lo_dfs[""chronData""][filename]
            elif filename in lo_dfs[""paleoData""]:
                dfs[filename] = lo_dfs[""paleoData""][filename]
        except KeyError:
            logger_dataframes.info(""filter_dfs: KeyError: missing data frames keys"")

    logger_dataframes.info(""exit match_filenames_w_dfs"")
    return dfs","Match a list of filenames to their data frame counterparts. Return data frames
    :param list filenames: Filenames of data frames to retrieve
    :param dict lo_dfs: All data frames
    :return dict: Filenames and data frames (filtered)",0,3,1,4
"def _merge_filters(self) -> None:
        
        for opts in ([""-filter:a"", ""-af""], [""-filter:v"", ""-vf""]):
            filter_list = []
            new_argv = []
            cmd_iter = iter(self._argv)
            for element in cmd_iter:
                if element in opts:
                    filter_list.insert(0, next(cmd_iter))
                else:
                    new_argv.append(element)

            
            if filter_list:
                new_argv.extend([opts[0], "","".join(filter_list)])
                self._argv = new_argv.copy()",Merge all filter config in command line.,0,0,1,1
"def _message_from_token(self, token: Text, payload: Any) \
            -> Optional[BaseMessage]:
        

        try:
            tk = jwt.decode(token, settings.WEBVIEW_SECRET_KEY)
        except jwt.InvalidTokenError:
            return

        try:
            user_id = tk['fb_psid']
            assert isinstance(user_id, Text)
            page_id = tk['fb_pid']
            assert isinstance(page_id, Text)
        except (KeyError, AssertionError):
            return

        if self.settings()['page_id'] == page_id:
            return self._make_fake_message(user_id, page_id, payload)",Analyzes a signed token and generates the matching message,0,0,2,2
"def _message_in_range(self, message):
        
        
        if self.entity:
            if self.reverse:
                if message.id <= self.last_id or message.id >= self.max_id:
                    return False
            else:
                if message.id >= self.last_id or message.id <= self.min_id:
                    return False

        return True","Determine whether the given message is in the range or
        it should be ignored (and avoid loading more chunks).",0,0,1,1
"def _move_leadership(self, state):
        
        partition = random.randint(0, len(self.cluster_topology.partitions) - 1)

        
        
        
        if state.partition_weights[partition] == 0:
            return None
        if len(state.replicas[partition]) <= 1:
            return None
        dest_index = random.randint(1, len(state.replicas[partition]) - 1)
        dest = state.replicas[partition][dest_index]
        if (self.args.max_leader_changes is not None and
                state.leader_movement_count >= self.args.max_leader_changes):
            return None

        return state.move_leadership(partition, dest)","Attempt to move a random partition to a random broker. If the
        chosen movement is not possible, None is returned.

        :param state: The starting state.

        :return: The resulting State object if a leader change is found. None
            if no change is found.",0,0,2,2
"def _new(self, dx_hash, **kwargs):
        
        for field in 'runSpec', 'dxapi':
            if field not in kwargs:
                raise DXError(""%s: Keyword argument %s is required"" % (self.__class__.__name__, field))
            dx_hash[field] = kwargs[field]
            del kwargs[field]
        for field in 'inputSpec', 'outputSpec', 'access', 'title', 'summary', 'description':
            if field in kwargs:
                dx_hash[field] = kwargs[field]
                del kwargs[field]

        resp = dxpy.api.applet_new(dx_hash, **kwargs)
        self.set_ids(resp[""id""], dx_hash[""project""])",":param dx_hash: Standard hash populated in :func:`dxpy.bindings.DXDataObject.new()` containing attributes common to all data object classes.
        :type dx_hash: dict
        :param runSpec: Run specification
        :type runSpec: dict
        :param dxapi: API version string
        :type dxapi: string
        :param inputSpec: Input specification (optional)
        :type inputSpec: dict
        :param outputSpec: Output specification (optional)
        :type outputSpec: dict
        :param access: Access specification (optional)
        :type access: dict
        :param title: Title string (optional)
        :type title: string
        :param summary: Summary string (optional)
        :type summary: string
        :param description: Description string (optional)
        :type description: string

        .. note:: It is highly recommended that the higher-level module
           :mod:`dxpy.app_builder` or (preferably) its frontend `dx build
           <https://wiki.dnanexus.com/Command-Line-Client/Index-of-dx-Commands#build>`_
           be used instead for applet creation.

        Creates an applet with the given parameters. See the API
        documentation for the `/applet/new
        <https://wiki.dnanexus.com/API-Specification-v1.0.0/Applets-and-Entry-Points#API-method:-/applet/new>`_
        method for more info. The applet is not run until :meth:`run()`
        is called.",1,1,1,3
"def _nginx_http_spec(port_spec, bridge_ip):
    
    server_string_spec = ""\t server {\n""
    server_string_spec += ""\t \t {}\n"".format(_nginx_max_file_size_string())
    server_string_spec += ""\t \t {}\n"".format(_nginx_listen_string(port_spec))
    server_string_spec += ""\t \t {}\n"".format(_nginx_server_name_string(port_spec))
    server_string_spec += _nginx_location_spec(port_spec, bridge_ip)
    server_string_spec += _custom_502_page()
    server_string_spec += ""\t }\n""
    return server_string_spec",This will output the nginx HTTP config string for specific port spec,0,0,1,1
"def _normalizeRect(self, x, y, width, height):
        
        x, y = self._normalizePoint(x, y) 

        assert width is None or isinstance(width, _INTTYPES), 'width must be an integer or None, got %s' % repr(width)
        assert height is None or isinstance(height, _INTTYPES), 'height must be an integer or None, got %s' % repr(height)

        
        if width is None:
            width = self.width - x
        elif width < 0: 
            width += self.width
            width = max(0, width) 
        if height is None:
            height = self.height - y
            height = max(0, height)
        elif height < 0:
            height += self.height

        
        width = min(width, self.width - x)
        height = min(height, self.height - y)

        return x, y, width, height","Check if the rectangle is in bounds and make minor adjustments.
        raise AssertionError's for any problems",0,0,3,3
"def _normalize_and_accrue_digits_and_plus_sign(self, next_char, remember_position):
        
        if next_char == _PLUS_SIGN:
            normalized_char = next_char
            self._accrued_input_without_formatting += next_char
        else:
            next_digit = unicode_digit(next_char, -1)
            if next_digit != -1:
                normalized_char = unicod(next_digit)
            else:  
                normalized_char = next_char
            self._accrued_input_without_formatting += normalized_char
            self._national_number += normalized_char
        if remember_position:
            self._position_to_remember = len(self._accrued_input_without_formatting)
        return normalized_char","Accrues digits and the plus sign to
        _accrued_input_without_formatting for later use. If next_char contains
        a digit in non-ASCII format (e.g. the full-width version of digits),
        it is first normalized to the ASCII version. The return value is
        next_char itself, or its normalized version, if next_char is a digit
        in non-ASCII format. This method assumes its input is either a digit
        or the plus sign.",0,0,2,2
"def _normalize_sort_SQL(self, field_name, field_vals, sort_dir_str):
        
        fvi = None
        if sort_dir_str == 'ASC':
            fvi = (t for t in enumerate(field_vals)) 

        else:
            fvi = (t for t in enumerate(reversed(field_vals))) 

        query_sort_str = ['  CASE {}'.format(self._normalize_name(field_name))]
        query_args = []
        for i, v in fvi:
            query_sort_str.append('    WHEN {} THEN {}'.format(self.val_placeholder, i))
            query_args.append(v)

        query_sort_str.append('  END')
        query_sort_str = ""\n"".join(query_sort_str)
        return query_sort_str, query_args","allow sorting by a set of values

        http://stackoverflow.com/questions/3303851/sqlite-and-custom-order-by",0,0,1,1
"def _objective_function(X, W, R, S, gamma):
        
        subjs = len(X)
        func = .0
        for i in range(subjs):
            func += 0.5 * np.sum((X[i] - W[i].dot(R) - S[i])**2) \
                    + gamma * np.sum(np.abs(S[i]))
        return func","Evaluate the objective function.

        .. math:: \\sum_{i=1}^{N} 1/2 \\| X_i - W_i R - S_i \\|_F^2
        .. math:: + /\\gamma * \\|S_i\\|_1

        Parameters
        ----------

        X : list of array, element i has shape=[voxels_i, timepoints]
            Each element in the list contains the fMRI data for alignment of
            one subject.

        W : list of array, element i has shape=[voxels_i, features]
            The orthogonal transforms (mappings) :math:`W_i` for each subject.

        R : array, shape=[features, timepoints]
            The shared response.

        S : list of array, element i has shape=[voxels_i, timepoints]
            The individual component :math:`S_i` for each subject.

        gamma : float, default: 1.0
            Regularization parameter for the sparseness of the individual
            components.

        Returns
        -------

        func : float
            The RSRM objective function evaluated on the parameters to this
            function.",0,0,1,1
"def _objective_qubitcount(self, old, new):
        
        (oldscore, oldthing) = old
        (newscore, newthing) = new

        def measure(chains):
            return sum(map(len, chains))

        if oldscore is None:
            return True
        if newscore is None:
            return False
        if len(newthing):
            if not len(oldthing):
                return True
            elif isinstance(newthing, tuple):
                newlengths = sum(map(measure, newthing))
                oldlengths = sum(map(measure, oldthing))
                return newlengths < oldlengths
            else:
                return measure(newthing) < measure(oldthing)
        else:
            return False","An objective function that returns True if new uses fewer qubits
        than old, and False otherwise.  This objective function should only be
        used to compare embeddings of the same graph (or at least embeddings of
        graphs with the same number of qubits).

        INPUTS:
            old (tuple): a tuple (score, embedding)

            new (tuple): a tuple (score, embedding)",0,0,3,3
"def _on_decisions_event(self, event=None, **kwargs):
        
        if not self.ran_ready_function:
            logger.warning('ignoring decision from {} before when_all_players_ready: {}'.format(event.participant.code, event.value))
            return
        with track('_on_decisions_event'):
            self.group_decisions[event.participant.code] = event.value
            self._group_decisions_updated = True
            self.save(update_fields=['group_decisions', '_group_decisions_updated'])
            if not self.num_subperiods() and not self.rate_limit():
                self.send('group_decisions', self.group_decisions)","Called when an Event is received on the decisions channel. Saves
        the value in group_decisions. If num_subperiods is None, immediately
        broadcasts the event back out on the group_decisions channel.",0,2,1,3
"def _palette_cmd(self, event):
        
        label = event.widget
        label.master.focus_set()
        label.master.configure(relief=""sunken"")
        r, g, b = self.winfo_rgb(label.cget(""background""))
        r = round2(r * 255 / 65535)
        g = round2(g * 255 / 65535)
        b = round2(b * 255 / 65535)
        args = (r, g, b)
        if self.alpha_channel:
            a = self.alpha.get()
            args += (a,)
            self.alphabar.set_color(args)
        color = rgb_to_hexa(*args)
        h, s, v = rgb_to_hsv(r, g, b)
        self.red.set(r)
        self.green.set(g)
        self.blue.set(b)
        self.hue.set(h)
        self.saturation.set(s)
        self.value.set(v)
        self.hexa.delete(0, ""end"")
        self.hexa.insert(0, color.upper())
        self.bar.set(h)
        self.square.set_hsv((h, s, v))
        self._update_preview()",Respond to user click on a palette item.,1,0,0,1
"def _parallel_downloader(voxforge_url, archive_dir, total, counter):
    
    def download(d):
        
        (i, file) = d
        download_url = voxforge_url + '/' + file
        c = counter.increment()
        print('Downloading file {} ({}/{})...'.format(i+1, c, total))
        maybe_download(filename_of(download_url), archive_dir, download_url)
    return download","Generate a function to download a file based on given parameters
    This works by currying the above given arguments into a closure
    in the form of the following function.

    :param voxforge_url: the base voxforge URL
    :param archive_dir:  the location to store the downloaded file
    :param total:        the total number of files to download
    :param counter:      an atomic counter to keep track of # of downloaded files
    :return:             a function that actually downloads a file given these params",0,0,1,1
"def _parse(cls, data, key=None):
        
        parse = cls.parse if cls.parse is not None else cls.get_endpoint()

        if callable(parse):
            data = parse(data)
        elif isinstance(parse, str):
            data = data[key]
        else:
            raise Exception('""parse"" should be a callable or string got, {0}'
                            .format(parse))
        return data","Parse a set of data to extract entity-only data.

        Use classmethod `parse` if available, otherwise use the `endpoint`
        class variable to extract data from a data blob.",1,0,1,2
"def _parse_optional_params(self, oauth_params, req_kwargs):
        
        params = req_kwargs.get('params', {})
        data = req_kwargs.get('data') or {}

        for oauth_param in OPTIONAL_OAUTH_PARAMS:
            if oauth_param in params:
                oauth_params[oauth_param] = params.pop(oauth_param)
            if oauth_param in data:
                oauth_params[oauth_param] = data.pop(oauth_param)

            if params:
                req_kwargs['params'] = params

            if data:
                req_kwargs['data'] = data","Parses and sets optional OAuth parameters on a request.

        :param oauth_param: The OAuth parameter to parse.
        :type oauth_param: str
        :param req_kwargs: The keyworded arguments passed to the request
            method.
        :type req_kwargs: dict",0,0,2,2
"def _parse_split_shape(split_shape):
        
        if isinstance(split_shape, int):
            return split_shape, split_shape
        if isinstance(split_shape, (tuple, list)):
            if len(split_shape) == 2 and isinstance(split_shape[0], int) and isinstance(split_shape[1], int):
                return split_shape[0], split_shape[1]
            raise ValueError(""Content of split_shape {} must be 2 integers."".format(split_shape))
        raise ValueError(""Split shape must be an int or a tuple of 2 integers."")","Parses the parameter `split_shape`

        :param split_shape: The parameter `split_shape` from class initialization
        :type split_shape: int or (int, int)
        :return: A tuple of n
        :rtype: (int, int)
        :raises: ValueError",2,0,3,5
"def _parse_time(self, date_string, settings):
        
        date_string = PATTERN.sub('', date_string)
        date_string = re.sub(r'\b(?:ago|in)\b', '', date_string)
        try:
            return time_parser(date_string)
        except:
            pass","Attemps to parse time part of date strings like '1 day ago, 2 PM'",0,0,1,1
"def _parse_typed_parameter(param):
    
    global _current_parameter_value
    type_, value = _expand_one_key_dictionary(param)
    _current_parameter.type = type_

    if _is_simple_type(value) and value != '':
        _current_parameter_value = SimpleParameterValue(value)
        _current_parameter.add_value(_current_parameter_value)
    elif isinstance(value, list):
        for i in value:
            if _is_simple_type(i):
                _current_parameter_value = SimpleParameterValue(i)
                _current_parameter.add_value(_current_parameter_value)
            elif isinstance(i, dict):
                _current_parameter_value = TypedParameterValue()
                _parse_typed_parameter_typed_value(i)
                _current_parameter.add_value(_current_parameter_value)",Parses a TypedParameter and fills it with values.,0,0,2,2
"def _parse_url(host, provided_protocol=None):
        
        protocol = ""http"" 
        
        if host.startswith(""http://""):
            protocol = ""http""
        elif host.startswith(""https://""):
            protocol = ""https""
        elif provided_protocol is not None:
            provided_protocol = provided_protocol.replace(""://"", """")
            if provided_protocol in (""http"", ""https""):
                protocol = provided_protocol
        
        host = host.replace(""http://"", """").replace(""https://"", """")
        if host.endswith(""/""):
            host = host[:-1]

        return host, protocol","Process the provided host and protocol to return them in a standardized
        way that can be subsequently used by Cytomine methods.
        If the protocol is not specified, HTTP is the default.
        Only HTTP and HTTPS schemes are supported.

        Parameters
        ----------
        host: str
            The host, with or without the protocol
        provided_protocol: str (""http"", ""http://"", ""https"", ""https://"")
            The default protocol - used only if the host value does not specify one

        Return
        ------
        (host, protocol): tuple
            The host and protocol in a standardized way (host without protocol,
            and protocol in (""http"", ""https""))
            
        Examples
        --------
        >>> Cytomine._parse_url(""localhost-core"")
        (""localhost-core"", ""http"")
        >>> Cytomine._parse_url(""https://demo.cytomine.coop"", ""http"")
        (""demo.cytomine.coop"", ""https"")",0,0,1,1
"def _parse_xfs_info(data):
    
    ret = {}
    spr = re.compile(r'\s+')
    entry = None
    for line in [spr.sub("" "", l).strip().replace("", "", "" "") for l in data.split(""\n"")]:
        if not line:
            continue
        nfo = _xfs_info_get_kv(line)
        if not line.startswith(""=""):
            entry = nfo.pop(0)
            ret[entry[0]] = {'section': entry[(entry[1] != '***' and 1 or 0)]}
        ret[entry[0]].update(dict(nfo))

    return ret","Parse output from ""xfs_info"" or ""xfs_growfs -n"".",0,0,1,1
"def _patch(self, doc, source, patches, setter=None):
        
        old = self._saved_copy()

        for name, patch in patches.items():
            for ind, value in patch:
                if isinstance(ind, (int, slice)):
                    self[name][ind] = value
                else:
                    shape = self[name][ind[0]][tuple(ind[1:])].shape
                    self[name][ind[0]][tuple(ind[1:])] = np.array(value, copy=False).reshape(shape)

        from ...document.events import ColumnsPatchedEvent

        self._notify_owners(old,
                            hint=ColumnsPatchedEvent(doc, source, patches, setter))","Internal implementation to handle special-casing patch events
        on ``ColumnDataSource`` columns.

        Normally any changes to the ``.data`` dict attribute on a
        ``ColumnDataSource`` triggers a notification, causing all of the data
        to be synchronized between server and clients.

        The ``.patch`` method on column data sources exists to provide a
        more efficient way to perform patching (i.e. random access) updates
        to a data source, without having to perform a full synchronization,
        which would needlessly re-send all the data.

        To accomplish this, this function bypasses the wrapped methods on
        ``PropertyValueDict`` and uses the unwrapped versions on the dict
        superclass directly. It then explicitly makes a notification, adding
        a special ``ColumnsPatchedEvent`` hint to the message containing
        only the small patched data that BokehJS needs in order to efficiently
        synchronize.

        .. warning::
            This function assumes the integrity of ``patches`` has already
            been verified.",0,1,1,2
"def _pick_palette_key(self, palette_name, selected=False, allow_input_state=True):
        
        key = palette_name
        if self._custom_colour:
            key = self._custom_colour
        elif self.disabled:
            key = ""disabled""
        elif not self._is_valid:
            key = ""invalid""
        elif allow_input_state:
            if self._has_focus:
                key = ""focus_"" + palette_name
            if selected:
                key = ""selected_"" + key
        return key","Pick the rendering colour for a widget based on the current state.

        :param palette_name: The stem name for the widget - e.g. ""button"".
        :param selected: Whether this item is selected or not.
        :param allow_input_state: Whether to allow input state (e.g. focus) to affect result.
        :returns: A colour palette key to be used.",0,0,1,1
"def _plugin_get(self, plugin_name):
        
        if not plugin_name:
            return None, u""Plugin name not set""
        for plugin in self.controller.plugins:
            if not isinstance(plugin, SettablePlugin):
                continue
            if plugin.name == plugin_name:
                return plugin, """"
        return None, u""Settable plugin '{}' not found"".format(plugin_name)","Find plugins in controller

        :param plugin_name: Name of the plugin to find
        :type plugin_name: str | None
        :return: Plugin or None and error message
        :rtype: (settable_plugin.SettablePlugin | None, str)",0,0,1,1
"def _post(url, headers, body, retries=3, timeout=3.0):
    
    retry = 0
    out = None
    while out is None:
        try:
            out = requests.post(url, headers=headers, data=body,
                                timeout=timeout)
        
        
        
        
        except (requests.exceptions.Timeout, socket.timeout) as exception:
            retry += 1
            if retry == retries:
                
                raise requests.exceptions.Timeout(exception.message)
    return out","Try 3 times to request the content.

    :param headers: The HTTP headers
    :type headers: dict
    :param body: The body of the HTTP post
    :type body: str
    :param retries: The number of times to retry before giving up
    :type retries: int
    :param timeout: The time to wait for the post to complete, before timing
        out
    :type timeout: float",1,1,2,4
"def _pre_heat_deploy(self):
        
        clients = self.app.client_manager
        compute_client = clients.compute

        self.log.debug(""Checking hypervisor stats"")
        if utils.check_hypervisor_stats(compute_client) is None:
            raise exceptions.DeploymentError(
                ""Expected hypervisor stats not met"")
        return True",Setup before the Heat stack create or update has been done.,1,1,2,4
"def _prepare_init_params_from_job_description(cls, job_details, model_channel_name=None):
        
        init_params = super(AmazonAlgorithmEstimatorBase, cls)._prepare_init_params_from_job_description(
            job_details, model_channel_name)

        
        
        
        for attribute, value in cls.__dict__.items():
            if isinstance(value, hp):
                if value.name in init_params['hyperparameters']:
                    init_params[attribute] = init_params['hyperparameters'][value.name]

        del init_params['hyperparameters']
        del init_params['image']
        return init_params","Convert the job description to init params that can be handled by the class constructor

        Args:
            job_details: the returned job details from a describe_training_job API call.
            model_channel_name (str): Name of the channel where pre-trained model data will be downloaded.

        Returns:
             dictionary: The transformed init_params",0,0,1,1
"def _prepare_outputs(self, data_out, outputs):
        
        compress = ROOTModule.ROOT.CompressionSettings(ROOTModule.ROOT.kZLIB, 1)
        if isinstance(data_out, (str, unicode)):
            self.file_emulation = True
            outputs.append(ROOTModule.TFile.Open(data_out, 'RECREATE', '', compress))
        
        elif isinstance(data_out, ROOTModule.TFile):
            outputs.append(data_out)
        else:  
            self.file_emulation = True
            filename = os.path.join(tempfile.mkdtemp(),'tmp.root')
            outputs.append(ROOTModule.TFile.Open(filename, 'RECREATE', '', compress))","Open a ROOT file with option 'RECREATE' to create a new file (the file will
        be overwritten if it already exists), and using the ZLIB compression algorithm
        (with compression level 1) for better compatibility with older ROOT versions
        (see https://root.cern.ch/doc/v614/release-notes.html#important-notice ).

        :param data_out:
        :param outputs:
        :return:",2,0,1,3
"def _process_delivery(self, pn_delivery):
        
        if pn_delivery.readable and not pn_delivery.partial:
            data = self._pn_link.recv(pn_delivery.pending)
            msg = proton.Message()
            msg.decode(data)
            self._pn_link.advance()

            if self._handler:
                handle = ""rmsg-%s:%x"" % (self._name, self._next_handle)
                self._next_handle += 1
                self._unsettled_deliveries[handle] = pn_delivery
                with self._callback_lock:
                    self._handler.message_received(self, msg, handle)
            else:
                
                pn_delivery.settle()",Check if the delivery can be processed.,0,1,1,2
"def _process_slice(self, arg):
        
        start = arg.start
        stop = arg.stop
        step = arg.step

        nrows = self._info['nrows']
        if step is None:
            step = 1
        if start is None:
            start = 0
        if stop is None:
            stop = nrows

        if start < 0:
            start = nrows + start
            if start < 0:
                raise IndexError(""Index out of bounds"")

        if stop < 0:
            stop = nrows + start + 1

        if stop < start:
            
            stop = start

        if stop > nrows:
            stop = nrows
        return slice(start, stop, step)",process the input slice for use calling the C code,1,0,2,3
"def _progressbar(self, msg, iter_num):
        
        if self.display_progress and \
                                (iter_num + 1) % self.display_progress == 0:
            sys.stderr.write('\r')
            sys.stderr.write(""Iteration {}: {}"".format(iter_num + 1, msg))
            sys.stderr.flush()","Display a progress bar with current loss.

        Parameters
        ----------
        msg : str
            Message to print alongside the progress bar

        iter_num : int
            Iteration number.
            Progress is only printed if this is a multiple of
            `self.display_progress`.",1,0,0,1
"def _proxy(self):
        
        if self._context is None:
            self._context = FunctionVersionContext(
                self._version,
                service_sid=self._solution['service_sid'],
                function_sid=self._solution['function_sid'],
                sid=self._solution['sid'],
            )
        return self._context","Generate an instance context for the instance, the context is capable of
        performing various actions.  All instance actions are proxied to the context

        :returns: FunctionVersionContext for this FunctionVersionInstance
        :rtype: twilio.rest.serverless.v1.service.function.function_version.FunctionVersionContext",0,0,2,2
"def _put_options(self, options_list):
        
        new_options = self._options.copy()  
        new_options.update({""value_choices"": options_list})
        validate(new_options, options_json_schema)

        url = self._client._build_url('property', property_id=self.id)
        response = self._client._request('PUT', url, json={'options': new_options})

        if response.status_code != 200:  
            raise APIError(""Could not update property value. Response: {}"".format(str(response)))
        else:
            self._options = new_options","Save the options to KE-chain.

        Makes a single API call.

        :param options_list: list of options to set.
        :raises APIError: when unable to update the options",1,1,2,4
"def _query(
        self,
        sql,
        url,
        fmt,
        log
    ):
        
        self.log.info('starting the ``_query`` method')

        try:
            response = requests.get(
                url=url,
                params={
                    ""cmd"": self._filtercomment(sql),
                    ""format"": fmt,
                },
                headers={
                    ""Cookie"": ""ASP.NET_SessionId=d0fiwrodvk4rdf21gh3jzr3t; SERVERID=dsa003"",
                },
            )
            
            
            
            
        except requests.exceptions.RequestException:
            print('HTTP Request failed')

        self.log.info('completed the ``_query`` method')
        return response.content",* query*,0,3,0,3
"def _rdump_value_to_numpy(s):
    
    if ""structure"" in s:
        vector_str, shape_str = re.findall(r'c\([^\)]+\)', s)
        shape = [int(d) for d in shape_str[2:-1].split(',')]
        if '.' in vector_str:
            arr = np.array([float(v) for v in vector_str[2:-1].split(',')])
        else:
            arr = np.array([int(v) for v in vector_str[2:-1].split(',')])
        
        arr = arr.reshape(shape, order='F')
    elif ""c("" in s:
        if '.' in s:
            arr = np.array([float(v) for v in s[2:-1].split(',')], order='F')
        else:
            arr = np.array([int(v) for v in s[2:-1].split(',')], order='F')
    else:
        arr = np.array(float(s) if '.' in s else int(s))
    return arr","Convert a R dump formatted value to Numpy equivalent

    For example, ""c(1, 2)"" becomes ``array([1, 2])``

    Only supports a few R data structures. Will not work with European decimal format.",0,0,1,1
"def _read_ancestry(file):
        
        df = pd.read_csv(
            file,
            comment=""
            header=0,
            sep=""\t"",
            na_values=0,
            names=[""rsid"", ""chrom"", ""pos"", ""allele1"", ""allele2""],
            index_col=0,
            dtype={""chrom"": object},
        )

        
        df[""genotype""] = df[""allele1""] + df[""allele2""]

        
        
        del df[""allele1""]
        del df[""allele2""]

        
        df.ix[np.where(df[""chrom""] == ""23"")[0], ""chrom""] = ""X""
        df.ix[np.where(df[""chrom""] == ""24"")[0], ""chrom""] = ""Y""
        df.ix[np.where(df[""chrom""] == ""25"")[0], ""chrom""] = ""PAR""
        df.ix[np.where(df[""chrom""] == ""26"")[0], ""chrom""] = ""MT""

        return sort_snps(df), ""AncestryDNA""","Read and parse Ancestry.com file.

        http://www.ancestry.com

        Parameters
        ----------
        file : str
            path to file

        Returns
        -------
        pandas.DataFrame
            individual's genetic data normalized for use with `lineage`
        str
            name of data source",1,0,1,2
"def _read_bin(self, stream, byte_order):
        
        self._data = _np.empty(self.count, dtype=self.dtype(byte_order))

        for k in _range(self.count):
            for prop in self.properties:
                try:
                    self._data[prop.name][k] = \
                        prop._read_bin(stream, byte_order)
                except StopIteration:
                    raise PlyElementParseError(""early end-of-file"",
                                               self, k, prop)","Load a PLY element from a binary PLY file.  The element may
        contain list properties.",2,0,1,3
"def _read_configuration_file(self, config_filename):
        
        config = configparser.ConfigParser()
        config.read(config_filename)

        self._constants_filename = config.get('constants', 'columns')
        self._prefix = config.get('constants', 'prefix')
        self._class_name = config.get('constants', 'class')","Reads parameters from the configuration file.

        :param str config_filename: The name of the configuration file.",1,0,0,1
"def _read_s3_config(self):
        
        try:
            import boto3
            import botocore.exceptions
        except ImportError:
            boto3, botocore = None, None

        if not boto3:
            raise ValueError(
                's3 URL specified for configuration but boto3 not installed')
        parsed = parse.urlparse(self._file_path)
        try:
            response = boto3.client(
                's3', endpoint_url=os.environ.get('S3_ENDPOINT')).get_object(
                    Bucket=parsed.netloc, Key=parsed.path.lstrip('/'))
        except botocore.exceptions.ClientError as e:
            raise ValueError(
                'Failed to download configuration from S3: {}'.format(e))
        return response['Body'].read().decode('utf-8')","Read in the value of the configuration file in Amazon S3.

        :rtype: str
        :raises: ValueError",2,1,2,5
"def _read_yaml_file(self, yaml_file):
        
        
        with yaml_file.open('r') as f:
            if self.config is None:
                self.config = ruamel.yaml.load(
                    f, Loader=ruamel.yaml.RoundTripLoader)
            else:
                self.config = blobxfer.util.merge_dict(
                    ruamel.yaml.load(f, Loader=ruamel.yaml.RoundTripLoader),
                    self.config)","Read a yaml file into self.config
        :param CliContext self: this
        :param pathlib.Path yaml_file: yaml file to load",2,0,0,2
"def _reads_per_position(bam_in, loci_file, out_dir):
    
    data = Counter()
    a = pybedtools.BedTool(bam_in)
    b = pybedtools.BedTool(loci_file)
    c = a.intersect(b, s=True, bed=True, wo=True)
    for line in c:
        end = int(line[1]) + 1 + int(line[2]) if line[5] == ""+"" else int(line[1]) + 1
        start = int(line[1]) + 1 if line[5] == ""+"" else int(line[1]) + 1 + int(line[2])
        side5 = ""%s\t5p\t%s"" % (line[15], start)
        side3 = ""%s\t3p\t%s"" % (line[15], end)
        data[side5] += 1
        data[side3] += 1

    counts_reads = op.join(out_dir, 'locus_readpos.counts')
    with open(counts_reads, 'w') as out_handle:
        for k in data:
            print(k, file=out_handle, end="""")

    return counts_reads",Create input for compute entropy,1,0,1,2
"def _rectForce(x,pot,t=0.):
    
    
    R= nu.sqrt(x[0]**2.+x[1]**2.)
    phi= nu.arccos(x[0]/R)
    sinphi= x[1]/R
    cosphi= x[0]/R
    if x[1] < 0.: phi= 2.*nu.pi-phi
    
    Rforce= _evaluateplanarRforces(pot,R,phi=phi,t=t)
    phiforce= _evaluateplanarphiforces(pot,R,phi=phi,t=t)
    return nu.array([cosphi*Rforce-1./R*sinphi*phiforce,
                     sinphi*Rforce+1./R*cosphi*phiforce])","NAME:
       _rectForce
    PURPOSE:
       returns the force in the rectangular frame
    INPUT:
       x - current position
       t - current time
       pot - (list of) Potential instance(s)
    OUTPUT:
       force
    HISTORY:
       2011-02-02 - Written - Bovy (NYU)",0,0,1,1
"def _reduce(nodes):
  
  i = 0
  while i < len(nodes):
    if isinstance(nodes[i], OperatorNode):
      break
    else:
      i += 1

  if i == len(nodes):
    raise OperatorError(""No operator found"")

  operator_node = nodes[i]
  operator = operator_node.operator
  operands_lbound = i - operator.cardinality

  if operands_lbound < 0:
    raise OperatorError(""Insufficient operands for operator {0}"".format(operator.symbol))

  return nodes[:operands_lbound] + \
         [OptreeNode(operator_node, tuple(nodes[operands_lbound:i]))] + \
         nodes[i+1:]","Finds the first operator in the list, converts it and its operands to a OptreeNode, then
  returns a new list with the operator and operands replaced by the new OptreeNode.",2,0,5,7
"def _reg_name(self, reg_id):
        
        if reg_id >= X86_REG_ENDING:
            logger.warning(""Trying to get register name for a non-register"")
            return None
        cs_reg_name = self.cpu.instruction.reg_name(reg_id)
        if cs_reg_name is None or cs_reg_name.lower() == '(invalid)':
            return None
        return self.cpu._regfile._alias(cs_reg_name.upper())","Translates a register ID from the disassembler object into the
        register name based on manticore's alias in the register file

        :param int reg_id: Register ID",0,1,1,2
"def _register_namespace_and_command(self, namespace):
        
        self._add_namespace(namespace)
        
        cmd_name = namespace.source_name.split(""."", 1)[0] + "".$cmd""
        dest_cmd_name = namespace.dest_name.split(""."", 1)[0] + "".$cmd""
        self._add_namespace(Namespace(dest_name=dest_cmd_name, source_name=cmd_name))",Add a Namespace and the corresponding command namespace.,0,0,1,1
"def _register_transaction(self, send_msg, recv_msg, coroutine_recv, coroutine_abrt, get_key=None, inter_msg=None):
        
        if get_key is None:
            get_key = lambda x: None
        if inter_msg is None:
            inter_msg = []

        
        
        self._msgs_registered[send_msg.__msgtype__] = ([recv_msg.__msgtype__] + [x.__msgtype__ for x, _ in inter_msg], get_key, None, None, [])
        self._msgs_registered[recv_msg.__msgtype__] = (
        [], get_key, coroutine_recv, coroutine_abrt, [recv_msg.__msgtype__] + [x.__msgtype__ for x, _ in inter_msg])

        self._transactions[recv_msg.__msgtype__] = {}
        for msg_class, handler in inter_msg:
            self._msgs_registered[msg_class.__msgtype__] = ([], get_key, handler, None, [])
            self._transactions[msg_class.__msgtype__] = {}","Register a type of message to be sent.
        After this message has been sent, if the answer is received, callback_recv is called.
        If the remote server becomes dones, calls callback_abrt.

        :param send_msg: class of message to be sent
        :param recv_msg: message that the server should send in response
        :param get_key: receive a `send_msg` or `recv_msg` as input, and returns the ""key"" (global identifier) of the message
        :param coroutine_recv: callback called (on the event loop) when the transaction succeed, with, as input, `recv_msg` and eventually other args
        given to .send
        :param coroutine_abrt: callback called (on the event loop) when the transaction fails, with, as input, `recv_msg` and eventually other args
        given to .send
        :param inter_msg: a list of `(message_class, coroutine_recv)`, that can be received during the resolution of the transaction but will not
        finalize it. `get_key` is used on these `message_class` to get the key of the transaction.",0,2,4,6
"def _remove_persistent_module(mod, comment):
    
    if not mod or mod not in mod_list(True):
        return set()

    if comment:
        __salt__['file.comment'](_LOADER_CONF, _MODULE_RE.format(mod))
    else:
        __salt__['file.sed'](_LOADER_CONF, _MODULE_RE.format(mod), '')

    return set([mod])","Remove module from loader.conf. If comment is true only comment line where
    module is.",0,0,3,3
"def _remove_sig(signature, idempotent=False):
    
    try:
        signaturep = next(signature.iterancestors())
    except StopIteration:
        if idempotent:
            return
        raise ValueError(""Can't remove the root signature node"")
    if signature.tail is not None:
        try:
            signatures = next(signature.itersiblings(preceding=True))
        except StopIteration:
            if signaturep.text is not None:
                signaturep.text = signaturep.text + signature.tail
            else:
                signaturep.text = signature.tail
        else:
            if signatures.tail is not None:
                signatures.tail = signatures.tail + signature.tail
            else:
                signatures.tail = signature.tail
    signaturep.remove(signature)","Remove the signature node from its parent, keeping any tail element.
    This is needed for eneveloped signatures.

    :param signature: Signature to remove from payload
    :type signature: XML ElementTree Element
    :param idempotent:
        If True, don't raise an error if signature is already detached from parent.
    :type idempotent: boolean",1,0,3,4
"def _render_serializable(self, obj, context):
        
        logging.info()
        if obj is None:
            logging.debug(
                    ""_render_serializable passed a None obj, returning None"")
            return None
        output = {}
        if self._fields_to_render is None:
            return output
        for field in self._fields_to_render:
            renderer = self._fields[field].render
            output[field] = renderer(obj, field, context)
        return output","Renders a JSON-serializable version of the object passed in.
        Usually this means turning a Python object into a dict, but sometimes
        it might make sense to render a list, or a string, or a tuple.

        In this base class, we provide a default implementation that assumes
        some things about your application architecture, namely, that your
        models specified in `underlying_model` have properties with the same
        name as all of the `_fields` that you've specified on a
        resource, and that all of those fields are public.

        Obviously this may not be appropriate for your app, so your
        subclass(es) of Resource should implement this method to serialize
        your things in the way that works for you.

        Do what you need to do.  The world is your oyster.",0,1,1,2
"def _replace_booleans(tok):
    
    toknum, tokval = tok
    if toknum == tokenize.OP:
        if tokval == '&':
            return tokenize.NAME, 'and'
        elif tokval == '|':
            return tokenize.NAME, 'or'
        return toknum, tokval
    return toknum, tokval","Replace ``&`` with ``and`` and ``|`` with ``or`` so that bitwise
    precedence is changed to boolean precedence.

    Parameters
    ----------
    tok : tuple of int, str
        ints correspond to the all caps constants in the tokenize module

    Returns
    -------
    t : tuple of int, str
        Either the input or token or the replacement values",0,0,2,2
"def _request(self, text, properties, retries=0):
        
        text = to_unicode(text)  
        try:
            r = requests.post(self.server, params={'properties': str(properties)}, data=text.encode('utf-8'))
            r.raise_for_status()
            return r
        except requests.ConnectionError as e:
            if retries > 5:
                logging.critical('Max retries exceeded!')
                raise e
            else:
                logging.critical(repr(e))
                logging.critical(""It seems like we've temporarily ran out of ports. Taking a 30s break..."")
                time.sleep(30)
                logging.critical(""Retrying..."")
                return self._request(text, properties, retries=retries+1)
        except requests.HTTPError:
            if r.text == ""CoreNLP request timed out. Your document may be too long."":
                raise TimeoutException(r.text)
            else:
                raise AnnotationException(r.text)","Send a request to the CoreNLP server.

        :param (str | unicode) text: raw text for the CoreNLPServer to parse
        :param (dict) properties: properties that the server expects
        :return: request result",2,4,2,8
"def _request(self, typ, id=0, method='GET', params=None, data=None,
                 url=None):
        
        backend, backend_version = peeringdb.get_backend_info()
        user_agent = 'PeeringDB/{} {}/{}'.format(peeringdb.__version__,
                                                 backend, backend_version)
        headers = {
            ""Accept"": ""application/json"",
            ""User-Agent"": user_agent,
        }
        auth = None

        if self.user:
            auth = (self.user, self.password)
        if not url:
            if id:
                url = ""%s/%s/%s"" % (self.url, typ, id)
            else:
                url = ""%s/%s"" % (self.url, typ)

        return requests.request(method, url, params=params, data=data,
                                auth=auth, headers=headers)","send the request, return response obj",0,1,0,1
"def _requests_post(self, url,
                       json=None,
                       data=None,
                       username="""",
                       password="""",
                       xapikey="""",
                       headers=None,
                       timeout=30):
        
        if headers is None:
            headers = {}

        
        auth = None
        if username and password:
            auth = requests.auth.HTTPBasicAuth(username, password)
        elif xapikey:
            headers['x-api-key'] = xapikey

        
        
        headers['User-Agent'] = self.user_agent

        request = requests.post(url, auth=auth, data=data, json=json,
                                headers=headers, timeout=timeout)

        
        
        
        message = json

        return request.text, message, request.status_code, request.headers","This function will POST to the url endpoint using requests.
        Returning an AdyenResult object on 200 HTTP response.
        Either json or data has to be provided.
        If username and password are provided, basic auth will be used.


        Args:
            url (str): url to send the POST
            json (dict, optional): Dict of the JSON to POST
            data (dict, optional): Dict, presumed flat structure of key/value
                of request to place
            username (str, optionl): Username for basic auth. Must be included
                as part of password.
            password (str, optional): Password for basic auth. Must be included
                as part of username.
            headers (dict, optional): Key/Value pairs of headers to include
            timeout (int, optional): Default 30. Timeout for the request.

        Returns:
            str:    Raw response received
            str:    Raw request placed
            int:    HTTP status code, eg 200,404,401
            dict:   Key/Value pairs of the headers received.",1,1,2,4
"def _restore_transfers(self, response):
        

        transfers = []
        for transfer_data in response.json()['transfers']:
            transfer = Transfer(self, _restore=True)
            transfer.transfer_info.update(transfer_data)
            transfer.get_files()
            transfers.append(transfer)

        return transfers","Restore transfers from josn retreived Filemail
        :param response: response object from request
        :rtype: ``list`` with :class:`Transfer` objects",0,0,1,1
"def _retrocom(rx, tx, game, kwargs):
    
    env = RetroWrapper.retro_make_func(game, **kwargs)

    
    while True:
        attr, args, kwargs = rx.get()

        
        
        if attr == RetroWrapper.symbol:
            result = env.__getattribute__(args)
            tx.put(callable(result))
        elif attr == ""close"":
            env.close()
            break
        else:
            
            result = getattr(env, attr)
            if callable(result):
                result = result(*args, **kwargs)
            tx.put(result)","This function is the target for RetroWrapper's internal
    process and does all the work of communicating with the
    environment.",0,3,0,3
"def _roads_extract(resp):
    

    try:
        j = resp.json()
    except:
        if resp.status_code != 200:
            raise googlemaps.exceptions.HTTPError(resp.status_code)

        raise googlemaps.exceptions.ApiError(""UNKNOWN_ERROR"",
                                             ""Received a malformed response."")

    if ""error"" in j:
        error = j[""error""]
        status = error[""status""]

        if status == ""RESOURCE_EXHAUSTED"":
            raise googlemaps.exceptions._OverQueryLimit(status,
                                                        error.get(""message""))

        raise googlemaps.exceptions.ApiError(status, error.get(""message""))

    if resp.status_code != 200:
        raise googlemaps.exceptions.HTTPError(resp.status_code)

    return j",Extracts a result from a Roads API HTTP response.,5,0,6,11
"def _router_request(router, method, data=None):
    
    if router not in ROUTERS:
        return False

    req_data = salt.utils.json.dumps([dict(
        action=router,
        method=method,
        data=data,
        type='rpc',
        tid=1)])

    config = __salt__['config.option']('zenoss')
    log.debug('Making request to router %s with method %s', router, method)
    url = '{0}/zport/dmd/{1}_router'.format(config.get('hostname'), ROUTERS[router])
    response = _session().post(url, data=req_data)

    
    
    
    if re.search('name=""__ac_name""', response.content):
        log.error('Request failed. Bad username/password.')
        raise Exception('Request failed. Bad username/password.')

    return salt.utils.json.loads(response.content).get('result', None)",Make a request to the Zenoss API router,1,2,1,4
"def _safe_path(filepath, can_be_cwl=False):
    
    
    if filepath in {'.gitignore', '.gitattributes'}:
        return False

    
    if filepath.startswith('.renku'):
        
        if can_be_cwl and filepath.endswith('.cwl'):
            return True
        return False

    return True",Check if the path should be used in output.,0,0,1,1
"def _sample_in_stratum(self, stratum_idx, replace = True):
        
        if replace:
            stratum_loc = np.random.choice(self.sizes_[stratum_idx])
        else:
            
            stratum_locs = np.where(~self._sampled[stratum_idx])[0]
            stratum_loc = np.random.choice(stratum_locs)

        
        self._sampled[stratum_idx][stratum_loc] = True
        self._n_sampled[stratum_idx] += 1
        
        loc = self.allocations_[stratum_idx][stratum_loc]
        return loc","Sample an item uniformly from a stratum

        Parameters
        ----------
        stratum_idx : int
            stratum index to sample from

        replace : bool, optional, default True
            whether to sample with replacement

        Returns
        -------
        int
            location of the randomly selected item in the original input array",0,0,3,3
"def _save(self):
        
        collection = JSONClientValidated('assessment',
                                         collection='AssessmentSection',
                                         runtime=self._runtime)
        if '_id' in self._my_map:  
            collection.save(self._my_map)
        else:
            insert_result = collection.insert_one(self._my_map)
            self._my_map = collection.find_one({'_id': insert_result.inserted_id})","Saves the current state of this AssessmentSection to database.

        Should be called every time the question map changes.",1,0,0,1
"def _save_function_initial_state(self, function_key, function_address, state):
        

        l.debug('Saving the initial state for function %
                function_address,
                function_key
                )
        if function_key in self._function_initial_states[function_address]:
            existing_state = self._function_initial_states[function_address][function_key]
            merged_state, _, _ = existing_state.merge(state)
            self._function_initial_states[function_address][function_key] = merged_state

        else:
            self._function_initial_states[function_address][function_key] = state","Save the initial state of a function, and merge it with existing ones if there are any.

        :param FunctionKey function_key: The key to this function.
        :param int function_address: Address of the function.
        :param SimState state: Initial state of the function.
        :return: None",0,1,3,4
"def _schedulerServiceSpecialCase(empowered, pups):
    
    from axiom.scheduler import _SiteScheduler, _UserScheduler

    
    for pup in pups:
        return pup
    
    if isinstance(empowered, Store):
        if getattr(empowered, '_schedulerService', None) is None:
            if empowered.parent is None:
                sched = _SiteScheduler(empowered)
            else:
                sched = _UserScheduler(empowered)
            empowered._schedulerService = sched
        return empowered._schedulerService
    return None","This function creates (or returns a previously created) L{IScheduler}
    powerup.

    If L{IScheduler} powerups were found on C{empowered}, the first of those
    is given priority.  Otherwise, a site L{Store} or a user L{Store} will
    have any pre-existing L{IScheduler} powerup associated with them (on the
    hackish cache attribute C{_schedulerService}) returned, or a new one
    created if none exists already.",0,0,5,5
"def _send(self, message):
        
        message['command'] = 'zappa.asynchronous.route_sns_task'
        payload = json.dumps(message).encode('utf-8')
        if len(payload) > LAMBDA_ASYNC_PAYLOAD_LIMIT: 
            raise AsyncException(""Payload too large for SNS"")
        self.response = self.client.publish(
                                TargetArn=self.arn,
                                Message=payload
                            )
        self.sent = self.response.get('MessageId')","Given a message, publish to this topic.",1,1,1,3
"def _send(self, value, mode):
        

        
        params = [mode]
        params.extend([(value >> i) & 0x01 for i in range(8)])
        
        pigpio.exceptions = False
        while True:
            ret = self.pi.run_script(self._writescript, params)
            if ret >= 0:
                break
            elif ret != pigpio.PI_SCRIPT_NOT_READY:
                raise pigpio.error(pigpio.error_text(ret))
            
            c.usleep(1)
        
        pigpio.exceptions = True","Send the specified value to the display with automatic 4bit / 8bit
        selection. The rs_mode is either ``RS_DATA`` or ``RS_INSTRUCTION``.",1,1,1,3
"def _send_api_request(self, request, captcha_response=None):
        
        url = self.API_URL + request.method_name

        
        method_kwargs = {'v': self.api_version}

        
        for values in (request.method_args,):
            method_kwargs.update(stringify_values(values))

        if self.is_token_required() or self._service_token:
            
            method_kwargs['access_token'] = self.access_token

        if captcha_response:
            method_kwargs['captcha_sid'] = captcha_response['sid']
            method_kwargs['captcha_key'] = captcha_response['key']

        http_params = dict(url=url,
                           data=method_kwargs,
                           **request.http_params)
        logger.debug('send_api_request:http_params: %s', http_params)
        response = self.http_session.post(**http_params)
        return response","Prepare and send HTTP API request

        :param request: vk_requests.api.Request instance
        :param captcha_response: None or dict 
        :return: HTTP response",0,2,1,3
"def _send_event_to_project(self, project_id, action, event):
        
        try:
            project_listeners = self._listeners[project_id]
        except KeyError:
            return
        for listener in project_listeners:
            listener.put_nowait((action, event, {}))","Send an event to all the client listening for notifications for
        this project

        :param project: Project where we need to send the event
        :param action: Action name
        :param event: Event to send",0,1,0,1
"def _send_http_request(self, xml_request):
        
        headers = {""Host"": self._host, ""Content-Type"": ""text/xml"", ""Recipient"": self._storage}
        try: 
            self._connection.request(""POST"", self._selector_url, xml_request, headers)
            response = self._connection.getresponse()
        except (httplib.CannotSendRequest, httplib.BadStatusLine):
            Debug.warn(""\nRestarting socket, resending message!"")
            self._open_connection()
            self._connection.request(""POST"", self._selector_url, xml_request, headers)
            response = self._connection.getresponse()
        data = response.read()
        return data","Send a request via HTTP protocol.

            Args:
                xml_request -- A fully formed xml request string for the CPS.

            Returns:
                The raw xml response string.",0,2,0,2
"def _send_request(url_id, data=None, json=None, req_type=None):
    
    url = settings.SEEDER_INFO_URL % url_id

    if not req_type:
        req_type = requests.get

    resp = req_type(
        url,
        data=data,
        json=json,
        timeout=settings.SEEDER_TIMEOUT,
        headers={
            ""User-Agent"": settings.USER_AGENT,
            ""Authorization"": settings.SEEDER_TOKEN,
        }
    )
    resp.raise_for_status()
    data = resp.json()

    return data","Send request to Seeder's API.

    Args:
        url_id (str): ID used as identification in Seeder.
        data (obj, default None): Optional parameter for data.
        json (obj, default None): Optional parameter for JSON body.
        req_type (fn, default None): Request method used to send/download the
            data. If none, `requests.get` is used.

    Returns:
        dict: Data from Seeder.",0,1,1,2
"def _send_scp(self, cabinet, frame, board, *args, **kwargs):
        
        
        
        connection = self.connections.get((cabinet, frame, board), None)
        if connection is None:
            connection = self.connections.get((cabinet, frame), None)
        assert connection is not None, \
            ""No connection available to ({}, {}, {})"".format(cabinet,
                                                             frame,
                                                             board)

        
        
        
        if self._scp_data_length is None:
            length = consts.SCP_SVER_RECEIVE_LENGTH_MAX
        else:
            length = self._scp_data_length

        return connection.send_scp(length, 0, 0, board, *args, **kwargs)","Determine the best connection to use to send an SCP packet and use
        it to transmit.

        See the arguments for
        :py:meth:`~rig.machine_control.scp_connection.SCPConnection` for
        details.",1,3,2,6
"def _send_stream_start(self, stream_id = None, stream_to = None):
        
        if self._output_state in (""open"", ""closed""):
            raise StreamError(""Stream start already sent"")
        if not self.language:
            self.language = self.settings[""language""]
        if stream_to:
            stream_to = unicode(stream_to)
        elif self.peer and self.initiator:
            stream_to = unicode(self.peer)
        stream_from = None
        if self.me and (self.tls_established or not self.initiator):
            stream_from = unicode(self.me)
        if stream_id:
            self.stream_id = stream_id
        else:
            self.stream_id = None
        self.transport.send_stream_head(self.stanza_namespace,
                                        stream_from, stream_to,
                                    self.stream_id, language = self.language)
        self._output_state = ""open""",Send stream start tag.,1,1,2,4
"def _send_with_auth(values, secret_key, url):
  

  data = urllib.urlencode(values)

  
  
  request = Request.from_values(
    content_length=len(data),
    input_stream=StringIO(data),
    content_type='application/x-www-form-urlencoded',
    method='POST')

  
  values['auth_token'] = create_token(secret_key, dict(request.form))
  data = urllib.urlencode(values)
  req = urllib2.Request(url, data)
  response = urllib2.urlopen(req)
  return json.loads(response.read())","Send dictionary of JSON serializable `values` as a POST body to `url`
     along with `auth_token` that's generated from `secret_key` and `values`

  scheduler.auth.create_token expects a JSON serializable payload, so we send
  a dictionary. On the receiving end of the POST request, the Flask view will
  have access to a werkzeug.datastructures.ImmutableMultiDict. The easiest
  and most surefire way to ensure that the payload sent to create_token will
  be consistent on both ends is to generate an ImmutableMultiDict using the
  werkzeug.Request.",0,2,1,3
"def _separate(self, kwargs):
        
        self._pop_none(kwargs)
        result = {}
        for field in Resource.config_fields:
            if field in kwargs:
                result[field] = kwargs.pop(field)
                if field in Resource.json_fields:

                    
                    if not isinstance(result[field], six.string_types):
                        continue

                    try:
                        data = json.loads(result[field])
                        result[field] = data
                    except ValueError:
                        raise exc.TowerCLIError('Provided json file format '
                                                'invalid. Please recheck.')
        return result",Remove None-valued and configuration-related keyworded arguments,1,0,2,3
"def _set_BC(self, pores, bctype, bcvalues=None, mode='merge'):
        r
        
        bctype = self._parse_mode(bctype, allowed=['value', 'rate'],
                                  single=True)
        mode = self._parse_mode(mode, allowed=['merge', 'overwrite', 'remove'],
                                single=True)
        pores = self._parse_indices(pores)

        values = np.array(bcvalues)
        if values.size > 1 and values.size != pores.size:
            raise Exception('The number of boundary values must match the ' +
                            'number of locations')

        
        if ('pore.bc_'+bctype not in self.keys()) or (mode == 'overwrite'):
            self['pore.bc_'+bctype] = np.nan
        self['pore.bc_'+bctype][pores] = values","r""""""
        Apply boundary conditions to specified pores

        Parameters
        ----------
        pores : array_like
            The pores where the boundary conditions should be applied

        bctype : string
            Specifies the type or the name of boundary condition to apply. The
            types can be one one of the following:

            - *'value'* : Specify the value of the quantity in each location
            - *'rate'* : Specify the flow rate into each location

        bcvalues : int or array_like
            The boundary value to apply, such as concentration or rate.  If
            a single value is given, it's assumed to apply to all locations.
            Different values can be applied to all pores in the form of an
            array of the same length as ``pores``.

        mode : string, optional
            Controls how the conditions are applied.  Options are:

            *'merge'*: (Default) Adds supplied boundary conditions to already
            existing conditions.

            *'overwrite'*: Deletes all boundary condition on object then add
            the given ones

        Notes
        -----
        It is not possible to have multiple boundary conditions for a
        specified location in one algorithm. Use ``remove_BCs`` to
        clear existing BCs before applying new ones or ``mode='overwrite'``
        which removes all existing BC's before applying the new ones.",1,0,2,3
"def _set_auth(self, v, load=False):
    
    if hasattr(v, ""_utype""):
      v = v._utype(v)
    try:
      t = YANGDynClass(v,base=auth.auth, is_container='container', presence=False, yang_name=""auth"", rest_name=""auth"", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'info': u'Authentication type configuration', u'cli-incomplete-no': None}}, namespace='urn:brocade.com:mgmt:brocade-fc-auth', defining_module='brocade-fc-auth', yang_type='container', is_config=True)
    except (TypeError, ValueError):
      raise ValueError({
          'error-string': ,
          'defined-type': ""container"",
          'generated-type': ,
        })

    self.__auth = t
    if hasattr(self, '_set'):
      self._set()","Setter method for auth, mapped from YANG variable /rbridge_id/fcsp/auth (container)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_auth is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_auth() directly.",1,0,3,4
"def _set_default_headers(self):
        
        self.headers.setdefault('Date', self.get_current_time)
        self.headers.setdefault('Server', self.SERVER_INFO)
        self.headers.setdefault('Content-Length', ""%d"" % len(self.message))
        if self.app.enabled('x-powered-by'):
            self.headers.setdefault('X-Powered-By', 'Growler')","Create some default headers that should be sent along with every HTTP
        response",0,0,1,1
"def _set_fwdl_state(self, v, load=False):
    
    if hasattr(v, ""_utype""):
      v = v._utype(v)
    try:
      t = YANGDynClass(v,base=RestrictedClassType(base_type=unicode,                                     restriction_type=""dict_key"",                                     restriction_arg={u'in-progress': {'value': 1}, u'downloaded': {'value': 2}, u'completed': {'value': 4}, u'failed': {'value': 3}},), is_leaf=True, yang_name=""fwdl-state"", rest_name=""fwdl-state"", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=False, extensions={u'tailf-common': {u'info': u'Firmware download state'}}, namespace='urn:brocade.com:mgmt:brocade-firmware', defining_module='brocade-firmware', yang_type='enumeration', is_config=True)
    except (TypeError, ValueError):
      raise ValueError({
          'error-string': ,
          'defined-type': ""brocade-firmware:enumeration"",
          'generated-type': ,
        })

    self.__fwdl_state = t
    if hasattr(self, '_set'):
      self._set()","Setter method for fwdl_state, mapped from YANG variable /brocade_firmware_rpc/fwdl_status/output/fwdl_state (enumeration)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_fwdl_state is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_fwdl_state() directly.",1,0,3,4
"def _set_input(self, v, load=False):
    
    if hasattr(v, ""_utype""):
      v = v._utype(v)
    try:
      t = YANGDynClass(v,base=input.input, is_leaf=True, yang_name=""input"", rest_name=""input"", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=False, extensions=None, namespace='urn:brocade.com:mgmt:brocade-system-monitor-ext', defining_module='brocade-system-monitor-ext', yang_type='input', is_config=True)
    except (TypeError, ValueError):
      raise ValueError({
          'error-string': ,
          'defined-type': ""brocade-system-monitor-ext:input"",
          'generated-type': ,
        })

    self.__input = t
    if hasattr(self, '_set'):
      self._set()","Setter method for input, mapped from YANG variable /brocade_system_monitor_ext_rpc/show_system_monitor/input (input)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_input is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_input() directly.",1,0,2,3
"def _set_lsp_secpath_autobw_template(self, v, load=False):
    
    if hasattr(v, ""_utype""):
      v = v._utype(v)
    try:
      t = YANGDynClass(v,base=ReferenceType(referenced_path='../../../../autobw-template/autobw-template-name', caller=self._path() + ['lsp-secpath-autobw-template'], path_helper=self._path_helper, require_instance=True), is_leaf=True, yang_name=""lsp-secpath-autobw-template"", rest_name=""template"", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'cli-full-command': None, u'info': u'Inherit Auto-bandwidth parameters from a template', u'alt-name': u'template'}}, namespace='urn:brocade.com:mgmt:brocade-mpls', defining_module='brocade-mpls', yang_type='leafref', is_config=True)
    except (TypeError, ValueError):
      raise ValueError({
          'error-string': ,
          'defined-type': ""leafref"",
          'generated-type': ,
        })

    self.__lsp_secpath_autobw_template = t
    if hasattr(self, '_set'):
      self._set()","Setter method for lsp_secpath_autobw_template, mapped from YANG variable /mpls_config/router/mpls/mpls_cmds_holder/lsp/secondary_path/lsp_secpath_auto_bandwidth/lsp_secpath_autobw_template (leafref)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_lsp_secpath_autobw_template is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_lsp_secpath_autobw_template() directly.",1,0,3,4
"def _set_priv(self, v, load=False):
    
    if hasattr(v, ""_utype""):
      v = v._utype(v)
    try:
      t = YANGDynClass(v,base=RestrictedClassType(base_type=unicode,                                     restriction_type=""dict_key"",                                     restriction_arg={u'AES128': {'value': 2}, u'DES': {'value': 0}, u'nopriv': {'value': 1}},), default=unicode(""nopriv""), is_leaf=True, yang_name=""priv"", rest_name=""priv"", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'info': u'Privacy protocol for username (Default=nopriv)'}}, namespace='urn:brocade.com:mgmt:brocade-snmp', defining_module='brocade-snmp', yang_type='enumeration', is_config=True)
    except (TypeError, ValueError):
      raise ValueError({
          'error-string': ,
          'defined-type': ""brocade-snmp:enumeration"",
          'generated-type': ,
        })

    self.__priv = t
    if hasattr(self, '_set'):
      self._set()","Setter method for priv, mapped from YANG variable /rbridge_id/snmp_server/user/priv (enumeration)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_priv is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_priv() directly.",1,0,3,4
"def _set_protocol_vrrp(self, v, load=False):
    
    if hasattr(v, ""_utype""):
      v = v._utype(v)
    try:
      t = YANGDynClass(v,base=protocol_vrrp.protocol_vrrp, is_container='container', presence=False, yang_name=""protocol-vrrp"", rest_name="""", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'cli-drop-node-name': None, u'sort-priority': u'RUNNCFG_LEVEL_ROUTER_GLOBAL'}}, namespace='urn:brocade.com:mgmt:brocade-vrrp', defining_module='brocade-vrrp', yang_type='container', is_config=True)
    except (TypeError, ValueError):
      raise ValueError({
          'error-string': ,
          'defined-type': ""container"",
          'generated-type': ,
        })

    self.__protocol_vrrp = t
    if hasattr(self, '_set'):
      self._set()","Setter method for protocol_vrrp, mapped from YANG variable /protocol_vrrp (container)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_protocol_vrrp is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_protocol_vrrp() directly.

    YANG Description: An intermediary node that separates the protocol vrrp from other protocols.",1,0,3,4
"def _set_rowcount(self, query_results):
        
        total_rows = 0
        num_dml_affected_rows = query_results.num_dml_affected_rows

        if query_results.total_rows is not None and query_results.total_rows > 0:
            total_rows = query_results.total_rows
        if num_dml_affected_rows is not None and num_dml_affected_rows > 0:
            total_rows = num_dml_affected_rows
        self.rowcount = total_rows","Set the rowcount from query results.

        Normally, this sets rowcount to the number of rows returned by the
        query, but if it was a DML statement, it sets rowcount to the number
        of modified rows.

        :type query_results:
            :class:`~google.cloud.bigquery.query._QueryResults`
        :param query_results: results of a query",0,0,3,3
"def _set_show_mpls_policy(self, v, load=False):
    
    if hasattr(v, ""_utype""):
      v = v._utype(v)
    try:
      t = YANGDynClass(v,base=show_mpls_policy.show_mpls_policy, is_leaf=True, yang_name=""show-mpls-policy"", rest_name=""show-mpls-policy"", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=False, extensions={u'tailf-common': {u'hidden': u'full', u'actionpoint': u'showMpls'}}, namespace='urn:brocade.com:mgmt:brocade-mpls', defining_module='brocade-mpls', yang_type='rpc', is_config=True)
    except (TypeError, ValueError):
      raise ValueError({
          'error-string': ,
          'defined-type': ""rpc"",
          'generated-type': ,
        })

    self.__show_mpls_policy = t
    if hasattr(self, '_set'):
      self._set()","Setter method for show_mpls_policy, mapped from YANG variable /brocade_mpls_rpc/show_mpls_policy (rpc)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_show_mpls_policy is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_show_mpls_policy() directly.",1,0,2,3
"def _set_system_qos(self, v, load=False):
    
    if hasattr(v, ""_utype""):
      v = v._utype(v)
    try:
      t = YANGDynClass(v,base=system_qos.system_qos, is_container='container', presence=False, yang_name=""system-qos"", rest_name="""", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'cli-drop-node-name': None, u'sort-priority': u'70'}}, namespace='urn:brocade.com:mgmt:brocade-policer', defining_module='brocade-policer', yang_type='container', is_config=True)
    except (TypeError, ValueError):
      raise ValueError({
          'error-string': ,
          'defined-type': ""container"",
          'generated-type': ,
        })

    self.__system_qos = t
    if hasattr(self, '_set'):
      self._set()","Setter method for system_qos, mapped from YANG variable /system_qos (container)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_system_qos is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_system_qos() directly.",1,0,3,4
"def _set_threshold_memory(self, v, load=False):
    
    if hasattr(v, ""_utype""):
      v = v._utype(v)
    try:
      t = YANGDynClass(v,base=RestrictedClassType(base_type=RestrictedClassType(base_type=long, restriction_dict={'range': ['0..4294967295']}, int_size=32), restriction_dict={'range': [u'50 .. 200']}), is_leaf=True, yang_name=""threshold-memory"", rest_name=""threshold"", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'info': u'Threshold for free low memory', u'alt-name': u'threshold', u'cli-suppress-no': None}}, namespace='urn:brocade.com:mgmt:brocade-resource-monitor', defining_module='brocade-resource-monitor', yang_type='uint32', is_config=True)
    except (TypeError, ValueError):
      raise ValueError({
          'error-string': ,
          'defined-type': ""uint32"",
          'generated-type': ,
        })

    self.__threshold_memory = t
    if hasattr(self, '_set'):
      self._set()","Setter method for threshold_memory, mapped from YANG variable /rbridge_id/resource_monitor/memory/threshold_memory (uint32)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_threshold_memory is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_threshold_memory() directly.",2,0,2,4
"def _set_verbose(self, verbose):
        
        string_levels = ('basic', 'advanced', 'expert', 'guru')

        if verbose is True:
            
            
            verbose = 'basic'

        if (isinstance(verbose, str) and not (verbose in string_levels)):
            verbose = 'basic'

        self.verbose = verbose","Check and set our :data:`verbose` attribute.
        The debug-level must be a string or an integer. If it is one of
        the allowed strings, GnuPG will translate it internally to it's
        corresponding integer level:

        basic     = 1-2
        advanced  = 3-5
        expert    = 6-8
        guru      = 9+

        If it's not one of the recognised string levels, then then
        entire argument is ignored by GnuPG. :(

        To fix that stupid behaviour, if they wanted debugging but typo'd
        the string level (or specified ``verbose=True``), we'll default to
        'basic' logging.",0,0,1,1
"def _setup_output_file(self, output_filename, args, write_header=True):
        
        
        
        try:
            output_file = open(output_filename, 'w') 
        except IOError as e:
            sys.exit(e)

        if write_header:
            output_file.write(' '.join(map(util.escape_string_shell, self._build_cmdline(args)))
                              + '\n\n\n' + '-' * 80 + '\n\n\n')
            output_file.flush()

        return output_file",Open and prepare output file.,2,0,0,2
"def _shift_headings(self, content: str, shift: int) -> str:
        

        def _sub(heading):
            new_heading_level = len(heading.group('hashes')) + shift

            self.logger.debug(f'Shift heading level to {new_heading_level}, heading title: {heading.group(""title"")}')

            if new_heading_level <= 6:
                return f'{""

            else:
                self.logger.debug('New heading level is out of range, using bold paragraph text instead of heading')

                return f'**{heading.group(""title"")}**{heading.group(""tail"")}'

        return self._heading_pattern.sub(_sub, content)","Shift Markdown headings in a string by a given value. The shift can
        be positive or negative.

        :param content: Markdown content
        :param shift: Heading shift

        :returns: Markdown content with headings shifted by ``shift``",0,2,1,3
"def _shutdown_unlocked(self, context, lru=None, new_context=None):
        
        LOG.info('%r._shutdown_unlocked(): shutting down %r', self, context)
        context.shutdown()
        via = self._via_by_context.get(context)
        if via:
            lru = self._lru_by_via.get(via)
            if lru:
                if context in lru:
                    lru.remove(context)
                if new_context:
                    lru.append(new_context)
        self._forget_context_unlocked(context)","Arrange for `context` to be shut down, and optionally add `new_context`
        to the LRU list while holding the lock.",0,1,3,4
"def _signature_hash(self, tx_out_script, unsigned_txs_out_idx, hash_type):
        

        if hash_type & SIGHASH_FORKID != SIGHASH_FORKID:
            raise self.ScriptError()

        return self._signature_for_hash_type_segwit(tx_out_script, unsigned_txs_out_idx, hash_type)","Return the canonical hash for a transaction. We need to
        remove references to the signature, since it's a signature
        of the hash before the signature is applied.

        tx_out_script: the script the coins for unsigned_txs_out_idx are coming from
        unsigned_txs_out_idx: where to put the tx_out_script
        hash_type: one of SIGHASH_NONE, SIGHASH_SINGLE, SIGHASH_ALL,
        optionally bitwise or'ed with SIGHASH_ANYONECANPAY",1,0,2,3
"def _smooth(aprob, bprob, Ndiff):
    
    gamma, epsilon = _smoothing_parameters(aprob, bprob, Ndiff)

    
    in_a = [i for i,v in enumerate(aprob) if abs(v) > 0.0]
    aprob = list([list(aprob)[i] for i in in_a])
    bprob = list([list(bprob)[i]*gamma for i in in_a])

    
    bprob = list(map(lambda v: v if v != 0. else epsilon, bprob))

    return aprob, bprob","Smooth distributions for KL-divergence according to `Bigi 2003
    <http://link.springer.com/chapter/10.1007%2F3-540-36618-0_22?LI=true>`_.",0,0,1,1
"def _solve_base_feature_map(self, model_name, dependency_cache=None):
        
        features = list(trim(self[model_name].features))
        feature_values = self.extractor.solve(features, cache=dependency_cache)
        return {str(f): v
                for f, v in zip(features, feature_values)}","Solves the leaf :class:`revscoring.Feature` from the dependency for
        `model_name` using `dependency_cache`.  This will return a mapping
        between the `str` name of the base features and the solved values.",0,0,2,2
"def _sortAttributeValue(self, offset):
        
        if self._currentResults:
            pageStart = (self._currentResults[offset][
                self.currentSortColumn.attributeID],
                         self._currentResults[offset][
                    '__item__'].storeID)
        else:
            pageStart = None
        return pageStart","return the value of the sort attribute for the item at
        'offset' in the results of the last query, otherwise None.",0,0,2,2
"def _split_path(self, path):
        
        if '\\' in path:
            p = path.find('\\')
            hive = path[:p]
            path = path[p+1:]
        else:
            hive = path
            path = None
        handle = self._hives_by_name[ hive.upper() ]
        return handle, path","Splits a Registry path and returns the hive and key.

        @type  path: str
        @param path: Registry path.

        @rtype:  tuple( int, str )
        @return: Tuple containing the hive handle and the subkey path.
            The hive handle is always one of the following integer constants:
             - L{win32.HKEY_CLASSES_ROOT}
             - L{win32.HKEY_CURRENT_USER}
             - L{win32.HKEY_LOCAL_MACHINE}
             - L{win32.HKEY_USERS}
             - L{win32.HKEY_PERFORMANCE_DATA}
             - L{win32.HKEY_CURRENT_CONFIG}",0,0,1,1
"def _srvc_load_several_items(self, iterable, *args, **kwargs):
        
        for input_tuple in iterable:
            msg = input_tuple[0]
            item = input_tuple[1]
            if len(input_tuple) > 2:
                args = input_tuple[2]
            if len(input_tuple) > 3:
                kwargs = input_tuple[3]
            if len(input_tuple) > 4:
                raise RuntimeError('You shall not pass!')

            self.load(msg, item, *args, **kwargs)","Loads several items from an iterable

        Iterables are supposed to be of a format like `[(msg, item, args, kwarg),...]`
        If `args` and `kwargs` are not part of a tuple, they are taken from the
        current `args` and `kwargs` provided to this function.",1,0,1,2
"def _start_again_message(self, message=None):
        
        logging.debug(""Start again message delivered: {}"".format(message))
        the_answer = ', '.join(
            [str(d) for d in self.game.answer][:-1]
        ) + ', and ' + [str(d) for d in self.game.answer][-1]

        return ""{0}{1} The correct answer was {2}. Please start a new game."".format(
            message,
            ""."" if message[-1] not in [""."", "","", "";"", "":"", ""!""] else """",
            the_answer
        )",Simple method to form a start again message and give the answer in readable form.,0,1,1,2
"def _start_element (self, tag, attrs, end):
        
        tag = tag.encode(self.encoding, ""ignore"")
        self.fd.write(""<%s"" % tag.replace(""/"", """"))
        for key, val in attrs.items():
            key = key.encode(self.encoding, ""ignore"")
            if val is None:
                self.fd.write("" %s"" % key)
            else:
                val = val.encode(self.encoding, ""ignore"")
                self.fd.write(' %s=""%s""' % (key, quote_attrval(val)))
        self.fd.write(end)","Print HTML element with end string.

        @param tag: tag name
        @type tag: string
        @param attrs: tag attributes
        @type attrs: dict
        @param end: either > or />
        @type end: string
        @return: None",1,0,0,1
"def _status_new(self):
        

        self._update_status()
        new_comp = self._group_report(self._comp_report, 'Completed')
        new_dead = self._group_report(self._dead_report,
                                      'Dead, call jobs.traceback() for details')
        self._comp_report[:] = []
        self._dead_report[:] = []
        return new_comp or new_dead","Print the status of newly finished jobs.

        Return True if any new jobs are reported.

        This call resets its own state every time, so it only reports jobs
        which have finished since the last time it was called.",0,0,3,3
"def _storage_list(args, _):
  
  target = args['object'] if args['object'] else args['bucket']
  project = args['project']
  if target is None:
    return _storage_list_buckets(project, '*')  

  bucket_name, key = datalab.storage._bucket.parse_name(target)
  if bucket_name is None:
    raise Exception('Cannot list %s; not a valid bucket name' % target)

  if key or not re.search('\?|\*|\[', target):
    
    if not key:
      key = '*'
    if project:
      
      for bucket in datalab.storage.Buckets(project_id=project):
        if bucket.name == bucket_name:
          break
      else:
        raise Exception('%s does not exist in project %s' % (target, project))
    else:
      bucket = datalab.storage.Bucket(bucket_name)

    if bucket.exists():
      return _storage_list_keys(bucket, key)
    else:
      raise Exception('Bucket %s does not exist' % target)

  else:
    
    
    return _storage_list_buckets(project, target[5:])","List the buckets or the contents of a bucket.

  This command is a bit different in that we allow wildchars in the bucket name and will list
  the buckets that match.",3,0,6,9
"def _tag_matches(tags, filter):
        
        for tag_name, tag_value in filter.items():
            
            
            
            
            series_tag_value = tags.get(tag_name, _sentinel)
            if series_tag_value != tag_value:
                return False

        return True",Check if all key/values in filter match in tags.,0,0,1,1
"def _tls_aead_auth_decrypt(alg, c, read_seq_num):
    
    
    
    
    
    plen = c.len - getattr(alg, ""nonce_explicit_len"", 0) - alg.tag_len
    read_seq_num = struct.pack(""!Q"", read_seq_num)
    A = read_seq_num + struct.pack('!BHH', c.type, c.version, plen)

    p = TLSCompressed()
    p.type = c.type
    p.version = c.version
    p.len = plen
    p.data = alg.auth_decrypt(A, c.data, read_seq_num)

    if p.data is None:  
        return None
    return p","Provided with a TLSCiphertext instance c, the function applies AEAD
    cipher alg auth_decrypt function to c.data (and additional data)
    in order to authenticate the data and decrypt c.data. When those
    steps succeed, the result is a newly created TLSCompressed instance.
    On error, None is returned. Note that it is the caller's responsibility to
    increment read_seq_num afterwards.",0,0,2,2
"def _tls_mac_verify(alg, p, read_seq_num):
    
    h_size = alg.hash_len
    if p.len < h_size:
        return False
    received_h = p.data[-h_size:]
    p.len -= h_size
    p.data = p.data[:-h_size]

    read_seq_num = struct.pack(""!Q"", read_seq_num)
    h = alg.digest(read_seq_num + bytes(p))
    return h == received_h","Verify if the MAC in provided message (message resulting from decryption
    and padding removal) is valid. Current read sequence number is used in
    the verification process.

    If the MAC is valid:
     - The function returns True
     - The packet p is updated in the following way: trailing MAC value is
       removed from p.data and length is updated accordingly.

    In case of error, False is returned, and p may have been modified.

    Also note that it is the caller's responsibility to update the read
    sequence number after the operation.",0,0,1,1
"def _transcoding(cls, data):
        
        if not data:
            return data

        result = None
        if isinstance(data, str) and hasattr(data, 'decode'):
            result = data.decode('utf-8')
        else:
            result = data
        return result","Encoding conversion
:param data: data to be converted
:return: converted data",0,0,1,1
"def _traverse_toc(pdf_base, visitor_fn, log):
    

    visited = set()
    queue = set()
    link_keys = ('/Parent', '/First', '/Last', '/Prev', '/Next')

    if not '/Outlines' in pdf_base.root:
        return

    queue.add(pdf_base.root.Outlines.objgen)
    while queue:
        objgen = queue.pop()
        visited.add(objgen)
        node = pdf_base.get_object(objgen)
        log.debug('fix toc: exploring outline entries at %r', objgen)

        
        for key in link_keys:
            if key not in node:
                continue
            item = node[key]
            if not item.is_indirect:
                
                
                
                continue
            objgen = item.objgen
            if objgen not in visited:
                queue.add(objgen)

        if visitor_fn:
            visitor_fn(pdf_base, node, log)","Walk the table of contents, calling visitor_fn() at each node

    The /Outlines data structure is a messy data structure, but rather than
    navigating hierarchically we just track unique nodes.  Enqueue nodes when
    we find them, and never visit them again.  set() is awesome.  We look for
    the two types of object in the table of contents that can be page bookmarks
    and update the page entry.",0,1,2,3
"def _trunc_logser_solver(bins, b):
    

    if bins == b:
        p = 0

    else:
        BOUNDS = [0, 1]
        DIST_FROM_BOUND = 10 ** -15
        m = np.array(np.arange(1, np.int(b) + 1))
        y = lambda x: np.sum(x ** m / b * bins) - np.sum((x ** m) / m)
        p = optim.bisect(y, BOUNDS[0] + DIST_FROM_BOUND,
                   min((sys.float_info[0] / bins) ** (1 / b), 2),
                   xtol=1.490116e-08, maxiter=1000)
    return p","Given bins (S) and b (N) solve for MLE of truncated logseries
    parameter p

    Parameters
    -----------
    bins : float
        Number of bins. Considered S in an ecological context
    b : float
        Upper truncation of distribution. Considered N in an ecological context

    Returns
    -------
    : float
        MLE estimate of p

    Notes
    ------
    Adapted from Ethan White's macroecology_tools",0,0,1,1
"def _try_get_solutions(self, address, size, access, max_solutions=0x1000, force=False):
        
        assert issymbolic(address)

        solutions = solver.get_all_values(self.constraints, address, maxcnt=max_solutions)

        crashing_condition = False
        for base in solutions:
            if not self.access_ok(slice(base, base + size), access, force):
                crashing_condition = Operators.OR(address == base, crashing_condition)

        if solver.can_be_true(self.constraints, crashing_condition):
            raise InvalidSymbolicMemoryAccess(address, access, size, crashing_condition)

        return solutions","Try to solve for a symbolic address, checking permissions when reading/writing size bytes.

        :param Expression address: The address to solve for
        :param int size: How many bytes to check permissions for
        :param str access: 'r' or 'w'
        :param int max_solutions: Will raise if more solutions are found
        :param force: Whether to ignore permission failure
        :rtype: list",1,0,3,4
"def _union_lcs(evaluated_sentences, reference_sentence, prev_union=None):
    
    if prev_union is None:
        prev_union = set()

    if len(evaluated_sentences) <= 0:
        raise ValueError(""Collections must contain at least 1 sentence."")

    lcs_union = prev_union
    prev_count = len(prev_union)
    reference_words = _split_into_words([reference_sentence])

    combined_lcs_length = 0
    for eval_s in evaluated_sentences:
        evaluated_words = _split_into_words([eval_s])
        lcs = set(_recon_lcs(reference_words, evaluated_words))
        combined_lcs_length += len(lcs)
        lcs_union = lcs_union.union(lcs)

    new_lcs_count = len(lcs_union) - prev_count
    return new_lcs_count, lcs_union","Returns LCS_u(r_i, C) which is the LCS score of the union longest common
    subsequence between reference sentence ri and candidate summary C.
    For example:
    if r_i= w1 w2 w3 w4 w5, and C contains two sentences: c1 = w1 w2 w6 w7 w8
    and c2 = w1 w3 w8 w9 w5, then the longest common subsequence of r_i and c1
    is ""w1 w2"" and the longest common subsequence of r_i and c2 is ""w1 w3 w5"".
    The union longest common subsequence of r_i, c1, and c2 is ""w1 w2 w3 w5""
    and LCS_u(r_i, C) = 4/5.

    Args:
      evaluated_sentences: The sentences that have been picked by the
                           summarizer
      reference_sentence: One of the sentences in the reference summaries

    Returns:
      float: LCS_u(r_i, C)

    ValueError:
      Raises exception if a param has len <= 0",1,0,2,3
"def _unique_resource_identifier_from_kwargs(**kwargs):
    
    name = kwargs.pop('name', '')
    uuid = kwargs.pop('uuid', '')
    id = kwargs.pop('id', '')
    if uuid:
        return uuid, kwargs
    elif id:
        
        return id, kwargs
    else:
        return name, kwargs","Chooses an identifier given different choices

    The unique identifier in BIG-IP's REST API at the time of this writing
    is called 'name'. This is in contrast to the unique identifier that is
    used by iWorkflow and BIG-IQ which at some times is 'name' and other
    times is 'uuid'.

    For example, in iWorkflow, there consider this URI

      * https://10.2.2.3/mgmt/cm/cloud/tenants/{0}/services/iapp

    Then consider this iWorkflow URI

      * https://localhost/mgmt/cm/cloud/connectors/local/{0}

    In the first example, the identifier, {0}, is what we would normally
    consider a name. For example, ""tenant1"". In the second example though,
    the value is expected to be what we would normally consider to be a
    UUID. For example, '244bd478-374e-4eb2-8c73-6e46d7112604'.

    This method only tries to rectify the problem of which to use.

    I believe there might be some change that the two can appear together,
    although I have not yet experienced it. If it is possible, I believe it
    would happen in BIG-IQ/iWorkflow land where the UUID and Name both have
    significance. That's why I deliberately prefer the UUID when it exists
    in the parameters sent to the URL.

    :param kwargs:
    :return:",0,0,1,1
"def _update_flags(compiler_flags, remove_flags=()):
    
    for flag in GFORTRAN_SHARED_FLAGS:
        if flag not in compiler_flags:
            compiler_flags.append(flag)
    if DEBUG_ENV in os.environ:
        to_add = GFORTRAN_DEBUG_FLAGS
        to_remove = GFORTRAN_OPTIMIZE_FLAGS
    else:
        to_add = GFORTRAN_OPTIMIZE_FLAGS
        if os.environ.get(WHEEL_ENV) is None:
            to_add += (GFORTRAN_NATIVE_FLAG,)
        to_remove = GFORTRAN_DEBUG_FLAGS
    for flag in to_add:
        if flag not in compiler_flags:
            compiler_flags.append(flag)
    return [
        flag
        for flag in compiler_flags
        if not (flag in to_remove or flag in remove_flags)
    ]","Update a given set of compiler flags.

    Args:
        compiler_flags (List[str]): Existing flags associated with a compiler.
        remove_flags (Optional[Container[str]]): A container of flags to remove
            that will override any of the defaults.

    Returns:
        List[str]: The modified list (i.e. some flags added and some removed).",0,0,2,2
"def _update_internal_column_state(self, column_names):
        
        for k in column_names:
            if k not in self._column_name_idx:
                self._column_name_idx[k] = len(self._column_name_list)
                self._column_name_list.append(k)","Update the internal state with some (possibly) new columns

        :param column_names: an iterable which contains new column names",0,0,1,1
"def _update_request_uri_query(self, request):
        

        if '?' in request.path:
            request.path, _, query_string = request.path.partition('?')
            if query_string:
                query_params = query_string.split('&')
                for query in query_params:
                    if '=' in query:
                        name, _, value = query.partition('=')
                        request.query.append((name, value))

        request.path = url_quote(request.path, '/()$=\',')

        
        if request.query:
            request.path += '?'
            for name, value in request.query:
                if value is not None:
                    request.path += name + '=' + url_quote(value, '/()$=\',') + '&'
            request.path = request.path[:-1]

        return request.path, request.query","pulls the query string out of the URI and moves it into
        the query portion of the request object.  If there are already
        query parameters on the request the parameters in the URI will
        appear after the existing parameters",0,0,2,2
"def _update_zipimporter_cache(normalized_path, cache, updater=None):
    
    for p in _collect_zipimporter_cache_entries(normalized_path, cache):
        
        
        
        
        
        
        
        
        
        old_entry = cache[p]
        del cache[p]
        new_entry = updater and updater(p, old_entry)
        if new_entry is not None:
            cache[p] = new_entry","Update zipimporter cache data for a given normalized path.

    Any sub-path entries are processed as well, i.e. those corresponding to zip
    archives embedded in other zip archives.

    Given updater is a callable taking a cache entry key and the original entry
    (after already removing the entry from the cache), and expected to update
    the entry and possibly return a new one to be inserted in its place.
    Returning None indicates that the entry should not be replaced with a new
    one. If no updater is given, the cache entries are simply removed without
    any additional processing, the same as if the updater simply returned None.",0,0,2,2
"def _upsert(context, params, data):
    
    table = params.get(""table"")
    table = datastore.get_table(table, primary_id=False)
    unique_keys = ensure_list(params.get(""unique""))
    data[""__last_seen""] = datetime.datetime.utcnow()
    if len(unique_keys):
        updated = table.update(data, unique_keys, return_count=True)
        if updated:
            return
    data[""__first_seen""] = data[""__last_seen""]
    table.insert(data)",Insert or update data and add/update appropriate timestamps,0,0,2,2
"def _urllib_send(opener, req, **kwargs):
    
    if req.content and not any(h.lower() == 'content-type'
                               for h in req.headers):
        req = req.with_headers({'Content-Type': 'application/octet-stream'})
    url = req.url + '?' + urlencode(req.params)
    raw_req = urllib_request.Request(url, req.content, headers=req.headers)
    set_urllib_method(raw_req, req.method)
    try:
        res = opener.open(raw_req, **kwargs)
    except urllib_http_error_cls as http_err:
        res = http_err
    return Response(res.getcode(), content=res.read(), headers=res.headers)",Send a request with an :mod:`urllib` opener,0,1,1,2
"def _validate(self):
        
        if self.data_format is FormatType.PYTHON:
            self.data = self.raw_data
        elif self.data_format is FormatType.JSON:
            self._validate_json()
        elif self.data_format is FormatType.YAML:
            self._validate_yaml()",Validate the input data.,0,0,1,1
"def _validate_aud(claims, audience=None):
    

    if 'aud' not in claims:
        
        
        return

    audience_claims = claims['aud']
    if isinstance(audience_claims, string_types):
        audience_claims = [audience_claims]
    if not isinstance(audience_claims, list):
        raise JWTClaimsError('Invalid claim format in token')
    if any(not isinstance(c, string_types) for c in audience_claims):
        raise JWTClaimsError('Invalid claim format in token')
    if audience not in audience_claims:
        raise JWTClaimsError('Invalid audience')","Validates that the 'aud' claim is valid.

    The ""aud"" (audience) claim identifies the recipients that the JWT is
    intended for.  Each principal intended to process the JWT MUST
    identify itself with a value in the audience claim.  If the principal
    processing the claim does not identify itself with a value in the
    ""aud"" claim when this claim is present, then the JWT MUST be
    rejected.  In the general case, the ""aud"" value is an array of case-
    sensitive strings, each containing a StringOrURI value.  In the
    special case when the JWT has one audience, the ""aud"" value MAY be a
    single case-sensitive string containing a StringOrURI value.  The
    interpretation of audience values is generally application specific.
    Use of this claim is OPTIONAL.

    Args:
        claims (dict): The claims dictionary to validate.
        audience (str): The audience that is verifying the token.",3,0,4,7
"def _validate_nested_list_type(self, name, obj, nested_level, *args):
        
        if nested_level <= 1:
            self._validate_list_type(name, obj, *args)
        else:
            if obj is None:
                return
            if not isinstance(obj, list):
                raise TypeError(self.__class__.__name__ + '.' + name + ' contains value of type ' +
                                type(obj).__name__ + ' where a list is expected')
            for sub_obj in obj:
                self._validate_nested_list_type(name, sub_obj, nested_level - 1, *args)","Helper function that checks the input object as a list then recursively until nested_level is 1.

        :param name: Name of the object.
        :param obj: Object to check the type of.
        :param nested_level: Integer with the current nested level.
        :param args: List of classes.
        :raises TypeError: if the input object is not of any of the allowed types.",1,0,2,3
"def _validate_python_version(s):
    ""Return True if a string is of the form <int>.<int>, False otherwise.""
    if not s:
        return True
    toks = s.split('.')
    if len(toks) != 2:
        return False
    try:
        int(toks[0])
        int(toks[1])
    except ValueError:
        return False
    return True","Return True if a string is of the form <int>.<int>, False otherwise.",0,0,2,2
"def _validate_record_field_positions_global(record):
    
    all_fields = []
    for tag, fields in record.items():
        previous_field_position_global = -1
        for field in fields:
            if field[4] < previous_field_position_global:
                return (""Non ascending global field positions in tag '%s'."" %
                        tag)
            previous_field_position_global = field[4]
            if field[4] in all_fields:
                return (""Duplicate global field position '%d' in tag '%s'"" %
                        (field[4], tag))","Check if the global field positions in the record are valid.

    I.e., no duplicate global field positions and local field positions in the
    list of fields are ascending.

    :param record: the record data structure
    :return: the first error found as a string or None if no error was found",0,0,1,1
"def _warn_if_deprecated(key):
    

    d = _get_deprecated_option(key)
    if d:
        if d.msg:
            print(d.msg)
            warnings.warn(d.msg, DeprecationWarning)
        else:
            msg = ""'%s' is deprecated"" % key
            if d.removal_ver:
                msg += ' and will be removed in %s' % d.removal_ver
            if d.rkey:
                msg += "", please use '%s' instead."" % d.rkey
            else:
                msg += ', please refrain from using it.'

            warnings.warn(msg, DeprecationWarning)
        return True
    return False","Checks if `key` is a deprecated option and if so, prints a warning.
    Returns
    -------
    bool - True if `key` is deprecated, False otherwise.",1,0,1,2
"def _worker(self, constructor, conn):
    
    try:
      env = constructor()
      while True:
        try:
          
          if not conn.poll(0.1):
            continue
          message, payload = conn.recv()
        except (EOFError, KeyboardInterrupt):
          break
        if message == self._ACCESS:
          name = payload
          result = getattr(env, name)
          conn.send((self._RESULT, result))
          continue
        if message == self._CALL:
          name, args, kwargs = payload
          result = getattr(env, name)(*args, **kwargs)
          conn.send((self._RESULT, result))
          continue
        if message == self._CLOSE:
          assert payload is None
          break
        raise KeyError('Received message of unknown type {}'.format(message))
    except Exception:  
      stacktrace = ''.join(traceback.format_exception(*sys.exc_info()))
      tf.logging.error('Error in environment process: {}'.format(stacktrace))
      conn.send((self._EXCEPTION, stacktrace))
    conn.close()","The process waits for actions and sends back environment results.

    Args:
      constructor: Constructor for the OpenAI Gym environment.
      conn: Connection for communication to the main process.

    Raises:
      KeyError: When receiving a message of unknown type.",1,4,2,7
"def _writeBlock(block, blockID):
    
    with open(""blockIDs.txt"", ""a"") as fp:
        fp.write(""blockID: "" + str(blockID) + ""\n"")
        sentences = """"
        for sentence in block:
            sentences += sentence+"",""
        fp.write(""block sentences: ""+sentences[:-1]+""\n"")
        fp.write(""\n"")",writes the block to a file with the id,1,0,0,1
"def _write_entries(self, stream, entries, converter, properties=None):
        
        def iter_entries():
            for c in entries:
                entry = converter(c)
                if entry is None:
                    continue
                if properties is not None:
                    entry = OrderedDict(
                        (key, value) for key, value in iteritems(entry)
                        if key == 'id' or key in properties)
                yield entry

        self._dump(stream, list(iter_entries()))","Write iterable of entries as YAML object to stream.

        Args:
            stream: File-like object.
            entries: Iterable of entries.
            converter: Conversion function from entry to YAML object.
            properties: Set of compartment properties to output (or None to
                output all).",0,1,1,2
"def _write_frame(self, data):
        
        assert data is not None and 0 < len(data) < 255, 'Data must be array of 1 to 255 bytes.'
        
        
        
        
        
        
        
        
        
        length = len(data)
        frame = bytearray(length+8)
        frame[0] = PN532_SPI_DATAWRITE
        frame[1] = PN532_PREAMBLE
        frame[2] = PN532_STARTCODE1
        frame[3] = PN532_STARTCODE2
        frame[4] = length & 0xFF
        frame[5] = self._uint8_add(~length, 1)
        frame[6:-2] = data
        checksum = reduce(self._uint8_add, data, 0xFF)
        frame[-2] = ~checksum & 0xFF
        frame[-1] = PN532_POSTAMBLE
        
        logger.debug('Write frame: 0x{0}'.format(binascii.hexlify(frame)))
        self._gpio.set_low(self._cs)
        self._busy_wait_ms(2)
        self._spi.write(frame)
        self._gpio.set_high(self._cs)",Write a frame to the PN532 with the specified data bytearray.,0,1,2,3
"def _writedoc(doc, thing, forceload=0):
    
    try:
        object, name = pydoc.resolve(thing, forceload)
        page = pydoc.html.page(pydoc.describe(object), pydoc.html.document(object, name))
        fname = os.path.join(doc, name + '.html')
        file = open(fname, 'w')
        file.write(page)
        file.close()
    except (ImportError, pydoc.ErrorDuringImport), value:
        traceback.print_exc(sys.stderr)
    else:
        return name + '.html'",Write HTML documentation to a file in the current directory.,1,0,0,1
"def _writeloop(self, consumer):
        
        while True:
            bytes = self._inputFile.read(self._readSize)
            if not bytes:
                self._inputFile.close()
                break
            consumer.write(bytes)
            yield None","Return an iterator which reads one chunk of bytes from the input file
        and writes them to the consumer for each time it is iterated.",2,0,0,2
"def _zip_files(self):
        
        
        zip = zipfile.ZipFile(self.name + '.zip', 'w', zipfile.ZIP_DEFLATED)

        
        zip.write(MANIFEST_FILE_NAME)

        
        for file in self.support_files:
            zip.write(file)

        
        zip.close()","Adds the manifest and all support files to the zip file
        :return:",2,0,0,2
"def abp_n4(image, intensity_truncation=(0.025,0.975,256), mask=None, usen3=False):
    
    if (not isinstance(intensity_truncation, (list,tuple))) or (len(intensity_truncation) != 3):
        raise ValueError('intensity_truncation must be list/tuple with 3 values')
    outimage = iMath(image, 'TruncateIntensity',
            intensity_truncation[0], intensity_truncation[1], intensity_truncation[2])
    if usen3 == True:
        outimage = n3_bias_field_correction(outimage, 4)
        outimage = n3_bias_field_correction(outimage, 2)
        return outimage
    else:
        outimage = n4_bias_field_correction(outimage, mask)
        return outimage","Truncate outlier intensities and bias correct with the N4 algorithm.

    ANTsR function: `abpN4`

    Arguments
    ---------
    image : ANTsImage
        image to correct and truncate

    intensity_truncation : 3-tuple
        quantiles for intensity truncation

    mask : ANTsImage (optional)
        mask for bias correction

    usen3 : boolean
        if True, use N3 bias correction instead of N4

    Returns
    -------
    ANTsImage

    Example
    -------
    >>> import ants
    >>> image = ants.image_read(ants.get_ants_data('r16'))
    >>> image2 = ants.abp_n4(image)",1,0,2,3
"def accept(self):
        
        with self._registered('re'):
            while 1:
                try:
                    client, addr = self._sock.accept()
                except socket.error, exc:
                    if not self._blocking or exc[0] not in _BLOCKING_OP:
                        raise
                    sys.exc_clear()
                    if self._readable.wait(self.gettimeout()):
                        raise socket.timeout(""timed out"")
                    if scheduler.state.interrupted:
                        raise IOError(errno.EINTR,
                                ""interrupted system call"")
                    continue
                return type(self)(fromsock=client), addr","accept a connection on the host/port to which the socket is bound

        .. note::

            if there is no connection attempt already queued, this method will
            block until a connection is made

        :returns:
            a two-tuple of ``(socket, address)`` where the socket is connected,
            and the address is the ``(ip_address, port)`` of the remote end",2,1,2,5
"def accept(self, *args):
        

        token = self.peek()
        if token is None:
            return None

        for arg in args:
            if token.type == arg:
                self.position += 1
                return token

        return None","Consume and return the next token if it has the correct type

        Multiple token types (as strings, e.g. 'integer64') can be given
        as arguments. If the next token is one of them, consume and return it.

        If the token type doesn't match, return None.",0,0,3,3
"def accept(self, context):
        
        _next = 0
        for _s in context:
            _data = [self.data[j] for j in self.trn[_next]]
            if _s in _data:
                _next = self.trn[_next][_data.index(_s)]
            else:
                return 0, _next
        return 1, _next","Check if the context could be accepted by the oracle
        
        Args:
            context: s sequence same type as the oracle data
        
        Returns:
            bAccepted: whether the sequence is accepted or not
            _next: the state where the sequence is accepted",0,0,1,1
"def accuracy(self, outputs):
        
        output = outputs[self.output_name]
        predict = TT.argmax(output, axis=-1)
        correct = TT.eq(predict, self._target)
        acc = correct.mean()
        if self._weights is not None:
            acc = (self._weights * correct).sum() / self._weights.sum()
        return acc","Build a Theano expression for computing the accuracy of graph output.

        Parameters
        ----------
        outputs : dict of Theano expressions
            A dictionary mapping network output names to Theano expressions
            representing the outputs of a computation graph.

        Returns
        -------
        acc : Theano expression
            A Theano expression representing the accuracy of the output compared
            to the target data.",0,0,1,1
"def accuracy(self, test_set, format=None):
        
        if isinstance(test_set, basestring):  
            test_data = self._read_data(test_set)
        else:  
            test_data = test_set
        test_features = [(self.extract_features(d), c) for d, c in test_data]
        return nltk.classify.accuracy(self.classifier, test_features)","Compute the accuracy on a test set.

        :param test_set: A list of tuples of the form ``(text, label)``, or a
            filename.
        :param format: If ``test_set`` is a filename, the file format, e.g.
            ``""csv""`` or ``""json""``. If ``None``, will attempt to detect the
            file format.",0,0,1,1
"def acquire_partition(self, partition, source_broker):
        
        broker_dest = self._elect_dest_broker(partition)
        if not broker_dest:
            raise NotEligibleGroupError(
                ""No eligible brokers to accept partition {p}"".format(p=partition),
            )
        source_broker.move_partition(partition, broker_dest)","Move a partition from a broker to any of the eligible brokers
        of the replication group.

        :param partition: Partition to move
        :param source_broker: Broker the partition currently belongs to",1,1,1,3
"def activate_bounce(bounce_id, api_key=None, secure=None, test=None,
                    **request_args):
    
    return _default_bounce_activate.activate(bounce_id, api_key=api_key,
                                             secure=secure, test=test,
                                             **request_args)","Activate a deactivated bounce.

    :param bounce_id: The bounce's id. Get the id with :func:`get_bounces`.
    :param api_key: Your Postmark API key. Required, if `test` is not `True`.
    :param secure: Use the https scheme for the Postmark API.
        Defaults to `True`
    :param test: Use the Postmark Test API. Defaults to `False`.
    :param \*\*request_args: Keyword arguments to pass to
        :func:`requests.request`.
    :rtype: :class:`BounceActivateResponse`",0,1,0,1
"def add(self, path, default_handler = None, **http_methods):
        
        if default_handler:
            methods = defaultdict(lambda: default_handler, http_methods.copy())
        else:
            methods = http_methods.copy()
        self.mappings.append((re.compile(path.decode('utf8')), methods, (path.find(r'\?')>-1) ))","Add a selector mapping.

        add(path, default_handler, **named_handlers)

        Adding order is important. Firt added = first matched.
        If you want to hand special case URI handled by one app and shorter
        version of the same regex string by anoter app,
        .add() special case first.

        Inputs:
         path - A regex string. We will compile it.
          Highly recommend using grouping of type: ""(?P<groupname>.+)""
          These will be exposed to WSGI app through environment key
          per http://www.wsgi.org/wsgi/Specifications/routing_args

         default_handler - (optional) A pointer to the function / iterable
          class instance that will handle ALL HTTP methods (verbs)

         **named_handlers - (optional) An unlimited list of named args or
          an unpacked dict of handlers allocated to handle specific HTTP
          methods (HTTP verbs). See ""Examples"" below.

        Matched named method handlers override default handler.

        If neither default_handler nor named_handlers point to any methods,
        ""Method not implemented"" is returned for the requests on this URI.

        Examples:
        selectorInstance.add('^(?P<working_path>.*)$',generic_handler,
                              POST=post_handler, HEAD=head_handler)

        custom_assembled_dict = {'GET':wsgi_app_a,'POST':wsgi_app_b}:
        ## note the unpacking - ""**"" - of the dict in this case.
        selectorInstance.add('^(?P<working_path>.*)$', **custom_assembled_dict)


        If the string contains '\?' (escaped ?, which translates to '?' in
        non-regex strings) we understand that as ""do regex matching on
        QUERY_PATH + '?' + QUERY_STRING""

        When lookup matches are met, results are injected into
        environ['wsgiorg.routing_args'] per
        http://www.wsgi.org/wsgi/Specifications/routing_args",0,0,3,3
"def add_arguments(self):
        
        if self.parser is None:
            raise TypeError(""Parser cannot be None, has create_parser been called?"")

        for keys, kwargs in self.args.items():
            if not isinstance(keys, tuple):
                keys = (keys,)
            self.parser.add_argument(*keys, **kwargs)",Definition and addition of all arguments.,1,0,2,3
"def add_attribute_label(self, attribute_id, label):
        
        if not self.can_update():
            self._tcex.handle_error(910, [self.type])

        return self.tc_requests.add_attribute_label(
            self.api_type, self.api_sub_type, self.unique_id, attribute_id, label, owner=self.owner
        )","Adds a security labels to a attribute

        Args:
            attribute_id:
            label:

        Returns: A response json",0,0,1,1
"def add_batch_parser(subparsers, parent_parser):
    
    parser = subparsers.add_parser(
        'batch',
        help='Displays information about batches and submit new batches',
        description='Provides subcommands to display Batch information and '
        'submit Batches to the validator via the REST API.')

    grand_parsers = parser.add_subparsers(title='subcommands',
                                          dest='subcommand')
    grand_parsers.required = True
    add_batch_list_parser(grand_parsers, parent_parser)
    add_batch_show_parser(grand_parsers, parent_parser)
    add_batch_status_parser(grand_parsers, parent_parser)
    add_batch_submit_parser(grand_parsers, parent_parser)","Adds arguments parsers for the batch list, batch show and batch status
    commands

        Args:
            subparsers: Add parsers to this subparser object
            parent_parser: The parent argparse.ArgumentParser object",0,0,3,3
"def add_columns(self, C_k_new, columns_new):
        r
        added = []
        for (i, c) in enumerate(columns_new):
            if self.add_column(C_k_new[:, i], c, update_error=False):
                added.append(c)
        
        self._compute_error()
        
        return np.array(added)","r"""""" Attempts to adds a set of new columns of :math:`A` to the Nystroem approximation and updates the local matrices

        Parameters
        ----------
        C_k_new : ndarray((N,k), dtype=float)
            :math:`k` new columns of :math:`A`
        columns_new : int
            indices of new columns within :math:`A`, in the same order as the C_k_new columns

        Return
        ------
        cols_added : ndarray of int
            Columns that were added successfully. Columns are only added when their Schur complement exceeds 0,
            which is normally true for columns that were not yet added, but the Schur complement may become 0 even
            for new columns as a result of numerical cancellation errors.",0,0,3,3
"def add_context_menu_items(self, items, replace_items=False):
        
        for label, action in items:
            assert isinstance(label, basestring)
            assert isinstance(action, basestring)
        if replace_items:
            self._context_menu_items = []
        self._context_menu_items.extend(items)
        self._listitem.addContextMenuItems(items, replace_items)","Adds context menu items. If replace_items is True all
        previous context menu items will be removed.",1,0,3,4
"def add_cookie_header(self, request, referrer_host=None):
        
        new_request = convert_http_request(request, referrer_host)
        self._cookie_jar.add_cookie_header(new_request)

        request.fields.clear()

        for name, value in new_request.header_items():
            request.fields.add(name, value)","Wrapped ``add_cookie_header``.

        Args:
            request: An instance of :class:`.http.request.Request`.
            referrer_host (str): An hostname or IP address of the referrer
                URL.",0,0,1,1
"def add_devs_custom_views(custom_view_name, dev_list, auth, url):
    
    view_id = get_custom_views(auth, url, name=custom_view_name)[0]['symbolId']
    add_devs_custom_views_url = '/imcrs/plat/res/view/custom/'+str(view_id)
    payload = + json.dumps(dev_list) + 
    f_url = url + add_devs_custom_views_url
    r = requests.put(f_url, data = payload, auth=auth, headers=HEADERS)  
    try:
        if r.status_code == 204:
            print ('View ' + custom_view_name +' : Devices Successfully Added')
            return r.status_code
    except requests.exceptions.RequestException as e:
            return ""Error:\n"" + str(e) + ' get_custom_views: An Error has occured'","function takes a list of devIDs from devices discovered in the HPE IMC platform and and issues a RESTFUL call to
     add the list of devices to a specific custom views from HPE IMC.

    :param dev_list: list containing the devID of all devices to be contained in this custom view.

    :param auth: requests auth object #usually auth.creds from auth pyhpeimc.auth.class

    :param url: base url of IMC RS interface #usually auth.url from pyhpeimc.auth.authclass

    :return: str of creation results ( ""view "" + name + ""created successfully""

    :rtype: str

    >>> from pyhpeimc.auth import *

    >>> from pyhpeimc.plat.groups import *

    >>> auth = IMCAuth(""http://"", ""10.101.0.203"", ""8080"", ""admin"", ""admin"")",2,1,0,3
"def add_edge(self, u, v, **attr):
        
        if u not in self.vertices:
            self.vertices[u] = []
            self.pred[u] = []
            self.succ[u] = []
        if v not in self.vertices:
            self.vertices[v] = []
            self.pred[v] = []
            self.succ[v] = []
        vertex = (u, v)
        self.edges[vertex] = {}
        self.edges[vertex].update(attr)
        self.vertices[u].append(v)
        self.pred[v].append(u)
        self.succ[u].append(v)",Add an edge from u to v and update edge attributes,0,0,1,1
"def add_embedded_campaign(self, id, collection, campaign, confidence,
                              analyst, date, description):
        
        if type(id) is not ObjectId:
            id = ObjectId(id)
        
        
        obj = getattr(self.db, collection)
        result = obj.find({'_id': id, 'campaign.name': campaign})
        if result.count() > 0:
            return
        else:
            log.debug('Adding campaign to set: {}'.format(campaign))
            campaign_obj = {
                'analyst': analyst,
                'confidence': confidence,
                'date': date,
                'description': description,
                'name': campaign
            }
            result = obj.update(
                {'_id': id},
                {'$push': {'campaign': campaign_obj}}
            )
            return result","Adds an embedded campaign to the TLO.

        Args:
            id: the CRITs object id of the TLO
            collection: The db collection. See main class documentation.
            campaign: The campaign to assign.
            confidence: The campaign confidence
            analyst: The analyst making the assignment
            date: The date of the assignment
            description: A description
        Returns:
            The resulting mongo object",2,1,1,4
"def add_exception_handler(self, exception_handler):
        
        
        if exception_handler is None:
            raise RuntimeConfigException(
                ""Valid Exception Handler instance to be provided"")

        if not isinstance(exception_handler, AbstractExceptionHandler):
            raise RuntimeConfigException(
                ""Input should be an ExceptionHandler instance"")

        self.exception_handlers.append(exception_handler)","Register input to the exception handlers list.

        :param exception_handler: Exception Handler instance to be
            registered.
        :type exception_handler: AbstractExceptionHandler
        :return: None",2,0,3,5
"def add_file(self, fileGrp, mimetype=None, url=None, ID=None, pageId=None, force=False, local_filename=None, **kwargs):
        
        if not ID:
            raise Exception(""Must set ID of the mets:file"")
        el_fileGrp = self._tree.getroot().find("".//mets:fileGrp[@USE='%s']"" % (fileGrp), NS)
        if el_fileGrp is None:
            el_fileGrp = self.add_file_group(fileGrp)
        if ID is not None and self.find_files(ID=ID) != []:
            if not force:
                raise Exception(""File with ID='%s' already exists"" % ID)
            mets_file = self.find_files(ID=ID)[0]
        else:
            mets_file = OcrdFile(ET.SubElement(el_fileGrp, TAG_METS_FILE), mets=self)
        mets_file.url = url
        mets_file.mimetype = mimetype
        mets_file.ID = ID
        mets_file.pageId = pageId
        mets_file.local_filename = local_filename

        self._file_by_id[ID] = mets_file

        return mets_file","Add a `OcrdFile </../../ocrd_models/ocrd_models.ocrd_file.html>`_.

        Arguments:
            fileGrp (string): Add file to ``mets:fileGrp`` with this ``USE`` attribute
            mimetype (string):
            url (string):
            ID (string):
            pageId (string):
            force (boolean): Whether to add the file even if a ``mets:file`` with the same ``ID`` already exists.
            local_filename (string):
            mimetype (string):",3,0,2,5
"def add_fraction_correct_values_to_dataframe(dataframe, x_series, y_series, new_label, x_cutoff = 1.0, y_cutoff = 1.0, ignore_null_values = False):
    
    new_series_values = fraction_correct_values(dataframe.index.values.tolist(), dataframe[x_series].values.tolist(), dataframe[y_series].values.tolist(), x_cutoff = x_cutoff, y_cutoff = y_cutoff, ignore_null_values = ignore_null_values)
    if new_label in dataframe.columns.values:
        del dataframe[new_label]
    dataframe.insert(len(dataframe.columns), new_label, new_series_values)",Adds a new column (new_label) to the dataframe with the fraction correct computed over X and Y values.,0,0,1,1
"def add_gene_family_to_graph(self, family_id):
        
        family = Family(self.graph)
        gene_family = self.globaltt['gene_family']

        
        
        self.model.addIndividualToGraph(family_id, None, gene_family)

        
        family.addMember(family_id, self.sub)
        family.addMember(family_id, self.obj)

        return","Make an association between a group of genes and some grouping class.
        We make the assumption that the genes in the association
        are part of the supplied family_id, and that the genes have
        already been declared as classes elsewhere.
        The family_id is added as an individual of type DATA:gene_family.

        Triples:
        <family_id> a EDAM-DATA:gene_family
        <family_id> RO:has_member <gene1>
        <family_id> RO:has_member <gene2>

        :param family_id:
        :param g: the graph to modify
        :return:",0,0,1,1
"def add_generic_error_message_with_code(request, error_code):
    
    messages.error(
        request,
        _(
            '{strong_start}Something happened.{strong_end} '
            '{span_start}Please reach out to your learning administrator with '
            'the following error code and they will be able to help you out.{span_end}'
            '{span_start}Error code: {error_code}{span_end}'
        ).format(
            error_code=error_code,
            strong_start='<strong>',
            strong_end='</strong>',
            span_start='<span>',
            span_end='</span>',
        )
    )","Add message to request indicating that there was an issue processing request.

    Arguments:
        request: The current request.
        error_code: A string error code to be used to point devs to the spot in
                    the code where this error occurred.",0,0,1,1
"def add_injectable(
        name, value, autocall=True, cache=False, cache_scope=_CS_FOREVER,
        memoize=False):
    
    if isinstance(value, Callable):
        if autocall:
            value = _InjectableFuncWrapper(
                name, value, cache=cache, cache_scope=cache_scope)
            
            value.clear_cached()
        elif not autocall and memoize:
            value = _memoize_function(value, name, cache_scope=cache_scope)

    logger.debug('registering injectable {!r}'.format(name))
    _INJECTABLES[name] = value","Add a value that will be injected into other functions.

    Parameters
    ----------
    name : str
    value
        If a callable and `autocall` is True then the function's
        argument names and keyword argument values will be matched
        to registered variables when the function needs to be
        evaluated by Orca. The return value will
        be passed to any functions using this injectable. In all other
        cases, `value` will be passed through untouched.
    autocall : bool, optional
        Set to True to have injectable functions automatically called
        (with argument matching) and the result injected instead of
        the function itself.
    cache : bool, optional
        Whether to cache the return value of an injectable function.
        Only applies when `value` is a callable and `autocall` is True.
    cache_scope : {'step', 'iteration', 'forever'}, optional
        Scope for which to cache data. Default is to cache forever
        (or until manually cleared). 'iteration' caches data for each
        complete iteration of the pipeline, 'step' caches data for
        a single step of the pipeline.
    memoize : bool, optional
        If autocall is False it is still possible to cache function results
        by setting this flag to True. Cached values are stored in a dictionary
        keyed by argument values, so the argument values must be hashable.
        Memoized functions have their caches cleared according to the same
        rules as universal caching.",0,1,4,5
"def add_logger(name, level=None, format=None):
    
    format = format or '%(filename)-11s %(lineno)-3d: %(message)s'
    log = logging.getLogger(name)

    
    log.setLevel(level or logging.INFO)

    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter(format))
    log.addHandler(ch)

    return log","Set up a stdout logger.

    Args:
        name (str): name of the logger
        level: defaults to logging.INFO
        format (str): format string for logging output.
                      defaults to ``%(filename)-11s %(lineno)-3d: %(message)s``.

    Returns:
        The logger object.",0,1,1,2
"def add_message(request, level, message, extra_tags='', fail_silently=False, *args, **kwargs):
    
    if hasattr(request, '_messages'):
        return request._messages.add(level, message, extra_tags, *args, **kwargs)
    if not fail_silently:
        raise MessageFailure('You cannot add messages without installing '
                             'django.contrib.messages.middleware.MessageMiddleware')",Attempts to add a message to the request using the 'messages' app.,1,0,2,3
"def add_mongo_config(app, simple_connection_string,
                     mongo_uri, collection_name):
    
    if mongo_uri != (None, None):
        add_mongo_config_with_uri(app, mongo_uri[0], mongo_uri[1],
                                  collection_name)
        if simple_connection_string is not None:
            print(""Ignoring the -m option. Overridden by ""
                  ""a more specific option (-mu)."", file=sys.stderr)
    else:
        
        if simple_connection_string is None:
            simple_connection_string = ""sacred""
        add_mongo_config_simple(app, simple_connection_string, collection_name)","Configure the application to use MongoDB.

    :param app: Flask application
    :param simple_connection_string:
                Expects host:port:database_name or database_name
                Mutally_exclusive with mongo_uri
    :param mongo_uri: Expects mongodb://... as defined
                in https://docs.mongodb.com/manual/reference/connection-string/
                Mutually exclusive with simple_connection_string (must be None)
    :param collection_name: The collection containing Sacred's runs
    :return:",0,0,1,1
"def add_network_ipv6(
            self,
            id_vlan,
            id_tipo_rede,
            id_ambiente_vip=None,
            prefix=None):
        
        vlan_map = dict()
        vlan_map['id_vlan'] = id_vlan
        vlan_map['id_tipo_rede'] = id_tipo_rede
        vlan_map['id_ambiente_vip'] = id_ambiente_vip
        vlan_map['prefix'] = prefix

        code, xml = self.submit(
            {'vlan': vlan_map}, 'POST', 'network/ipv6/add/')

        return self.response(code, xml)","Add new networkipv6

        :param id_vlan: Identifier of the Vlan. Integer value and greater than zero.
        :param id_tipo_rede: Identifier of the NetworkType. Integer value and greater than zero.
        :param id_ambiente_vip: Identifier of the Environment Vip. Integer value and greater than zero.
        :param prefix: Prefix.

        :return: Following dictionary:

        ::

          {'vlan': {'id': < id_vlan >,
          'nome': < nome_vlan >,
          'num_vlan': < num_vlan >,
          'id_tipo_rede': < id_tipo_rede >,
          'id_ambiente': < id_ambiente >,
          'rede_oct1': < rede_oct1 >,
          'rede_oct2': < rede_oct2 >,
          'rede_oct3': < rede_oct3 >,
          'rede_oct4': < rede_oct4 >,
          'rede_oct5': < rede_oct4 >,
          'rede_oct6': < rede_oct4 >,
          'rede_oct7': < rede_oct4 >,
          'rede_oct8': < rede_oct4 >,
          'bloco': < bloco >,
          'mascara_oct1': < mascara_oct1 >,
          'mascara_oct2': < mascara_oct2 >,
          'mascara_oct3': < mascara_oct3 >,
          'mascara_oct4': < mascara_oct4 >,
          'mascara_oct5': < mascara_oct4 >,
          'mascara_oct6': < mascara_oct4 >,
          'mascara_oct7': < mascara_oct4 >,
          'mascara_oct8': < mascara_oct4 >,
          'broadcast': < broadcast >,
          'descricao': < descricao >,
          'acl_file_name': < acl_file_name >,
          'acl_valida': < acl_valida >,
          'ativada': < ativada >}}

        :raise TipoRedeNaoExisteError: NetworkType not found.
        :raise InvalidParameterError: Invalid ID for Vlan or NetworkType.
        :raise EnvironmentVipNotFoundError: Environment VIP not registered.
        :raise IPNaoDisponivelError: Network address unavailable to create a NetworkIPv6.
        :raise ConfigEnvironmentInvalidError: Invalid Environment Configuration or not registered
        :raise DataBaseError: Networkapi failed to access the database.
        :raise XMLError: Networkapi failed to generate the XML response.",0,1,0,1
"def add_occurrences(self, start_time, end_time, **rrule_params):
        
        count = rrule_params.get('count')
        until = rrule_params.get('until')
        if not (count or until):
            self.occurrence_set.create(start_time=start_time, end_time=end_time)
        else:
            rrule_params.setdefault('freq', rrule.DAILY)
            delta = end_time - start_time
            occurrences = []
            for ev in rrule.rrule(dtstart=start_time, **rrule_params):
                occurrences.append(Occurrence(start_time=ev, end_time=ev + delta, event=self))
            self.occurrence_set.bulk_create(occurrences)","Add one or more occurences to the event using a comparable API to
        ``dateutil.rrule``.

        If ``rrule_params`` does not contain a ``freq``, one will be defaulted
        to ``rrule.DAILY``.

        Because ``rrule.rrule`` returns an iterator that can essentially be
        unbounded, we need to slightly alter the expected behavior here in order
        to enforce a finite number of occurrence creation.

        If both ``count`` and ``until`` entries are missing from ``rrule_params``,
        only a single ``Occurrence`` instance will be created using the exact
        ``start_time`` and ``end_time`` values.",2,0,1,3
"def add_parameters(self, traj):
        
        self._logger.info('Adding Parameters of Components')

        for component in self.components:
            component.add_parameters(traj)

        if self.analysers:
            self._logger.info('Adding Parameters of Analysers')

            for analyser in self.analysers:
                analyser.add_parameters(traj)

        self._logger.info('Adding Parameters of Runner')

        self.network_runner.add_parameters(traj)","Adds parameters for a network simulation.

        Calls :func:`~pypet.brian2.network.NetworkComponent.add_parameters` for all components,
        analyser, and the network runner (in this order).

        :param traj:  Trajectory container",0,4,1,5
"def add_property_to_response(self, code='200', prop_name='data', **kwargs):
        
        self['responses'] \
            .setdefault(str(code), self._new_operation()) \
            .setdefault('schema', {'type': 'object'}) \
            .setdefault('properties', {}) \
            .setdefault(prop_name, {}) \
            .update(**kwargs)","Add a property (http://json-schema.org/latest/json-schema-validation.html#anchor64)  # noqa: E501
        to the schema of the response identified by the code",0,0,1,1
"def add_query_parameters_to_url(url, query_parameters):
    
    
    url_parts = urllib.parse.urlparse(url)

    
    qs_args = urllib.parse.parse_qs(url_parts[4])
    qs_args.update(query_parameters)

    
    sorted_qs_args = OrderedDict()
    for k in sorted(qs_args.keys()):
        sorted_qs_args[k] = qs_args[k]

    
    new_qs = urllib.parse.urlencode(sorted_qs_args, True)
    return urllib.parse.urlunparse(list(url_parts[0:4]) + [new_qs] + list(url_parts[5:]))","Merge a dictionary of query parameters into the given URL.
    Ensures all parameters are sorted in dictionary order when returning the URL.",0,0,2,2
"def add_remote(self, path, name, remote_url, use_sudo=False, user=None, fetch=True):
        
        if path is None:
            raise ValueError(""Path to the working copy is needed to add a remote"")

        if fetch:
            cmd = 'git remote add -f %s %s' % (name, remote_url)
        else:
            cmd = 'git remote add %s %s' % (name, remote_url)

        with cd(path):
            if use_sudo and user is None:
                run_as_root(cmd)
            elif use_sudo:
                sudo(cmd, user=user)
            else:
                run(cmd)","Add a remote Git repository into a directory.

        :param path: Path of the working copy directory.  This directory must exist
                     and be a Git working copy with a default remote to fetch from.
        :type path: str

        :param use_sudo: If ``True`` execute ``git`` with
                         :func:`fabric.operations.sudo`, else with
                         :func:`fabric.operations.run`.
        :type use_sudo: bool

        :param user: If ``use_sudo is True``, run :func:`fabric.operations.sudo`
                     with the given user.  If ``use_sudo is False`` this parameter
                     has no effect.
        :type user: str

        :param name: name for the remote repository
        :type name: str

        :param remote_url: URL of the remote repository
        :type remote_url: str

        :param fetch: If ``True`` execute ``git remote add -f``
        :type fetch: bool",1,3,3,7
"def add_resource_subscription_async(self, device_id, resource_path, callback_fn,
                                        fix_path=True, queue_size=5):
        
        queue = self.add_resource_subscription(device_id, resource_path, fix_path, queue_size)

        
        t = threading.Thread(target=self._subscription_handler,
                             args=[queue, device_id, resource_path, callback_fn])
        t.daemon = True
        t.start()","Subscribe to resource updates with callback function.

        When called on a valid device and resource path a subscription is setup so that
        any update on the resource path value triggers an update on the callback function.

        :param device_id: Name of device to set the subscription on (Required)
        :param resource_path: The resource path on device to observe (Required)
        :param callback_fn: Callback function to be executed on update to subscribed resource
        :param fix_path: Removes leading / on resource_path if found
        :param queue_size: Sets the Queue size. If set to 0, no queue object will be created
        :returns: void",0,1,0,1
"def add_roll_pitch_yaw(self):
        

        self._add_roll_pitch_yaw_to_message('vehicle_attitude')
        self._add_roll_pitch_yaw_to_message('vehicle_vision_attitude')
        self._add_roll_pitch_yaw_to_message('vehicle_attitude_groundtruth')
        self._add_roll_pitch_yaw_to_message('vehicle_attitude_setpoint', '_d')","convenience method to add the fields 'roll', 'pitch', 'yaw' to the
        loaded data using the quaternion fields (does not update field_data).

        Messages are: 'vehicle_attitude.q' and 'vehicle_attitude_setpoint.q_d',
        'vehicle_attitude_groundtruth.q' and 'vehicle_vision_attitude.q'",0,0,3,3
"def add_stream_alias(self, localStreamName, aliasName, **kwargs):
        
        return self.protocol.execute('addStreamAlias',
                                     localStreamName=localStreamName,
                                     aliasName=aliasName, **kwargs)","Allows you to create secondary name(s) for internal streams. Once an
        alias is created the localstreamname cannot be used to request
        playback of that stream. Once an alias is used (requested by a client)
        the alias is removed. Aliases are designed to be used to protect/hide
        your source streams.

        :param localStreamName: The original stream name
        :type localStreamName: str

        :param aliasName: The alias alternative to the localStreamName
        :type aliasName: str

        :param expirePeriod: The expiration period for this alias. Negative
            values will be treated as one-shot but no longer than the absolute
            positive value in seconds, 0 means it will not expire, positive
            values mean the alias can be used multiple times but expires after
            this many seconds. The default is -600 (one-shot, 10 mins)
        :type expirePeriod: int

        :link: http://docs.evostream.com/ems_api_definition/addstreamalias",0,1,0,1
"def add_subtract_locally(st, region_depth=3, filter_size=5, sigma_cutoff=8,
                         **kwargs):
    
    
    tiles = identify_misfeatured_regions(
        st, filter_size=filter_size, sigma_cutoff=sigma_cutoff)
    
    n_empty = 0
    n_added = 0
    new_poses = []
    for t in tiles:
        curn, curinds = add_subtract_misfeatured_tile(st, t, **kwargs)
        if curn == 0:
            n_empty += 1
        else:
            n_added += curn
            new_poses.extend(st.obj_get_positions()[curinds])
        if n_empty > region_depth:
            break  
    else:  
        pass
        
        
    return n_added, new_poses","Automatically adds and subtracts missing particles based on local
    regions of poor fit.

    Calls identify_misfeatured_regions to identify regions, then
    add_subtract_misfeatured_tile on the tiles in order of size until
    region_depth tiles have been checked without adding any particles.

    Parameters
    ----------
    st: :class:`peri.states.State`
        The state to add and subtract particles to.
    region_depth : Int
        The minimum amount of regions to try; the algorithm terminates if
        region_depth regions have been tried without adding particles.

    Other Parameters
    ----------------
    filter_size : Int, optional
        The size of the filter for calculating the local standard deviation;
        should approximately be the size of a poorly featured region in each
        dimension. Best if odd. Default is 5.
    sigma_cutoff : Float, optional
        The max allowed deviation of the residuals from what is expected,
        in units of the residuals' standard deviation. Lower means more
        sensitive, higher = less sensitive. Default is 8.0, i.e. one pixel
        out of every ``7*10^11`` is mis-identified randomly. In practice the
        noise is not Gaussian so there are still some regions mis-
        identified as improperly featured.
    rad : Float or 'calc', optional
        The initial radius for added particles; added particles radii are
        not fit until the end of add_subtract. Default is ``'calc'``, which
        uses the median radii of active particles.
    max_iter : Int, optional
        The maximum number of loops for attempted adds at one tile location.
        Default is 3.
    invert : Bool, optional
        Whether to invert the image for feature_guess. Default is ``True``,
        i.e. dark particles on bright background.
    max_allowed_remove : Int, optional
        The maximum number of particles to remove. If the misfeatured tile
        contains more than this many particles, raises an error. If it
        contains more than half as many particles, throws a warning. If more
        than this many particles are added, they are optimized in blocks of
        ``max_allowed_remove``. Default is 20.
    im_change_frac : Float, between 0 and 1.
        If adding or removing a particle decreases the error less than
        ``im_change_frac *`` the change in the image, the particle is deleted.
        Default is 0.2.
    min_derr : Float
        The minimum change in the state's error to keep a particle in the
        image. Default is ``'3sig'`` which uses ``3*st.sigma``.
    do_opt : Bool, optional
        Set to False to avoid optimizing particle positions after adding
        them. Default is True
    minmass : Float, optional
        The minimum mass for a particle to be identified as a feature, as
        used by trackpy. Defaults to a decent guess.
    use_tp : Bool, optional
        Set to True to use trackpy to find missing particles inside the
        image. Not recommended since trackpy deliberately cuts out
        particles at the edge of the image. Default is False.
    max_allowed_remove : Int, optional
        The maximum number of particles to remove. If the misfeatured tile
        contains more than this many particles, raises an error. If it
        contains more than half as many particles, throws a warning. If more
        than this many particles are added, they are optimized in blocks of
        ``max_allowed_remove``. Default is 20.

    Returns
    -------
    n_added : Int
        The change in the number of particles; i.e the number added - number
        removed.
    new_poses : List
        [N,3] element list of the added particle positions.

    Notes
    -----
    Algorithm Description

    1. Identify mis-featured regions by how much the local residuals
       deviate from the global residuals, as measured by the standard
       deviation of both.
    2. Loop over each of those regions, and:

       a. Remove every particle in the current region.
       b. Try to add particles in the current region until no more
          can be added while adequately decreasing the error.
       c. Terminate if at least region_depth regions have been
          checked without successfully adding a particle.

    Because this algorithm is more judicious about chooosing regions to
    check, and more aggressive about removing particles in those regions,
    it runs faster and does a better job than the (global) add_subtract.
    However, this function usually does not work better as an initial add-
    subtract on an image, since (1) it doesn't check for removing small/big
    particles per se, and (2) when the poorly-featured regions of the image
    are large or when the fit is bad, it will remove essentially all of the
    particles, taking a long time. As a result, it's usually best to do a
    normal add_subtract first and using this function for tough missing or
    double-featured particles.",0,0,2,2
"def add_task(self, task_id, backend, category, backend_args,
                 archive_args=None, sched_args=None):
        
        try:
            archiving_cfg = self.__parse_archive_args(archive_args)
            scheduling_cfg = self.__parse_schedule_args(sched_args)
            self.__validate_args(task_id, backend, category, backend_args)
        except ValueError as e:
            raise e

        try:
            task = self._tasks.add(task_id, backend, category, backend_args,
                                   archiving_cfg=archiving_cfg,
                                   scheduling_cfg=scheduling_cfg)
        except AlreadyExistsError as e:
            raise e

        self._scheduler.schedule_task(task.task_id)

        return task","Add and schedule a task.

        :param task_id: id of the task
        :param backend: name of the backend
        :param category: category of the items to fecth
        :param backend_args: args needed to initialize the backend
        :param archive_args: args needed to initialize the archive
        :param sched_args: scheduling args for this task

        :returns: the task created",2,2,2,6
"def add_timing_signal_1d(x,
                         min_timescale=1.0,
                         max_timescale=1.0e4,
                         start_index=0):
  
  length = common_layers.shape_list(x)[1]
  channels = common_layers.shape_list(x)[2]
  signal = get_timing_signal_1d(length, channels, min_timescale, max_timescale,
                                start_index)
  return x + common_layers.cast_like(signal, x)","Adds a bunch of sinusoids of different frequencies to a Tensor.

  Each channel of the input Tensor is incremented by a sinusoid of a different
  frequency and phase.

  This allows attention to learn to use absolute and relative positions.
  Timing signals should be added to some precursors of both the query and the
  memory inputs to attention.

  The use of relative position is possible because sin(x+y) and cos(x+y) can be
  expressed in terms of y, sin(x) and cos(x).

  In particular, we use a geometric sequence of timescales starting with
  min_timescale and ending with max_timescale.  The number of different
  timescales is equal to channels / 2. For each timescale, we
  generate the two sinusoidal signals sin(timestep/timescale) and
  cos(timestep/timescale).  All of these sinusoids are concatenated in
  the channels dimension.

  Args:
    x: a Tensor with shape [batch, length, channels]
    min_timescale: a float
    max_timescale: a float
    start_index: index of first position

  Returns:
    a Tensor the same shape as x.",0,0,1,1
"def add_upsert(self, action, meta_action, doc_source, update_spec):
        

        
        
        
        
        if update_spec:
            self.bulk_index(action, meta_action)

            
            
            
            self.add_doc_to_update(action, update_spec, len(self.action_buffer) - 2)
        else:
            
            
            
            
            
            if doc_source:
                self.add_to_sources(action, doc_source)
            self.bulk_index(action, meta_action)","Function which stores sources for ""insert"" actions
        and decide if for ""update"" action has to add docs to
        get source buffer",1,0,1,2
"def add_user(
            self, user,
            first_name=None, last_name=None,
            email=None, password=None
        ):
        
        self.project_service.set_auth(self._token_project)
        self.project_service.add_user(
            user, first_name, last_name, email, password)","Add a new user.

        Args:
            user (string): User name.
            first_name (optional[string]): User's first name.  Defaults to None.
            last_name (optional[string]): User's last name.  Defaults to None.
            email: (optional[string]): User's email address.  Defaults to None.
            password: (optional[string]): User's password.  Defaults to None.

        Raises:
            requests.HTTPError on failure.",1,0,0,1
"def add_virtual_columns_aitoff(self, alpha, delta, x, y, radians=True):
        
        transform = """" if radians else ""*pi/180.""
        aitoff_alpha = ""__aitoff_alpha_%s_%s"" % (alpha, delta)
        
        aitoff_alpha = re.sub(""[^a-zA-Z_]"", ""_"", aitoff_alpha)

        self.add_virtual_column(aitoff_alpha, ""arccos(cos({delta}{transform})*cos({alpha}{transform}/2))"".format(**locals()))
        self.add_virtual_column(x, ""2*cos({delta}{transform})*sin({alpha}{transform}/2)/sinc({aitoff_alpha}/pi)/pi"".format(**locals()))
        self.add_virtual_column(y, ""sin({delta}{transform})/sinc({aitoff_alpha}/pi)/pi"".format(**locals()))","Add aitoff (https://en.wikipedia.org/wiki/Aitoff_projection) projection

        :param alpha: azimuth angle
        :param delta: polar angle
        :param x: output name for x coordinate
        :param y: output name for y coordinate
        :param radians: input and output in radians (True), or degrees (False)
        :return:",0,1,0,1
"def add_virtual_columns_polar_velocities_to_cartesian(self, x='x', y='y', azimuth=None, vr='vr_polar', vazimuth='vphi_polar', vx_out='vx', vy_out='vy', propagate_uncertainties=False):
        
        x = self._expr(x)
        y = self._expr(y)
        vr = self._expr(vr)
        vazimuth = self._expr(vazimuth)
        if azimuth is not None:
            azimuth = self._expr(azimuth)
            azimuth = np.deg2rad(azimuth)
        else:
            azimuth = np.arctan2(y, x)
        azimuth = self._expr(azimuth)
        self[vx_out] = vr * np.cos(azimuth) - vazimuth * np.sin(azimuth)
        self[vy_out] = vr * np.sin(azimuth) + vazimuth * np.cos(azimuth)
        if propagate_uncertainties:
            self.propagate_uncertainties([self[vx_out], self[vy_out]])","Convert cylindrical polar velocities to Cartesian.

        :param x:
        :param y:
        :param azimuth: Optional expression for the azimuth in degrees , may lead to a better performance when given.
        :param vr:
        :param vazimuth:
        :param vx_out:
        :param vy_out:
        :param propagate_uncertainties: {propagate_uncertainties}",0,0,1,1
"def add_vlan(self, vlan_id, full_name, port_mode, qnq, c_tag):
        

        try:
            action_result = self.add_vlan_flow.execute_flow(vlan_range=vlan_id,
                                                            port_mode=port_mode,
                                                            port_name=full_name,
                                                            qnq=qnq,
                                                            c_tag=c_tag)
            self.result[current_thread().name].append((True, action_result))
        except Exception as e:
            self._logger.error(traceback.format_exc())
            self.result[current_thread().name].append((False, e.message))","Run flow to add VLAN(s) to interface

        :param vlan_id: Already validated number of VLAN(s)
        :param full_name: Full interface name. Example: 2950/Chassis 0/FastEthernet0-23
        :param port_mode: port mode type. Should be trunk or access
        :param qnq:
        :param c_tag:",0,2,0,2
"def add_widget(self, widget, column=0):
        
        
        if self._frame is None:
            raise RuntimeError(""You must add the Layout to the Frame before you can add a Widget."")

        
        self._columns[column].append(widget)
        widget.register_frame(self._frame)

        if widget.name in self._frame.data:
            widget.value = self._frame.data[widget.name]","Add a widget to this Layout.

        If you are adding this Widget to the Layout dynamically after starting to play the Scene,
        don't forget to ensure that the value is explicitly set before the next update.

        :param widget: The widget to be added.
        :param column: The column within the widget for this widget.  Defaults to zero.",2,0,1,3
"def addinstance(self, testfile, features, classlabel=""?""):
        

        features = self.validatefeatures(features)

        if self.delimiter in classlabel:
            raise ValueError(""Class label contains delimiter: "" + self.delimiter)


        f = io.open(testfile,'a', encoding=self.encoding)
        f.write(self.delimiter.join(features) + self.delimiter + classlabel + ""\n"")
        f.close()",Adds an instance to a specific file. Especially suitable for generating test files,2,0,1,3
"def adjacent(self, other):
        

        if not self.is_valid_range(other):
            raise TypeError(
                ""Unsupported type to test for inclusion '{0.__class__.__name__}'"".format(
                    other))
        
        elif not self or not other:
            return False
        return (
            (self.lower == other.upper and self.lower_inc != other.upper_inc) or
            (self.upper == other.lower and self.upper_inc != other.lower_inc))","Returns True if ranges are directly next to each other but does not
        overlap.

            >>> intrange(1, 5).adjacent(intrange(5, 10))
            True
            >>> intrange(1, 5).adjacent(intrange(10, 15))
            False

        The empty set is not adjacent to any set.

        This is the same as the ``-|-`` operator for two ranges in PostgreSQL.

        :param other: Range to test against.
        :return: ``True`` if this range is adjacent with `other`, otherwise
                 ``False``.
        :raises TypeError: If given argument is of invalid type",1,0,2,3
"def adrest_errors_mail(response, request):
    

    if not response.status_code in ADREST_MAIL_ERRORS:
        return False

    subject = 'ADREST API Error (%s): %s' % (
        response.status_code, request.path)
    stack_trace = '\n'.join(traceback.format_exception(*sys.exc_info()))
    message =  % (stack_trace, repr(getattr(request, 'data', None)), repr(request))
    return mail_admins(subject, message, fail_silently=True)","Send a mail about ADRest errors.

    :return bool: status of operation",0,1,0,1
"def after_insert(mapper, connection, target):
    
    record_after_update.send(CmtRECORDCOMMENT, recid=target.id_bibrec)

    from .api import get_reply_order_cache_data
    if target.in_reply_to_id_cmtRECORDCOMMENT > 0:
        parent = CmtRECORDCOMMENT.query.get(
            target.in_reply_to_id_cmtRECORDCOMMENT)
        if parent:
            trans = connection.begin()
            parent_reply_order = parent.reply_order_cached_data \
                if parent.reply_order_cached_data else ''
            parent_reply_order += get_reply_order_cache_data(target.id)
            connection.execute(
                db.update(CmtRECORDCOMMENT.__table__).
                where(CmtRECORDCOMMENT.id == parent.id).
                values(reply_order_cached_data=parent_reply_order))
            trans.commit()",Update reply order cache  and send record-after-update signal.,1,1,0,2
"def agp(args):
    
    p = OptionParser(agp.__doc__)
    opts, args = p.parse_args(args)

    if len(args) != 1:
        sys.exit(not p.print_help())

    tablefile, = args
    fp = open(tablefile)
    for row in fp:
        atoms = row.split()
        hr = atoms[0]
        scaf = atoms[1]
        scaf_start = int(atoms[2]) + 1
        scaf_end = int(atoms[3])
        strand = atoms[4]
        hr_start = int(atoms[5]) + 1
        hr_end = int(atoms[6])

        print(""\t"".join(str(x) for x in \
                (hr, hr_start, hr_end, 1, 'W',
                 scaf, scaf_start, scaf_end, strand)))","%prog agp Siirt_Female_pistachio_23May2017_table.txt

    The table file, as prepared by Dovetail Genomics, is not immediately useful
    to convert gene model coordinates, as assumed by formats.chain.fromagp().
    This is a quick script to do such conversion. The file structure of this
    table file is described in the .manifest file shipped in the same package::

    pistachio_b_23May2017_MeyIy.table.txt
        Tab-delimited table describing positions of input assembly scaffolds
        in the Hirise scaffolds. The table has the following format:

            1. HiRise scaffold name
            2. Input sequence name
            3. Starting base (zero-based) of the input sequence
            4. Ending base of the input sequence
            5. Strand (- or +) of the input sequence in the scaffold
            6. Starting base (zero-based) in the HiRise scaffold
            7. Ending base in the HiRise scaffold

        where '-' in the strand column indicates that the sequence is reverse
        complemented relative to the input assembly.

    CAUTION: This is NOT a proper AGP format since it does not have gaps in
    them.",1,0,1,2
"def all(self, page=None, per_page=None, include_totals=False, extra_params=None):
        
        params = extra_params or {}
        params.update({
            'page': page,
            'per_page': per_page,
            'include_totals': str(include_totals).lower()
        })

        return self.client.get(self._url(), params=params)","Retrieves all grants.

        Args:
            page (int, optional): The result's page number (zero based).

            per_page (int, optional): The amount of entries per page.

            include_totals (bool, optional): True if the query summary is
                to be included in the result, False otherwise.

           extra_params (dictionary, optional): The extra parameters to add to
             the request. The page, per_page, and include_totals values
             specified as parameters take precedence over the ones defined here.
            
        See: https://auth0.com/docs/api/management/v2#!/Grants/get_grants",0,1,0,1
"def all(self, stage='login_success', enabled=True, fields=None,
            include_fields=True, page=None, per_page=None, include_totals=False):
        

        params = {
            'stage': stage,
            'fields': fields and ','.join(fields) or None,
            'include_fields': str(include_fields).lower(),
            'page': page,
            'per_page': per_page,
            'include_totals': str(include_totals).lower()
        }

        
        if enabled is not None:
            params['enabled'] = str(enabled).lower()

        return self.client.get(self._url(), params=params)","Retrieves a list of all rules.

        Args:
            stage (str, optional):  Retrieves rules that match the execution
                stage (defaults to login_success).

            enabled (bool, optional): If provided, retrieves rules that match
                the value, otherwise all rules are retrieved.

            fields (list, optional): A list of fields to include or exclude
                (depending on include_fields) from the result, empty to
                retrieve all fields.

            include_fields (bool, optional): True if the fields specified are
                to be included in the result, False otherwise
                (defaults to true).

            page (int, optional): The result's page number (zero based).

            per_page (int, optional): The amount of entries per page.

            include_totals (bool, optional): True if the query summary is
                to be included in the result, False otherwise.

        See: https://auth0.com/docs/api/management/v2#!/Rules/get_rules",0,1,1,2
"def allow_headers(self, domain, headers, secure=True):
        
        if self.site_control == SITE_CONTROL_NONE:
            raise TypeError(
                METAPOLICY_ERROR.format(""allow headers from a domain"")
            )
        self.header_domains[domain] = {'headers': headers,
                                       'secure': secure}","Allows ``domain`` to push data via the HTTP headers named in
        ``headers``.

        As with ``allow_domain``, ``domain`` may be either a full
        domain name or a wildcard. Again, use of wildcards is
        discouraged for security reasons.

        The value for ``headers`` should be a list of header names.

        To disable Flash's requirement of security matching (e.g.,
        retrieving a policy via HTTPS will require that SWFs also be
        retrieved via HTTPS), pass ``secure=False``. Due to security
        concerns, it is strongly recommended that you not disable
        this.",1,0,2,3
"def allowance(self, b58_owner_address: str, b58_spender_address: str):
        
        func = InvokeFunction('allowance')
        Oep4.__b58_address_check(b58_owner_address)
        owner = Address.b58decode(b58_owner_address).to_bytes()
        Oep4.__b58_address_check(b58_spender_address)
        spender = Address.b58decode(b58_spender_address).to_bytes()
        func.set_params_value(owner, spender)
        result = self.__sdk.get_network().send_neo_vm_transaction_pre_exec(self.__hex_contract_address, None, func)
        try:
            allowance = ContractDataParser.to_int(result['Result'])
        except SDKException:
            allowance = 0
        return allowance","This interface is used to call the Allowance method in ope4
        that query the amount of spender still allowed to withdraw from owner account.

        :param b58_owner_address: a base58 encode address that represent owner's account.
        :param b58_spender_address: a base58 encode address that represent spender's account.
        :return: the amount of oep4 token that owner allow spender to transfer from the owner account.",0,1,1,2
"def alltoall_pointtwise(xs, devices, split_axis, concat_axis):
  
  n = len(xs)
  if n == 1:
    return xs
  
  parts = mtf.transpose_list_of_lists(
      mtf.parallel(devices, tf.split, xs, [n] * n, axis=[split_axis] * n))
  return mtf.parallel(devices, tf.concat, parts, axis=[concat_axis] * n)","MPI alltoall operation.

  Implementation of alltoall using pointwise communication.

  Args:
    xs: a list of n tf.Tensors
    devices: a list of n strings
    split_axis: an integer
    concat_axis: an integer

  Returns:
    a list of n Tensors",0,0,1,1
"def ancestral_likelihood(self):
        
        log_lh = np.zeros(self.multiplicity.shape[0])
        for node in self.tree.find_clades(order='postorder'):

            if node.up is None: 
                
                profile = seq2prof(node.cseq, self.gtr.profile_map)
                
                profile *= self.gtr.Pi
                profile = profile.sum(axis=1)
                log_lh += np.log(profile) 
                continue

            t = node.branch_length

            indices = np.array([(np.argmax(self.gtr.alphabet==a),
                        np.argmax(self.gtr.alphabet==b)) for a, b in zip(node.up.cseq, node.cseq)])

            logQt = np.log(self.gtr.expQt(t))
            lh = logQt[indices[:, 1], indices[:, 0]]
            log_lh += lh

        return log_lh","Calculate the likelihood of the given realization of the sequences in
        the tree

        Returns
        -------

         log_lh : float
            The tree likelihood given the sequences",0,0,1,1
"def annotate_event(ev, key, ts=None, namespace=None, **kwargs):
    
    ann = {}
    if ts is None:
        ts = time.time()
    ann[""ts""] = ts
    ann[""key""] = key
    if namespace is None and ""HUMILIS_ENVIRONMENT"" in os.environ:
        namespace = ""{}:{}:{}"".format(
            os.environ.get(""HUMILIS_ENVIRONMENT""),
            os.environ.get(""HUMILIS_LAYER""),
            os.environ.get(""HUMILIS_STAGE""))

    if namespace is not None:
        ann[""namespace""] = namespace
    ann.update(kwargs)
    _humilis = ev.get(""_humilis"", {})
    if not _humilis:
        ev[""_humilis""] = {""annotation"": [ann]}
    else:
        ev[""_humilis""][""annotation""] = _humilis.get(""annotation"", [])
        
        delete_annotations(ev, key)
        ev[""_humilis""][""annotation""].append(ann)

    return ev",Add an annotation to an event.,0,0,1,1
"def any_date_field(field, **kwargs):
    
    if field.auto_now or field.auto_now_add:
        return None
    from_date = kwargs.get('from_date', date(1990, 1, 1))
    to_date = kwargs.get('to_date', date.today())
    return xunit.any_date(from_date=from_date, to_date=to_date)","Return random value for DateField,
    skips auto_now and auto_now_add fields

    >>> result = any_field(models.DateField())
    >>> type(result)
    <type 'datetime.date'>",0,0,2,2
"def api(self, action, args=None):
        
        if args is None:
            args = {}
        args['action'] = action.upper()

        try:
            res, con_info = self.solr.transport.send_request(endpoint='admin/collections', params=args)
        except Exception as e:
            self.logger.error(""Error querying SolrCloud Collections API. "")
            self.logger.exception(e)
            raise e

        if 'responseHeader' in res and res['responseHeader']['status'] == 0:
            return res, con_info
        else:
            raise SolrError(""Error Issuing Collections API Call for: {} +"".format(con_info, res))","Sends a request to Solr Collections API.
        Documentation is here: https://cwiki.apache.org/confluence/display/solr/Collections+API

        :param string action: Name of the collection for the action
        :param dict args: Dictionary of specific parameters for action",1,3,1,5
"def append(self, cpe):
        

        if cpe.VERSION != CPE2_3.VERSION:
            errmsg = ""CPE Name version {0} not valid, version 2.3 expected"".format(
                cpe.VERSION)
            raise ValueError(errmsg)

        for k in self.K:
            if cpe._str == k._str:
                return None

        if isinstance(cpe, CPE2_3_WFN):
            self.K.append(cpe)
        else:
            
            wfn = CPE2_3_WFN(cpe.as_wfn())
            self.K.append(wfn)","Adds a CPE element to the set if not already.
        Only WFN CPE Names are valid, so this function converts the input CPE
        object of version 2.3 to WFN style.

        :param CPE cpe: CPE Name to store in set
        :returns: None
        :exception: ValueError - invalid version of CPE Name",1,0,2,3
"def append_instances(cls, inst1, inst2):
        
        msg = inst1.equal_headers(inst2)
        if msg is not None:
            raise Exception(""Cannot appent instances: "" + msg)
        result = cls.copy_instances(inst1)
        for i in xrange(inst2.num_instances):
            result.add_instance(inst2.get_instance(i))
        return result","Merges the two datasets (one-after-the-other). Throws an exception if the datasets aren't compatible.

        :param inst1: the first dataset
        :type inst1: Instances
        :param inst2: the first dataset
        :type inst2: Instances
        :return: the combined dataset
        :rtype: Instances",2,0,1,3
"def application(environ, start_response):
    
    path = environ.get('PATH_INFO', '').lstrip('/')
    logger.info(""<application> PATH: %s"", path)

    if path == ""metadata"":
        return metadata(environ, start_response)

    user = environ.get(""REMOTE_USER"", """")
    if not user:
        user = environ.get(""repoze.who.identity"", """")
        logger.info(""repoze.who.identity: '%s'"", user)
    else:
        logger.info(""REMOTE_USER: '%s'"", user)
    
    for regex, callback in urls:
        if user:
            match = re.search(regex, path)
            if match is not None:
                try:
                    environ['myapp.url_args'] = match.groups()[0]
                except IndexError:
                    environ['myapp.url_args'] = path
                return callback(environ, start_response, user)
        else:
            return not_authn(environ, start_response)

    return not_found(environ, start_response)","The main WSGI application. Dispatch the current request to
    the functions from above and store the regular expression
    captures in the WSGI environment as `myapp.url_args` so that
    the functions from above can access the url placeholders.

    If nothing matches, call the `not_found` function.

    :param environ: The HTTP application environment
    :param start_response: The application to run when the handling of the
        request is done
    :return: The response as a list of lines",0,3,4,7
"def apply(self, byte, value, data, i, count):
        
        if byte & self.mask == self.value:
            value <<= self.bits
            value |= byte & self.mask2
        else:
            raise UnicodeDecodeError(
                NAME, data, i, i + count, ""invalid {}-byte sequence"".format(self.count)
            )
        return value","Apply mask, compare to expected value, shift and return result.
        Eventually, this could become a ``reduce`` function.

        :param byte: The byte to compare
        :param value: The currently accumulated value.
        :param data: The data buffer, (array of bytes).
        :param i: The position within the data buffer.
        :param count: The position of this comparison.
        :return: A new value with the bits merged in.
        :raises UnicodeDecodeError: if marked bits don't match.",1,0,5,6
"def apply(self, func, *args, cont=False, tag=None, **kwargs):
        
        new_lhs = func(self.lhs, *args, **kwargs)
        if new_lhs == self.lhs and cont:
            new_lhs = None
        new_rhs = func(self.rhs, *args, **kwargs)
        new_tag = tag
        return self._update(new_lhs, new_rhs, new_tag, cont)","Apply `func` to both sides of the equation

        Returns a new equation where the left-hand-side and right-hand side
        are replaced by the application of `func`::

            lhs=func(lhs, *args, **kwargs)
            rhs=func(rhs, *args, **kwargs)

        If ``cont=True``, the resulting equation will keep a history of its
        previous state (resulting in multiple lines of equations when printed,
        as in the main example above).

        The resulting equation with have the given `tag`.",0,0,2,2
"def approximating_model_reg(self, beta, T, Z, R, Q, h_approx, data, X, state_no):
             
        
        H = np.ones(data.shape[0])
        mu = np.zeros(data.shape[0])

        alpha = np.zeros([state_no, data.shape[0]])
        tol = 100.0
        it = 0
        while tol > 10**-7 and it < 5:
            old_alpha = np.sum(X*alpha.T,axis=1)
            alpha, V = nld_univariate_KFS(data,Z,H,T,Q,R,mu)
            H = np.exp(-np.sum(X*alpha.T,axis=1))
            mu = data - np.sum(X*alpha.T,axis=1) - np.exp(-np.sum(X*alpha.T,axis=1))*(data - np.exp(np.sum(X*alpha.T,axis=1)))
            tol = np.mean(np.abs(np.sum(X*alpha.T,axis=1)-old_alpha))
            it += 1

        return H, mu","Creates approximating Gaussian model for Poisson measurement density
        - dynamic regression model

        Parameters
        ----------
        beta : np.array
            Contains untransformed starting values for latent variables

        T, Z, R, Q : np.array
            State space matrices used in KFS algorithm

        data: np.array
            The univariate time series data

        X: np.array
            The regressors

        state_no : int
            Number of states

        Returns
        ----------

        H : np.array
            Approximating measurement variance matrix

        mu : np.array
            Approximating measurement constants",0,0,1,1
"def archive_if_exists(filename):
    
    if os.path.exists(filename):
        current_time = datetime.datetime.now()
        dt_format = '%Y-%m-%dT%H:%M:%S%z'
        timestamp = current_time.strftime(dt_format)
        dst = filename + '_' + timestamp
        shutil.move(filename, dst)","Move `filename` out of the way, archiving it by appending the current datetime
    Can be a file or a directory",0,0,2,2
"def args(self):
        
        return url_decode(wsgi_get_bytes(self.environ.get('QUERY_STRING', '')),
                          self.url_charset, errors=self.encoding_errors,
                          cls=self.parameter_storage_class)","The parsed URL parameters.  By default an
        :class:`~werkzeug.datastructures.ImmutableMultiDict`
        is returned from this function.  This can be changed by setting
        :attr:`parameter_storage_class` to a different type.  This might
        be necessary if the order of the form data is important.",0,0,2,2
"def argsplit(args, sep=','):
    
    parsed_len = 0
    last = 0
    splits = []
    for e in bracket_split(args, brackets=['()', '[]', '{}']):
        if e[0] not in {'(', '[', '{'}:
            for i, char in enumerate(e):
                if char == sep:
                    splits.append(args[last:parsed_len + i])
                    last = parsed_len + i + 1
        parsed_len += len(e)
    splits.append(args[last:])
    return splits","used to split JS args (it is not that simple as it seems because
       sep can be inside brackets).

       pass args *without* brackets!

       Used also to parse array and object elements, and more",0,0,1,1
"def array(input, separator=',', strip=True, empty=False):
    
    
    if input is None:
        return []
    
    if isinstance(input, list):
        if not empty:
            return [i for i in input if i]
        
        return input
    
    if not isinstance(input, (binary, unicode)):
        if not empty:
            return [i for i in list(input) if i]
        
        return list(input)
    
    if not strip:
        if not empty:
            return [i for i in input.split(separator) if i]
        
        return input.split(separator)
    
    if not empty:
        return [i for i in [i.strip() for i in input.split(separator)] if i]
    
    return [i.strip() for i in input.split(separator)]","Convert the given input to a list.
    
    Intelligently handles list and non-string values, returning
    as-is and passing to the list builtin respectively.
    
    The default optional keyword arguments allow for lists in the form::
    
        ""foo,bar, baz   , diz"" -> ['foo', 'bar', 'baz', 'diz']
    
    For a far more advanced method of converting a string to a list of
    values see :class:`KeywordProcessor`.
    
    :param input: the value to convert to a list
    :type input: any
    
    :param separator: The character (or string) to use to split the
                      input.  May be None to split on any whitespace.
    :type separator: basestring or None
    
    :param strip: If True, the values found by splitting will be stripped
                  of extraneous whitespace.
    :type strip: bool
    
    :param empty: If True, allow empty list items.
    :type empty: bool
    
    :returns: converted values as a list
    :rtype: list",0,0,1,1
"def articles(self):
		

		result = [None, None]
		element = self._first('NN')
		if element:
			if re.search('(de|het/?de|het);', element, re.U):
				result[0] = re.findall('(de|het/?de|het);', element, re.U)[0].split('/')
			if re.search('meervoud: (\w+)', element, re.U):
				
				result[1] = ['de']
			else:
				
				result[1] = ['']
		return result",Tries to scrape the correct articles for singular and plural from vandale.nl.,0,1,0,1
"def as_create_table(self, table_name, overwrite=False):
        
        exec_sql = ''
        sql = self.stripped()
        if overwrite:
            exec_sql = f'DROP TABLE IF EXISTS {table_name};\n'
        exec_sql += f'CREATE TABLE {table_name} AS \n{sql}'
        return exec_sql","Reformats the query into the create table as query.

        Works only for the single select SQL statements, in all other cases
        the sql query is not modified.
        :param superset_query: string, sql query that will be executed
        :param table_name: string, will contain the results of the
            query execution
        :param overwrite, boolean, table table_name will be dropped if true
        :return: string, create table as query",0,0,3,3
"def as_fn(self, *binding_order):
    
    if len(binding_order) != len(self.unbound_vars):
      raise ValueError('All vars must be specified.')
    for arg in binding_order:
      if arg not in self.unbound_vars:
        raise ValueError('Unknown binding: %s' % arg)

    def func(*args, **kwargs):
      
      if len(binding_order) != len(args):
        raise ValueError('Missing values, expects: %s' % binding_order)
      values = dict(zip(binding_order, args))
      values.update(kwargs)
      return self.construct(**values)

    func.__doc__ = _gen_ipython_string(func, binding_order, [], func.__doc__)
    return func","Creates a function by binding the arguments in the given order.

    Args:
      *binding_order: The unbound variables. This must include all values.
    Returns:
      A function that takes the arguments of binding_order.
    Raises:
      ValueError: If the bindings are missing values or include unknown values.",2,0,3,5
"def as_ord_matrix(matrix, alphabet):
    
    ords = [ord(c) for c in alphabet]
    ord_matrix = np.zeros((max(ords) + 1, max(ords) + 1), dtype=np.integer)
    for i, row_ord in enumerate(ords):
        for j, col_ord in enumerate(ords):
            ord_matrix[row_ord, col_ord] = matrix[i, j]

    return ord_matrix","Given the SubstitutionMatrix input, generate an equivalent matrix that
    is indexed by the ASCII number of each residue (e.g. A -> 65).",0,0,1,1
"def as_search_document_update(self, *, index, update_fields):
        
        if UPDATE_STRATEGY == UPDATE_STRATEGY_FULL:
            return self.as_search_document(index=index)

        if UPDATE_STRATEGY == UPDATE_STRATEGY_PARTIAL:
            
            
            return {
                k: getattr(self, k)
                for k in self.clean_update_fields(
                    index=index, update_fields=update_fields
                )
            }","Return a partial update document based on which fields have been updated.

        If an object is saved with the `update_fields` argument passed
        through, then it is assumed that this is a 'partial update'. In
        this scenario we need a {property: value} dictionary containing
        just the fields we want to update.

        This method handles two possible update strategies - 'full' or 'partial'.
        The default 'full' strategy simply returns the value of `as_search_document`
        - thereby replacing the entire document each time. The 'partial' strategy is
        more intelligent - it will determine whether the fields passed are in the
        search document mapping, and return a partial update document that contains
        only those that are. In addition, if any field that _is_ included cannot
        be automatically serialized (e.g. a RelatedField object), then this method
        will raise a ValueError. In this scenario, you should override this method
        in your subclass.

        >>> def as_search_document_update(self, index, update_fields):
        ...     if 'user' in update_fields:
        ...         update_fields.remove('user')
        ...         doc = super().as_search_document_update(index, update_fields)
        ...         doc['user'] = self.user.get_full_name()
        ...         return doc
        ...     return super().as_search_document_update(index, update_fields)

        You may also wish to subclass this method to perform field-specific logic
        - in this example if only the timestamp is being saved, then ignore the
        update if the timestamp is later than a certain time.

        >>> def as_search_document_update(self, index, update_fields):
        ...     if update_fields == ['timestamp']:
        ...         if self.timestamp > today():
        ...            return {}
        ...     return super().as_search_document_update(index, update_fields)",0,0,3,3
"def assertDateTimesLagLess(self, sequence, lag, msg=None):
        
        if not isinstance(sequence, collections.Iterable):
            raise TypeError('First argument is not iterable')
        if not isinstance(lag, timedelta):
            raise TypeError('Second argument is not a timedelta object')

        
        
        if isinstance(max(sequence), datetime):
            target = datetime.today()
        elif isinstance(max(sequence), date):
            target = date.today()
        else:
            raise TypeError('Expected iterable of datetime or date objects')

        self.assertLess(target - max(sequence), lag, msg=msg)","Fail if max element in ``sequence`` is separated from
        the present by ``lag`` or more as determined by the '<'
        operator.

        If the max element is a datetime, ""present"" is defined as
        ``datetime.now()``; if the max element is a date, ""present""
        is defined as ``date.today()``.

        This is equivalent to
        ``self.assertLess(present - max(sequence), lag)``.

        Parameters
        ----------
        sequence : iterable
        lag : timedelta
        msg : str
            If not provided, the :mod:`marbles.mixins` or
            :mod:`unittest` standard message will be used.

        Raises
        ------
        TypeError
            If ``sequence`` is not iterable.
        TypeError
            If ``lag`` is not a timedelta object.
        TypeError
            If max element in ``sequence`` is not a datetime or date
            object.",3,0,4,7
"def assignPrivilege(self, rolename, privilege=""ACCESS""):
        
        aURL = self._url + ""/roles/assignPrivilege""
        params = {
            ""f"" : ""json"",
            ""rolename"" : rolename,
            ""privilege"" : privilege
        }
        return self._post(url=aURL,
                             param_dict=params,
                             securityHandler=self._securityHandler,
                             proxy_url=self._proxy_url,
                             proxy_port=self._proxy_port)","Administrative access to ArcGIS Server is modeled as three broad
           tiers of privileges:
               ADMINISTER - A role that possesses this privilege has
                           unrestricted administrative access to ArcGIS
                          Server.
               PUBLISH - A role with PUBLISH privilege can only publish GIS
                       services to ArcGIS Server.
               ACCESS-No administrative access. A role with this privilege
                      can only be granted permission to access one or more
                      GIS services.
           By assigning these privileges to one or more roles in the role
           store, ArcGIS Server's security model supports role-based access
           control to its administrative functionality.
           These privilege assignments are stored independent of ArcGIS
           Server's role store. As a result, you don't need to update your
           enterprise identity stores (like Active Directory).
           Inputs:
              rolename - The name of the role.
              privilege - The capability to assign to the role. The default
                          capability is ACCESS.
                          Values: ADMINISTER | PUBLISH | ACCESS
           Output:
              JSON Message",0,1,0,1
"def assign_version_default_trigger(plpy, td):
    
    modified_state = ""OK""
    portal_type = td['new']['portal_type']
    version = td['new']['version']
    minor_version = td['new']['minor_version']

    
    
    if minor_version is None and portal_type in ('Collection',
                                                 'SubCollection'):
        modified_state = ""MODIFY""
        td['new']['minor_version'] = 1

    
    if version is None:
        major_version = td['new']['major_version']
        version = ""1.{}"".format(major_version)
        modified_state = ""MODIFY""
        td['new']['version'] = version

    return modified_state","Trigger to fill in legacy data fields.

    A compatibilty trigger to fill in legacy data fields that are not
    populated when inserting publications from cnx-publishing.

    If this is not a legacy publication the ``version`` will be set
    based on the ``major_version`` value.",0,0,3,3
"def asterisk_to_min_max(field, time_filter, search_engine_endpoint, actual_params=None):
    

    if actual_params:
        raise NotImplemented(""actual_params"")

    start, end = parse_solr_time_range_as_pair(time_filter)
    if start == '*' or end == '*':
        params_stats = {
            ""q"": ""*:*"",
            ""rows"": 0,
            ""stats.field"": field,
            ""stats"": ""true"",
            ""wt"": ""json""
        }
        res_stats = requests.get(search_engine_endpoint, params=params_stats)

        if res_stats.ok:

            stats_date_field = res_stats.json()[""stats""][""stats_fields""][field]
            date_min = stats_date_field[""min""]
            date_max = stats_date_field[""max""]

            if start != '*':
                date_min = start
            if end != '*':
                date_max = end

            time_filter = ""[{0} TO {1}]"".format(date_min, date_max)

    return time_filter","traduce [* TO *] to something like [MIN-INDEXED-DATE TO MAX-INDEXED-DATE]
    :param field: map the stats to this field.
    :param time_filter: this is the value to be translated. think in ""[* TO 2000]""
    :param search_engine_endpoint: solr core
    :param actual_params: (not implemented) to merge with other params.
    :return: translated time filter",1,1,2,4
"def astra_conebeam_2d_geom_to_vec(geometry):
    
    
    
    
    rot_minus_90 = euler_matrix(-np.pi / 2)
    angles = geometry.angles
    vectors = np.zeros((angles.size, 6))

    
    src_pos = geometry.src_position(angles)
    vectors[:, 0:2] = rot_minus_90.dot(src_pos.T).T  

    
    mid_pt = geometry.det_params.mid_pt
    
    
    centers = geometry.det_point_position(angles, float(mid_pt))
    vectors[:, 2:4] = rot_minus_90.dot(centers.T).T

    
    det_axis = rot_minus_90.dot(geometry.det_axis(angles).T).T
    px_size = geometry.det_partition.cell_sides[0]
    vectors[:, 4:6] = det_axis * px_size

    return vectors","Create vectors for ASTRA projection geometries from ODL geometry.

    The 2D vectors are used to create an ASTRA projection geometry for
    fan beam geometries, see ``'fanflat_vec'`` in the
    `ASTRA projection geometry documentation`_.

    Each row of the returned vectors corresponds to a single projection
    and consists of ::

        (srcX, srcY, dX, dY, uX, uY)

    with

        - ``src``: the ray source position
        - ``d``  : the center of the detector
        - ``u``  : the vector from detector pixel 0 to 1

    Parameters
    ----------
    geometry : `Geometry`
        ODL projection geometry from which to create the ASTRA geometry.

    Returns
    -------
    vectors : `numpy.ndarray`
        Array of shape ``(num_angles, 6)`` containing the vectors.

    References
    ----------
    .. _ASTRA projection geometry documentation:
       http://www.astra-toolbox.com/docs/geom2d.html#projection-geometries",0,0,1,1
"def asyncPipeSubstr(context=None, _INPUT=None, conf=None, **kwargs):
    
    conf['start'] = conf.pop('from', dict.get(conf, 'start'))
    splits = yield asyncGetSplits(_INPUT, conf, **cdicts(opts, kwargs))
    parsed = yield asyncDispatch(splits, *get_async_dispatch_funcs())
    _OUTPUT = yield asyncStarMap(partial(maybeDeferred, parse_result), parsed)
    returnValue(iter(_OUTPUT))","A string module that asynchronously returns a substring. Loopable.

    Parameters
    ----------
    context : pipe2py.Context object
    _INPUT : twisted Deferred iterable of items or strings
    conf : {
        'from': {'type': 'number', value': <starting position>},
        'length': {'type': 'number', 'value': <count of characters to return>}
    }

    returns
    -------
    _OUTPUT : twisted.internet.defer.Deferred generator of substrings",0,3,0,3
"def at_time_validate(ctx, param, value):
    
    
    if value is not None:
        if (re.search(r'([0-2]\d)(:[0-5]\d){1,2}', value) is None and
            re.search(r'\d{4}-[01]\d-[0-3]\d [0-2]\d:[0-5]\d(:[0-5]\d)?',
                      value) is None):
            raise click.BadParameter(""A commit at time must be in one of the ""
                                     ""two formats: 'hh:mm[:ss]' or ""
                                     ""'yyyy-mm-dd hh:mm[:ss]' (seconds are ""
                                     ""optional)."")
    ctx.obj['at_time'] = value
    return value","Callback validating the at_time commit option.

    Purpose: Validates the `at time` option for the commit command. Only the
           | the following two formats are supported: 'hh:mm[:ss]' or
           | 'yyyy-mm-dd hh:mm[:ss]' (seconds are optional).

    @param ctx: The click context paramter, for receiving the object dictionary
              | being manipulated by other previous functions. Needed by any
              | function with the @click.pass_context decorator. Callback
              | functions such as this one receive this automatically.
    @type ctx: click.Context
    @param param: param is passed into a validation callback function by click.
                | We do not use it.
    @type param: None
    @param value: The value that the user supplied for the at_time option.
    @type value: str

    @returns: The value that the user supplied, if it passed validation.
            | Otherwise, raises click.BadParameter
    @rtype: str",1,0,2,3
"def atmospheric_station_pressure(self, value=999999):
        
        if value is not None:
            try:
                value = int(value)
            except ValueError:
                raise ValueError(
                    'value {} need to be of type int '
                    'for field `atmospheric_station_pressure`'.format(value))
            if value <= 31000:
                raise ValueError('value need to be greater 31000 '
                                 'for field `atmospheric_station_pressure`')
            if value >= 120000:
                raise ValueError('value need to be smaller 120000 '
                                 'for field `atmospheric_station_pressure`')

        self._atmospheric_station_pressure = value","Corresponds to IDD Field `atmospheric_station_pressure`

        Args:
            value (int): value for IDD Field `atmospheric_station_pressure`
                Unit: Pa
                value > 31000
                value < 120000
                Missing value: 999999
                if `value` is None it will not be checked against the
                specification and is assumed to be a missing value

        Raises:
            ValueError: if `value` is not a valid value",3,0,4,7
"def attach_keypress(fig, scaling=1.1):
    

    def press(event):
        if event.key == 'q':
            plt.close(fig)
        elif event.key == 'e':
            fig.set_size_inches(scaling * fig.get_size_inches(), forward=True)
        elif event.key == 'c':
            fig.set_size_inches(fig.get_size_inches() / scaling, forward=True)

    
    if not hasattr(fig, '_sporco_keypress_cid'):
        cid = fig.canvas.mpl_connect('key_press_event', press)
        fig._sporco_keypress_cid = cid

    return press","Attach a key press event handler that configures keys for closing a
    figure and changing the figure size. Keys 'e' and 'c' respectively
    expand and contract the figure, and key 'q' closes it.

    **Note:** Resizing may not function correctly with all matplotlib
    backends (a
    `bug <https://github.com/matplotlib/matplotlib/issues/10083>`__
    has been reported).

    Parameters
    ----------
    fig : :class:`matplotlib.figure.Figure` object
        Figure to which event handling is to be attached
   scaling : float, optional (default 1.1)
      Scaling factor for figure size changes

    Returns
    -------
    press : function
      Key press event handler function",0,0,4,4
"def authenticate(self, request: AxesHttpRequest, username: str = None, password: str = None, **kwargs: dict):
        

        if request is None:
            raise AxesBackendRequestParameterRequired('AxesBackend requires a request as an argument to authenticate')

        credentials = get_credentials(username=username, password=password, **kwargs)

        if AxesProxyHandler.is_allowed(request, credentials):
            return

        
        

        error_msg = get_lockout_message()
        response_context = kwargs.get('response_context', {})
        response_context['error'] = error_msg

        
        
        
        
        
        

        raise AxesBackendPermissionDenied('AxesBackend detected that the given user is locked out')","Checks user lockout status and raise a PermissionDenied if user is not allowed to log in.

        This method interrupts the login flow and inserts  error message directly to the
        ``response_context`` attribute that is supplied as a keyword argument.

        :keyword response_context: kwarg that will be have its ``error`` attribute updated with context.
        :raises AxesBackendRequestParameterRequired: if request parameter is not passed.
        :raises AxesBackendPermissionDenied: if user is already locked out.",2,0,3,5
"def authenticate(self, session: Session, listener):
        
        auth_plugins = None
        auth_config = self.config.get('auth', None)
        if auth_config:
            auth_plugins = auth_config.get('plugins', None)
        returns = yield from self.plugins_manager.map_plugin_coro(
            ""authenticate"",
            session=session,
            filter_plugins=auth_plugins)
        auth_result = True
        if returns:
            for plugin in returns:
                res = returns[plugin]
                if res is False:
                    auth_result = False
                    self.logger.debug(""Authentication failed due to '%s' plugin result: %s"" % (plugin.name, res))
                else:
                    self.logger.debug(""'%s' plugin result: %s"" % (plugin.name, res))
        
        return auth_result","This method call the authenticate method on registered plugins to test user authentication.
        User is considered authenticated if all plugins called returns True.
        Plugins authenticate() method are supposed to return :
         - True if user is authentication succeed
         - False if user authentication fails
         - None if authentication can't be achieved (then plugin result is then ignored)
        :param session:
        :param listener:
        :return:",0,3,1,4
"def authenticate(username, password):
    
    session = MoJOAuth2Session(
        client=LegacyApplicationClient(
            client_id=settings.API_CLIENT_ID
        )
    )

    token = session.fetch_token(
        token_url=get_request_token_url(),
        username=username,
        password=password,
        auth=HTTPBasicAuth(settings.API_CLIENT_ID, settings.API_CLIENT_SECRET),
        timeout=15,
        encoding='utf-8'
    )

    user_data = session.get('/users/{username}/'.format(username=username)).json()

    return {
        'pk': user_data.get('pk'),
        'token': token,
        'user_data': user_data
    }","Returns:
        a dict with:
            pk: the pk of the user
            token: dict containing all the data from the api
                (access_token, refresh_token, expires_at etc.)
            user_data: dict containing user data such as
                first_name, last_name etc.
        if the authentication succeeds
        Raises Unauthorized if the authentication fails",0,2,0,2
"def authenticateRequest(self, service_request, username, password, **kwargs):
        
        authenticator = self.getAuthenticator(service_request)

        if self.logger:
            self.logger.debug('Authenticator expands to: %r' % authenticator)

        if authenticator is None:
            return defer.succeed(True)

        args = (username, password)

        if hasattr(authenticator, '_pyamf_expose_request'):
            http_request = kwargs.get('http_request', None)
            args = (http_request,) + args

        return defer.maybeDeferred(authenticator, *args)","Processes an authentication request. If no authenticator is supplied,
        then authentication succeeds.

        @return: C{Deferred}.
        @rtype: C{twisted.internet.defer.Deferred}",0,1,1,2
"def authorize(self, client_id, redirect_uri, scope, 
                  state=None, user_name=None, user_email=None):
        
        query = [
            ('client_id', client_id),
            ('redirect_uri', redirect_uri),
            ('scope', scope)
        ]
        if user_name is not None:
            query.append(('user_name', user_name))
        if user_email is not None:
            query.append(('user_email', user_email))
        if state is not None:
            query.append(('state', state))
        return '%s/oauth2/authorize?%s' % (
            self._api.browser_endpoint, urllib.parse.urlencode(query))","Documentation: `/oauth2/authorize
        <https://www.wepay.com/developer/reference/oauth2#authorize>`_.

        .. note::
    
           This is not an API call but an actual uri that you send the user to.",0,0,1,1
"def autodict_decorate(cls,                         
                      include=None,                
                      exclude=None,                
                      only_constructor_args=True,  
                      only_public_fields=True      
                      ):
    
    

    
    _check_known_decorators(cls, '@autodict')

    
    _execute_autodict_on_class(cls, include=include, exclude=exclude, only_constructor_args=only_constructor_args,
                               only_public_fields=only_public_fields)

    return cls","To automatically generate the appropriate methods so that objects of this class behave like a `dict`,
    manually, without using @autodict decorator.

    :param cls: the class on which to execute. Note that it won't be wrapped.
    :param include: a tuple of explicit attribute names to include (None means all)
    :param exclude: a tuple of explicit attribute names to exclude. In such case, include should be None.
    :param only_constructor_args: if True (default), only constructor arguments will be exposed through the dictionary
    view, not any other field that would be created in the constructor or dynamically. This makes it very convenient
    to use in combination with @autoargs. If set to False, the dictionary is a direct view of public object fields.
    :param only_public_fields: this parameter is only used when only_constructor_args is set to False. If
    only_public_fields is set to False, all fields are visible. Otherwise (default), class-private fields will be hidden
    :return:",0,0,1,1
"def automatic_parser(result, dtypes={}, converters={}):
    
    np.seterr(all='raise')
    parsed = {}

    for filename, contents in result['output'].items():
        if dtypes.get(filename) is None:
            dtypes[filename] = None
        if converters.get(filename) is None:
            converters[filename] = None

        with warnings.catch_warnings():
            warnings.simplefilter(""ignore"")
            parsed[filename] = np.genfromtxt(io.StringIO(contents),
                                             dtype=dtypes[filename],
                                             converters=converters[filename]
                                             ).tolist()

    return parsed","Try and automatically convert strings formatted as tables into nested
    list structures.

    Under the hood, this function essentially applies the genfromtxt function
    to all files in the output, and passes it the additional kwargs.

    Args:
      result (dict): the result to parse.
      dtypes (dict): a dictionary containing the dtype specification to perform
        parsing for each available filename. See the numpy genfromtxt
        documentation for more details on how to format these.",0,0,2,2
"def average_returns(ts, **kwargs):
    
    average_type = kwargs.get('type', 'net')
    if average_type == 'net':
        relative = 0
    else:
        relative = -1  
    
    
    
    period = kwargs.get('period', None)
    if isinstance(period, int):
        pass
    
        
        
    avg_ret = 1
    for idx in range(len(ts.index)):
        if idx % period == 0:
            avg_ret *= (1 + ts[idx] + relative)
    return avg_ret - 1",Compute geometric average returns from a returns time serie,0,0,1,1
"def backprop(self, input_data, targets, cache=None):
        

        df_input = gpuarray.zeros_like(input_data)

        if cache is None: cache = self.n_tasks * [None]

        gradients = []
        for targets_task, cache_task, task, task_weight  in \
          izip(targets, cache, self.tasks, self.task_weights):
            gradients_task, df_input_task = \
              task.backprop(input_data, targets_task,
                            cache_task)

            df_input = df_input.mul_add(1., df_input_task, task_weight)

            gradients.extend(gradients_task)

        return gradients, df_input","Compute gradients for each task and combine the results.

        **Parameters:**

        input_data : ``GPUArray``
            Inpute data to compute activations for.

        targets : ``GPUArray``
            The target values of the units.

        cache : list of ``GPUArray``
            Cache obtained from forward pass. If the cache is
            provided, then the activations are not recalculated.

        **Returns:**

        gradients : list
            Gradients with respect to the weights and biases for each task

        df_input : ``GPUArray``
            Gradients with respect to the input, obtained by adding
            the gradients with respect to the inputs from each task,
            weighted by ``MultitaskTopLayer.task_weights``.",0,0,4,4
"def backup_db(self):
        
        with self.db_mutex:
            if os.path.exists(self.json_db_path):
                try:
                    shutil.copy2(self.json_db_path, self.backup_json_db_path)
                except (IOError, OSError):
                    _logger.debug(""*** No file to copy."")",""" Generate a xxxxx.backup.json.",1,1,0,2
"def bandstructure_flow(workdir, scf_input, nscf_input, dos_inputs=None, manager=None, flow_class=Flow, allocate=True):
    
    flow = flow_class(workdir, manager=manager)
    work = BandStructureWork(scf_input, nscf_input, dos_inputs=dos_inputs)
    flow.register_work(work)

    
    flow.scf_task, flow.nscf_task, flow.dos_tasks = work.scf_task, work.nscf_task, work.dos_tasks

    if allocate: flow.allocate()
    return flow","Build a :class:`Flow` for band structure calculations.

    Args:
        workdir: Working directory.
        scf_input: Input for the GS SCF run.
        nscf_input: Input for the NSCF run (band structure run).
        dos_inputs: Input(s) for the NSCF run (dos run).
        manager: :class:`TaskManager` object used to submit the jobs
                 Initialized from manager.yml if manager is None.
        flow_class: Flow subclass
        allocate: True if the flow should be allocated before returning.

    Returns:
        :class:`Flow` object",0,0,1,1
"def base62_decode(string):
    

    alphabet = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'

    base = len(alphabet)
    strlen = len(string)
    num = 0

    idx = 0
    for char in string:
        power = (strlen - (idx + 1))
        num += alphabet.index(char) * (base ** power)
        idx += 1

    return int(num)","Decode a Base X encoded string into the number

    Arguments:
    - `string`: The encoded string
    - `alphabet`: The alphabet to use for encoding
    Stolen from: http://stackoverflow.com/a/1119769/1144479",0,0,1,1
"def base_id(self):
        
        
        if self._base_id is not None:
            return self._base_id

        
        self.send(Packet(PACKET.COMMON_COMMAND, data=[0x08]))
        
        
        
        for i in range(0, 10):
            try:
                packet = self.receive.get(block=True, timeout=0.1)
                
                if packet.packet_type == PACKET.RESPONSE and packet.response == RETURN_CODE.OK and len(packet.response_data) == 4:
                    
                    self._base_id = packet.response_data
                    
                    self.receive.put(packet)
                    break
                
                self.receive.put(packet)
            except queue.Empty:
                continue
        
        return self._base_id","Fetches Base ID from the transmitter, if required. Otherwise returns the currently set Base ID.",0,3,1,4
"def basic_publish(self, msg, exchange='', routing_key='',
        mandatory=False, immediate=False, ticket=None):
        
        args = AMQPWriter()
        if ticket is not None:
            args.write_short(ticket)
        else:
            args.write_short(self.default_ticket)
        args.write_shortstr(exchange)
        args.write_shortstr(routing_key)
        args.write_bit(mandatory)
        args.write_bit(immediate)

        self._send_method((60, 40), args, msg)","publish a message

        This method publishes a message to a specific exchange. The
        message will be routed to queues as defined by the exchange
        configuration and distributed to any active consumers when the
        transaction, if any, is committed.

        PARAMETERS:
            exchange: shortstr

                Specifies the name of the exchange to publish to.  The
                exchange name can be empty, meaning the default
                exchange.  If the exchange name is specified, and that
                exchange does not exist, the server will raise a
                channel exception.

                RULE:

                    The server MUST accept a blank exchange name to
                    mean the default exchange.

                RULE:

                    If the exchange was declared as an internal
                    exchange, the server MUST raise a channel
                    exception with a reply code 403 (access refused).

                RULE:

                    The exchange MAY refuse basic content in which
                    case it MUST raise a channel exception with reply
                    code 540 (not implemented).

            routing_key: shortstr

                Message routing key

                Specifies the routing key for the message.  The
                routing key is used for routing messages depending on
                the exchange configuration.

            mandatory: boolean

                indicate mandatory routing

                This flag tells the server how to react if the message
                cannot be routed to a queue.  If this flag is True, the
                server will return an unroutable message with a Return
                method.  If this flag is False, the server silently
                drops the message.

                RULE:

                    The server SHOULD implement the mandatory flag.

            immediate: boolean

                request immediate delivery

                This flag tells the server how to react if the message
                cannot be routed to a queue consumer immediately.  If
                this flag is set, the server will return an
                undeliverable message with a Return method. If this
                flag is zero, the server will queue the message, but
                with no guarantee that it will ever be consumed.

                RULE:

                    The server SHOULD implement the immediate flag.

            ticket: short

                RULE:

                    The client MUST provide a valid access ticket
                    giving ""write"" access rights to the access realm
                    for the exchange.",0,1,1,2
"def batch_means(x, f=lambda y: y, theta=.5, q=.95, burn=0):
    

    try:
        import scipy
        from scipy import stats
    except ImportError:
        raise ImportError('SciPy must be installed to use batch_means.')

    x = x[burn:]

    n = len(x)

    b = np.int(n ** theta)
    a = n / b

    t_quant = stats.t.isf(1 - q, a - 1)

    Y = np.array([np.mean(f(x[i * b:(i + 1) * b])) for i in xrange(a)])
    sig = b / (a - 1.) * sum((Y - np.mean(f(x))) ** 2)

    return t_quant * sig / np.sqrt(n)","TODO: Use Bayesian CI.

    Returns the half-width of the frequentist confidence interval
    (q'th quantile) of the Monte Carlo estimate of E[f(x)].

    :Parameters:
        x : sequence
            Sampled series. Must be a one-dimensional array.
        f : function
            The MCSE of E[f(x)] will be computed.
        theta : float between 0 and 1
            The batch length will be set to len(x) ** theta.
        q : float between 0 and 1
            The desired quantile.

    :Example:
        >>>batch_means(x, f=lambda x: x**2, theta=.5, q=.95)

    :Reference:
        Flegal, James M. and Haran, Murali and Jones, Galin L. (2007).
        Markov chain Monte Carlo: Can we trust the third significant figure?
        <Publication>

    :Note:
        Requires SciPy",1,0,2,3
"def batched_index_select(target: torch.Tensor,
                         indices: torch.LongTensor,
                         flattened_indices: Optional[torch.LongTensor] = None) -> torch.Tensor:
    
    if flattened_indices is None:
        
        flattened_indices = flatten_and_batch_shift_indices(indices, target.size(1))

    
    flattened_target = target.view(-1, target.size(-1))

    
    flattened_selected = flattened_target.index_select(0, flattened_indices)
    selected_shape = list(indices.size()) + [target.size(-1)]
    
    selected_targets = flattened_selected.view(*selected_shape)
    return selected_targets","The given ``indices`` of size ``(batch_size, d_1, ..., d_n)`` indexes into the sequence
    dimension (dimension 2) of the target, which has size ``(batch_size, sequence_length,
    embedding_size)``.

    This function returns selected values in the target with respect to the provided indices, which
    have size ``(batch_size, d_1, ..., d_n, embedding_size)``. This can use the optionally
    precomputed :func:`~flattened_indices` with size ``(batch_size * d_1 * ... * d_n)`` if given.

    An example use case of this function is looking up the start and end indices of spans in a
    sequence tensor. This is used in the
    :class:`~allennlp.models.coreference_resolution.CoreferenceResolver`. Model to select
    contextual word representations corresponding to the start and end indices of mentions. The key
    reason this can't be done with basic torch functions is that we want to be able to use look-up
    tensors with an arbitrary number of dimensions (for example, in the coref model, we don't know
    a-priori how many spans we are looking up).

    Parameters
    ----------
    target : ``torch.Tensor``, required.
        A 3 dimensional tensor of shape (batch_size, sequence_length, embedding_size).
        This is the tensor to be indexed.
    indices : ``torch.LongTensor``
        A tensor of shape (batch_size, ...), where each element is an index into the
        ``sequence_length`` dimension of the ``target`` tensor.
    flattened_indices : Optional[torch.Tensor], optional (default = None)
        An optional tensor representing the result of calling :func:~`flatten_and_batch_shift_indices`
        on ``indices``. This is helpful in the case that the indices can be flattened once and
        cached for many batch lookups.

    Returns
    -------
    selected_targets : ``torch.Tensor``
        A tensor with shape [indices.size(), target.size(-1)] representing the embedded indices
        extracted from the batch flattened target tensor.",0,0,1,1
"def bbox_transform(ex_rois, gt_rois, box_stds):
    
    assert ex_rois.shape[0] == gt_rois.shape[0], 'inconsistent rois number'

    ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + 1.0
    ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + 1.0
    ex_ctr_x = ex_rois[:, 0] + 0.5 * (ex_widths - 1.0)
    ex_ctr_y = ex_rois[:, 1] + 0.5 * (ex_heights - 1.0)

    gt_widths = gt_rois[:, 2] - gt_rois[:, 0] + 1.0
    gt_heights = gt_rois[:, 3] - gt_rois[:, 1] + 1.0
    gt_ctr_x = gt_rois[:, 0] + 0.5 * (gt_widths - 1.0)
    gt_ctr_y = gt_rois[:, 1] + 0.5 * (gt_heights - 1.0)

    targets_dx = (gt_ctr_x - ex_ctr_x) / (ex_widths + 1e-14) / box_stds[0]
    targets_dy = (gt_ctr_y - ex_ctr_y) / (ex_heights + 1e-14) / box_stds[1]
    targets_dw = np.log(gt_widths / ex_widths) / box_stds[2]
    targets_dh = np.log(gt_heights / ex_heights) / box_stds[3]

    targets = np.vstack((targets_dx, targets_dy, targets_dw, targets_dh)).transpose()
    return targets","compute bounding box regression targets from ex_rois to gt_rois
    :param ex_rois: [N, 4]
    :param gt_rois: [N, 4]
    :return: [N, 4]",0,0,2,2
"def bcs_parameters(n_site, n_fermi, u, t) :
    

    
    wave_num = np.linspace(0, 1, n_site, endpoint=False)
    
    hop_erg = -2 * t * np.cos(2 * np.pi * wave_num)
    
    fermi_erg = hop_erg[n_fermi // 2]
    
    hop_erg = hop_erg - fermi_erg

    def _bcs_gap(x):
        

        s = 0.
        for i in range(n_site):
            s += 1. / np.sqrt(hop_erg[i] ** 2 + x ** 2)
        return 1 + s * u / (2 * n_site)

    
    delta = scipy.optimize.bisect(_bcs_gap, 0.01, 10000. * abs(u))
    
    bcs_v = np.sqrt(0.5 * (1 - hop_erg / np.sqrt(hop_erg ** 2 + delta ** 2)))
    
    bog_theta = np.arcsin(bcs_v)

    return delta, bog_theta","Generate the parameters for the BCS ground state, i.e., the
    superconducting gap and the rotational angles in the Bogoliubov
    transformation.

     Args:
        n_site: the number of sites in the Hubbard model
        n_fermi: the number of fermions
        u: the interaction strength
        t: the tunneling strength

    Returns:
        float delta, List[float] bog_theta",0,0,1,1
"def before_insert(mapper, conn, target):
        
        from sqlalchemy import text

        if not target.id:
            sql = text('SELECT max(f_id)+1 FROM files WHERE f_d_vid = :did')

            target.id, = conn.execute(sql, did=target.d_vid).fetchone()

            if not target.id:
                target.id = 1

        if target.contents and isinstance(target.contents, six.text_type):
            target.contents = target.contents.encode('utf-8')

        File.before_update(mapper, conn, target)","event.listen method for Sqlalchemy to set the sequence for this
        object and create an ObjectNumber value for the id_",1,0,1,2
"def begin(self, address=MPR121_I2CADDR_DEFAULT, i2c=None, **kwargs):
                
        
        if i2c is None:
            import Adafruit_GPIO.I2C as I2C
            i2c = I2C
            
            
            
            I2C.require_repeated_start()
        
        self._device = i2c.get_i2c_device(address, **kwargs)
        return self._reset()","Initialize communication with the MPR121. 

        Can specify a custom I2C address for the device using the address 
        parameter (defaults to 0x5A). Optional i2c parameter allows specifying a 
        custom I2C bus source (defaults to platform's I2C bus).

        Returns True if communication with the MPR121 was established, otherwise
        returns False.",0,1,0,1
"def between(self, time_):
        
        hour = int(time_[0:2])
        minute = int(time_[3:5])
        return not (
            hour < self.h1 or hour > self.h2 or
            (hour == self.h1 and minute < self.m1) or
            (hour == self.h2 and minute > self.m2)
        )",Compare if the parameter HH:MM is in the time range.,0,0,1,1
"def between_time(self, start_time, end_time, include_start=True,
                     include_end=True, axis=None):
        
        if axis is None:
            axis = self._stat_axis_number
        axis = self._get_axis_number(axis)

        index = self._get_axis(axis)
        try:
            indexer = index.indexer_between_time(
                start_time, end_time, include_start=include_start,
                include_end=include_end)
        except AttributeError:
            raise TypeError('Index must be DatetimeIndex')

        return self._take(indexer, axis=axis)","Select values between particular times of the day (e.g., 9:00-9:30 AM).

        By setting ``start_time`` to be later than ``end_time``,
        you can get the times that are *not* between the two times.

        Parameters
        ----------
        start_time : datetime.time or str
        end_time : datetime.time or str
        include_start : bool, default True
        include_end : bool, default True
        axis : {0 or 'index', 1 or 'columns'}, default 0

            .. versionadded:: 0.24.0

        Returns
        -------
        Series or DataFrame

        Raises
        ------
        TypeError
            If the index is not  a :class:`DatetimeIndex`

        See Also
        --------
        at_time : Select values at a particular time of the day.
        first : Select initial periods of time series based on a date offset.
        last : Select final periods of time series based on a date offset.
        DatetimeIndex.indexer_between_time : Get just the index locations for
            values between particular times of the day.

        Examples
        --------
        >>> i = pd.date_range('2018-04-09', periods=4, freq='1D20min')
        >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)
        >>> ts
                             A
        2018-04-09 00:00:00  1
        2018-04-10 00:20:00  2
        2018-04-11 00:40:00  3
        2018-04-12 01:00:00  4

        >>> ts.between_time('0:15', '0:45')
                             A
        2018-04-10 00:20:00  2
        2018-04-11 00:40:00  3

        You get the times that are *not* between two times by setting
        ``start_time`` later than ``end_time``:

        >>> ts.between_time('0:45', '0:15')
                             A
        2018-04-09 00:00:00  1
        2018-04-12 01:00:00  4",1,0,2,3
"def bids_from_config(sess, scans_metadata, config, out_base):
    
    
    _item = next(iter(scans_metadata))
    session,subject = _item['session_label'],_item['subject_label']
    
    sourcedata_base = os.path.join(
        out_base,
        'sourcedata',
        'sub-{0}'.format(legal.sub('', subject)),
        'ses-{0}'.format(legal.sub('', session))
    )
    bids_base = os.path.join(
        out_base,
        'sub-{0}'.format(legal.sub('', subject)),
        'ses-{0}'.format(legal.sub('', session))
    )
    
    args = commons.struct(
        xnat=sess,
        subject=subject,
        session=session,
        bids=bids_base,
        sourcedata=sourcedata_base
    )
    
    func_refs = proc_func(config, args)
    anat_refs = proc_anat(config, args)
    fmap_refs = proc_fmap(config, args, func_refs)",Create a BIDS output directory from configuration file,0,0,1,1
"def bin(y, bins):
    
    if np.ndim(y) == 1:
        k = 1
        n = np.shape(y)[0]
    else:
        n, k = np.shape(y)
    b = np.zeros((n, k), dtype='int')
    i = len(bins)
    if type(bins) != list:
        bins = bins.tolist()
    binsc = copy.copy(bins)
    while binsc:
        i -= 1
        c = binsc.pop(-1)
        b[np.nonzero(y <= c)] = i
    return b","bin interval/ratio data

    Parameters
    ----------
    y : array
        (n,q), values to bin
    bins : array
           (k,1), upper bounds of each bin (monotonic)

    Returns
    -------
    b : array
        (n,q), values of values between 0 and k-1

    Examples
    --------
    >>> import numpy as np
    >>> import mapclassify as mc
    >>> np.random.seed(1)
    >>> x = np.random.randint(2, 20, (10, 3))
    >>> bins = [10, 15, 20]
    >>> b = mc.classifiers.bin(x, bins)
    >>> x
    array([[ 7, 13, 14],
           [10, 11, 13],
           [ 7, 17,  2],
           [18,  3, 14],
           [ 9, 15,  8],
           [ 7, 13, 12],
           [16,  6, 11],
           [19,  2, 15],
           [11, 11,  9],
           [ 3,  2, 19]])
    >>> b
    array([[0, 1, 1],
           [0, 1, 1],
           [0, 2, 0],
           [2, 0, 1],
           [0, 1, 0],
           [0, 1, 1],
           [2, 0, 1],
           [2, 0, 1],
           [1, 1, 0],
           [0, 0, 2]])",0,0,1,1
"def bind(self, source=None, destination=None, node=None,
             edge_title=None, edge_label=None, edge_color=None, edge_weight=None,
             point_title=None, point_label=None, point_color=None, point_size=None):
        
        res = copy.copy(self)
        res._source = source or self._source
        res._destination = destination or self._destination
        res._node = node or self._node

        res._edge_title = edge_title or self._edge_title
        res._edge_label = edge_label or self._edge_label
        res._edge_color = edge_color or self._edge_color
        res._edge_weight = edge_weight or self._edge_weight

        res._point_title = point_title or self._point_title
        res._point_label = point_label or self._point_label
        res._point_color = point_color or self._point_color
        res._point_size = point_size or self._point_size

        return res","Relate data attributes to graph structure and visual representation.

        To facilitate reuse and replayable notebooks, the binding call is chainable. Invocation does not effect the old binding: it instead returns a new Plotter instance with the new bindings added to the existing ones. Both the old and new bindings can then be used for different graphs.


        :param source: Attribute containing an edge's source ID
        :type source: String.

        :param destination: Attribute containing an edge's destination ID
        :type destination: String.

        :param node: Attribute containing a node's ID
        :type node: String.

        :param edge_title: Attribute overriding edge's minimized label text. By default, the edge source and destination is used.
        :type edge_title: HtmlString.

        :param edge_label: Attribute overriding edge's expanded label text. By default, scrollable list of attribute/value mappings.
        :type edge_label: HtmlString.

        :param edge_color: Attribute overriding edge's color. `See palette definitions <https://graphistry.github.io/docs/legacy/api/0.9.2/api.html#extendedpalette>`_ for values. Based on Color Brewer.
        :type edge_color: String.

        :param edge_weight: Attribute overriding edge weight. Default is 1. Advanced layout controls will relayout edges based on this value.
        :type edge_weight: String.

        :param point_title: Attribute overriding node's minimized label text. By default, the node ID is used.
        :type point_title: HtmlString.

        :param point_label: Attribute overriding node's expanded label text. By default, scrollable list of attribute/value mappings.
        :type point_label: HtmlString.

        :param point_color: Attribute overriding node's color. `See palette definitions <https://graphistry.github.io/docs/legacy/api/0.9.2/api.html#extendedpalette>`_ for values. Based on Color Brewer.
        :type point_color: Integer.

        :param point_size: Attribute overriding node's size. By default, uses the node degree. The visualization will normalize point sizes and adjust dynamically using semantic zoom.
        :type point_size: HtmlString.

        :returns: Plotter.
        :rtype: Plotter.

        **Example: Minimal**
            ::

                import graphistry
                g = graphistry.bind()
                g = g.bind(source='src', destination='dst')

        **Example: Node colors**
            ::

                import graphistry
                g = graphistry.bind()
                g = g.bind(source='src', destination='dst',
                           node='id', point_color='color')

        **Example: Chaining**
            ::

                import graphistry
                g = graphistry.bind(source='src', destination='dst', node='id')

                g1 = g.bind(point_color='color1', point_size='size1')

                g.bind(point_color='color1b')

                g2a = g1.bind(point_color='color2a')
                g2b = g1.bind(point_color='color2b', point_size='size2b')

                g3a = g2a.bind(point_size='size3a')
                g3b = g2b.bind(point_size='size3b')

        In the above **Chaining** example, all bindings use src/dst/id. Colors and sizes bind to:
            ::

                g: default/default
                g1: color1/size1
                g2a: color2a/size1
                g2b: color2b/size2b
                g3a: color2a/size3a
                g3b: color2b/size3b",0,0,1,1
"def bitmask(*args):
    
    mask = 0

    for a in args:
        if type(a) is tuple:
            for b in range(a[1], a[0]+1):
                mask |= 1 << b
        elif type(a) is list:
            for b in a:
                mask |= 1 << b
        elif type(a) is int:
            mask |= 1 << a

    return mask","! @brief Returns a mask with specified bit ranges set.
    
    An integer mask is generated based on the bits and bit ranges specified by the
    arguments. Any number of arguments can be provided. Each argument may be either
    a 2-tuple of integers, a list of integers, or an individual integer. The result
    is the combination of masks produced by the arguments.
    
    - 2-tuple: The tuple is a bit range with the first element being the MSB and the
          second element the LSB. All bits from LSB up to and included MSB are set.
    - list: Each bit position specified by the list elements is set.
    - int: The specified bit position is set.
    
    @return An integer mask value computed from the logical OR'ing of masks generated
      by each argument.
    
    Example:
    @code
      >>> hex(bitmask((23,17),1))
      0xfe0002
      >>> hex(bitmask([4,0,2],(31,24))
      0xff000015
    @endcode",0,0,2,2
"def board(self):
        
        board = []
        if self.flop:
            board.extend(self.flop.cards)
            if self.turn:
                board.append(self.turn)
                if self.river:
                    board.append(self.river)
        return tuple(board) if board else None","Calculates board from flop, turn and river.",0,0,1,1
"def bodc2n(code, lenout=_default_len_out):
    
    code = ctypes.c_int(code)
    name = stypes.stringToCharP("" "" * lenout)
    lenout = ctypes.c_int(lenout)
    found = ctypes.c_int()
    libspice.bodc2n_c(code, lenout, name, ctypes.byref(found))
    return stypes.toPythonString(name), bool(found.value)","Translate the SPICE integer code of a body into a common name
    for that body.

    http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/bodc2n_c.html

    :param code: Integer ID code to be translated into a name.
    :type code: int
    :param lenout: Maximum length of output name.
    :type lenout: int
    :return: A common name for the body identified by code.
    :rtype: str",0,0,1,1
"def bokeh_palette(name, rawtext, text, lineno, inliner, options=None, content=None):
    
    try:
        exec(""palette = %s"" % text, _globals)
    except Exception as e:
        raise SphinxError(""cannot evaluate palette expression '%r', reason: %s"" % (text, e))
    p = _globals.get('palette', None)
    if not isinstance(p, (list, tuple)) or not all(isinstance(x, str) for x in p):
        raise SphinxError(""palette expression '%r' generated invalid or no output: %s"" % (text, p))
    w = 20 if len(p) < 15 else 10 if len(p) < 32 else 5 if len(p) < 64 else 2 if len(p) < 128 else 1
    html = PALETTE_DETAIL.render(palette=p, width=w)
    node = nodes.raw('', html, format=""html"")
    return [node], []","Generate an inline visual representations of a single color palette.

    This function evaluates the expression ``""palette = %s"" % text``, in the
    context of a ``globals`` namespace that has previously imported all of
    ``bokeh.plotting``. The resulting value for ``palette`` is used to
    construct a sequence of HTML ``<span>`` elements for each color.

    If evaluating the palette expression fails or does not produce a list or
    tuple of all strings, then a SphinxError is raised to terminate the build.

    For details on the arguments to this function, consult the Docutils docs:

    http://docutils.sourceforge.net/docs/howto/rst-roles.html#define-the-role-function",3,0,0,3
"def bool_from_exists_clause(session: Session,
                            exists_clause: Exists) -> bool:
      
    if session.get_bind().dialect.name == SqlaDialectName.MSSQL:
        
        result = session.query(literal(True)).filter(exists_clause).scalar()
    else:
        
        result = session.query(exists_clause).scalar()
    return bool(result)","Database dialects are not consistent in how ``EXISTS`` clauses can be
    converted to a boolean answer. This function manages the inconsistencies.

    See:
    
    - https://bitbucket.org/zzzeek/sqlalchemy/issues/3212/misleading-documentation-for-queryexists
    - http://docs.sqlalchemy.org/en/latest/orm/query.html#sqlalchemy.orm.query.Query.exists
    
    Specifically, we want this:
    
    *SQL Server*
    
    .. code-block:: sql
    
        SELECT 1 WHERE EXISTS (SELECT 1 FROM table WHERE ...)
        -- ... giving 1 or None (no rows)
        -- ... fine for SQL Server, but invalid for MySQL (no FROM clause)
        
    *Others, including MySQL*
    
    .. code-block:: sql
    
        SELECT EXISTS (SELECT 1 FROM table WHERE ...)
        -- ... giving 1 or 0
        -- ... fine for MySQL, but invalid syntax for SQL Server",2,0,1,3
"def boolean(cls, true_code, false_code=None):
        
        def func(response):
            if response is not None:
                status_code = response.status
                if status_code == true_code:
                    return True
                if false_code is not None and status_code == false_code:
                    return False
                raise error_for(response)
        return func","Callback to validate a response code.

        The returned callback checks whether a given response has a
        ``status_code`` that is considered good (``true_code``) and
        raise an appropriate error if not.

        The optional ``false_code`` allows for a non-successful status
        code to return False instead of throwing an error. This is used,
        for example in relationship mutation to indicate that the
        relationship was not modified.

        Args:

            true_code(int): The http status code to consider as a success

        Keyword Args:

            false_code(int): The http status code to consider a failure

        Returns:

            A function that given a response returns ``True`` if the
                response's status code matches the given code. Raises
                a :class:`HeliumError` if the response code does not
                match.",1,0,1,2
"def boot_packet(sock, cmd, arg1=0, arg2=0, arg3=0, data=b""""):
    
    PROTOCOL_VERSION = 1

    
    header = struct.pack(""!H4I"", PROTOCOL_VERSION, cmd, arg1, arg2, arg3)

    assert len(data) % 4 == 0  
    fdata = b""""

    
    while len(data) > 0:
        word, data = (data[:4], data[4:])
        fdata += struct.pack(""!I"", struct.unpack(""<I"", word)[0])

    
    sock.send(header + fdata)","Create and transmit a packet to boot the machine.

    Parameters
    ----------
    sock : :py:class:`~socket.socket`
        Connected socket to use to transmit the packet.
    cmd : int
    arg1 : int
    arg2 : int
    arg3 : int
    data : :py:class:`bytes`
        Optional data to include in the packet.",0,1,1,2
"def bootstrap_prompt(prompt_kwargs, group):
    
    prompt_kwargs = prompt_kwargs or {}

    defaults = {
        ""history"": InMemoryHistory(),
        ""completer"": ClickCompleter(group),
        ""message"": u""> "",
    }

    for key in defaults:
        default_value = defaults[key]
        if key not in prompt_kwargs:
            prompt_kwargs[key] = default_value

    return prompt_kwargs","Bootstrap prompt_toolkit kwargs or use user defined values.

    :param prompt_kwargs: The user specified prompt kwargs.",0,0,2,2
"def bsrch(self, domain):
        
        logger = _get_logger(self.debug)
        request = self.exrService.createRequest('ExcelGetGridRequest')
        request.set('Domain', domain)
        logger.info('Sending Request:\n{}'.format(request))
        self._session.sendRequest(request, identity=self._identity)
        data = []
        for msg in self._receive_events(to_dict=False):
            for v in msg.getElement(""DataRecords"").values():
                for f in v.getElement(""DataFields"").values():
                    data.append(f.getElementAsString(""StringValue""))
        return pd.DataFrame(data)","This function uses the Bloomberg API to retrieve 'bsrch' (Bloomberg
        SRCH Data) queries. Returns list of tickers.

        Parameters
        ----------
        domain: string
            A character string with the name of the domain to execute.
            It can be a user defined SRCH screen, commodity screen or
            one of the variety of Bloomberg examples. All domains are in the
            format <domain>:<search_name>. Example ""COMDTY:NGFLOW""

        Returns
        -------
        data: pandas.DataFrame
            List of bloomberg tickers from the BSRCH",0,2,1,3
"def build_authorization_endpoint(self, request, disable_sso=None):
        
        self.load_config()
        redirect_to = request.GET.get(REDIRECT_FIELD_NAME, None)
        if not redirect_to:
            redirect_to = django_settings.LOGIN_REDIRECT_URL
        redirect_to = base64.urlsafe_b64encode(redirect_to.encode()).decode()
        query = QueryDict(mutable=True)
        query.update({
            ""response_type"": ""code"",
            ""client_id"": settings.CLIENT_ID,
            ""resource"": settings.RELYING_PARTY_ID,
            ""redirect_uri"": self.redirect_uri(request),
            ""state"": redirect_to,
        })
        if self._mode == ""openid_connect"":
            query[""scope""] = ""openid""
            if (disable_sso is None and settings.DISABLE_SSO) or disable_sso is True:
                query[""prompt""] = ""login""

        return ""{0}?{1}"".format(self.authorization_endpoint, query.urlencode())","This function returns the ADFS authorization URL.

        Args:
            request(django.http.request.HttpRequest): A django Request object
            disable_sso(bool): Whether to disable single sign-on and force the ADFS server to show a login prompt.

        Returns:
            str: The redirect URI",0,0,3,3
"def build_circles(self, circles):
        
        if not circles:
            return
        if not isinstance(circles, list):
            raise AttributeError('circles accepts only lists')

        for circle in circles:
            if isinstance(circle, dict):
                self.add_circle(**circle)
            elif isinstance(circle, (tuple, list)):
                if len(circle) != 3:
                    raise AttributeError('circle requires center and radius')
                circle_dict = self.build_circle_dict(circle[0],
                                                     circle[1],
                                                     circle[2])
                self.add_circle(**circle_dict)","Process data to construct rectangles

        This method is built from the assumption that the circles parameter
        is a list of:
            lists : a list with 3 elements indicating
                [center_latitude, center_longitude, radius]
            tuples : a tuple with 3 elements indicating
                (center_latitude, center_longitude, radius)
            dicts: a dictionary with circle attributes

        So, for instance, we have this general scenario as a input parameter:
            [[22.345,45.44, 1000],
             (22.345,45.44,200),
             {
            'stroke_color': stroke_color,
            'stroke_opacity': stroke_opacity,
            'stroke_weight': stroke_weight,
            'fill_color': fill_color,
            'fill_opacity': fill_opacity,
            'center': {'lat': center_latitude,
                       'lng': center_longitude,
                       },
            'radius': radius
            }]",2,0,4,6
"def build_collection(df, **kwargs):
    

    print 'Generating the Record Collection...\n'

    df['index_original'] = df.index
    df.reset_index(drop=True, inplace=True)

    if pd.__version__ >= '0.15.0':
        d = df.T.to_dict(orient='series')
    else:
        d = df.T.to_dict(outtype='series')

    collection = [load_record(item, kwargs) for item in d.items()]
    return collection","Generates a list of Record objects given a DataFrame.
    Each Record instance has a series attribute which is a pandas.Series of the same attributes 
    in the DataFrame.
    Optional data can be passed in through kwargs which will be included by the name of each object.

    parameters
    ----------
    df : pandas.DataFrame
    kwargs : alternate arguments to be saved by name to the series of each object

    Returns
    -------
    collection : list
        list of Record objects where each Record represents one row from a dataframe

    Examples
    --------
    This is how we generate a Record Collection from a DataFrame.

    >>> import pandas as pd
    >>> import turntable
    >>>
    >>> df = pd.DataFrame({'Artist':""""""Michael Jackson, Pink Floyd, Whitney Houston, Meat Loaf, 
        Eagles, Fleetwood Mac, Bee Gees, AC/DC"""""".split(', '),
    >>> 'Album' :""""""Thriller, The Dark Side of the Moon, The Bodyguard, Bat Out of Hell, 
        Their Greatest Hits (1971-1975), Rumours, Saturday Night Fever, Back in Black"""""".split(', ')})
    >>> collection = turntable.press.build_collection(df, my_favorite_record = 'nevermind')
    >>> record = collection[0]
    >>> print record.series",0,0,1,1
"def build_directories(root_dir):
    
    
    if not os.path.exists(root_dir):
        os.mkdir(root_dir)

    
    directories = [os.path.join(root_dir, subdir) for subdir in
                   (""output"", ""stderr"", ""stdout"", ""jobs"")]
    for dirname in directories:
        os.makedirs(dirname, exist_ok=True)","Constructs the subdirectories output, stderr, stdout, and jobs in the
    passed root directory. These subdirectories have the following roles:

        jobs             Stores the scripts for each job
        stderr           Stores the stderr output from SGE
        stdout           Stores the stdout output from SGE
        output           Stores output (if the scripts place the output here)

    - root_dir   Path to the top-level directory for creation of subdirectories",1,0,0,1
"def build_docx(path_jinja2, template_name, path_outfile, template_kwargs=None):
    
    latex_template_object = LatexBuild(
            path_jinja2,
            template_name,
            template_kwargs,
            )
    return latex_template_object.build_docx(path_outfile)","Helper function for building a docx file from a latex jinja2 template

    :param path_jinja2: the root directory for latex jinja2 templates
    :param template_name: the relative path, to path_jinja2, to the desired
        jinja2 Latex template
    :param path_outfile: the full path to the desired final output file
        Must contain the same file extension as files generated by
        cmd_wo_infile, otherwise the process will fail
    :param template_kwargs: a dictionary of key/values for jinja2 variables",2,0,0,2
"def build_message(self, checker):
        
        solution = ' (%s)' % checker.solution if self.with_solutions else ''
        return '{} {}{}'.format(checker.code,
                                checker.msg,
                                solution)",Builds the checker's error message to report,0,0,1,1
"def build_message(self, metric, value, timestamp, tags={}):
        
        if not metric or metric.split(None, 1)[0] != metric:
            raise ValueError('""metric"" must not have whitespace in it')
        if not isinstance(value, (int, float)):
            raise TypeError('""value"" must be an int or a float, not a {}'.format(
                type(value).__name__))

        tags_suffix = ''.join(';{}={}'.format(x[0], x[1]) for x in sorted(tags.items()))

        message = u'{}{}{} {} {}\n'.format(
            self.prefix + '.' if self.prefix else '',
            metric,
            tags_suffix,
            value,
            int(round(timestamp))
        )
        message = message.encode('utf-8')
        return message",Build a Graphite message to send and return it as a byte string.,2,0,4,6
"def build_metadata_service(did, metadata, service_endpoint):
        
        return Service(service_endpoint,
                       ServiceTypes.METADATA,
                       values={'metadata': metadata},
                       did=did)","Build a metadata service.

        :param did: DID, str
        :param metadata: conforming to the Metadata accepted by Ocean Protocol, dict
        :param service_endpoint: identifier of the service inside the asset DDO, str
        :return: Service",0,0,1,1
"def build_plane_arrays(x, y, qlist):
    
    if type(qlist) is not list:
        return_list = False
        qlist = [qlist]
    else:
        return_list = True
    xv = x[np.where(y==y[0])[0]]
    yv = y[np.where(x==x[0])[0]]
    qlistp = []
    for n in range(len(qlist)):
        qlistp.append(np.zeros((len(yv), len(xv))))
    for j in range(len(qlist)):
        for n in range(len(yv)):
            i = np.where(y==yv[n])[0]
            qlistp[j][n,:] = qlist[j][i]
    if not return_list:
        qlistp = qlistp[0]
    return xv, yv, qlistp","Build a 2-D array out of data taken in the same plane, for contour
    plotting.",0,0,1,1
"def build_response(self, req, resp):
        
        response = Response()

        
        response.status_code = getattr(resp, 'status', None)

        
        response.headers = CaseInsensitiveDict(getattr(resp, 'headers', {}))

        
        response.encoding = get_encoding_from_headers(response.headers)
        response.raw = resp
        response.reason = response.raw.reason

        if isinstance(req.url, bytes):
            response.url = req.url.decode('utf-8')
        else:
            response.url = req.url

        
        extract_cookies_to_jar(response.cookies, req, resp)

        
        response.request = req
        response.connection = self

        return response","Builds a :class:`Response <requests.Response>` object from a urllib3
        response. This should not be called from user code, and is only exposed
        for use when subclassing the
        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`

        :param req: The :class:`PreparedRequest <PreparedRequest>` used to generate the response.
        :param resp: The urllib3 response object.
        :rtype: requests.Response",0,0,1,1
"def build_riskinputs(self, kind):
        
        logging.info('Building risk inputs from %d realization(s)', self.R)
        imtls = self.oqparam.imtls
        if not set(self.oqparam.risk_imtls) & set(imtls):
            rsk = ', '.join(self.oqparam.risk_imtls)
            haz = ', '.join(imtls)
            raise ValueError('The IMTs in the risk models (%s) are disjoint '
                             ""from the IMTs in the hazard (%s)"" % (rsk, haz))
        self.riskmodel.taxonomy = self.assetcol.tagcol.taxonomy
        with self.monitor('building riskinputs', autoflush=True):
            riskinputs = list(self._gen_riskinputs(kind))
        assert riskinputs
        logging.info('Built %d risk inputs', len(riskinputs))
        return riskinputs",":param kind:
            kind of hazard getter, can be 'poe' or 'gmf'
        :returns:
            a list of RiskInputs objects, sorted by IMT.",1,2,3,6
"def build_tree(X, y, criterion, max_depth, current_depth=1):
    

    
    if max_depth >= 0 and current_depth >= max_depth:
        return Leaf(y)

    
    gain, question = find_best_question(X, y, criterion)
    if gain == 0:
        return Leaf(y)

    
    true_X, false_X, true_y, false_y = split(X, y, question)

    
    true_branch = build_tree(
        true_X, true_y,
        criterion,
        max_depth,
        current_depth=current_depth+1
    )
    
    
    false_branch = build_tree(
        false_X, false_y,
        criterion,
        max_depth,
        current_depth=current_depth+1
    )

    
    return Node(
        question=question,
        true_branch=true_branch,
        false_branch=false_branch
    )",Builds the decision tree.,0,0,1,1
"def build_url(host, path, params=None):
    
    path = quote(path)
    params = params or {}

    if params:
        path = '/{}?{}'.format(path, urlencode(params))
    else:
        path = '/{}'.format(path)

    if not host.startswith(http.URL_SCHEME):
        host = '{}{}'.format(http.URL_SCHEME, host)

    return urljoin(host, path)","Build a URL.

    This method encodes the parameters and adds them
    to the end of the base URL, then adds scheme and hostname.

    Parameters
        host (str)
            Base URL of the Uber Server that handles API calls.
        path (str)
            Target path to add to the host (e.g. 'v1.2/products').
        params (dict)
            Optional dictionary of parameters to add to the URL.

    Returns
        (str)
            The fully formed URL.",0,0,1,1
"def build_url(self):
        
        url = '{protocol}/{url}/{rest}/{version}/{restapi}/{rscpath}/' \
              '{query}'.format(protocol=self.schema.protocol,
                               url=self.schema.main_url,
                               rest=self.schema.rest,
                               version=self.schema.version,
                               restapi=self.schema.restApi,
                               rscpath=self.schema.resourcePath,
                               query=self.schema.query)
        return url.replace('/None/', '/')","Builds the URL for elevations API services based on the data given
        by the user.

        Returns:
            url (str): URL for the elevations API services",0,0,1,1
"def build_verified_certificate_chain(self, received_certificate_chain: List[Certificate]) -> List[Certificate]:
        
        
        if not self._is_certificate_chain_order_valid(received_certificate_chain):
            raise InvalidCertificateChainOrderError()

        
        verified_certificate_chain = []
        anchor_cert = None
        
        for cert in received_certificate_chain:
            anchor_cert = self._get_certificate_with_subject(cert.issuer)
            verified_certificate_chain.append(cert)
            if anchor_cert:
                verified_certificate_chain.append(anchor_cert)
                break

        if anchor_cert is None:
            
            raise AnchorCertificateNotInTrustStoreError()

        return verified_certificate_chain","Try to figure out the verified chain by finding the anchor/root CA the received chain chains up to in the
        trust store.

        This will not clean the certificate chain if additional/invalid certificates were sent and the signatures and
        fields (notBefore, etc.) are not verified.",2,0,3,5
"def build_yt_api():
    
    data = datatools.get_data()
    if ""google_api_key"" not in data[""discord""][""keys""]:
        logger.warning(""No API key found with name 'google_api_key'"")
        logger.info(""Please add your Google API key with name 'google_api_key' ""
                    ""in data.json to use YouTube features of the music module"")
        return False

    logger.debug(""Building YouTube discovery API"")
    ytdevkey = data[""discord""][""keys""][""google_api_key""]

    try:
        global ytdiscoveryapi
        ytdiscoveryapi = googleapiclient.discovery.build(""youtube"", ""v3"", developerKey=ytdevkey)
        logger.debug(""YouTube API build successful"")
        return True
    except Exception as e:
        logger.exception(e)
        logger.warning(""HTTP error connecting to YouTube API, YouTube won't be available"")
        return False",Build the YouTube API for future use,0,5,0,5
"def bulk_create_posts(self, posts, post_categories, post_tags, post_media_attachments):
        
        Post.objects.bulk_create(posts)

        
        for post_wp_id, categories in six.iteritems(post_categories):
            Post.objects.get(site_id=self.site_id, wp_id=post_wp_id).categories.add(*categories)

        for post_id, tags in six.iteritems(post_tags):
            Post.objects.get(site_id=self.site_id, wp_id=post_id).tags.add(*tags)

        for post_id, attachments in six.iteritems(post_media_attachments):
            Post.objects.get(site_id=self.site_id, wp_id=post_id).attachments.add(*attachments)","Actually do a db bulk creation of posts, and link up the many-to-many fields

        :param posts: the list of Post objects to bulk create
        :param post_categories: a mapping of Categories to add to newly created Posts
        :param post_tags: a mapping of Tags to add to newly created Posts
        :param post_media_attachments: a mapping of Medias to add to newly created Posts
        :return: None",4,0,0,4
"def bulk_insert_extras(dialect_name: str,
                       fileobj: TextIO,
                       start: bool) -> None:
    
    lines = []
    if dialect_name == SqlaDialectName.MYSQL:
        if start:
            lines = [
                ""SET autocommit=0;"",
                ""SET unique_checks=0;"",
                ""SET foreign_key_checks=0;"",
            ]
        else:
            lines = [
                ""SET foreign_key_checks=1;"",
                ""SET unique_checks=1;"",
                ""COMMIT;"",
            ]
    writelines_nl(fileobj, lines)","Writes bulk ``INSERT`` preamble (start=True) or end (start=False).

    For MySQL, this temporarily switches off autocommit behaviour and index/FK
    checks, for speed, then re-enables them at the end and commits.

    Args:
        dialect_name: SQLAlchemy dialect name (see :class:`SqlaDialectName`)
        fileobj: file-like object to write to
        start: if ``True``, write preamble; if ``False``, write end",1,0,1,2
"def bundle_view(parser, token):
    

    bits = token.split_contents()
    if len(bits) < 3:
        raise TemplateSyntaxError(""'%s' takes at least two arguments""
                                  "" bundle and view_name"" % bits[0])

    bundle = parser.compile_filter(bits[1])
    viewname = parser.compile_filter(bits[2])

    asvar = None
    bits = bits[2:]
    if len(bits) >= 2 and bits[-2] == 'as':
        asvar = bits[-1]
        bits = bits[:-2]

    return ViewNode(bundle, viewname, asvar)","Returns an string version of a bundle view. This is done by
    calling the `get_string_from_view` method of the provided bundle.

    This tag expects that the request object as well as the
    the original url_params are available in the context.

    Requires two arguments bundle and the name of the view
    you want to render. In addition, this tag also accepts
    the 'as xxx' syntax.

    Example:

    {% bundle_url bundle main_list as html %}",1,0,2,3
"def by_name(self, country, language=""en""):
        
        with override(language):
            for code, name in self:
                if name.lower() == country.lower():
                    return code
                if code in self.OLD_NAMES:
                    for old_name in self.OLD_NAMES[code]:
                        if old_name.lower() == country.lower():
                            return code
        return """"","Fetch a country's ISO3166-1 two letter country code from its name.

        An optional language parameter is also available.
        Warning: This depends on the quality of the available translations.

        If no match is found, returns an empty string.

        ..warning:: Be cautious about relying on this returning a country code
            (especially with any hard-coded string) since the ISO names of
            countries may change over time.",0,0,1,1
"def cache_clean(path=None, runas=None, env=None, force=False):
    
    env = env or {}

    if runas:
        uid = salt.utils.user.get_uid(runas)
        if uid:
            env.update({'SUDO_UID': uid, 'SUDO_USER': ''})

    cmd = ['npm', 'cache', 'clean']
    if path:
        cmd.append(path)
    if force is True:
        cmd.append('--force')

    cmd = ' '.join(cmd)
    result = __salt__['cmd.run_all'](
        cmd, cwd=None, runas=runas, env=env, python_shell=True, ignore_retcode=True)

    if result['retcode'] != 0:
        log.error(result['stderr'])
        return False
    return True","Clean cached NPM packages.

    If no path for a specific package is provided the entire cache will be cleared.

    path
        The cache subpath to delete, or None to clear the entire cache

    runas
        The user to run NPM with

    env
        Environment variables to set when invoking npm. Uses the same ``env``
        format as the :py:func:`cmd.run <salt.modules.cmdmod.run>` execution
        function.

    force
        Force cleaning of cache.  Required for npm@5 and greater

        .. versionadded:: 2016.11.6

    CLI Example:

    .. code-block:: bash

        salt '*' npm.cache_clean force=True",0,3,1,4
"def calc_next_run(self):
        
        base_time = self.last_run
        if self.last_run == HAS_NOT_RUN:
            if self.wait_for_schedule is False:
                self.next_run = timezone.now()
                self.wait_for_schedule = False 
                self.save()
                return
            else:
                base_time = timezone.now()
        self.next_run = croniter(self.schedule, base_time).get_next(datetime)
        self.save()",Calculate next run time of this task,0,0,1,1
"def calc_resize_factor(prediction, image_size):
    
    resize_factor_x = prediction.shape[1] / float(image_size[1])
    resize_factor_y = prediction.shape[0] / float(image_size[0])
    if abs(resize_factor_x - resize_factor_y) > 1.0/image_size[1] :
        raise RuntimeError(
                              %(resize_factor_y, resize_factor_x))
    return (resize_factor_y, resize_factor_x)",Calculates how much prediction.shape and image_size differ.,1,0,2,3
"def calculate(self, T, P, zs, ws, method):
        r
        if method == MIXING_LOG_MOLAR:
            mus = [i(T, P) for i in self.ViscosityLiquids]
            return mixing_logarithmic(zs, mus)
        elif method == MIXING_LOG_MASS:
            mus = [i(T, P) for i in self.ViscosityLiquids]
            return mixing_logarithmic(ws, mus)
        elif method == LALIBERTE_MU:
            ws = list(ws) ; ws.pop(self.index_w)
            return Laliberte_viscosity(T, ws, self.wCASs)
        else:
            raise Exception('Method not valid')","r'''Method to calculate viscosity of a liquid mixture at 
        temperature `T`, pressure `P`, mole fractions `zs` and weight fractions
        `ws` with a given method.

        This method has no exception handling; see `mixture_property`
        for that.

        Parameters
        ----------
        T : float
            Temperature at which to calculate the property, [K]
        P : float
            Pressure at which to calculate the property, [Pa]
        zs : list[float]
            Mole fractions of all species in the mixture, [-]
        ws : list[float]
            Weight fractions of all species in the mixture, [-]
        method : str
            Name of the method to use

        Returns
        -------
        mu : float
            Viscosity of the liquid mixture, [Pa*s]",1,0,2,3
"def calculateProbableRootOfGeneTree(speciesTree, geneTree, processID=lambda x : x):
    
    
    
    
    if geneTree.traversalID.midEnd <= 3:
        return (0, 0, geneTree)
    checkGeneTreeMatchesSpeciesTree(speciesTree, geneTree, processID)
    l = []
    def fn(tree):
        if tree.traversalID.mid != geneTree.left.traversalID.mid and tree.traversalID.mid != geneTree.right.traversalID.mid:
            newGeneTree = moveRoot(geneTree, tree.traversalID.mid)
            binaryTree_depthFirstNumbers(newGeneTree)
            dupCount, lossCount = calculateDupsAndLossesByReconcilingTrees(speciesTree, newGeneTree, processID)
            l.append((dupCount, lossCount, newGeneTree))
        if tree.internal:
            fn(tree.left)
            fn(tree.right)
    fn(geneTree)
    l.sort()
    return l[0][2], l[0][0], l[0][1]","Goes through each root possible branch making it the root. 
    Returns tree that requires the minimum number of duplications.",0,0,2,2
"def calculate_signatures(self):
        
        if not self.signing_algorithm:
            return []

        algo_id = {'sha1': 1, 'sha384': 2}[self.signing_algorithm]
        hashers = [(algo_id, make_hasher(algo_id))]
        for block in get_signature_data(self.fileobj, self.filesize):
            [h.update(block) for (_, h) in hashers]

        signatures = [(algo_id, sign_hash(self.signing_key, h.finalize(), h.algorithm.name)) for (algo_id, h) in hashers]
        return signatures","Calculate the signatures for this MAR file.

        Returns:
            A list of signature tuples: [(algorithm_id, signature_data), ...]",0,0,1,1
"def calculate_size(credentials, uuid, owner_uuid, is_owner_connection, client_type, serialization_version, client_hazelcast_version):
    
    data_size = 0
    data_size += calculate_size_data(credentials)
    data_size += BOOLEAN_SIZE_IN_BYTES
    if uuid is not None:
        data_size += calculate_size_str(uuid)
    data_size += BOOLEAN_SIZE_IN_BYTES
    if owner_uuid is not None:
        data_size += calculate_size_str(owner_uuid)
    data_size += BOOLEAN_SIZE_IN_BYTES
    data_size += calculate_size_str(client_type)
    data_size += BYTE_SIZE_IN_BYTES
    data_size += calculate_size_str(client_hazelcast_version)
    return data_size",Calculates the request payload size,0,0,1,1
"def calculate_size(name, start_sequence, min_count, max_count, filter):
    
    data_size = 0
    data_size += calculate_size_str(name)
    data_size += LONG_SIZE_IN_BYTES
    data_size += INT_SIZE_IN_BYTES
    data_size += INT_SIZE_IN_BYTES
    data_size += BOOLEAN_SIZE_IN_BYTES
    if filter is not None:
        data_size += calculate_size_data(filter)
    return data_size",Calculates the request payload size,0,0,1,1
"def calculate_windows(self, **kwargs):
        
        windows = find_windows(self.elements, self.coordinates, **kwargs)
        if windows:
            self.properties.update(
                {
                    'windows': {
                        'diameters': windows[0], 'centre_of_mass': windows[1],
                    }
                }
            )
            return windows[0]
        else:
            self.properties.update(
                {'windows': {'diameters': None,  'centre_of_mass': None, }}
            )
        return None","Return the diameters of all windows in a molecule.

        This function first finds and then measures the diameters of all the
        window in the molecule.

        Returns
        -------
        :class:`numpy.array`
            An array of windows' diameters.

        :class:`NoneType`
            If no windows were found.",0,0,2,2
"def call(self):
        
        data = self._downloader.download()

        
        if sys.version >= ""2.7"":
            data = data.decode(""utf-8"", errors=""ignore"")
        else:
            data = data.decode(""utf-8"")

        self.update(json.loads(data))
        self._fetched = True",Make the API call again and fetch fresh data.,0,1,1,2
"def call(self, method, params=None):
        
        start_time = time()
        self.check_auth()
        self.log(INFO, '[%s-%05d] Calling Zabbix API method ""%s""', start_time, self.id, method)
        self.log(DEBUG, '\twith parameters: %s', params)

        try:
            return self.do_request(self.json_obj(method, params=params))
        except ZabbixAPIError as ex:
            if self.relogin_interval and any(i in ex.error['data'] for i in self.LOGIN_ERRORS):
                self.log(WARNING, 'Zabbix API not logged in (%s). Performing Zabbix API relogin', ex)
                self.relogin()  
                return self.do_request(self.json_obj(method, params=params))
            raise  
        finally:
            self.log(INFO, '[%s-%05d] Zabbix API method ""%s"" finished in %g seconds',
                     start_time, self.id, method, (time() - start_time))",Check authentication and perform actual API request and relogin if needed,0,5,1,6
"def call_cmd(cmdlist, stdin=None):
    
    termenc = urwid.util.detected_encoding
    if isinstance(stdin, str):
        stdin = stdin.encode(termenc)
    try:

        logging.debug(""Calling %s"" % cmdlist)
        proc = subprocess.Popen(
            cmdlist,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            stdin=subprocess.PIPE if stdin is not None else None)
    except OSError as e:
        out = b''
        err = e.strerror
        ret = e.errno
    else:
        out, err = proc.communicate(stdin)
        ret = proc.returncode

    out = string_decode(out, termenc)
    err = string_decode(err, termenc)
    return out, err, ret","get a shell commands output, error message and return value and immediately
    return.

    .. warning::

        This returns with the first screen content for interactive commands.

    :param cmdlist: shellcommand to call, already splitted into a list accepted
                    by :meth:`subprocess.Popen`
    :type cmdlist: list of str
    :param stdin: string to pipe to the process
    :type stdin: str, bytes, or None
    :return: triple of stdout, stderr, return value of the shell command
    :rtype: str, str, int",0,2,1,3
"def camelcase_search_options(self, options):
        
        new_options = {}
        for key in options:
            value = options[key]
            new_key = SEARCH_OPTIONS_DICT.get(key, key)
            if new_key == 'sort':
                value = SORT_OPTIONS_DICT.get(value, value)
            elif new_key == 'timePivot':
                value = TIME_PIVOT_OPTIONS_DICT.get(value, value)
            elif new_key in BOOLEAN_SEARCH_OPTIONS:
                value = str(value).lower()
            new_options[new_key] = value
        return new_options",change all underscored variants back to what the API is expecting,0,0,1,1
"def cancelTickByTickData(self, contract: Contract, tickType: str):
        
        ticker = self.ticker(contract)
        reqId = self.wrapper.endTicker(ticker, tickType)
        if reqId:
            self.client.cancelTickByTickData(reqId)
        else:
            self._logger.error(
                f'cancelMktData: No reqId found for contract {contract}')","Unsubscribe from tick-by-tick data

        Args:
            contract: The exact contract object that was used to
                subscribe with.",0,2,0,2
"def canonical_averages_dtype(spanning_cluster=True):
    
    fields = list()
    fields.extend([
        ('number_of_runs', 'uint32'),
    ])
    if spanning_cluster:
        fields.extend([
            ('percolation_probability_mean', 'float64'),
            ('percolation_probability_m2', 'float64'),
        ])
    fields.extend([
        ('max_cluster_size_mean', 'float64'),
        ('max_cluster_size_m2', 'float64'),
        ('moments_mean', '(5,)float64'),
        ('moments_m2', '(5,)float64'),
    ])
    return _ndarray_dtype(fields)","The NumPy Structured Array type for canonical averages over several
    runs

    Helper function

    Parameters
    ----------
    spanning_cluster : bool, optional
        Whether to detect a spanning cluster or not.
        Defaults to ``True``.

    Returns
    -------
    ret : list of pairs of str
        A list of tuples of field names and data types to be used as ``dtype``
        argument in numpy ndarray constructors

    See Also
    --------
    http://docs.scipy.org/doc/numpy/user/basics.rec.html
    canonical_statistics_dtype
    finalized_canonical_averages_dtype",0,0,1,1
"def cdefs(features):
    

    code = ''

    
    is_64bits = sys.maxsize > 2 ** 32

    
    
    if is_64bits:
        code += 
    else:
        code += 

    code += 

    
    if _at_least(features, 8, 4):
        code += 
    else:
        code += 

    code += 

    
    if _at_least(features, 8, 4):
        code += 
    else:
        code += 

    code += 

    if _at_least(features, 8, 5):
        code += 

    if _at_least(features, 8, 6):
        code += 

    if _at_least(features, 8, 7):
        code += 

    if _at_least(features, 8, 8):
        code += 

    
    
    if features['api']:
        code += 

    
    code += '
    code += '
    code += '

    
    for key, value in features.items():
        code += '//%s = %s\n' % (key, value)

    return code","Return the C API declarations for libvips.

    features is a dict with the features we want. Some features were only
    added in later libvips, for example, and some need to be disabled in
    some FFI modes.",0,0,1,1
"def challenges(self):
        
        if self._challenges is None:
            self._challenges = ChallengeList(
                self._version,
                service_sid=self._solution['service_sid'],
                identity=self._solution['identity'],
                factor_sid=self._solution['sid'],
            )
        return self._challenges","Access the challenges

        :returns: twilio.rest.authy.v1.service.entity.factor.challenge.ChallengeList
        :rtype: twilio.rest.authy.v1.service.entity.factor.challenge.ChallengeList",0,0,1,1
"def change_disk_usage(self, usage_change, file_path, st_dev):
        
        mount_point = self._mount_point_for_device(st_dev)
        if mount_point:
            total_size = mount_point['total_size']
            if total_size is not None:
                if total_size - mount_point['used_size'] < usage_change:
                    self.raise_io_error(errno.ENOSPC, file_path)
            mount_point['used_size'] += usage_change","Change the used disk space by the given amount.

        Args:
            usage_change: Number of bytes added to the used space.
                If negative, the used space will be decreased.

            file_path: The path of the object needing the disk space.

            st_dev: The device ID for the respective file system.

        Raises:
            IOError: if usage_change exceeds the free file system space",1,0,2,3
"def change_password(self, user, new_password):
        
        from boiler.user.models import UpdateSchema
        from flask_login import logout_user

        schema = UpdateSchema()
        user.password = new_password
        user.password_link=None
        user.password_link_expires=None
        valid = schema.validate(user)

        if not valid:
            return valid

        db.session.add(user)
        db.session.commit()

        
        if has_request_context():
            logout_user()

        events.password_changed_event.send(user)
        return user",Change user password and logout,2,2,0,4
"def channel(self, channel_id=None, auto_encode_decode=True):
        
        try:
            return self.channels[channel_id]
        except KeyError:
            return self.Channel(self, channel_id,
                                auto_encode_decode=auto_encode_decode)","Fetch a Channel object identified by the numeric channel_id, or
        create that object if it doesn't already exist. See Channel for meaning
        of auto_encode_decode. If the channel already exists, the auto_* flag
        will not be updated.",1,0,3,4
"def charge(self, code, each_amount, quantity=1, description=None):
        
        each_amount = Decimal(each_amount)
        each_amount = each_amount.quantize(Decimal('.01'))
        data = {
            'chargeCode': code,
            'eachAmount': '%.2f' % each_amount,
            'quantity': quantity,
        }
        if description:
            data['description'] = description
        
        response = self.product.client.make_request(
            path='customers/add-charge',
            params={'code': self.code},
            data=data,
        )
        return self.load_data_from_xml(response.content)","Add an arbitrary charge or credit to a customer's account.  A positive
        number will create a charge.  A negative number will create a credit.
        
        each_amount is normalized to a Decimal with a precision of 2 as that
        is the level of precision which the cheddar API supports.",1,1,1,3
"def charts_get(self, **kwargs):
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('callback'):
            return self.charts_get_with_http_info(**kwargs)
        else:
            (data) = self.charts_get_with_http_info(**kwargs)
            return data","Charts
        Returns a list of Charts, ordered by creation date (newest first).  A Chart is chosen by Pollster editors. One example is \""Obama job approval - Democrats\"". It is always based upon a single Question.  Users should strongly consider basing their analysis on Questions instead. Charts are derived data; Pollster editors publish them and change them as editorial priorities change. 

        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please define a `callback` function
        to be invoked when receiving the response.
        >>> def callback_function(response):
        >>>     pprint(response)
        >>>
        >>> thread = api.charts_get(callback=callback_function)

        :param callback function: The callback function
            for asynchronous request. (optional)
        :param str cursor: Special string to index into the Array
        :param str tags: Comma-separated list of tag slugs. Only Charts with one or more of these tags and Charts based on Questions with one or more of these tags will be returned.
        :param date election_date: Date of an election, in YYYY-MM-DD format. Only Charts based on Questions pertaining to an election on this date will be returned.
        :return: InlineResponse200
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def checkArgs(args):
    
    
    if not os.path.isfile(args.mds):
        msg = ""{}: no such file"".format(args.mds)
        raise ProgramError(msg)

    if not os.path.isfile(args.population_file):
        msg = ""{}: no such file"".format(args.population_file)
        raise ProgramError(msg)

    
    if args.xaxis == args.yaxis:
        msg = ""xaxis must be different than yaxis""
        raise ProgramError(msg)

    return True","Checks the arguments and options.

    :param args: a :py:class:`argparse.Namespace` object containing the options
                 of the program.

    :type args: argparse.Namespace

    :returns: ``True`` if everything was OK.

    If there is a problem with an option, an exception is raised using the
    :py:class:`ProgramError` class, a message is printed to the
    :class:`sys.stderr` and the program exists with code 1.",1,0,3,4
"def check_altitude_units(self, ds):
        
        if 'z' in ds.variables:
            msgs = []
            val = 'units' in ds.variables['z'].ncattrs()
            if not val:
                msgs.append(""Variable 'z' has no units attr"")
            return Result(BaseCheck.LOW, val, 'Altitude Units', msgs)

        return Result(BaseCheck.LOW, (0, 0), 'Altitude Units', [""Dataset has no 'z' variable""])","If there's a variable named z, it must have units.

        @TODO: this is duplicated with check_variable_units
        :param netCDF4.Dataset ds: An open netCDF dataset",0,0,1,1
"def check_boundary_lines_similar(l_1, l_2):
    
    num_matches = 0
    if (type(l_1) != list) or (type(l_2) != list) or (len(l_1) != len(l_2)):
        
        return 0

    num_elements = len(l_1)
    for i in xrange(0, num_elements):
        if l_1[i].isdigit() and l_2[i].isdigit():
            
            num_matches += 1
        else:
            l1_str = l_1[i].lower()
            l2_str = l_2[i].lower()
            if (l1_str[0] == l2_str[0]) and \
                    (l1_str[len(l1_str) - 1] == l2_str[len(l2_str) - 1]):
                num_matches = num_matches + 1
    if (len(l_1) == 0) or (float(num_matches) / float(len(l_1)) < 0.9):
        return 0
    else:
        return 1","Compare two lists to see if their elements are roughly the same.
    @param l_1: (list) of strings.
    @param l_2: (list) of strings.
    @return: (int) 1/0.",0,0,1,1
"def check_coordinate_types(self, ds):
        
        ret_val = []

        for variable in ds.get_variables_by_attributes(axis=lambda x: x is not None):
            name = variable.name
            
            
            
            if cfutil.is_compression_coordinate(ds, name):
                continue

            variable = ds.variables[name]
            
            
            if hasattr(variable, 'cf_role'):
                continue

            
            
            
            if variable.dtype.char == 'S':
                continue

            axis = getattr(variable, 'axis', None)

            if axis is not None:
                valid_axis = self._check_axis(ds, name)
                ret_val.append(valid_axis)

        return ret_val","Check the axis attribute of coordinate variables

        CF §4 The attribute axis may be attached to a coordinate variable and
        given one of the values X, Y, Z or T which stand for a longitude,
        latitude, vertical, or time axis respectively. Alternatively the
        standard_name attribute may be used for direct identification.

        :param netCDF4.Dataset ds: An open netCDF dataset
        :rtype: list
        :return: List of results",0,0,1,1
"def check_dict_expected_keys(self, expected_keys, current, dict_name):
        
        if not isinstance(current, dict):
            raise ParseError(u""'{}' key must be a dict"".format(dict_name),
                             YAML_EXAMPLE)
        expected_keys = set(expected_keys)
        current_keys = {key for key in current}
        extra_keys = current_keys - expected_keys
        if extra_keys:
            message = u""{}: the keys {} are unexpected. (allowed keys: {})""
            raise ParseError(
                message.format(
                    dict_name,
                    list(extra_keys),
                    list(expected_keys),
                ),
                YAML_EXAMPLE,
            )","Check that we don't have unknown keys in a dictionary.

        It does not raise an error if we have less keys than expected.",2,0,3,5
"def check_ensembl_api_version(self):
        
        
        self.attempt = 0
        headers = {""content-type"": ""application/json""}
        ext = ""/info/rest""
        r = self.ensembl_request(ext, headers)
        
        response = json.loads(r)
        self.cache.set_ensembl_api_version(response[""release""])","check the ensembl api version matches a currently working version
        
        This function is included so when the api version changes, we notice the
        change, and we can manually check the responses for the new version.",0,1,0,1
"def check_expected_future_model_list_is_empty(target_state_m, msg, delete=True, with_logger=None):
    
    if with_logger is None:
        with_logger = logger
    
    if target_state_m.expected_future_models:
        with_logger.warning(""{0} -> still in are: {1} Please inform the developer how to reproduce this.""
                            """".format(msg, target_state_m.expected_future_models))
        if delete:
            
            target_state_m.expected_future_models.clear()
        return False
    return True","Checks if the expected future models list/set is empty

    Return False if there are still elements in and also creates a warning message as feedback.

    :param StateModel target_state_m: The state model which expected_future_models attribute should be checked
    :param str msg: Message for the logger if a model is still in.
    :param bool delete: Flag to delete respective model from list/set.
    :param with_logger: A optional logger to use in case of logging messages
    :rtype: bool
    :return: True if empty and False if still model in set/list",0,1,4,5
"def check_for_old_config(ipython_dir=None):
    
    if ipython_dir is None:
        ipython_dir = get_ipython_dir()

    old_configs = ['ipy_user_conf.py', 'ipythonrc', 'ipython_config.py']
    warned = False
    for cfg in old_configs:
        f = os.path.join(ipython_dir, cfg)
        if os.path.exists(f):
            if filehash(f) == old_config_md5.get(cfg, ''):
                os.unlink(f)
            else:
                warnings.warn(""Found old IPython config file %r (modified by user)""%f)
                warned = True

    if warned:
        warnings.warn()","Check for old config files, and present a warning if they exist.

    A link to the docs of the new config is included in the message.

    This should mitigate confusion with the transition to the new
    config system in 0.11.",0,0,2,2
"def check_for_rate_limiting(response, response_lambda, timeout=1, attempts=0):
    
    if attempts >= 3:
        raise RateLimitingException()
    if response.status_code == 429:
        sleep(timeout)
        new_timeout = timeout + 1
        new_attempts = attempts + 1
        return check_for_rate_limiting(response_lambda(timeout, attempts), response_lambda, timeout=new_timeout, attempts=new_attempts)
    return response","Takes an initial response, and a way to repeat the request that produced it and retries the request with an 
increasing sleep period between requests if rate limiting resposne codes are encountered.

    If more than 3 attempts are made, a RateLimitingException is raised

    :param response: A response from Citrination
    :type response: requests.Response
    :param response_lambda: a callable that runs the request that returned the
        response
    :type response_lambda: function
    :param timeout: the time to wait before retrying
    :type timeout: int
    :param attempts: the number of the retry being executed
    :type attempts: int",1,1,2,4
"def check_for_required_columns(
    problems: List, table: str, df: DataFrame
) -> List:
    
    r = cs.GTFS_REF
    req_columns = r.loc[
        (r[""table""] == table) & r[""column_required""], ""column""
    ].values
    for col in req_columns:
        if col not in df.columns:
            problems.append([""error"", f""Missing column {col}"", table, []])

    return problems","Check that the given GTFS table has the required columns.

    Parameters
    ----------
    problems : list
        A four-tuple containing

        1. A problem type (string) equal to ``'error'`` or ``'warning'``;
           ``'error'`` means the GTFS is violated;
           ``'warning'`` means there is a problem but it is not a
           GTFS violation
        2. A message (string) that describes the problem
        3. A GTFS table name, e.g. ``'routes'``, in which the problem
           occurs
        4. A list of rows (integers) of the table's DataFrame where the
           problem occurs

    table : string
        Name of a GTFS table
    df : DataFrame
        The GTFS table corresponding to ``table``

    Returns
    -------
    list
        The ``problems`` list extended as follows.
        Check that the DataFrame contains the colums required by GTFS
        and append to the problems list one error for each column
        missing.",0,0,1,1
"def check_for_required_columns(problems, table, df):
    
    r = cs.PROTOFEED_REF
    req_columns = r.loc[(r['table'] == table) & r['column_required'],
      'column'].values
    for col in req_columns:
        if col not in df.columns:
            problems.append(['error', 'Missing column {!s}'.format(col),
              table, []])

    return problems","Check that the given ProtoFeed table has the required columns.

    Parameters
    ----------
    problems : list
        A four-tuple containing

        1. A problem type (string) equal to ``'error'`` or ``'warning'``;
           ``'error'`` means the ProtoFeed is violated;
           ``'warning'`` means there is a problem but it is not a
           ProtoFeed violation
        2. A message (string) that describes the problem
        3. A ProtoFeed table name, e.g. ``'meta'``, in which the problem
           occurs
        4. A list of rows (integers) of the table's DataFrame where the
           problem occurs

    table : string
        Name of a ProtoFeed table
    df : DataFrame
        The ProtoFeed table corresponding to ``table``

    Returns
    -------
    list
        The ``problems`` list extended as follows.
        Check that the DataFrame contains the colums required by
        the ProtoFeed spec
        and append to the problems list one error for each column
        missing.",1,0,2,3
"def check_hosted_service_name_availability(self, service_name):
        
        _validate_not_none('service_name', service_name)
        return self._perform_get(
            '/' + self.subscription_id +
            '/services/hostedservices/operations/isavailable/' +
            _str(service_name) + '',
            AvailabilityResponse)","Checks to see if the specified hosted service name is available, or if
        it has already been taken.

        service_name:
            Name of the hosted service.",0,1,0,1
"def check_install_build_global(options, check_options=None):
    
    
    if check_options is None:
        check_options = options

    def getname(n):
        return getattr(check_options, n, None)
    names = [""build_options"", ""global_options"", ""install_options""]
    if any(map(getname, names)):
        control = options.format_control
        control.disallow_binaries()
        warnings.warn(
            'Disabling all use of wheels due to the use of --build-options '
            '/ --global-options / --install-options.', stacklevel=2,
        )","Disable wheels if per-setup.py call options are set.

    :param options: The OptionParser options to update.
    :param check_options: The options to check, if not supplied defaults to
        options.",0,0,3,3
"def check_key_cert_match(keyfile, certfile):
    
    key_modulus = subprocess.check_output(
        'openssl rsa -noout -modulus -in {}'.format(keyfile),
        shell=True)
    cert_modulus = subprocess.check_output(
        'openssl x509 -noout -modulus -in {}'.format(certfile),
        shell=True)
    return key_modulus == cert_modulus","check if the ssl key matches the certificate
    :param keyfile: file path to the ssl key
    :param certfile: file path to the ssl certificate
    :returns: true or false",0,0,1,1
"def check_misc(text):
    
    err = ""mixed_metaphors.misc.misc""
    msg = u""Mixed metaphor. Try '{}'.""

    preferences = [

        [""cream rises to the top"",    [""cream rises to the crop""]],
        [""fasten your seatbelts"",     [""button your seatbelts""]],
        [""a minute to decompress"",    [""a minute to decompose""]],
        [""sharpest tool in the shed"", [""sharpest marble in the (shed|box)""]],
        [""not rocket science"",        [""not rocket surgery""]],
    ]

    return preferred_forms_check(text, preferences, err, msg)","Avoid mixing metaphors.

    source:     Garner's Modern American Usage
    source_url: http://bit.ly/1T4alrY",0,0,1,1
"def check_num_tasks(chain, task_count):
    
    errors = []
    
    
    min_decision_tasks = 1
    if task_count['decision'] < min_decision_tasks:
        errors.append(""{} decision tasks; we must have at least {}!"".format(
            task_count['decision'], min_decision_tasks
        ))
    raise_on_errors(errors)","Make sure there are a specific number of specific task types.

    Currently we only check decision tasks.

    Args:
        chain (ChainOfTrust): the chain we're operating on
        task_count (dict): mapping task type to the number of links.

    Raises:
        CoTError: on failure.",1,0,2,3
"def check_output_files(self,
                           return_found=True,
                           return_missing=True):
        
        all_output_files = self.files.chain_output_files + \
            self.sub_files.chain_output_files
        return check_files(all_output_files, self._file_stage,
                           return_found, return_missing)","Check if output files exist.

        Parameters
        ----------
        return_found : list
            A list with the paths of the files that were found.

        return_missing : list
            A list with the paths of the files that were missing.

        Returns
        -------
        found : list
            List of the found files, if requested, otherwise `None`

        missing : list
            List of the missing files, if requested, otherwise `None`",0,0,3,3
"def check_url_accessibility(url, timeout=10):       
    
    if(url=='localhost'):
        url = 'http://127.0.0.1'
    try:
        req = urllib2.urlopen(url, timeout=timeout)
        if (req.getcode()==200):
            return True
    except Exception:
        pass
    fail(""URL '%s' is not accessible from this machine"" % url)","Check whether the URL accessible and returns HTTP 200 OK or not
    if not raises ValidationError",0,1,1,2
"def check_who_am_i(self):
        
        register = self.MMA8452Q_Register['WHO_AM_I']

        self.board.i2c_read_request(self.address, register, 1,
                                    Constants.I2C_READ | Constants.I2C_END_TX_MASK,
                                    self.data_val, Constants.CB_TYPE_DIRECT)


        reply = self.wait_for_read_result()

        if reply[self.data_start] == self.device_id:
            rval = True
        else:
            rval = False
        return rval","This method checks verifies the device ID.
        @return: True if valid, False if not",0,1,2,3
"def checkin(self, place_id, sensor=False):
        
        data = {'placeid': place_id}
        url, checkin_response = _fetch_remote_json(
                GooglePlaces.CHECKIN_API_URL % (str(sensor).lower(),
                        self.api_key), json.dumps(data), use_http_post=True)
        _validate_response(url, checkin_response)","Checks in a user to a place.

        keyword arguments:
        place_id  -- The unique Google identifier for the relevant place.
        sensor    -- Boolean flag denoting if the location came from a
                     device using its location sensor (default False).",0,1,0,1
"def checkpoint(self, message, header=None, delay=0, **kwargs):
        
        if not self.transport:
            raise ValueError(
                ""This RecipeWrapper object does not contain ""
                ""a reference to a transport object.""
            )

        if not self.recipe_step:
            raise ValueError(
                ""This RecipeWrapper object does not contain ""
                ""a recipe with a selected step.""
            )

        kwargs[""delay""] = delay

        self._send_to_destination(
            self.recipe_pointer, header, message, kwargs, add_path_step=False
        )","Send a message to the current recipe destination. This can be used to
        keep a state for longer processing tasks.
        :param delay: Delay transport of message by this many seconds",2,1,2,5
"def checksum_ip(ipvx, length, payload):
    
    if ipvx.version == 4:
        header = struct.pack(_IPV4_PSEUDO_HEADER_PACK_STR,
                             addrconv.ipv4.text_to_bin(ipvx.src),
                             addrconv.ipv4.text_to_bin(ipvx.dst),
                             ipvx.proto, length)
    elif ipvx.version == 6:
        header = struct.pack(_IPV6_PSEUDO_HEADER_PACK_STR,
                             addrconv.ipv6.text_to_bin(ipvx.src),
                             addrconv.ipv6.text_to_bin(ipvx.dst),
                             length, ipvx.nxt)
    else:
        raise ValueError('Unknown IP version %d' % ipvx.version)

    buf = header + payload
    return checksum(buf)","calculate checksum of IP pseudo header

    IPv4 pseudo header
    UDP RFC768
    TCP RFC793 3.1

     0      7 8     15 16    23 24    31
    +--------+--------+--------+--------+
    |          source address           |
    +--------+--------+--------+--------+
    |        destination address        |
    +--------+--------+--------+--------+
    |  zero  |protocol|    length       |
    +--------+--------+--------+--------+


    IPv6 pseudo header
    RFC2460 8.1
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    |                                                               |
    +                                                               +
    |                                                               |
    +                         Source Address                        +
    |                                                               |
    +                                                               +
    |                                                               |
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    |                                                               |
    +                                                               +
    |                                                               |
    +                      Destination Address                      +
    |                                                               |
    +                                                               +
    |                                                               |
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    |                   Upper-Layer Packet Length                   |
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    |                      zero                     |  Next Header  |
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+",1,0,2,3
"def choices(self):
        
        
        
        model_parent_part = self.part.model()  
        property_model = model_parent_part.property(self.name)
        referenced_model = self._client.model(pk=property_model._value['id'])  
        possible_choices = self._client.parts(model=referenced_model)  

        return possible_choices","Retrieve the parts that you can reference for this `ReferenceProperty`.

        This method makes 2 API calls: 1) to retrieve the referenced model, and 2) to retrieve the instances of
        that model.

        :return: the :class:`Part`'s that can be referenced as a :class:`~pykechain.model.PartSet`.
        :raises APIError: When unable to load and provide the choices

        Example
        -------

        >>> property = project.part('Bike').property('RefTest')
        >>> reference_part_choices = property.choices()",0,2,0,2
"def cidr_notation(ip_address, netmask):
    
    try:
        inet_aton(ip_address)
    except:
        raise Exception(""Invalid ip address '%s'"" % ip_address)
    try:
        inet_aton(netmask)
    except:
        raise Exception(""Invalid netmask '%s'"" % netmask)

    ip_address_split = ip_address.split('.')
    netmask_split = netmask.split('.')
    
    
    net_start = [str(int(ip_address_split[x]) & int(netmask_split[x]))
                    for x in range(0,4)]
    
    return '.'.join(net_start) + '/' + get_net_size(netmask_split)","Retrieve the cidr notation given an ip address and netmask.
    
    For example:
    
        cidr_notation('12.34.56.78', '255.255.255.248')
    
    Would return: 12.34.56.72/29 
    
    @see http://terminalmage.net/2012/06/10/how-to-find-out-the-cidr-notation-for-a-subne-given-an-ip-and-netmask/
    @see http://www.aelius.com/njh/subnet_sheet.html",2,0,3,5
"def circTOcutout(wcs,ra,dec,rad):
    
    (x1,y1)=wcs.rd2xy((ra+rad/2.0,dec-rad/2.0))
    (x2,y2)=wcs.rd2xy((ra-rad/2.0,dec+rad/2.0))
    xl=min(x1,x2)
    xr=max(x1,x2)
    yl=min(y1,y2)
    yu=max(y1,y2)
    
    
    x1=max(xl,1)
    x1=int(min(x1,wcs.naxis1))
    
    x2=max(xr,x1,1)
    x2=int(min(x2,wcs.naxis1))
    
    y1=max(yl,1)
    y1=int(min(y1,wcs.naxis2))
    y2=max(yu,y1,1)
    y2=int(min(y2,wcs.naxis2))

    area=(x2-x1)*(y2-y1)
    cutout=""[%d:%d,%d:%d]"" % ( x1,x2,y1,y2)

    if not y1<y2 or not x1<x2: cutout=None
    return (cutout, area)",Convert an RA/DEC/RADIUS to an imcopy cutout,0,0,1,1
"def cl_input(cls, filename, unpack=True, anonpipe=True):
        
        command, prog = cls._download_cl(filename)
        if prog == ""awscli"":
            command.append(""-"")

        command = "" "".join(command)
        if filename.endswith("".gz"") and unpack:
            command = ""%(command)s | gunzip -c"" % {""command"": command}
        elif filename.endswith("".bz2"") and unpack:
            command = ""%(command)s | bunzip2 -c"" % {""command"": command}
        if anonpipe:
            command = ""<(%(command)s)"" % {""command"": command}

        return command","Return command line input for a file, handling streaming
        remote cases.",1,0,1,2
"def classify(self, feature_names, classifier, transformer, candidates,
                 query_fc=None):
        
        if query_fc is None:
            query_fc = self.query_fc
        dis = {}
        for name in feature_names:
            vec = dict_vector()
            query = vec.fit_transform([get_feat(query_fc, name)])
            cans = vec.transform(get_feat(fc, name) for _, fc in candidates)
            dis[name] = 1 - pairwise_distances(
                cans, query, metric='cosine', n_jobs=1)[:,0]

        
        phi_dicts = transformer.transform(
            [dict([(name, dis[name][i]) for name in feature_names])
             for i in xrange(len(candidates))])
        return classifier.predict_proba(phi_dicts)[:,1]","Returns ``[probability]`` in correspondence with
        ``candidates``.

        Where each ``probability`` corresponds to the probability that
        the corresponding candidate is classified with a positive label
        given the training data.

        The list returned is in correspondence with the list of
        candidates given.

        N.B. The contract of this method should be simplified by
        bundling ``feature_names``, ``classifier`` and ``transformer``
        into one thing known as ""the model."" ---AG",0,0,1,1
"def clean(image, mask=None, iterations = 1):
    
    global clean_table
    if mask is None:
        masked_image = image
    else:
        masked_image = image.astype(bool).copy()
        masked_image[~mask] = False
    result = table_lookup(masked_image, clean_table, False, iterations)
    if not mask is None:
        result[~mask] = image[~mask]
    return result","Remove isolated pixels
    
    0 0 0     0 0 0
    0 1 0 ->  0 0 0
    0 0 0     0 0 0
    
    Border pixels and pixels adjoining masks are removed unless one valid
    neighbor is true.",0,0,1,1
"def clean_community_indexes(communityID):
    
    communityID = np.array(communityID)
    cid_shape = communityID.shape
    if len(cid_shape) > 1:
        communityID = communityID.flatten()
    new_communityID = np.zeros(len(communityID))
    for i, n in enumerate(np.unique(communityID)):
        new_communityID[communityID == n] = i
    if len(cid_shape) > 1:
        new_communityID = new_communityID.reshape(cid_shape)
    return new_communityID","Takes input of community assignments. Returns reindexed community assignment by using smallest numbers possible.

    Parameters
    ----------

    communityID : array-like
        list or array of integers. Output from community detection algorithems.

    Returns
    -------

    new_communityID : array
        cleaned list going from 0 to len(np.unique(communityID))-1

    Note
    -----

    Behaviour of funciton entails that the lowest community integer in communityID will recieve the lowest integer in new_communityID.",0,0,1,1
"def clean_item_no_list(i):
    
    itype = type(i)
    if itype == dict:
        return clean_dict(i, clean_item_no_list)
    elif itype == list:
        return clean_tuple(i, clean_item_no_list)
    elif itype == tuple:
        return clean_tuple(i, clean_item_no_list)
    elif itype == numpy.float32:
        return float(i)
    elif itype == numpy.float64:
        return float(i)
    elif itype == numpy.int16:
        return int(i)
    elif itype == numpy.uint16:
        return int(i)
    elif itype == numpy.int32:
        return int(i)
    elif itype == numpy.uint32:
        return int(i)
    elif itype == float:
        return i
    elif itype == str:
        return i
    elif itype == int:
        return i
    elif itype == bool:
        return i
    elif itype == type(None):
        return i
    logging.info(""[2] Unable to handle type %s"", itype)
    return None",Return a json-clean item or None. Will log info message for failure.,0,1,1,2
"def clean_obj(obj, remove):
    
    if isinstance(remove, string_types):
        remove = [remove]
    try:
        iter(remove)
    except TypeError:
        raise ClientException(""`remove` could not be removed from object: {}"".format(repr(remove)))
    else:
        if isinstance(obj, dict):
            obj = {
                key: clean_obj(value, remove)
                for key, value in iteritems(obj)
                if key not in remove
            }
        elif isinstance(obj, list):
            obj = [
                clean_obj(item, remove)
                for item in obj
                if item not in remove
            ]
        return obj","Recursively remove keys from list/dict/dict-of-lists/list-of-keys/nested ...,
     e.g. remove all sharing keys or remove all 'user' fields
    This should result in the same as if running in bash: `jq del(.. | .publicAccess?, .userGroupAccesses?)`
    :param obj: the dict to remove keys from
    :param remove: keys to remove - can be a string or iterable",1,0,2,3
"def clean_series_name(seriesname):
    
    if not seriesname:
        return seriesname
    
    
    seriesname = re.sub(r'(\D)[.](\D)', '\\1 \\2', seriesname)
    seriesname = re.sub(r'(\D)[.]', '\\1 ', seriesname)
    seriesname = re.sub(r'[.](\D)', ' \\1', seriesname)
    seriesname = seriesname.replace('_', ' ')
    seriesname = re.sub('-$', '', seriesname)
    return _replace_series_name(seriesname.strip(),
                                cfg.CONF.input_series_replacements)","Cleans up series name.

    By removing any . and _ characters, along with any trailing hyphens.

    Is basically equivalent to replacing all _ and . with a
    space, but handles decimal numbers in string, for example:

    >>> _clean_series_name(""an.example.1.0.test"")
    'an example 1.0 test'
    >>> _clean_series_name(""an_example_1.0_test"")
    'an example 1.0 test'",0,0,1,1
"def clear_agent(self, short_name, client_id):
        

        if short_name not in self.services:
            raise ArgumentError(""Unknown service name"", short_name=short_name)

        if short_name not in self.agents:
            raise ArgumentError(""No agent registered for service"", short_name=short_name)

        if client_id != self.agents[short_name]:
            raise ArgumentError(""Client was not registered for service"", short_name=short_name,
                                client_id=client_id, current_client=self.agents[short_name])

        del self.agents[short_name]","Remove a client id from being the command handler for a service.

        Args:
            short_name (str): The name of the service to set an agent
                for.
            client_id (str): A globally unique id for the client that
                should no longer receive commands for this service.",3,0,4,7
"def clear_indexes(self, chunk_size=1000, aggressive=False, index_class=None):
        
        assert self.indexable, ""Field not indexable""
        assert self.attached_to_model, \
            '`rebuild_indexes` can only be called on a field attached to the model'

        for index in self._indexes:
            if index_class and not isinstance(index, index_class):
                continue
            index.clear(chunk_size=chunk_size, aggressive=aggressive)","Clear all indexes tied to this field

        Parameters
        ----------
        chunk_size: int
            Default to 1000, it's the number of instances to load at once if not in aggressive mode.
        aggressive: bool
            Default to ``False``. When ``False``, the actual collection of instances will
            be ran through to deindex all the values.
            But when ``True``, the database keys will be scanned to find keys that matches the
            pattern of the keys used by the indexes. This is a lot faster and may find forgotten keys.
            But may also find keys not related to the index.
            Should be set to ``True`` if you are not sure about the already indexed values.
        index_class: type
            Allow to clear only index(es) for this index class instead of all indexes.

        Raises
        ------
        AssertionError
            If called from an instance field. It must be called from the model field
            Also raised if the field is not indexable

        Examples
        --------

        >>> MyModel.get_field('myfield').clear_indexes()
        >>> MyModel.get_field('myfield').clear_indexes(index_class=MyIndex)",2,0,1,3
"def cleartextRoot(self, hostname=None):
        
        warnings.warn(
            ""Use ISiteURLGenerator.rootURL instead of WebSite.cleartextRoot."",
            category=DeprecationWarning,
            stacklevel=2)
        if self.store.parent is not None:
            generator = ISiteURLGenerator(self.store.parent)
        else:
            generator = ISiteURLGenerator(self.store)
        return generator.cleartextRoot(hostname)","Return a string representing the HTTP URL which is at the root of this
        site.

        @param hostname: An optional unicode string which, if specified, will
        be used as the hostname in the resulting URL, regardless of the
        C{hostname} attribute of this item.",0,0,1,1
"def clef_error_check(func_to_decorate):
    
    def wrapper(*args, **kwargs):
        try:
            response = func_to_decorate(*args, **kwargs)
        except requests.exceptions.RequestException:
            raise ConnectionError(
                'Clef encountered a network connectivity problem. Are you sure you are connected to the Internet?'
            )
        else:
            raise_right_error(response)
        return response.json() 
    return wrapper",Return JSON decoded response from API call. Handle errors when bad response encountered.,3,1,1,5
"def cli():
    
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        ""dataset_uri"",
        help=""Dtool dataset URI""
    )
    parser.add_argument(
        ""-q"",
        ""--quiet"",
        action=""store_true"",
        help=""Only return the http URI""
    )
    args = parser.parse_args()
    access_uri = publish(args.dataset_uri)

    if args.quiet:
        print(access_uri)
    else:
        print(""Dataset accessible at: {}"".format(access_uri))",Command line utility to HTTP enable (publish) a dataset.,0,1,1,2
"def cli(env, account_id, content_url):
    

    manager = SoftLayer.CDNManager(env.client)
    content_list = manager.purge_content(account_id, content_url)

    table = formatting.Table(['url', 'status'])

    for content in content_list:
        table.add_row([
            content['url'],
            content['statusCode']
        ])

    env.fout(table)","Purge cached files from all edge nodes.

    Examples:
         slcli cdn purge 97794 http://example.com/cdn/file.txt
         slcli cdn purge 97794 http://example.com/cdn/file.txt https://dal01.example.softlayer.net/image.png",1,0,0,1
"def cli_progress_bar(start, end, bar_length=50):
    
    percent = float(start) / end
    hashes = '
    spaces = '-' * (bar_length - len(hashes))
    stdout.write(
        ""\r[{0}] {1}/{2} ({3}%)"".format(
            hashes + spaces,
            start,
            end,
            int(round(percent * 100))
        )
    )
    stdout.flush()","Prints out a Yum-style progress bar (via sys.stdout.write).
    `start`: The 'current' value of the progress bar.
    `end`: The '100%' value of the progress bar.
    `bar_length`: The size of the overall progress bar.

    Example output with start=20, end=100, bar_length=50:
    [###########----------------------------------------] 20/100 (100%)

    Intended to be used in a loop. Example:
    end = 100
    for i in range(end):
        cli_progress_bar(i, end)

    Based on an implementation found here:
        http://stackoverflow.com/a/13685020/1149774",1,0,0,1
"def close_link(self):
        
        logger.info('Closing link')
        if (self.link is not None):
            self.commander.send_setpoint(0, 0, 0, 0)
        if (self.link is not None):
            self.link.close()
            self.link = None
        self._answer_patterns = {}
        self.disconnected.call(self.link_uri)",Close the communication link.,0,2,0,2
"def cmd(self, *args, **kwargs):
        

        args = list(args)
        if self.socket_name:
            args.insert(0, '-L{0}'.format(self.socket_name))
        if self.socket_path:
            args.insert(0, '-S{0}'.format(self.socket_path))
        if self.config_file:
            args.insert(0, '-f{0}'.format(self.config_file))
        if self.colors:
            if self.colors == 256:
                args.insert(0, '-2')
            elif self.colors == 88:
                args.insert(0, '-8')
            else:
                raise ValueError('Server.colors must equal 88 or 256')

        return tmux_cmd(*args, **kwargs)","Execute tmux command and return output.

        Returns
        -------
        :class:`common.tmux_cmd`

        Notes
        -----
        .. versionchanged:: 0.8

            Renamed from ``.tmux`` to ``.cmd``.",1,1,1,3
"def cmd_pan(self, x=None, y=None, ch=None):
        
        viewer = self.get_viewer(ch)
        if viewer is None:
            self.log(""No current viewer/channel."")
            return

        pan_x, pan_y = viewer.get_pan()

        if x is None and y is None:
            self.log(""x=%f y=%f"" % (pan_x, pan_y))

        else:
            if x is not None:
                if y is None:
                    y = pan_y
            if y is not None:
                if x is None:
                    x = pan_x
            viewer.set_pan(x, y)","scale ch=chname x=pan_x y=pan_y

        Set the pan position for the given viewer/channel to the given
        pixel coordinates.
        If no coordinates are given, reports the current position.",0,2,2,4
"def collect_changes(self):
        

        file_diffs = self._collect_file_diffs()
        candidate_feature_diffs, valid_init_diffs, inadmissible_diffs = \
            self._categorize_file_diffs(file_diffs)
        new_feature_info = self._collect_feature_info(candidate_feature_diffs)

        return CollectedChanges(
            file_diffs, candidate_feature_diffs, valid_init_diffs,
            inadmissible_diffs, new_feature_info)","Collect file and feature changes

        Steps
        1. Collects the files that have changed in this pull request as
           compared to a comparison branch.
        2. Categorize these file changes into admissible or inadmissible file
           changes. Admissible file changes solely contribute python files to
           the contrib subdirectory.
        3. Collect features from admissible new files.

        Returns:
            CollectedChanges",1,0,2,3
"def colorize(self, colormap):
        

        if self.mode not in (""L"", ""LA""):
            raise ValueError(""Image should be grayscale to colorize"")
        if self.mode == ""LA"":
            alpha = self.channels[1]
        else:
            alpha = None
        self.channels = colormap.colorize(self.channels[0])
        if alpha is not None:
            self.channels.append(alpha)
            self.mode = ""RGBA""
        else:
            self.mode = ""RGB""","Colorize the current image using
        *colormap*. Works only on""L"" or ""LA"" images.",1,0,2,3
"def combination_step(self):
        

        
        tprv = self.t
        self.t = 0.5 * float(1. + np.sqrt(1. + 4. * tprv**2))

        
        if not self.opt['FastSolve']:
            self.Yprv = self.Y.copy()
        self.Y = self.X + ((tprv - 1.) / self.t) * (self.X - self.Xprv)","Build next update by a smart combination of previous updates.
        (standard FISTA :cite:`beck-2009-fast`).",0,0,1,1
"def combine_files(joinfile, sourcefiles, client):
    
    from mediasync.conf import msettings

    joinfile = joinfile.strip('/')

    if joinfile.endswith('.css'):
        dirname = msettings['CSS_PATH'].strip('/')
        separator = '\n'
    elif joinfile.endswith('.js'):
        dirname = msettings['JS_PATH'].strip('/')
        separator = ';\n'
    else:
        
        return None

    buffer = cStringIO.StringIO()

    for sourcefile in sourcefiles:
        sourcepath = os.path.join(client.media_root, dirname, sourcefile)
        if os.path.isfile(sourcepath):
            f = open(sourcepath)
            buffer.write(f.read())
            f.close()
            buffer.write(separator)

    filedata = buffer.getvalue()
    buffer.close()
    return (filedata, dirname)","Given a combo file name (joinfile), combine the sourcefiles into a single
    monolithic file.
    
    Returns a string containing the combo file, or None if the specified
    file can not be combo'd.",1,0,2,3
"def commit(self):
        
        obj = self.object
        while obj.type != 'commit':
            if obj.type == ""tag"":
                
                obj = obj.object
            else:
                raise ValueError((""Cannot resolve commit as tag %s points to a %s object - "" +
                                  ""use the `.object` property instead to access it"") % (self, obj.type))
        return obj",":return: Commit object the tag ref points to
        
        :raise ValueError: if the tag points to a tree or blob",1,0,1,2
"def compare_config(self):
        
        _show_merge = self._execute_config_show('show configuration merge')
        _show_run = self._execute_config_show('show running-config')

        diff = difflib.unified_diff(_show_run.splitlines(1)[2:-2], _show_merge.splitlines(1)[2:-2])
        return ''.join([x.replace('\r', '') for x in diff])","Compare configuration to be merged with the one on the device.

        Compare executed candidate config with the running config and
        return a diff, assuming the loaded config will be merged with the
        existing one.

        :return:  Config diff.",0,0,2,2
"def competitions_submissions_submit(self, blob_file_tokens, submission_description, id, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.competitions_submissions_submit_with_http_info(blob_file_tokens, submission_description, id, **kwargs)  
        else:
            (data) = self.competitions_submissions_submit_with_http_info(blob_file_tokens, submission_description, id, **kwargs)  
            return data","Submit to competition  # noqa: E501

        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.competitions_submissions_submit(blob_file_tokens, submission_description, id, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str blob_file_tokens: Token identifying location of uploaded submission file (required)
        :param str submission_description: Description of competition submission (required)
        :param str id: Competition name (required)
        :return: Result
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def compile_mpim_users(self):
        

        mpim_data = self._read_from_json(""mpims.json"")
        mpims = [c for c in mpim_data.values()]
        all_mpim_users = []

        for mpim in mpims:
            mpim_members = {""name"": mpim[""name""], ""users"": [self.__USER_DATA[m] for m in mpim[""members""]]}
            all_mpim_users.append(mpim_members)

        return all_mpim_users","Gets the info for the members within the multiple person instant message

        Returns a list of all dms with the members that have ever existed

        :rtype: [object]
        {
            name: <name>
            users: [<user_id>]
        }",1,0,1,2
"def complementary(clr):
    
    clr = color(clr)
    colors = colorlist(clr)

    
    c = clr.copy()
    if clr.brightness > 0.4:
        c.brightness = 0.1 + c.brightness * 0.25
    else:
        c.brightness = 1.0 - c.brightness * 0.25
    colors.append(c)

    
    c = clr.copy()
    c.brightness = 0.3 + c.brightness
    c.saturation = 0.1 + c.saturation * 0.3
    colors.append(c)

    
    clr = clr.complement
    c = clr.copy()
    if clr.brightness > 0.3:
        c.brightness = 0.1 + clr.brightness * 0.25
    else:
        c.brightness = 1.0 - c.brightness * 0.25
    colors.append(c)

    
    colors.append(clr)

    c = clr.copy()
    c.brightness = 0.3 + c.brightness
    c.saturation = 0.1 + c.saturation * 0.25
    colors.append(c)

    return colors","Returns a list of complementary colors.

    The complement is the color 180 degrees across
    the artistic RYB color wheel.
    The list contains darker and softer contrasting
    and complementing colors.",0,0,1,1
"def complex_el_from_dict(parent, data, key):
    
    el = ET.SubElement(parent, key)
    value = data[key]

    if isinstance(value, dict):
        if '_attr' in value:
            for a_name, a_value in viewitems(value['_attr']):
                el.set(a_name, a_value)

        if '_text' in value:
            el.text = value['_text']

    else:
        el.text = value

    return el","Create element from a dict definition and add it to ``parent``.

    :param parent: parent element
    :type parent: Element
    :param data: dictionary with elements definitions, it can be a simple \
    {element_name: 'element_value'} or complex \
    {element_name: {_attr: {name: value, name1: value1}, _text: 'text'}}
    :param key: element name and key in ``data``
    :return: created element",0,0,2,2
"def compute(self, x, y):
        
        if x in self.__memo_dict:
            x_v = self.__memo_dict[x]
        else:
            x_v = self.__cost_functionable.compute(self.__params_arr[x, :])
            self.__memo_dict.setdefault(x, x_v)
        if y in self.__memo_dict:
            y_v = self.__memo_dict[y]
        else:
            y_v = self.__cost_functionable.compute(self.__params_arr[y, :])
            self.__memo_dict.setdefault(y, y_v)

        return abs(x_v - y_v)","Compute distance.
        
        Args:
            x:    Data point.
            y:    Data point.
        
        Returns:
            Distance.",0,0,1,1
"def compute_err_score(true_positives, n_ref, n_est):
    
    n_ref_sum = float(n_ref.sum())

    if n_ref_sum == 0:
        warnings.warn(""Reference frequencies are all empty."")
        return 0., 0., 0., 0.

    
    e_sub = (np.min([n_ref, n_est], axis=0) - true_positives).sum()/n_ref_sum

    
    e_miss_numerator = n_ref - n_est
    e_miss_numerator[e_miss_numerator < 0] = 0
    
    e_miss = e_miss_numerator.sum()/n_ref_sum

    
    e_fa_numerator = n_est - n_ref
    e_fa_numerator[e_fa_numerator < 0] = 0
    
    e_fa = e_fa_numerator.sum()/n_ref_sum

    
    e_tot = (np.max([n_ref, n_est], axis=0) - true_positives).sum()/n_ref_sum

    return e_sub, e_miss, e_fa, e_tot","Compute error score metrics.

    Parameters
    ----------
    true_positives : np.ndarray
        Array containing the number of true positives at each time point.
    n_ref : np.ndarray
        Array containing the number of reference frequencies at each time
        point.
    n_est : np.ndarray
        Array containing the number of estimate frequencies at each time point.

    Returns
    -------
    e_sub : float
        Substitution error
    e_miss : float
        Miss error
    e_fa : float
        False alarm error
    e_tot : float
        Total error",0,0,1,1
"def compute_max_min_depth(germanet_db):
    
    gnet           = germanet.GermaNet(germanet_db)
    max_min_depths = defaultdict(lambda: -1)
    for synset in gnet.all_synsets():
        min_depth = synset.min_depth
        if max_min_depths[synset.category] < min_depth:
            max_min_depths[synset.category] = min_depth

    if germanet_db.metainfo.count() == 0:
        germanet_db.metainfo.insert({})
    metainfo = germanet_db.metainfo.find_one()
    metainfo['max_min_depths'] = max_min_depths
    germanet_db.metainfo.save(metainfo)

    print('Computed maximum min_depth for all parts of speech:')
    print(u', '.join(u'{0}: {1}'.format(k, v) for (k, v) in
                     sorted(max_min_depths.items())).encode('utf-8'))","For every part of speech in GermaNet, computes the maximum
    min_depth in that hierarchy.

    Arguments:
    - `germanet_db`: a pymongo.database.Database object",1,0,1,2
"def compute_nats_and_bits_per_dim(data_dim,
                                  latent_dim,
                                  average_reconstruction,
                                  average_prior):
  
  with tf.name_scope(None, default_name=""compute_nats_per_dim""):
    data_dim = tf.cast(data_dim, average_reconstruction.dtype)
    latent_dim = tf.cast(latent_dim, average_prior.dtype)
    negative_log_likelihood = data_dim * average_reconstruction
    negative_log_prior = latent_dim * average_prior
    negative_elbo = negative_log_likelihood + negative_log_prior
    nats_per_dim = tf.divide(negative_elbo, data_dim, name=""nats_per_dim"")
    bits_per_dim = tf.divide(nats_per_dim, tf.log(2.), name=""bits_per_dim"")
    return nats_per_dim, bits_per_dim","Computes negative ELBO, which is an upper bound on the negative likelihood.

  Args:
    data_dim: int-like indicating data dimensionality.
    latent_dim: int-like indicating latent dimensionality.
    average_reconstruction: Scalar Tensor indicating the reconstruction cost
      averaged over all data dimensions and any data batches.
    average_prior: Scalar Tensor indicating the negative log-prior probability
      averaged over all latent dimensions and any data batches.

  Returns:
    Tuple of scalar Tensors, representing the nats and bits per data dimension
    (e.g., subpixels) respectively.",0,0,1,1
"def compute_shape_centers(df_shapes, shape_i_column, inplace=False):
    
    if not isinstance(shape_i_column, bytes):
        raise KeyError('Shape index must be a single column.')

    if not inplace:
        df_shapes = df_shapes.copy()

    
    df_bounding_boxes = get_bounding_boxes(df_shapes, shape_i_column)
    path_centers = (df_bounding_boxes[['x', 'y']] + .5 *
                    df_bounding_boxes[['width', 'height']].values)
    df_shapes['x_center'] = path_centers.x[df_shapes[shape_i_column]].values
    df_shapes['y_center'] = path_centers.y[df_shapes[shape_i_column]].values

    
    
    center_offset = (df_shapes[['x', 'y']] -
                     df_shapes[['x_center', 'y_center']].values)
    return df_shapes.join(center_offset, rsuffix='_center_offset')","Compute the center point of each polygon shape, and the offset of each
    vertex to the corresponding polygon center point.

    Parameters
    ----------
    df_shapes : pandas.DataFrame
        Table of polygon shape vertices (one row per vertex).

        Must have at least the following columns:
         - ``vertex_i``: The index of the vertex within the corresponding
           shape.
         - ``x``: The x-coordinate of the vertex.
         - ``y``: The y-coordinate of the vertex.
    shape_i_column : str or list, optional
        Table rows with the same value in the :data:`shape_i_column` column are
        grouped together as a shape.
    in_place : bool, optional
        If ``True``, center coordinate columns are added directly to the input
        frame.

        Otherwise, center coordinate columns are added to copy of the input
        frame.

    Returns
    -------
    pandas.DataFrame
        Input frame with the following additional columns:
         - ``x_center``/``y_center``: Absolute coordinates of shape center.
         - ``x_center_offset``/``y_center_offset``:
             * Coordinates of each vertex coordinate relative to shape center.",1,0,2,3
"def compute_sizes(sizes, size_fn, scaling_factor, scaling_method, base_size):
    
    if sizes.dtype.kind not in ('i', 'f'):
        return None
    if scaling_method == 'area':
        pass
    elif scaling_method == 'width':
        scaling_factor = scaling_factor**2
    else:
        raise ValueError(
            'Invalid value for argument ""scaling_method"": ""{}"". '
            'Valid values are: ""width"", ""area"".'.format(scaling_method))
    sizes = size_fn(sizes)
    return (base_size*scaling_factor*sizes)","Scales point sizes according to a scaling factor,
    base size and size_fn, which will be applied before
    scaling.",1,0,2,3
"def concatenate(a, b=None):
    
    if b is None:
        b = []
    
    meshes = np.append(a, b)

    
    
    trimesh_type = type_named(meshes[0], 'Trimesh')

    
    vertices, faces = append_faces(
        [m.vertices.copy() for m in meshes],
        [m.faces.copy() for m in meshes])

    
    face_normals = None
    if all('face_normals' in m._cache for m in meshes):
        face_normals = np.vstack([m.face_normals
                                  for m in meshes])

    
    visual = meshes[0].visual.concatenate(
        [m.visual for m in meshes[1:]])

    
    mesh = trimesh_type(vertices=vertices,
                        faces=faces,
                        face_normals=face_normals,
                        visual=visual,
                        process=False)

    return mesh","Concatenate two or more meshes.

    Parameters
    ----------
    a: Trimesh object, or list of such
    b: Trimesh object, or list of such

    Returns
    ----------
    result: Trimesh object containing concatenated mesh",0,0,1,1
"def config(
    state, host, key, value,
    repo=None,
):
    

    existing_config = host.fact.git_config(repo)

    if key not in existing_config or existing_config[key] != value:
        if repo is None:
            yield 'git config --global {0} ""{1}""'.format(key, value)
        else:
            yield 'cd {0} && git config --local {1} ""{2}""'.format(repo, key, value)","Manage git config for a repository or globally.

    + key: the key of the config to ensure
    + value: the value this key should have
    + repo: specify the git repo path to edit local config (defaults to global)",0,2,2,4
"def config_filename(filename):
    
    global _ETC_PATHS
    if filename.startswith('/'):
        _LOGGER.info(""using absolute path for filename \""%s\"""" % filename)
        return filename

    import os.path
    for fpath in _ETC_PATHS:
        current_path = ""%s/%s"" % (fpath, filename)
        if os.path.isfile(current_path):
            current_path = os.path.realpath(current_path)
            _LOGGER.info(""using path \""%s\"" for filename \""%s\"""" % (current_path, filename))
            return current_path

    _LOGGER.info(""using path \""%s\"" for filename \""%s\"""" % (filename, filename))
    return filename","Obtains the first filename found that is included in one of the configuration folders.
        This function returs the full path for the file.
        
        * It is useful for files that are not config-formatted (e.g. hosts files, json, etc.)
          that will be read using other mechanisms",0,3,1,4
"def config_init(config_file, json_config_obj, config_dirname=None):
    
    HOME = os.environ['HOME']
    
    if config_dirname:
        dir_path = HOME + '/' + config_dirname
        if not os.path.exists(dir_path):
            os.mkdir(dir_path)
            os.chmod(dir_path, 0o755)
    else:
        dir_path = HOME
    
    r = export_json_object(
            dict_obj=json_config_obj,
            filename=dir_path + '/' + config_file
        )
    return r","Summary:
        Creates local config from JSON seed template
    Args:
        :config_file (str): filesystem object containing json dict of config values
        :json_config_obj (json):  data to be written to config_file
        :config_dirname (str):  dir name containing config_file
    Returns:
        TYPE: bool, Success | Failure",1,0,2,3
"def configure(self, org_name, api_token=None, base_url='okta.com', ttl=None, max_ttl=None, bypass_okta_mfa=False,
                  mount_point=DEFAULT_MOUNT_POINT):
        
        params = {
            'org_name': org_name,
            'api_token': api_token,
            'base_url': base_url,
            'ttl': ttl,
            'max_ttl': max_ttl,
            'bypass_okta_mfa': bypass_okta_mfa,
        }
        api_path = '/v1/auth/{mount_point}/config'.format(mount_point=mount_point)
        return self._adapter.post(
            url=api_path,
            json=params,
        )","Configure the connection parameters for Okta.

        This path honors the distinction between the create and update capabilities inside ACL policies.

        Supported methods:
            POST: /auth/{mount_point}/config. Produces: 204 (empty body)


        :param org_name: Name of the organization to be used in the Okta API.
        :type org_name: str | unicode
        :param api_token: Okta API token. This is required to query Okta for user group membership. If this is not
            supplied only locally configured groups will be enabled.
        :type api_token: str | unicode
        :param base_url:  If set, will be used as the base domain for API requests.  Examples are okta.com,
            oktapreview.com, and okta-emea.com.
        :type base_url: str | unicode
        :param ttl: Duration after which authentication will be expired.
        :type ttl: str | unicode
        :param max_ttl: Maximum duration after which authentication will be expired.
        :type max_ttl: str | unicode
        :param bypass_okta_mfa: Whether to bypass an Okta MFA request. Useful if using one of Vault's built-in MFA
            mechanisms, but this will also cause certain other statuses to be ignored, such as PASSWORD_EXPIRED.
        :type bypass_okta_mfa: bool
        :param mount_point: The ""path"" the method/backend was mounted on.
        :type mount_point: str | unicode
        :return: The response of the request.
        :rtype: requests.Response",0,1,0,1
"def configure_logger(simple_name, log_dest=None,
                     detail_level=DEFAULT_LOG_DETAIL_LEVEL,
                     log_filename=DEFAULT_LOG_FILENAME,
                     connection=None, propagate=False):
    
      
    

    global _CONN  
    if _CONN is None:
        from . import WBEMConnection
        _CONN = WBEMConnection

    _CONN._configure_logger(  
        simple_name,
        log_dest=log_dest,
        detail_level=detail_level,
        log_filename=log_filename,
        connection=connection,
        propagate=propagate)","Configure the pywbem loggers and optionally activate WBEM connections
    for logging and setting a log detail level.

    Parameters:

      simple_name (:term:`string`):
        Simple name (ex. `'api'`) of the single pywbem logger this method
        should affect, or `'all'` to affect all pywbem loggers.

        Must be one of the strings in
        :data:`~pywbem._logging.LOGGER_SIMPLE_NAMES`.

      log_dest (:term:`string`):
        Log destination for the affected pywbem loggers, controlling the
        configuration of its Python logging parameters (log handler,
        message format, and log level).

        If it is a :term:`string`, it must be one of the strings in
        :data:`~pywbem._logging.LOG_DESTINATIONS` and the Python logging
        parameters of the loggers will be configured accordingly for their
        log handler, message format, and with a logging level of
        :attr:`py:logging.DEBUG`.

        If `None`, the Python logging parameters of the loggers will not be
        changed.

      detail_level (:term:`string` or :class:`int` or `None`):
        Detail level for the data in each log record that is generated by
        the affected pywbem loggers.

        If it is a :term:`string`, it must be one of the strings in
        :data:`~pywbem._logging.LOG_DETAIL_LEVELS` and the loggers will
        be configured for the corresponding detail level.

        If it is an :class:`int`, it defines the maximum size of the log
        records created and the loggers will be configured to output all
        available information up to that size.

        If `None`, the detail level configuration will not be changed.

      log_filename (:term:`string`):
        Path name of the log file (required if the log destination is
        `'file'`; otherwise ignored).

      connection (:class:`~pywbem.WBEMConnection` or :class:`py:bool` or `None`):
        WBEM connection(s) that should be affected for activation and for
        setting the detail level.

        If it is a :class:`py:bool`, the information for activating logging
        and for the detail level of the affected loggers will be stored for
        use by subsequently created :class:`~pywbem.WBEMConnection` objects.
        A value of `True` will store the information to activate the
        connections for logging, and will add the detail level for the
        logger(s).
        A value of `False` will reset the stored information for future
        connections to be deactivated with no detail levels specified.

        If it is a :class:`~pywbem.WBEMConnection` object, logging will be
        activated for that WBEM connection only and the specified detail
        level will be set for the affected pywbem loggers on the
        connection.

        If `None`, no WBEM connection will be activated for logging.

      propagate (:class:`py:bool`): Flag controlling whether the
        affected pywbem logger should propagate log events to its
        parent loggers.

    Raises:

      ValueError: Invalid input parameters (loggers remain unchanged).",0,0,3,3
"def configure_logging(conf):
    
    root_logger = logging.getLogger()
    root_logger.setLevel(getattr(logging, conf.loglevel.upper()))
    if conf.logtostderr:
        add_stream_handler(root_logger, sys.stderr)
    if conf.logtostdout:
        add_stream_handler(root_logger, sys.stdout)",Initialize and configure logging.,0,0,2,2
"def connect(dbapi_connection, connection_record):
        
        try:
            cursor = dbapi_connection.cursor()
            try:
                cursor.execute(""PRAGMA foreign_keys = ON;"")
                cursor.execute(""PRAGMA foreign_keys;"")
                if cursor.fetchone()[0] != 1:
                    raise Exception()
            finally:
                cursor.close()
        except Exception:
            dbapi_connection.close()
            raise sqlite3.Error()","Called once by SQLAlchemy for each new SQLite DB-API connection.

        Here is where we issue some PRAGMA statements to configure how we're
        going to access the SQLite database.

        @param dbapi_connection:
            A newly connected raw SQLite DB-API connection.

        @param connection_record:
            Unused by this method.",4,0,0,4
"def connect(self):
        
        connection = psycopg2.connect(
            host=self.host,
            port=self.port,
            database=self.database,
            user=self.user,
            password=self.password)
        connection.set_client_encoding('utf-8')
        return connection",Get a psycopg2 connection object to the database where the table is.,1,0,0,1
"def connect(self):
        
        try:
            _, self.transport = yield from open_connection(
                self.hostname, self.port, loop=self.parent.app.loop)
        except OSError:
            if self.parent.cfg.fail_silently:
                return False
            raise
        return self",Connect to socket.,0,1,0,1
"def connect(self, config):
        
        if isinstance(config, str):
            conn = dbutil.get_database(config_file=config)
        elif isinstance(config, dict):
            conn = dbutil.get_database(settings=config)
        else:
            raise ValueError(""Configuration, '{}',  must be a path to ""
                             ""a configuration file or dict"".format(config))
        return conn","Connect to database with given configuration, which may be a dict or
        a path to a pymatgen-db configuration.",3,0,2,5
"def connect(self, receiver, sender):
        
        logger.info('Signal connected: {}'.format(receiver))
        
        assert callable(receiver)
        receiver_id = _make_id(receiver)
        sender_id = _make_id(sender)
        r = ref
        if hasattr(receiver, '__self__') and hasattr(receiver, '__func__'):
            r = WeakMethod

        receiver = r(receiver)

        self.receivers.setdefault((receiver_id, sender_id), receiver)","Connects a signal to a receiver function

        :param receiver:
            The callback function which will be connected to this signal

        :param sender:
            Specifies a particular sender to receive signals from.
            Used to limit the receiver function to signal from particular sender types",0,2,0,2
"def connect(token, protocol=RtmProtocol, factory=WebSocketClientFactory, factory_kwargs=None, api_url=None, debug=False):
	
	if factory_kwargs is None:
		factory_kwargs = dict()

	metadata = request_session(token, api_url)
	wsfactory = factory(metadata.url, **factory_kwargs)
	if debug:
		warnings.warn('debug=True has been deprecated in autobahn 0.14.0')

	wsfactory.protocol = lambda *a,**k: protocol(*a,**k)._seedMetadata(metadata)
	connection = connectWS(wsfactory)
	return connection","Creates a new connection to the Slack Real-Time API.

	Returns (connection) which represents this connection to the API server.",0,1,0,1
"def connect_head_namespaced_pod_proxy_with_path(self, name, namespace, path, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.connect_head_namespaced_pod_proxy_with_path_with_http_info(name, namespace, path, **kwargs)  
        else:
            (data) = self.connect_head_namespaced_pod_proxy_with_path_with_http_info(name, namespace, path, **kwargs)  
            return data","connect_head_namespaced_pod_proxy_with_path  # noqa: E501

        connect HEAD requests to proxy of Pod  # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.connect_head_namespaced_pod_proxy_with_path(name, namespace, path, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the PodProxyOptions (required)
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param str path: path to the resource (required)
        :param str path2: Path is the URL path to use for the current proxy request to pod.
        :return: str
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def construct_mapping(self, node, deep=False):
        
        mapping = super(ExtendedSafeConstructor, self).construct_mapping(
            node, deep)

        return {
            (str(key) if isinstance(key, int) else key): mapping[key]
            for key in mapping
        }","While yaml supports integer keys, these are not valid in
        json, and will break jsonschema. This method coerces all keys
        to strings.",0,0,1,1
"def construct_url(self, style):
        
        if style is API_URL:
            if self._ssl:
                return 'https://{}:{}'.format(self._host, self._port)
            else:
                return 'http://{}:{}'.format(self._host, self._port)
        elif style is SOCKET_URL:
            if self._ssl:
                return 'wss://{}:{}'.format(self._host, self._port)
            else:
                return 'ws://{}:{}'.format(self._host, self._port)
        else:
            return None",Return http/https or ws/wss url.,0,0,3,3
"def contains_merged_cell(self):
        
        for tc in self.iter_tcs():
            if tc.gridSpan > 1:
                return True
            if tc.rowSpan > 1:
                return True
            if tc.hMerge:
                return True
            if tc.vMerge:
                return True
        return False",True if one or more cells in range are part of a merged cell.,0,0,1,1
"def context_chunks(self, context):
        

        chunks = []
        papers = []
        for paper, feature in self.features.iteritems():
            if context in feature.contexts:
                new_chunks = feature.context_chunks(context)
            else:
                new_chunks = list(feature)
            indices = range(len(chunks), len(chunks) + len(new_chunks))
            papers.append((paper, indices))
            chunks += new_chunks
        return papers, chunks","Retrieves all tokens, divided into the chunks in context ``context``.

        If ``context`` is not found in a feature, then the feature will be
        treated as a single chunk.

        Parameters
        ----------
        context : str
            Context name.

        Returns
        -------
        papers : list
            2-tuples of (paper ID, chunk indices).
        chunks : list
            Each item in ``chunks`` is a list of tokens.",0,0,2,2
"def convert(element):
    
    element = linker.LinkPrimitives().convert(element)
    if isinstance(element, link.ChainLink.chain_types.base_bundle_type):
        return ThreadLinkPrimitives.base_bundle_type(element.elements)
    elif isinstance(element, link.ChainLink.chain_types.base_chain_type):
        return ThreadLinkPrimitives.base_chain_type(element.elements)
    return element","Convert a regular :term:`chainlink` to a thread based version

    :param element: the chainlink to convert
    :return: a threaded version of ``element`` if possible, or the element itself",0,0,2,2
"def convert(obj, ids, attr_type, item_func, cdata, parent='root'):
    
    
    LOG.info('Inside convert(). obj type is: ""%s"", obj=""%s""' % (type(obj).__name__, unicode_me(obj)))
    
    item_name = item_func(parent)
    
    if isinstance(obj, numbers.Number) or type(obj) in (str, unicode):
        return convert_kv(item_name, obj, attr_type, cdata)
        
    if hasattr(obj, 'isoformat'):
        return convert_kv(item_name, obj.isoformat(), attr_type, cdata)
        
    if type(obj) == bool:
        return convert_bool(item_name, obj, attr_type, cdata)
        
    if obj is None:
        return convert_none(item_name, '', attr_type, cdata)
        
    if isinstance(obj, dict):
        return convert_dict(obj, ids, parent, attr_type, item_func, cdata)
        
    if isinstance(obj, collections.Iterable):
        return convert_list(obj, ids, parent, attr_type, item_func, cdata)
        
    raise TypeError('Unsupported data type: %s (%s)' % (obj, type(obj).__name__))","Routes the elements of an object to the right function to convert them 
    based on their data type",1,1,3,5
"def convert(self, money, to_currency, date=None):
        
        if str(money.currency) == str(to_currency):
            return copy.copy(money)
        return Money(
            amount=money.amount
            * self.rate(money.currency, to_currency, date or datetime.date.today()),
            currency=to_currency,
        )","Convert the given ``money`` to ``to_currency`` using exchange rate on ``date``

        If ``date`` is omitted then the date given by ``money.date`` will be used.",0,0,1,1
"def convert(self, path, version, target):
        
        source = self.comparer.get_representation(path)
        lines = [ '

        for line in self.comparer.template.contents[version].preamble:
            lines.append(line.write(source.preamble, source.version, source.stored) + ""\n"")

        for line in self.comparer.template.contents[version].body:
            for valueset in source.body:
                lines.append(line.write(valueset, source.version, source.stored) + ""\n"")

        with open(os.path.expanduser(target), 'w') as f:
            f.writelines(lines)",Converts the specified source file to a new version number.,1,0,1,2
"def convertDay(self, day, prefix="""", weekday=False):
        
        def sameDay(d1, d2):
            d = d1.day == d2.day
            m = d1.month == d2.month
            y = d1.year == d2.year
            return d and m and y

        tom = self.now + datetime.timedelta(days=1)

        if sameDay(day, self.now):
            return ""today""
        elif sameDay(day, tom):
            return ""tomorrow""

        if weekday:
            dayString = day.strftime(""%A, %B %d"")
        else:
            dayString = day.strftime(""%B %d"")

        
        if not int(dayString[-2]):
            dayString = dayString[:-2] + dayString[-1]

        return prefix + "" "" + dayString","Convert a datetime object representing a day into a human-ready
        string that can be read, spoken aloud, etc.

        Args:
            day (datetime.date): A datetime object to be converted into text.
            prefix (str): An optional argument that prefixes the converted
                string. For example, if prefix=""in"", you'd receive ""in two
                days"", rather than ""two days"", while the method would still
                return ""tomorrow"" (rather than ""in tomorrow"").
            weekday (bool): An optional argument that returns ""Monday, Oct. 1""
                if True, rather than ""Oct. 1"".

        Returns:
            A string representation of the input day, ignoring any time-related
            information.",0,0,2,2
"def convertDict2Attrs(self, *args, **kwargs):
        
        for n,u in enumerate(self.attrs):
            try:
                params = self.params
            except AttributeError as aerr:
                params = {}
            kwargs.update(params)
            try:
                user = self.mambuuserclass(urlfunc=None, entid=None, *args, **kwargs)
            except AttributeError as ae:
                self.mambuuserclass = self.itemclass
                user = self.mambuuserclass(urlfunc=None, entid=None, *args, **kwargs)
            user.init(u, *args, **kwargs)
            self.attrs[n] = user","The trick for iterable Mambu Objects comes here:

        You iterate over each element of the responded List from Mambu,
        and create a Mambu User (or your own itemclass) object for each
        one, initializing them one at a time, and changing the attrs
        attribute (which just holds a list of plain dictionaries) with a
        MambuUser (or your own itemclass) just created.

.. todo:: pass a valid (perhaps default) urlfunc, and its
          corresponding id to entid to each itemclass, telling MambuStruct
          not to connect() by default. It's desirable to connect at any
          other further moment to refresh some element in the list.",2,0,1,3
"def convert_command_output(*command):
    
    captured_output = capture(command)
    converted_output = convert(captured_output)
    if connected_to_terminal():
        fd, temporary_file = tempfile.mkstemp(suffix='.html')
        with open(temporary_file, 'w') as handle:
            handle.write(converted_output)
        webbrowser.open(temporary_file)
    elif captured_output and not captured_output.isspace():
        output(converted_output)","Command line interface for ``coloredlogs --to-html``.

    Takes a command (and its arguments) and runs the program under ``script``
    (emulating an interactive terminal), intercepts the output of the command
    and converts ANSI escape sequences in the output to HTML.",1,1,2,4
"def convert_float_to_two_registers(floatValue):
      
    myList = list()
    s = bytearray(struct.pack('<f', floatValue) )      
    myList.append(s[0] | (s[1]<<8))         
    myList.append(s[2] | (s[3]<<8))         

    return myList","Convert 32 Bit real Value to two 16 Bit Value to send as Modbus Registers
    floatValue: Value to be converted
    return: 16 Bit Register values int[]",0,0,1,1
"def convert_markdown(message):
    
    assert message['Content-Type'].startswith(""text/markdown"")
    del message['Content-Type']
    
    message = make_message_multipart(message)
    for payload_item in set(message.get_payload()):
        
        
        
        if payload_item['Content-Type'].startswith('text/plain'):
            original_text = payload_item.get_payload()
            html_text = markdown.markdown(original_text)
            html_payload = future.backports.email.mime.text.MIMEText(
                ""<html><body>{}</body></html>"".format(html_text),
                ""html"",
            )
            message.attach(html_payload)
    return message",Convert markdown in message text to HTML.,0,0,2,2
"def coordinate_filter(self, query, mongo_query):
        
        LOG.debug('Adding genomic coordinates to the query')
        chromosome = query['chrom']
        mongo_query['chromosome'] = chromosome

        if (query.get('start') and query.get('end')):
            mongo_query['position'] = {'$lte': int(query['end'])}
            mongo_query['end'] = {'$gte': int(query['start'])}

        return mongo_query","Adds genomic coordinated-related filters to the query object

        Args:
            query(dict): a dictionary of query filters specified by the users
            mongo_query(dict): the query that is going to be submitted to the database

        Returns:
            mongo_query(dict): returned object contains coordinate filters",1,0,1,2
"def copy(self, deep=True, data=None):
        
        variable = self.variable.copy(deep=deep, data=data)
        coords = OrderedDict((k, v.copy(deep=deep))
                             for k, v in self._coords.items())
        return self._replace(variable, coords)","Returns a copy of this array.

        If `deep=True`, a deep copy is made of the data array.
        Otherwise, a shallow copy is made, so each variable in the new
        array's dataset is also a variable in this array's dataset.

        Use `data` to create a new object with the same structure as
        original but entirely new data.

        Parameters
        ----------
        deep : bool, optional
            Whether the data array and its coordinates are loaded into memory
            and copied onto the new object. Default is True.
        data : array_like, optional
            Data to use in the new object. Must have same shape as original.
            When `data` is used, `deep` is ignored for all data variables,
            and only used for coords.

        Returns
        -------
        object : DataArray
            New object with dimensions, attributes, coordinates, name,
            encoding, and optionally data copied from original.

        Examples
        --------

        Shallow versus deep copy

        >>> array = xr.DataArray([1, 2, 3], dims='x',
        ...                      coords={'x': ['a', 'b', 'c']})
        >>> array.copy()
        <xarray.DataArray (x: 3)>
        array([1, 2, 3])
        Coordinates:
        * x        (x) <U1 'a' 'b' 'c'
        >>> array_0 = array.copy(deep=False)
        >>> array_0[0] = 7
        >>> array_0
        <xarray.DataArray (x: 3)>
        array([7, 2, 3])
        Coordinates:
        * x        (x) <U1 'a' 'b' 'c'
        >>> array
        <xarray.DataArray (x: 3)>
        array([7, 2, 3])
        Coordinates:
        * x        (x) <U1 'a' 'b' 'c'

        Changing the data using the ``data`` argument maintains the
        structure of the original object, but with the new data. Original
        object is unaffected.

        >>> array.copy(data=[0.1, 0.2, 0.3])
        <xarray.DataArray (x: 3)>
        array([ 0.1,  0.2,  0.3])
        Coordinates:
        * x        (x) <U1 'a' 'b' 'c'
        >>> array
        <xarray.DataArray (x: 3)>
        array([1, 2, 3])
        Coordinates:
        * x        (x) <U1 'a' 'b' 'c'

        See also
        --------
        pandas.DataFrame.copy",0,0,3,3
"def copy_all_logs_to_file(filename: str,
                          fmt: str = LOG_FORMAT,
                          datefmt: str = LOG_DATEFMT) -> None:
    
    fh = logging.FileHandler(filename)
    
    formatter = logging.Formatter(fmt=fmt, datefmt=datefmt)
    fh.setFormatter(formatter)
    apply_handler_to_all_logs(fh)","Copy all currently configured logs to the specified file.

    Should ONLY be called from the ``if __name__ == 'main'`` script;
    see https://docs.python.org/3.4/howto/logging.html#library-config.

    Args:
        filename: file to send log output to
        fmt: passed to the ``fmt=`` argument of :class:`logging.Formatter`
        datefmt: passed to the ``datefmt=`` argument of
            :class:`logging.Formatter`",1,0,0,1
"def copy_files(self, files=None, path_patterns=None, symbolic_links=True,
                   root=None, conflicts='fail', **kwargs):
        
        _files = self.get(return_type='objects', **kwargs)
        if files:
            _files = list(set(files).intersection(_files))

        for f in _files:
            f.copy(path_patterns, symbolic_link=symbolic_links,
                   root=self.root, conflicts=conflicts)","Copies one or more BIDSFiles to new locations defined by each
        BIDSFile's entities and the specified path_patterns.

        Args:
            files (list): Optional list of BIDSFile objects to write out. If
                none provided, use files from running a get() query using
                remaining **kwargs.
            path_patterns (str, list): Write patterns to pass to each file's
                write_file method.
            symbolic_links (bool): Whether to copy each file as a symbolic link
                or a deep copy.
            root (str): Optional root directory that all patterns are relative
                to. Defaults to current working directory.
            conflicts (str):  Defines the desired action when the output path
                already exists. Must be one of:
                    'fail': raises an exception
                    'skip' does nothing
                    'overwrite': overwrites the existing file
                    'append': adds  a suffix to each file copy, starting with 1
            kwargs (kwargs): Optional key word arguments to pass into a get()
                query.",1,0,3,4
"def copy_files(source_files, target_directory, source_directory=None):
    
    try:
        os.makedirs(target_directory)
    except:     
        pass
    for f in source_files:
        source = os.path.join(source_directory, f) if source_directory else f
        target = os.path.join(target_directory, f)
        shutil.copy2(source, target)","Copies a list of files to the specified directory.
    If source_directory is provided, it will be prepended to each source file.",1,0,3,4
"def copy_many_times_events(self, other):
        
        events = self.events()
        other_events = other.events()
        if events and other_events:
            for name, event in other_events.items():
                handlers = event.handlers()
                if not event.onetime() and handlers:
                    ev = events.get(name)
                    
                    if ev:
                        for callback in handlers:
                            ev.bind(callback)","Copy :ref:`many times events <many-times-event>` from  ``other``.

        All many times events of ``other`` are copied to this handler
        provided the events handlers already exist.",0,0,2,2
"def corr2(X):
    
    X = np.atleast_3d(X)
    N = X.shape[0]
    D = X.shape[2]

    if D == 1:
        return np.zeros(N, dtype=np.float)

    trii = np.triu_indices(D, k=1)
    DD = len(trii[0])
    r = np.zeros((N, DD))
    for i in np.arange(N):
        rmat = np.corrcoef(X[i])  
        r[i] = rmat[trii]
    return r","computes correlations between all variable pairs in a segmented time series

    .. note:: this feature is expensive to compute with the current implementation, and cannot be
    used with univariate time series",0,0,1,1
"def count_dataset(train=False,
                  dev=False,
                  test=False,
                  train_rows=10000,
                  dev_rows=1000,
                  test_rows=1000,
                  seq_max_length=10):
    
    ret = []
    for is_requested, n_rows in [(train, train_rows), (dev, dev_rows), (test, test_rows)]:
        rows = []
        for i in range(n_rows):
            length = random.randint(1, seq_max_length)
            seq = []
            for _ in range(length):
                seq.append(str(random.randint(0, 9)))
            input_ = ' '.join(seq)
            rows.append({'numbers': input_, 'count': str(length)})

        
        
        
        if not is_requested:
            continue

        ret.append(Dataset(rows))

    if len(ret) == 1:
        return ret[0]
    else:
        return tuple(ret)","Load the Count dataset.

    The Count dataset is a simple task of counting the number of integers in a sequence. This
    dataset is useful for testing implementations of sequence to label models.

    Args:
        train (bool, optional): If to load the training split of the dataset.
        dev (bool, optional): If to load the development split of the dataset.
        test (bool, optional): If to load the test split of the dataset.
        train_rows (int, optional): Number of training rows to generate.
        dev_rows (int, optional): Number of development rows to generate.
        test_rows (int, optional): Number of test rows to generate.
        seq_max_length (int, optional): Maximum sequence length.

    Returns:
        :class:`tuple` of :class:`torchnlp.datasets.Dataset` or :class:`torchnlp.datasets.Dataset`:
        Returns between one and all dataset splits (train, dev and test) depending on if their
        respective boolean argument is ``True``.

    Example:
        >>> import random
        >>> random.seed(321)
        >>>
        >>> from torchnlp.datasets import count_dataset
        >>> train = count_dataset(train=True)
        >>> train[0:2]
        [{'numbers': '6 2 5 8 7', 'count': '5'}, {'numbers': '3 9 7 6 6 7', 'count': '6'}]",0,0,1,1
"def course_discovery_api_client(user, catalog_url):
    
    if JwtBuilder is None:
        raise NotConnectedToOpenEdX(
            _(""To get a Catalog API client, this package must be ""
              ""installed in an Open edX environment."")
        )

    jwt = JwtBuilder.create_jwt_for_user(user)
    return EdxRestApiClient(catalog_url, jwt=jwt)",Return a Course Discovery API client setup with authentication for the specified user.,1,1,1,3
"def covariance_between_points(self, kern, X, X1, X2):
        
        
        if self.woodbury_chol.ndim != 2:
            raise RuntimeError(""This method does not support posterior for missing data models"")

        Kx1 = kern.K(X, X1)
        Kx2 = kern.K(X, X2)
        K12 = kern.K(X1, X2)

        tmp1 = dtrtrs(self.woodbury_chol, Kx1)[0]
        tmp2 = dtrtrs(self.woodbury_chol, Kx2)[0]
        var = K12 - tmp1.T.dot(tmp2)

        return var","Computes the posterior covariance between points.

        :param kern: GP kernel
        :param X: current input observations
        :param X1: some input observations
        :param X2: other input observations",0,0,1,1
"def create(*context, **kwargs):
        
        items = context

        context = ContextStack()

        for item in items:
            if item is None:
                continue
            if isinstance(item, ContextStack):
                context._stack.extend(item._stack)
            else:
                context.push(item)

        if kwargs:
            context.push(kwargs)

        return context","Build a ContextStack instance from a sequence of context-like items.

        This factory-style method is more general than the ContextStack class's
        constructor in that, unlike the constructor, the argument list
        can itself contain ContextStack instances.

        Here is an example illustrating various aspects of this method:

        >>> obj1 = {'animal': 'cat', 'vegetable': 'carrot', 'mineral': 'copper'}
        >>> obj2 = ContextStack({'vegetable': 'spinach', 'mineral': 'silver'})
        >>>
        >>> context = ContextStack.create(obj1, None, obj2, mineral='gold')
        >>>
        >>> context.get('animal')
        'cat'
        >>> context.get('vegetable')
        'spinach'
        >>> context.get('mineral')
        'gold'

        Arguments:

          *context: zero or more dictionaries, ContextStack instances, or objects
            with which to populate the initial context stack.  None
            arguments will be skipped.  Items in the *context list are
            added to the stack in order so that later items in the argument
            list take precedence over earlier items.  This behavior is the
            same as the constructor's.

          **kwargs: additional key-value data to add to the context stack.
            As these arguments appear after all items in the *context list,
            in the case of key conflicts these values take precedence over
            all items in the *context list.  This behavior is the same as
            the constructor's.",0,0,2,2
"def create(cls, name, users=None):
        

        api = Client.instance().api

        database_data = {
            'name': name,
            'active': True,
        }

        if isinstance(users, list) or isinstance(users, tuple):
            database_data['users'] = users

        data = api.database.post(data=database_data)

        db = Database(
            name=name,
            api=api,
            kwargs=data
        )

        Client.instance().set_database(name=name)

        return db","Creates database and sets itself as the active database.

            :param name Database name

            :returns Database",1,0,1,2
"def create(cls, user_id, client_id, token, secret,
               token_type='', extra_data=None):
        
        account = RemoteAccount.get(user_id, client_id)

        with db.session.begin_nested():
            if account is None:
                account = RemoteAccount(
                    user_id=user_id,
                    client_id=client_id,
                    extra_data=extra_data or dict(),
                )
                db.session.add(account)

            token = cls(
                token_type=token_type,
                remote_account=account,
                access_token=token,
                secret=secret,
            )
            db.session.add(token)
        return token","Create a new access token.

        .. note:: Creates RemoteAccount as well if it does not exists.

        :param user_id: The user id.
        :param client_id: The client id.
        :param token: The token.
        :param secret: The secret key.
        :param token_type: The token type. (Default: ``''``)
        :param extra_data: Extra data to set in the remote account if the
            remote account doesn't exists. (Default: ``None``)
        :returns: A :class:`invenio_oauthclient.models.RemoteToken` instance.",3,1,2,6
"def create(self):
        
        self.service.create()

        os.environ[predix.config.get_env_key(self.use_class, 'host')] = self.get_eventhub_host()
        os.environ[predix.config.get_env_key(self.use_class, 'port')] = self.get_eventhub_grpc_port()
        os.environ[predix.config.get_env_key(self.use_class, 'wss_publish_uri')] = self.get_publish_wss_uri()
        os.environ[predix.config.get_env_key(self.use_class, 'zone_id')] = self.get_zone_id()","Create an instance of the Time Series Service with the typical
        starting settings.",0,0,1,1
"def create(self, client_id, subject, name, from_name, from_email, reply_to, html_url,
               text_url, list_ids, segment_ids):
        
        body = {
            ""Subject"": subject,
            ""Name"": name,
            ""FromName"": from_name,
            ""FromEmail"": from_email,
            ""ReplyTo"": reply_to,
            ""HtmlUrl"": html_url,
            ""TextUrl"": text_url,
            ""ListIDs"": list_ids,
            ""SegmentIDs"": segment_ids}
        response = self._post(""/campaigns/%s.json"" %
                              client_id, json.dumps(body))
        self.campaign_id = json_to_py(response)
        return self.campaign_id","Creates a new campaign for a client.

        :param client_id: String representing the ID of the client for whom the
          campaign will be created.
        :param subject: String representing the subject of the campaign.
        :param name: String representing the name of the campaign.
        :param from_name: String representing the from name for the campaign.
        :param from_email: String representing the from address for the campaign.
        :param reply_to: String representing the reply-to address for the campaign.
        :param html_url: String representing the URL for the campaign HTML content.
        :param text_url: String representing the URL for the campaign text content.
          Note that text_url is optional and if None or an empty string, text
          content will be automatically generated from the HTML content.
        :param list_ids: Array of Strings representing the IDs of the lists to
          which the campaign will be sent.
        :param segment_ids: Array of Strings representing the IDs of the segments to
          which the campaign will be sent.
        :returns String representing the ID of the newly created campaign.",0,1,0,1
"def create(self, data=None, uri=None, timeout=-1, custom_headers=None, force=False):
        
        if not uri:
            uri = self._base_uri

        if force:
            uri += '?force={}'.format(force)

        logger.debug('Create (uri = %s, resource = %s)' % (uri, str(data)))

        return self.do_post(uri, data, timeout, custom_headers)","Makes a POST request to create a resource when a request body is required.

        Args:
            data: Additional fields can be passed to create the resource.
            uri: Resouce uri
            timeout: Timeout in seconds. Wait for task completion by default. The timeout does not abort the operation
                in OneView; it just stops waiting for its completion.
            custom_headers: Allows set specific HTTP headers.
        Returns:
            Created resource.",0,2,1,3
"def create(self, dcid, vpsplanid, osid, params=None):
        
        params = update_params(params, {
            'DCID': dcid,
            'VPSPLANID': vpsplanid,
            'OSID': osid
        })
        return self.request('/v1/server/create', params, 'POST')","/v1/server/create
        POST - account
        Create a new virtual machine. You will start being billed for this
        immediately. The response only contains the SUBID for the new machine.
        You should use v1/server/list to poll and wait for the machine to be
        created (as this does not happen instantly).

        Link: https://www.vultr.com/api/#server_create",0,1,0,1
"def create(self, friendly_name=values.unset, domain_name=values.unset,
               disaster_recovery_url=values.unset,
               disaster_recovery_method=values.unset, recording=values.unset,
               secure=values.unset, cnam_lookup_enabled=values.unset):
        
        data = values.of({
            'FriendlyName': friendly_name,
            'DomainName': domain_name,
            'DisasterRecoveryUrl': disaster_recovery_url,
            'DisasterRecoveryMethod': disaster_recovery_method,
            'Recording': recording,
            'Secure': secure,
            'CnamLookupEnabled': cnam_lookup_enabled,
        })

        payload = self._version.create(
            'POST',
            self._uri,
            data=data,
        )

        return TrunkInstance(self._version, payload, )","Create a new TrunkInstance

        :param unicode friendly_name: A string to describe the resource
        :param unicode domain_name: The unique address you reserve on Twilio to which you route your SIP traffic
        :param unicode disaster_recovery_url: The HTTP URL that we should call if an error occurs while sending SIP traffic towards your configured Origination URL
        :param unicode disaster_recovery_method: The HTTP method we should use to call the disaster_recovery_url
        :param TrunkInstance.RecordingSetting recording: The recording settings for the trunk
        :param bool secure: Whether Secure Trunking is enabled for the trunk
        :param bool cnam_lookup_enabled: Whether Caller ID Name (CNAM) lookup should be enabled for the trunk

        :returns: Newly created TrunkInstance
        :rtype: twilio.rest.trunking.v1.trunk.TrunkInstance",0,0,1,1
"def create(self, from_=None, **kwargs):
        

        if ""dummy"" in kwargs and kwargs[""dummy""]:
            response, instance = self.request(""POST"", self.uri, data=kwargs)
            return instance
        if from_:
            kwargs[""from""] = from_
        return self.create_instance(kwargs)","Create and send a new outbound message.
        Returns :class:`Message` object contains next attributes: 'id', 'href', 'type',
        'sessionId', 'bulkId', 'messageId', 'scheduledId'.

        :Example:

        message = client.messages.create(from_=""447624800500"", phones=""999000001"", text=""Hello!"", lists=""1909100"")

        :param str text:         Message text. Required if templateId is not set.
        :param str templateId:   Template used instead of message text. Required if text is not set.
        :param str sendingTime:  Message sending time in unix timestamp format. Default is now.
                                 Optional (required with rrule set).
        :param str contacts:     Contacts ids, separated by comma, message will be sent to.
        :param str lists:        Lists ids, separated by comma, message will be sent to.
        :param str phones:       Phone numbers, separated by comma, message will be sent to.
        :param int cutExtra:     Should sending method cut extra characters
                                 which not fit supplied partsCount or return 400 Bad request response instead.
                                 Default is false.
        :param int partsCount:   Maximum message parts count (TextMagic allows sending 1 to 6 message parts).
                                 Default is 6.
        :param str referenceId:  Custom message reference id which can be used in your application infrastructure.
        :param str from_:        One of allowed Sender ID (phone number or alphanumeric sender ID).
        :param str rrule:        iCal RRULE parameter to create recurrent scheduled messages.
                                 When used, sendingTime is mandatory as start point of sending.
        :param int dummy:        If 1, just return message pricing. Message will not send.",0,1,1,2
"def create(self, identity, role_sid=values.unset, attributes=values.unset,
               friendly_name=values.unset):
        
        data = values.of({
            'Identity': identity,
            'RoleSid': role_sid,
            'Attributes': attributes,
            'FriendlyName': friendly_name,
        })

        payload = self._version.create(
            'POST',
            self._uri,
            data=data,
        )

        return UserInstance(self._version, payload, service_sid=self._solution['service_sid'], )","Create a new UserInstance

        :param unicode identity: The `identity` value that identifies the new resource's User
        :param unicode role_sid: The SID of the Role assigned to this user
        :param unicode attributes: A valid JSON string that contains application-specific data
        :param unicode friendly_name: A string to describe the new resource

        :returns: Newly created UserInstance
        :rtype: twilio.rest.chat.v2.service.user.UserInstance",0,1,1,2
"def create(self, name, backend_router_id, flavor, instances, test=False):
        

        
        args = (self.capacity_package, 0, [flavor])
        extras = {""backendRouterId"": backend_router_id, ""name"": name}
        kwargs = {
            'extras': extras,
            'quantity': instances,
            'complex_type': 'SoftLayer_Container_Product_Order_Virtual_ReservedCapacity',
            'hourly': True
        }
        if test:
            receipt = self.ordering_manager.verify_order(*args, **kwargs)
        else:
            receipt = self.ordering_manager.place_order(*args, **kwargs)
        return receipt","Orders a Virtual_ReservedCapacityGroup

        :param string name: Name for the new reserved capacity
        :param int backend_router_id: This selects the pod. See create_options for a list
        :param string flavor: Capacity KeyName, see create_options for a list
        :param int instances: Number of guest this capacity can support
        :param bool test: If True, don't actually order, just test.",0,0,3,3
"def create(self, name: str, descriptor: str, value: Constant=None) -> Field:
        
        field = Field(self._cf)
        name = self._cf.constants.create_utf8(name)
        descriptor = self._cf.constants.create_utf8(descriptor)
        field._name_index = name.index
        field._descriptor_index = descriptor.index
        field.access_flags.acc_public = True

        if value is not None:
            field.attributes.create(ConstantValueAttribute, value)
            field.access_flags.acc_static = True

        self.append(field)
        return field","Creates a new field from `name` and `descriptor`. For example::

            >>> from jawa.cf import ClassFile
            >>> cf = ClassFile.create('BeerCounter')
            >>> field = cf.fields.create('BeerCount', 'I')

        To automatically create a static field, pass a value::

            >>> from jawa.cf import ClassFile
            >>> cf = ClassFile.create('BeerCounter')
            >>> field = cf.fields.create(
            ...     'MaxBeer',
            ...     'I',
            ...     cf.constants.create_integer(99)
            ... )

        :param name: Name of the new field.
        :param descriptor: Type descriptor of the new field.
        :param value: Optional static value for the field.",0,0,1,1
"def create(self, resource, id=None, timeout=-1):
        
        if not id:
            available_id = self.__get_first_available_id()
            uri = '%s/%s' % (self.URI, str(available_id))
        else:
            uri = '%s/%s' % (self.URI, str(id))
        return self._client.create(resource, uri=uri, timeout=timeout)","Adds the specified trap forwarding destination.
        The trap destination associated with the specified id will be created if trap destination with that id does not exists.
        The id can only be an integer greater than 0.

        Args:
            resource (dict): Object to create.
            timeout:
                Timeout in seconds. Wait for task completion by default. The timeout does not abort the operation
                in OneView, just stop waiting for its completion.

        Returns:
            dict: Created resource.",0,1,0,1
"def create(self, status_callback=values.unset, unique_name=values.unset):
        
        data = values.of({'StatusCallback': status_callback, 'UniqueName': unique_name, })

        payload = self._version.create(
            'POST',
            self._uri,
            data=data,
        )

        return ModelBuildInstance(self._version, payload, assistant_sid=self._solution['assistant_sid'], )","Create a new ModelBuildInstance

        :param unicode status_callback: The URL we should call using a POST method to send status information to your application
        :param unicode unique_name: An application-defined string that uniquely identifies the new resource

        :returns: Newly created ModelBuildInstance
        :rtype: twilio.rest.autopilot.v1.assistant.model_build.ModelBuildInstance",1,1,0,2
"def create(self, stylename, **kwargs):
        
        if stylename == ""default"":    
            self[stylename] = style(stylename, self._ctx, **kwargs)
            return self[stylename]
        k = kwargs.get(""template"", ""default"")
        s = self[stylename] = self[k].copy(stylename)
        for attr in kwargs:
            if s.__dict__.has_key(attr):
                s.__dict__[attr] = kwargs[attr]
        return s","Creates a new style which inherits from the default style,
        or any other style which name is supplied to the optional template parameter.",0,0,2,2
"def createSQL(self, sql, args=()):
        
        before = time.time()
        self._execSQL(sql, args)
        after = time.time()
        if after - before > 2.0:
            log.msg('Extremely long CREATE: %s' % (after - before,))
            log.msg(sql)","For use with auto-committing statements such as CREATE TABLE or CREATE
        INDEX.",1,1,0,2
"def create_account_user(self, account_id, body, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('asynchronous'):
            return self.create_account_user_with_http_info(account_id, body, **kwargs)  
        else:
            (data) = self.create_account_user_with_http_info(account_id, body, **kwargs)  
            return data","Create a new user.  # noqa: E501

        An endpoint for creating or inviting a new user to the account. In case of invitation email address is used only, other attributes are set in the 2nd step.   **Example usage:** `curl -X POST https://api.us-east-1.mbedcloud.com/v3/accounts/{accountID}/users -d {\""email\"": \""myemail@company.com\""} -H 'content-type: application/json' -H 'Authorization: Bearer API_KEY'`  # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass asynchronous=True
        >>> thread = api.create_account_user(account_id, body, asynchronous=True)
        >>> result = thread.get()

        :param asynchronous bool
        :param str account_id: Account ID. (required)
        :param UserInfoReq body: A user object with attributes. (required)
        :param str action: Create or invite user.
        :return: UserInfoResp
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def create_appointment_group(self, appointment_group, **kwargs):
        
        from canvasapi.appointment_group import AppointmentGroup

        if (
                isinstance(appointment_group, dict) and
                'context_codes' in appointment_group and
                'title' in appointment_group
        ):
            kwargs['appointment_group'] = appointment_group

        elif (
            isinstance(appointment_group, dict) and
            'context_codes' not in appointment_group
        ):
            raise RequiredFieldMissing(
                ""Dictionary with key 'context_codes' is missing.""
            )

        elif isinstance(appointment_group, dict) and 'title' not in appointment_group:
            raise RequiredFieldMissing(""Dictionary with key 'title' is missing."")

        response = self.__requester.request(
            'POST',
            'appointment_groups',
            _kwargs=combine_kwargs(**kwargs)
        )

        return AppointmentGroup(self.__requester, response.json())","Create a new Appointment Group.

        :calls: `POST /api/v1/appointment_groups \
        <https://canvas.instructure.com/doc/api/appointment_groups.html#method.appointment_groups.create>`_

        :param appointment_group: The attributes of the appointment group.
        :type appointment_group: `dict`
        :param title: The title of the appointment group.
        :type title: `str`
        :rtype: :class:`canvasapi.appointment_group.AppointmentGroup`",0,1,0,1
"def create_attach_volumes(name, kwargs, call=None):
    
    if call != 'action':
        raise SaltCloudSystemExit(
            'The create_attach_volumes action must be called with '
            '-a or --action.'
        )

    volumes = literal_eval(kwargs['volumes'])
    node = kwargs['node']
    conn = get_conn()
    node_data = _expand_node(conn.ex_get_node(node))
    letter = ord('a') - 1

    for idx, volume in enumerate(volumes):
        volume_name = '{0}-sd{1}'.format(name, chr(letter + 2 + idx))

        volume_dict = {
          'disk_name': volume_name,
          'location': node_data['extra']['zone']['name'],
          'size': volume['size'],
          'type': volume.get('type', 'pd-standard'),
          'image': volume.get('image', None),
          'snapshot': volume.get('snapshot', None),
          'auto_delete': volume.get('auto_delete', False)
        }

        create_disk(volume_dict, 'function')
        attach_disk(name, volume_dict, 'action')",".. versionadded:: 2017.7.0

    Create and attach multiple volumes to a node. The 'volumes' and 'node'
    arguments are required, where 'node' is a libcloud node, and 'volumes'
    is a list of maps, where each map contains:

    size
        The size of the new disk in GB. Required.

    type
        The disk type, either pd-standard or pd-ssd. Optional, defaults to pd-standard.

    image
        An image to use for this new disk. Optional.

    snapshot
        A snapshot to use for this new disk. Optional.

    auto_delete
        An option(bool) to keep or remove the disk upon instance deletion.
        Optional, defaults to False.

    Volumes are attached in the order in which they are given, thus on a new
    node the first volume will be /dev/sdb, the second /dev/sdc, and so on.",1,0,2,3
"def create_bioset_lookup(lookupdb, spectrafns, set_names):
    
    unique_setnames = set(set_names)
    lookupdb.store_biosets(((x,) for x in unique_setnames))
    set_id_map = lookupdb.get_setnames()
    mzmlfiles = ((os.path.basename(fn), set_id_map[setname])
                 for fn, setname in zip(spectrafns, set_names))
    lookupdb.store_mzmlfiles(mzmlfiles)
    lookupdb.index_biosets()",Fills lookup database with biological set names,1,0,0,1
"def create_column_index(annotations):
    
    _column_index = OrderedDict({'Column Name' : annotations['Column Name']})
    categorical_rows = annotation_rows('C:', annotations)
    _column_index.update(categorical_rows)
    numerical_rows = {name: [float(x) if x != '' else float('NaN') for x in values]
            for name, values in annotation_rows('N:', annotations).items()} 
    _column_index.update(numerical_rows)
    column_index = pd.MultiIndex.from_tuples(list(zip(*_column_index.values())), names=list(_column_index.keys()))
    if len(column_index.names) == 1:
        
        name = column_index.names[0]
        column_index = column_index.get_level_values(name)
    return column_index","Create a pd.MultiIndex using the column names and any categorical rows.
    Note that also non-main columns will be assigned a default category ''.",0,0,1,1
"def create_community(self, token, name, **kwargs):
        
        parameters = dict()
        parameters['token'] = token
        parameters['name'] = name
        optional_keys = ['description', 'uuid', 'privacy', 'can_join']
        for key in optional_keys:
            if key in kwargs:
                if key == 'can_join':
                    parameters['canjoin'] = kwargs[key]
                    continue
                parameters[key] = kwargs[key]
        response = self.request('midas.community.create', parameters)
        return response","Create a new community or update an existing one using the uuid.

        :param token: A valid token for the user in question.
        :type token: string
        :param name: The community name.
        :type name: string
        :param description: (optional) The community description.
        :type description: string
        :param uuid: (optional) uuid of the community. If none is passed, will
            generate one.
        :type uuid: string
        :param privacy: (optional) Default 'Public', possible values
            [Public|Private].
        :type privacy: string
        :param can_join: (optional) Default 'Everyone', possible values
            [Everyone|Invitation].
        :type can_join: string
        :returns: The community dao that was created.
        :rtype: dict",2,0,1,3
"def create_credit_card(self, *, customer_id, name, document, number, exp_month, exp_year, type, address):
        
        payload = {
            ""name"": name,
            ""document"": document,
            ""number"": number,
            ""expMonth"": exp_month,
            ""expYear"": exp_year,
            ""type"": type,
            ""address"": address
        }
        fmt = 'customers/{}/creditCards'.format(customer_id)
        return self.client._post(self.url + fmt, json=payload, headers=self.get_headers())","Creating a credit card (Token) and assigning it to a user.

        Args:
            customer_id: Identifier of the client with whom you are going to associate the token with.

            name: Full name of the credit card holder as shown in the credit card.
            Alphanumeric. Min: 1 Max: 255.

            document: Identification number of the credit card holder.
            Alphanumeric. Min: 1 Max: 30.

            number: Credit card number.
            Numeric. Min: 13 Max: 20.

            exp_month: Credit card’s expiration month.
            Numeric. Min: 1 Max: 12.

            exp_year: Credit card’s expiration year. If it is a two digit value, it represents the years
            between 2000 (00) and 2099 (99). It the value has more than two digits it is used literally,
            being 2000 the minimum value.
            Numeric. Min: 00 Max: 2999.

            type: The franchise or credit card type.
            http://developers.payulatam.com/en/api/variables_table.html
            Alphanumeric.

            address: Credit card holder's billing address, associated to the credit card.

        Returns:",0,1,0,1
"def create_file(self, share_name, directory_name, file_name,
                    content_length, content_settings=None, metadata=None,
                    timeout=None):
        
        _validate_not_none('share_name', share_name)
        _validate_not_none('file_name', file_name)
        _validate_not_none('content_length', content_length)
        request = HTTPRequest()
        request.method = 'PUT'
        request.host_locations = self._get_host_locations()
        request.path = _get_path(share_name, directory_name, file_name)
        request.query = {'timeout': _int_to_str(timeout)}
        request.headers = {
            'x-ms-content-length': _to_str(content_length),
            'x-ms-type': 'file'
        }
        _add_metadata_headers(metadata, request)
        if content_settings is not None:
            request.headers.update(content_settings._to_headers())

        self._perform_request(request)","Creates a new file.

        See create_file_from_* for high level functions that handle the
        creation and upload of large files with automatic chunking and
        progress notifications.

        :param str share_name:
            Name of existing share.
        :param str directory_name:
            The path to the directory.
        :param str file_name:
            Name of file to create or update.
        :param int content_length:
            Length of the file in bytes.
        :param ~azure.storage.file.models.ContentSettings content_settings:
            ContentSettings object used to set file properties.
        :param metadata:
            Name-value pairs associated with the file as metadata.
        :type metadata: dict(str, str)
        :param int timeout:
            The timeout parameter is expressed in seconds.",1,1,0,2
"def create_floating_ip(self, droplet_id=None, region=None, **kwargs):
        
        if (droplet_id is None) == (region is None):
            
            raise TypeError('Exactly one of ""droplet_id"" and ""region"" must be'
                            ' specified')
        if droplet_id is not None:
            if isinstance(droplet_id, Droplet):
                droplet_id = droplet_id.id
            data = {""droplet_id"": droplet_id}
        else:
            if isinstance(region, Region):
                region = region.slug
            data = {""region"": region}
        data.update(kwargs)
        return self._floating_ip(self.request('/v2/floating_ips', method='POST',
                                              data=data)[""floating_ip""])","Create a new floating IP assigned to a droplet or reserved to a region.
        Either ``droplet_id`` or ``region`` must be specified, but not both.

        The returned `FloatingIP` object will represent the IP at the moment of
        creation; if the IP address is supposed to be assigned to a droplet,
        the assignment may not have been completed at the time the object is
        returned.  To wait for the assignment to complete, use the
        `FloatingIP`'s :meth:`~FloatingIP.wait_for_action` method.

        :param droplet_id: the droplet to assign the floating IP to as either
            an ID or a `Droplet` object
        :type droplet_id: integer or `Droplet`
        :param region: the region to reserve the floating IP to as either a
            slug or a `Region` object
        :type region: string or `Region`
        :param kwargs: additional fields to include in the API request
        :return: the new floating IP
        :rtype: FloatingIP
        :raises TypeError: if both ``droplet_id`` & ``region`` or neither of
            them are defined
        :raises DOAPIError: if the API endpoint replies with an error",1,1,1,3
"def create_free_shipping(cls, free_shipping, **kwargs):
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async'):
            return cls._create_free_shipping_with_http_info(free_shipping, **kwargs)
        else:
            (data) = cls._create_free_shipping_with_http_info(free_shipping, **kwargs)
            return data","Create FreeShipping

        Create a new FreeShipping
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async=True
        >>> thread = api.create_free_shipping(free_shipping, async=True)
        >>> result = thread.get()

        :param async bool
        :param FreeShipping free_shipping: Attributes of freeShipping to create (required)
        :return: FreeShipping
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def create_from_userdata(userdata):
        
        memhi = userdata.get('d3')
        memlo = userdata.get('d4')
        memory = memhi << 8 | memlo
        control_flags = userdata.get('d6')
        group = userdata.get('d7')
        addrhi = userdata.get('d8')
        addrmed = userdata.get('d9')
        addrlo = userdata.get('d10')
        addr = Address(bytearray([addrhi, addrmed, addrlo]))
        data1 = userdata.get('d11')
        data2 = userdata.get('d12')
        data3 = userdata.get('d13')
        return ALDBRecord(memory, control_flags, group, addr,
                          data1, data2, data3)",Create ALDB Record from the userdata dictionary.,0,0,1,1
"def create_gist(self, public, files, description=github.GithubObject.NotSet):
        
        assert isinstance(public, bool), public
        assert all(isinstance(element, github.InputFileContent) for element in files.itervalues()), files
        assert description is github.GithubObject.NotSet or isinstance(description, (str, unicode)), description
        post_parameters = {
            ""public"": public,
            ""files"": {key: value._identity for key, value in files.iteritems()},
        }
        if description is not github.GithubObject.NotSet:
            post_parameters[""description""] = description
        headers, data = self._requester.requestJsonAndCheck(
            ""POST"",
            ""/gists"",
            input=post_parameters
        )
        return github.Gist.Gist(self._requester, headers, data, completed=True)",":calls: `POST /gists <http://developer.github.com/v3/gists>`_
        :param public: bool
        :param files: dict of string to :class:`github.InputFileContent.InputFileContent`
        :param description: string
        :rtype: :class:`github.Gist.Gist`",1,1,0,2
"def create_jwt_token(secret, client_id):
    
    assert secret, ""Missing secret key""
    assert client_id, ""Missing client id""

    headers = {
        ""typ"": __type__,
        ""alg"": __algorithm__
    }

    claims = {
        'iss': client_id,
        'iat': epoch_seconds()
    }

    return jwt.encode(payload=claims, key=secret, headers=headers).decode()","Create JWT token for GOV.UK Notify

    Tokens have standard header:
    {
        ""typ"": ""JWT"",
        ""alg"": ""HS256""
    }

    Claims consist of:
    iss: identifier for the client
    iat: issued at in epoch seconds (UTC)

    :param secret: Application signing secret
    :param client_id: Identifier for the client
    :return: JWT token for this request",0,0,1,1
"def create_key(self, title, key, read_only=False):
        
        assert isinstance(title, (str, unicode)), title
        assert isinstance(key, (str, unicode)), key
        assert isinstance(read_only, bool), read_only
        post_parameters = {
            ""title"": title,
            ""key"": key,
            ""read_only"": read_only,
        }
        headers, data = self._requester.requestJsonAndCheck(
            ""POST"",
            self.url + ""/keys"",
            input=post_parameters
        )
        return github.RepositoryKey.RepositoryKey(self._requester, headers, data, completed=True)",":calls: `POST /repos/:owner/:repo/keys <http://developer.github.com/v3/repos/keys>`_
        :param title: string
        :param key: string
        :param read_only: bool
        :rtype: :class:`github.RepositoryKey.RepositoryKey`",0,2,2,4
"def create_kubernetes_configuration(self, kubernetes_host, kubernetes_ca_cert=None, token_reviewer_jwt=None, pem_keys=None, mount_point='kubernetes'):
        
        params = {
            'kubernetes_host': kubernetes_host,
            'kubernetes_ca_cert': kubernetes_ca_cert,
        }

        if token_reviewer_jwt is not None:
            params['token_reviewer_jwt'] = token_reviewer_jwt
        if pem_keys is not None:
            params['pem_keys'] = pem_keys

        url = 'v1/auth/{0}/config'.format(mount_point)
        return self._adapter.post(url, json=params)","POST /auth/<mount_point>/config

        :param kubernetes_host: A host:port pair, or a URL to the base of the Kubernetes API server.
        :type kubernetes_host: str.
        :param kubernetes_ca_cert: PEM encoded CA cert for use by the TLS client used to talk with the Kubernetes API.
        :type kubernetes_ca_cert: str.
        :param token_reviewer_jwt: A service account JWT used to access the TokenReview API to validate other
            JWTs during login. If not set the JWT used for login will be used to access the API.
        :type token_reviewer_jwt: str.
        :param pem_keys: Optional list of PEM-formated public keys or certificates used to verify the signatures of
            Kubernetes service account JWTs. If a certificate is given, its public key will be extracted. Not every
            installation of Kubernetes exposes these keys.
        :type pem_keys: list.
        :param mount_point: The ""path"" the k8s auth backend was mounted on. Vault currently defaults to ""kubernetes"".
        :type mount_point: str.
        :return: Will be an empty body with a 204 status code upon success
        :rtype: requests.Response.",0,1,1,2
"def create_logger_for_context(self, logger_name, context):
        

        if self._is_instance_of(context, 'AutoLoadCommandContext'):
            reservation_id = 'Autoload'
            handler_name = context.resource.name
        elif self._is_instance_of(context, 'UnreservedResourceCommandContext'):
            reservation_id = 'DeleteArtifacts'
            handler_name = context.resource.name
        else:
            reservation_id = self._get_reservation_id(context)

            if self._is_instance_of(context, 'ResourceCommandContext'):
                handler_name = context.resource.name
            elif self._is_instance_of(context, 'ResourceRemoteCommandContext'):
                handler_name = context.remote_endpoints[0].name
            else:
                raise Exception(ContextBasedLoggerFactory.UNSUPPORTED_CONTEXT_PROVIDED, context)

        logger = get_qs_logger(log_file_prefix=handler_name,
                               log_group=reservation_id,
                               log_category=logger_name)
        return logger","Create QS Logger for command context AutoLoadCommandContext or ResourceCommandContext
        :param logger_name:
        :type logger_name: str
        :param context:
        :return:",1,0,3,4
"def create_model(self, parent, name, multiplicity='ZERO_MANY', **kwargs):
        
        if parent.category != Category.MODEL:
            raise IllegalArgumentError(""The parent should be of category 'MODEL'"")

        data = {
            ""name"": name,
            ""parent"": parent.id,
            ""multiplicity"": multiplicity
        }

        return self._create_part(action=""create_child_model"", data=data, **kwargs)","Create a new child model under a given parent.

        In order to prevent the backend from updating the frontend you may add `suppress_kevents=True` as
        additional keyword=value argument to this method. This will improve performance of the backend
        against a trade-off that someone looking at the frontend won't notice any changes unless the page
        is refreshed.

        :param parent: parent model
        :param name: new model name
        :param parent: parent part instance
        :type parent: :class:`models.Part`
        :param name: new part name
        :type name: basestring
        :param multiplicity: choose between ZERO_ONE, ONE, ZERO_MANY, ONE_MANY or M_N
        :type multiplicity: basestring
        :param kwargs: (optional) additional keyword=value arguments
        :type kwargs: dict
        :return: :class:`models.Part` with category `MODEL`
        :raises IllegalArgumentError: When the provided arguments are incorrect
        :raises APIError: if the `Part` could not be created",1,0,2,3
"def create_namespaced_binding(self, namespace, body, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.create_namespaced_binding_with_http_info(namespace, body, **kwargs)  
        else:
            (data) = self.create_namespaced_binding_with_http_info(namespace, body, **kwargs)  
            return data","create_namespaced_binding  # noqa: E501

        create a Binding  # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.create_namespaced_binding(namespace, body, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param V1Binding body: (required)
        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed
        :param bool include_uninitialized: If IncludeUninitialized is specified, the object may be returned without completing initialization.
        :param str pretty: If 'true', then the output is pretty printed.
        :return: V1Binding
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def create_new_account(data_dir, password, **geth_kwargs):
    
    if os.path.exists(password):
        geth_kwargs['password'] = password

    command, proc = spawn_geth(dict(
        data_dir=data_dir,
        suffix_args=['account', 'new'],
        **geth_kwargs
    ))

    if os.path.exists(password):
        stdoutdata, stderrdata = proc.communicate()
    else:
        stdoutdata, stderrdata = proc.communicate(b""\n"".join((password, password)))

    if proc.returncode:
        raise ValueError(format_error_message(
            ""Error trying to create a new account"",
            command,
            proc.returncode,
            stdoutdata,
            stderrdata,
        ))

    match = account_regex.search(stdoutdata)
    if not match:
        raise ValueError(format_error_message(
            ""Did not find an address in process output"",
            command,
            proc.returncode,
            stdoutdata,
            stderrdata,
        ))

    return b'0x' + match.groups()[0]","Creates a new Ethereum account on geth.

    This is useful for testing when you want to stress
    interaction (transfers) between Ethereum accounts.

    This command communicates with ``geth`` command over
    terminal interaction. It creates keystore folder and new
    account there.

    This function only works against offline geth processes,
    because geth builds an account cache when starting up.
    If geth process is already running you can create new
    accounts using
    `web3.personal.newAccount()
    <https://github.com/ethereum/go-ethereum/wiki/JavaScript-Console#personalnewaccount>_`
    RPC API.


    Example py.test fixture for tests:

    .. code-block:: python

        import os

        from geth.wrapper import DEFAULT_PASSWORD_PATH
        from geth.accounts import create_new_account


        @pytest.fixture
        def target_account() -> str:
            '''Create a new Ethereum account on a running Geth node.

            The account can be used as a withdrawal target for tests.

            :return: 0x address of the account
            '''

            # We store keystore files in the current working directory
            # of the test run
            data_dir = os.getcwd()

            # Use the default password ""this-is-not-a-secure-password""
            # as supplied in geth/default_blockchain_password file.
            # The supplied password must be bytes, not string,
            # as we only want ASCII characters and do not want to
            # deal encoding problems with passwords
            account = create_new_account(data_dir, DEFAULT_PASSWORD_PATH)
            return account

    :param data_dir: Geth data fir path - where to keep ""keystore"" folder
    :param password: Path to a file containing the password
        for newly created account
    :param geth_kwargs: Extra command line arguments passwrord to geth
    :return: Account as 0x prefixed hex string",2,1,0,3
"def create_new_attachment_by_content_id(self, content_id, attachments, callback=None):
        
        if isinstance(attachments, list):
            assert all(isinstance(at, dict) and ""file"" in list(at.keys()) for at in attachments)
        elif isinstance(attachments, dict):
            assert ""file"" in list(attachments.keys())
        else:
            assert False
        return self._service_post_request(""rest/api/content/{id}/child/attachment"".format(id=content_id),
                                          headers={""X-Atlassian-Token"": ""nocheck""}, files=attachments,
                                          callback=callback)","Add one or more attachments to a Confluence Content entity, with optional comments.

        Comments are optional, but if included there must be as many comments as there are files, and the comments must
        be in the same order as the files.
        :param content_id (string): A string containing the id of the attachments content container.
        :param attachments (list of dicts or dict): This is a list of dictionaries or a dictionary.
                                                    Each dictionary must have the key
                                                    ""file"" with a value that is I/O like (file, StringIO, etc.), and
                                                    may also have a key ""comment"" with a string for file comments.
        :param callback: OPTIONAL: The callback to execute on the resulting data, before the method returns.
                         Default: None (no callback, raw data returned).
        :return: The JSON data returned from the content/{id}/child/attachment endpoint,
                 or the results of the callback. Will raise requests.HTTPError on bad input, potentially.",3,1,3,7
"def create_new_label_by_content_id(self, content_id, label_names, callback=None):
        
        assert isinstance(label_names, list)
        assert all(isinstance(ln, dict) and set(ln.keys()) == {""prefix"", ""name""} for ln in label_names)
        return self._service_post_request(""rest/api/content/{id}/label"".format(id=content_id),
                                         data=json.dumps(label_names), headers={""Content-Type"": ""application/json""},
                                         callback=callback)","Adds a list of labels to the specified content.
        :param content_id (string): A string containing the id of the labels content container.
        :param label_names (list): A list of labels (strings) to apply to the content.
        :param callback: OPTIONAL: The callback to execute on the resulting data, before the method returns.
                         Default: None (no callback, raw data returned).
        :return: The JSON data returned from the content/{id}/label endpoint,
                 or the results of the callback. Will raise requests.HTTPError on bad input, potentially.",1,1,0,2
"def create_node(hostname, username, password, name, address):
    

    ret = {'name': name, 'changes': {}, 'result': False, 'comment': ''}

    if __opts__['test']:
        return _test_output(ret, 'create', params={
            'hostname': hostname,
            'username': username,
            'password': password,
            'name': name,
            'address': address
        }
        )

    
    existing = __salt__['bigip.list_node'](hostname, username, password, name)

    
    if existing['code'] == 200:

        ret['result'] = True
        ret['comment'] = 'A node by this name currently exists.  No change made.'

    
    elif existing['code'] == 404:
        response = __salt__['bigip.create_node'](hostname, username, password, name, address)

        ret['result'] = True
        ret['changes']['old'] = {}
        ret['changes']['new'] = response['content']
        ret['comment'] = 'Node was successfully created.'

    
    else:
        ret = _load_result(existing, ret)

    return ret","Create a new node if it does not already exist.

    hostname
        The host/address of the bigip device
    username
        The iControl REST username
    password
        The iControl REST password
    name
        The name of the node to create
    address
        The address of the node",1,2,0,3
"def create_pgroup_snapshot(self, source, **kwargs):
        
        
        
        
        
        result = self.create_pgroup_snapshots([source], **kwargs)
        if self._rest_version >= LooseVersion(""1.4""):
            headers = result.headers
            result = ResponseDict(result[0])
            result.headers = headers
        return result","Create snapshot of pgroup from specified source.

        :param source: Name of pgroup of which to take snapshot.
        :type source: str
        :param \*\*kwargs: See the REST API Guide on your array for the
                           documentation on the request:
                           **POST pgroup**
        :type \*\*kwargs: optional

        :returns: A dictionary describing the created snapshot.
        :rtype: ResponseDict

        .. note::

            Requires use of REST API 1.2 or later.",0,1,1,2
"def create_project(self, name=None, project_id=None, path=None):
        

        if project_id is not None and project_id in self._projects:
            return self._projects[project_id]
        project = Project(name=name, project_id=project_id, path=path)
        self._check_available_disk_space(project)
        self._projects[project.id] = project
        return project","Create a project and keep a references to it in project manager.

        See documentation of Project for arguments",0,0,2,2
"def create_record(self, type, name, data, priority=None, port=None,
                      weight=None, **kwargs):
        
        
        api = self.doapi_manager
        data = {
            ""type"": type,
            ""name"": name,
            ""data"": data,
            ""priority"": priority,
            ""port"": port,
            ""weight"": weight,
        }
        data.update(kwargs)
        return self._record(api.request(self.record_url, method='POST',
                                        data=data)[""domain_record""])","Add a new DNS record to the domain

        :param str type: the type of DNS record to add (``""A""``, ``""CNAME""``,
            etc.)
        :param str name: the name (hostname, alias, etc.) of the new record
        :param str data: the value of the new record
        :param int priority: the priority of the new record (SRV and MX records
            only)
        :param int port: the port that the service is accessible on (SRV
            records only)
        :param int weight: the weight of records with the same priority (SRV
            records only)
        :param kwargs: additional fields to include in the API request
        :return: the new domain record
        :rtype: DomainRecord
        :raises DOAPIError: if the API endpoint replies with an error",0,1,0,1
"def create_request_with_query(self, kind, query, size=""thumb"", fmt=""json""):
        
        if kind == ""data"" or kind == ""files"":
            url = ""{}/{}.{}"".format(base_url, kind, fmt)
        elif kind == ""images"":
            url = ""{}/images/{}.{}"".format(base_url, size, fmt)
        self.url = url
        self.r = requests.get(url, params=unquote(urlencode(query)))","api/data.[fmt], api/images/[size].[fmt] api/files.[fmt]

        kind = ['data', 'images', 'files']",0,1,1,2
"def create_requests(self, request_to_create):
        
        content = self._serialize.body(request_to_create, 'Request')
        response = self._send(http_method='POST',
                              location_id='ebc09fe3-1b20-4667-abc5-f2b60fe8de52',
                              version='5.0-preview.1',
                              content=content)
        return self._deserialize('Request', response)","CreateRequests.
        [Preview API] Create a new symbol request.
        :param :class:`<Request> <azure.devops.v5_0.symbol.models.Request>` request_to_create: The symbol request to create.
        :rtype: :class:`<Request> <azure.devops.v5_0.symbol.models.Request>`",0,1,0,1
"def create_secret_link(self, title, description=None, expires_at=None):
        
        self.link = SecretLink.create(
            title,
            self.receiver,
            extra_data=dict(recid=self.recid),
            description=description,
            expires_at=expires_at,
        )
        return self.link",Create a secret link from request.,0,1,1,2
"def create_signature(self, base_url, payload=None):
        
        url = urlparse(base_url)

        url_to_sign = ""{path}?{query}"".format(path=url.path, query=url.query)

        converted_payload = self._convert(payload)

        decoded_key = base64.urlsafe_b64decode(self.private_key.encode('utf-8'))
        signature = hmac.new(decoded_key, str.encode(url_to_sign + converted_payload), hashlib.sha256)
        return bytes.decode(base64.urlsafe_b64encode(signature.digest()))","Creates unique signature for request.
        Make sure ALL 'GET' and 'POST' data is already included before
        creating the signature or receiver won't be able to re-create it.

        :param base_url:
            The url you'll using for your request.
        :param payload:
            The POST data that you'll be sending.",0,0,1,1
"def create_state(cls, state, **kwargs):
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async'):
            return cls._create_state_with_http_info(state, **kwargs)
        else:
            (data) = cls._create_state_with_http_info(state, **kwargs)
            return data","Create State

        Create a new State
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async=True
        >>> thread = api.create_state(state, async=True)
        >>> result = thread.get()

        :param async bool
        :param State state: Attributes of state to create (required)
        :return: State
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def create_subtask(self, cor, name=None, stop_timeout=1.0):
        

        if self.stopped:
            raise InternalError(""Cannot add a subtask to a parent that is already stopped"")

        subtask = BackgroundTask(cor, name, loop=self._loop, stop_timeout=stop_timeout)
        self.add_subtask(subtask)
        return subtask","Create and add a subtask from a coroutine.

        This function will create a BackgroundTask and then
        call self.add_subtask() on it.

        Args:
            cor (coroutine): The coroutine that should be wrapped
                in a background task.
            name (str): An optional name for the task.
            stop_timeout (float): The maximum time to wait for this
                subtask to die after stopping it.

        Returns:
            Backgroundtask: The created subtask.",1,0,3,4
"def create_task(self, task_name=None, script=None, hyper_parameters=None, saved_result_keys=None, **kwargs):
        
        if not isinstance(task_name, str):  
            raise Exception(""task_name should be string"")
        if not isinstance(script, str):  
            raise Exception(""script should be string"")
        if hyper_parameters is None:
            hyper_parameters = {}
        if saved_result_keys is None:
            saved_result_keys = []

        self._fill_project_info(kwargs)
        kwargs.update({'time': datetime.utcnow()})
        kwargs.update({'hyper_parameters': hyper_parameters})
        kwargs.update({'saved_result_keys': saved_result_keys})

        _script = open(script, 'rb').read()

        kwargs.update({'status': 'pending', 'script': _script, 'result': {}})
        self.db.Task.insert_one(kwargs)
        logging.info(""[Database] Saved Task - task_name: {} script: {}"".format(task_name, script))","Uploads a task to the database, timestamp will be added automatically.

        Parameters
        -----------
        task_name : str
            The task name.
        script : str
            File name of the python script.
        hyper_parameters : dictionary
            The hyper parameters pass into the script.
        saved_result_keys : list of str
            The keys of the task results to keep in the database when the task finishes.
        kwargs : other parameters
            Users customized parameters such as description, version number.

        Examples
        -----------
        Uploads a task
        >>> db.create_task(task_name='mnist', script='example/tutorial_mnist_simple.py', description='simple tutorial')

        Finds and runs the latest task
        >>> db.run_top_task(sess=sess, sort=[(""time"", pymongo.DESCENDING)])
        >>> db.run_top_task(sess=sess, sort=[(""time"", -1)])

        Finds and runs the oldest task
        >>> db.run_top_task(sess=sess, sort=[(""time"", pymongo.ASCENDING)])
        >>> db.run_top_task(sess=sess, sort=[(""time"", 1)])",3,1,3,7
"def create_topic(
            self, topic_name,
            default_message_time_to_live=None,
            max_size_in_megabytes=None, requires_duplicate_detection=None,
            duplicate_detection_history_time_window=None,
            enable_batched_operations=None):
        
        topic_properties = Topic(
            max_size_in_megabytes=max_size_in_megabytes,
            requires_duplicate_detection=requires_duplicate_detection,
            default_message_time_to_live=default_message_time_to_live,
            duplicate_detection_history_time_window=duplicate_detection_history_time_window,
            enable_batched_operations=enable_batched_operations)
        try:
            return self.mgmt_client.create_topic(topic_name, topic=topic_properties, fail_on_exist=True)
        except requests.exceptions.ConnectionError as e:
            raise ServiceBusConnectionError(""Namespace: {} not found"".format(self.service_namespace), e)","Create a topic entity.

        :param topic_name: The name of the new topic.
        :type topic_name: str
        :param max_size_in_megabytes: The max size to allow the topic to grow to.
        :type max_size_in_megabytes: int
        :param requires_duplicate_detection: Whether the topic will require every message with
         a specified time frame to have a unique ID. Non-unique messages will be discarded.
         Default value is False.
        :type requires_duplicate_detection: bool
        :param default_message_time_to_live: The length of time a message will remain in the topic
         before it is either discarded or moved to the dead letter queue.
        :type default_message_time_to_live: ~datetime.timedelta
        :param duplicate_detection_history_time_window: The period within which all incoming messages
         must have a unique message ID.
        :type duplicate_detection_history_time_window: ~datetime.timedelta
        :param enable_batched_operations:
        :type: enable_batched_operations: bool
        :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namespace is not found.
        :raises: ~azure.common.AzureConflictHttpError if a topic of the same name already exists.",1,1,1,3
"def create_zone(zone, private=False, vpc_id=None, vpc_region=None, region=None,
                key=None, keyid=None, profile=None):
    
    if region is None:
        region = 'universal'

    if private:
        if not vpc_id or not vpc_region:
            msg = 'vpc_id and vpc_region must be specified for a private zone'
            raise SaltInvocationError(msg)

    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)

    _zone = conn.get_zone(zone)

    if _zone:
        return False

    conn.create_zone(zone, private_zone=private, vpc_id=vpc_id,
                     vpc_region=vpc_region)
    return True","Create a Route53 hosted zone.

    .. versionadded:: 2015.8.0

    zone
        DNS zone to create

    private
        True/False if the zone will be a private zone

    vpc_id
        VPC ID to associate the zone to (required if private is True)

    vpc_region
        VPC Region (required if private is True)

    region
        region endpoint to connect to

    key
        AWS key

    keyid
        AWS keyid

    profile
        AWS pillar profile

    CLI Example::

        salt myminion boto_route53.create_zone example.org",1,1,1,3
"def crop_box(endpoint=None, filename=None):
        
        crop_size = current_app.config['AVATARS_CROP_BASE_WIDTH']

        if endpoint is None or filename is None:
            url = url_for('avatars.static', filename='default/default_l.jpg')
        else:
            url = url_for(endpoint, filename=filename)
        return Markup('<img src=""%s"" id=""crop-box"" style=""max-width: %dpx; display: block;"">' % (url, crop_size))","Create a crop box.

        :param endpoint: The endpoint of view function that serve avatar image file.
        :param filename: The filename of the image that need to be crop.",0,0,1,1
"def crystal(positions, molecules, group,
            cellpar=[1.0, 1.0, 1.0, 90, 90, 90], repetitions=[1, 1, 1]):
    
    sp = Spacegroup(group)
    sites, kind = sp.equivalent_sites(positions)

    nx, ny, nz = repetitions
    reptot = nx*ny*nz
    
    
    a,b,c = cellpar_to_cell(cellpar)
    
    cry = System()
    i = 0
    with cry.batch() as batch:
        for x in range(nx):
            for y in range(ny):
                for z in range(nz):
                    for s, ki in zip(sites, kind):
                        tpl = molecules[ki]
                        tpl.move_to(s[0]*a +s[1]*b + s[2]*c + a*x + b*y + c*z)
                        batch.append(tpl.copy())

    
    cry.box_vectors = np.array([a*nx, b*ny, c*nz])
    
    return cry","Build a crystal from atomic positions, space group and cell
    parameters.
    
    **Parameters**

    positions: list of coordinates
        A list of the atomic positions 
    molecules: list of Molecule
        The molecules corresponding to the positions, the molecule will be
        translated in all the equivalent positions.
    group: int | str
        Space group given either as its number in International Tables
        or as its Hermann-Mauguin symbol.
    repetitions:
        Repetition of the unit cell in each direction
    cellpar:
        Unit cell parameters

    This function was taken and adapted from the *spacegroup* module 
    found in `ASE <https://wiki.fysik.dtu.dk/ase/>`_.

    The module *spacegroup* module was originally developed by Jesper
    Frills.",0,0,1,1
"def csv(
            self,
            dirPath=None):
        

        if dirPath:
            p = self._file_prefix()
            csvSources = self.sourceResults.csv(
                filepath=dirPath + ""/"" + p + ""sources.csv"")
            csvPhot = self.photResults.csv(
                filepath=dirPath + ""/"" + p + ""phot.csv"")
            csvSpec = self.specResults.csv(
                filepath=dirPath + ""/"" + p + ""spec.csv"")
            csvFiles = self.relatedFilesResults.csv(
                filepath=dirPath + ""/"" + p + ""relatedFiles.csv"")
        else:
            csvSources = self.sourceResults.csv()
            csvPhot = self.photResults.csv()
            csvSpec = self.specResults.csv()
            csvFiles = self.relatedFilesResults.csv()
        return csvSources, csvPhot, csvSpec, csvFiles","*Render the results in csv format*

        **Key Arguments:**
            - ``dirPath`` -- the path to the directory to save the rendered results to. Default *None*

        **Return:**
            - `csvSources` -- the top-level transient data
            - `csvPhot` -- all photometry associated with the transients
            - `csvSpec` -- all spectral data associated with the transients
            - `csvFiles`  -- all files associated with the matched transients found on the tns

        **Usage:**

            To render the results in csv format:

            .. code-block:: python

                csvSources, csvPhot, csvSpec, csvFiles  = tns.csv()
                print csvSources

            .. code-block:: text

                TNSId,TNSName,discoveryName,discSurvey,raSex,decSex,raDeg,decDeg,transRedshift,specType,discMag,discMagFilter,discDate,objectUrl,hostName,hostRedshift,separationArcsec,separationNorthArcsec,separationEastArcsec
                2016asf,SN2016asf,ASASSN-16cs,ASAS-SN,06:50:36.73,+31:06:45.36,102.6530,31.1126,0.021,SN Ia,17.1,V-Johnson,2016-03-06 08:09:36,http://wis-tns.weizmann.ac.il/object/2016asf,KUG 0647+311,,0.66,0.65,-0.13

            You can save the results to file by passing in a directory path within which to save the files to. The four flavours of data (sources, photometry, spectra and files) are saved to separate files but all data can be assoicated with its transient source using the transient's unique `TNSId`.

            .. code-block:: python

                tns.csv(""~/tns"")

            .. image:: https://i.imgur.com/BwwqMBg.png
                :width: 800px
                :alt: csv output",5,0,1,6
"def cubehelix_palette(n_colors=6, start=0, rot=.4, gamma=1.0, hue=0.8,
                      light=.85, dark=.15, reverse=False, as_cmap=False):
    
    cdict = mpl._cm.cubehelix(gamma, start, rot, hue)
    cmap = mpl.colors.LinearSegmentedColormap(""cubehelix"", cdict)

    x = np.linspace(light, dark, n_colors)
    pal = cmap(x)[:, :3].tolist()
    if reverse:
        pal = pal[::-1]

    if as_cmap:
        x_256 = np.linspace(light, dark, 256)
        if reverse:
            x_256 = x_256[::-1]
        pal_256 = cmap(x_256)
        cmap = mpl.colors.ListedColormap(pal_256)
        return cmap
    else:
        return pal","Make a sequential palette from the cubehelix system.

    This produces a colormap with linearly-decreasing (or increasing)
    brightness. That means that information will be preserved if printed to
    black and white or viewed by someone who is colorblind.  ""cubehelix"" is
    also availible as a matplotlib-based palette, but this function gives the
    user more control over the look of the palette and has a different set of
    defaults.

    Parameters
    ----------
    n_colors : int
        Number of colors in the palette.
    start : float, 0 <= start <= 3
        The hue at the start of the helix.
    rot : float
        Rotations around the hue wheel over the range of the palette.
    gamma : float 0 <= gamma
        Gamma factor to emphasize darker (gamma < 1) or lighter (gamma > 1)
        colors.
    hue : float, 0 <= hue <= 1
        Saturation of the colors.
    dark : float 0 <= dark <= 1
        Intensity of the darkest color in the palette.
    light : float 0 <= light <= 1
        Intensity of the lightest color in the palette.
    reverse : bool
        If True, the palette will go from dark to light.
    as_cmap : bool
        If True, return a matplotlib colormap instead of a list of colors.

    Returns
    -------
    palette : list or colormap

    References
    ----------
    Green, D. A. (2011). ""A colour scheme for the display of astronomical
    intensity images"". Bulletin of the Astromical Society of India, Vol. 39,
    p. 289-295.",0,0,5,5
"def current_commit_parser() -> Callable:
    

    try:
        parts = config.get('semantic_release', 'commit_parser').split('.')
        module = '.'.join(parts[:-1])
        return getattr(importlib.import_module(module), parts[-1])
    except (ImportError, AttributeError) as error:
        raise ImproperConfigurationError('Unable to import parser ""{}""'.format(error))","Current commit parser

    :raises ImproperConfigurationError: if ImportError or AttributeError is raised",1,0,2,3
"def cutoff_filename(prefix, suffix, input_str):
    
    if prefix is not '':
        if input_str.startswith(prefix):
            input_str = input_str[len(prefix):]
    if suffix is not '':
        if input_str.endswith(suffix):
            input_str = input_str[:-len(suffix)]
    return input_str","Cuts off the start and end of a string, as specified by 2 parameters

    Parameters
    ----------
    prefix : string, if input_str starts with prefix, will cut off prefix
    suffix : string, if input_str end with suffix, will cut off suffix
    input_str : the string to be processed

    Returns
    -------
    A string, from which the start and end have been cut",0,0,1,1
"def cv_factory(data=None, folds=5, repeat=1, reporters=[], metrics=None,
               cv_runner=None, **kwargs):
    
    cv_runner = cv_runner or cross_validate
    md_kwargs = {}
    for arg in ModelDefinition.params:
        if arg in kwargs:
            md_kwargs[arg] = kwargs.pop(arg)
    model_def_fact = model_definition_factory(ModelDefinition(), **md_kwargs)
    results = []
    model_defs = list(model_def_fact)
    for model_def in model_defs:
        reporters = [reporter.copy() for reporter in reporters]
        cvr = cv_runner(model_def=model_def,
                        data=data,
                        folds=folds,
                        repeat=repeat,
                        reporters=reporters,
                        metrics=metrics,
                        **kwargs)
        results.append(cvr)

    return CVComparisonResult(model_defs, results)","Shortcut to iterate and cross-validate models.

    All ModelDefinition kwargs should be iterables that can be
    passed to model_definition_factory.

    Parameters:
    ___________

    data:
        Raw DataFrame

    folds:
        If an int, than basic k-fold cross-validation will be done.
        Otherwise must be an iterable of tuples of pandas Indexes
        [(train_index, test_index), ...]

    repeat:
        How many times to repeat each cross-validation run of each model. Only
        makes sense if cross-validation folds are randomized.

    kwargs:
        Can be any keyword accepted by `ModelDefinition`.
        Values should be iterables.",0,0,2,2
"def cycle(self, max_iter=None):
        

        count = 0
        while True:
            for obj in self.iterate():
                count += 1
                if max_iter is not None and count > max_iter:
                    return
                yield obj","Iterate from the streamer infinitely.

        This function will force an infinite stream, restarting
        the streamer even if a StopIteration is raised.

        Parameters
        ----------
        max_iter : None or int > 0
            Maximum number of iterations to yield.
            If `None`, iterate indefinitely.

        Yields
        ------
        obj : Objects yielded by the streamer provided on init.",1,0,4,5
"def dIbr_dV(Yf, Yt, V):
    


    Vnorm = div(V, abs(V))
    diagV = spdiag(V)
    diagVnorm = spdiag(Vnorm)
    dIf_dVa = Yf * 1j * diagV
    dIf_dVm = Yf * diagVnorm
    dIt_dVa = Yt * 1j * diagV
    dIt_dVm = Yt * diagVnorm

    
    If = Yf * V
    It = Yt * V

    return dIf_dVa, dIf_dVm, dIt_dVa, dIt_dVm, If, It","Computes partial derivatives of branch currents w.r.t. voltage.

        Ray Zimmerman, ""dIbr_dV.m"", MATPOWER, version 4.0b1,
        PSERC (Cornell), http://www.pserc.cornell.edu/matpower/",0,0,1,1
"def dag_to_file(self, dict_viz, output_file="".treeDag.json""):
        

        outfile_dag = open(os.path.join(dirname(self.nf_file), output_file)
                           , ""w"")
        outfile_dag.write(json.dumps(dict_viz))
        outfile_dag.close()","Writes dag to output file

        Parameters
        ----------
        dict_viz: dict
            Tree like dictionary that is used to export tree data of processes
            to html file and here for the dotfile .treeDag.json",1,0,0,1
"def data(self):
        
        content = {
            'form_data': self.form_data,
            'token': self.token,
            'viz_name': self.viz_type,
            'filter_select_enabled': self.datasource.filter_select_enabled,
        }
        return content",This is the data object serialized to the js layer,0,0,1,1
"def data(self, **kwargs):
        

        key = kwargs.pop('key', None)
        value = kwargs.pop('value', None)
        dictionary = kwargs.pop('dictionary', None)

        
        if (key is not None and dictionary is not None) or \
           (value is not None and dictionary is not None):
            raise ValueError

        
        if key is not None and value is None:
            return self._get_content(key)

        
        if key is not None and value is not None:
            self._set_content(key, value)

        if dictionary is not None:
            for key in dictionary.keys():
                value = dictionary[key]
                self._set_content(key, value)

        return self._get_content()","If a key is passed in, a corresponding value will be returned.
        If a key-value pair is passed in then the corresponding key in
        the database will be set to the specified value.
        A dictionary can be passed in as well.
        If a key does not exist and a value is provided then an entry
        will be created in the database.",5,0,4,9
"def data_dirpath(task=None, **kwargs):
    
    path = _base_dir()
    if task:
        path = os.path.join(path, _snail_case(task))
    for k, v in sorted(kwargs.items()):
        subdir_name = '{}_{}'.format(_snail_case(k), _snail_case(v))
        path = os.path.join(path, subdir_name)
    os.makedirs(path, exist_ok=True)
    return path","Get the path of the corresponding data directory.

    Parameters
    ----------
    task : str, optional
        The task for which datasets in the desired directory are used for. If
        not given, a path for the corresponding task-agnostic directory is
        returned.
    **kwargs : extra keyword arguments
        Extra keyword arguments, representing additional attributes of the
        datasets, are used to generate additional sub-folders on the path.
        For example, providing 'lang=en' will results in a path such as
        '/barn_base_dir/regression/lang_en/mydataset.csv'. Hierarchy always
        matches lexicographical order of keyword argument names, so 'lang=en'
        and 'animal=dog' will result in a path such as
        'barn_base_dir/task_name/animal_dof/lang_en/dset.csv'.

    Returns
    -------
    str
        The path to the desired dir.",0,0,2,2
"def data_received(self, data):
        
        _LOGGER.debug(""Starting: data_received"")
        _LOGGER.debug('Received %d bytes from PLM: %s',
                      len(data), binascii.hexlify(data))
        self._buffer.put_nowait(data)
        asyncio.ensure_future(self._peel_messages_from_buffer(),
                              loop=self._loop)

        _LOGGER.debug(""Finishing: data_received"")","Receive data from the protocol.

        Called when asyncio.Protocol detects received data from network.",0,3,0,3
"def dea3(v0, v1, v2, symmetric=False):
    
    e0, e1, e2 = np.atleast_1d(v0, v1, v2)
    with warnings.catch_warnings():
        warnings.simplefilter(""ignore"")  
        delta2, delta1 = e2 - e1, e1 - e0
        err2, err1 = np.abs(delta2), np.abs(delta1)
        tol2, tol1 = max_abs(e2, e1) * _EPS, max_abs(e1, e0) * _EPS
        delta1[err1 < _TINY] = _TINY
        delta2[err2 < _TINY] = _TINY  
        ss = 1.0 / delta2 - 1.0 / delta1 + _TINY
        smalle2 = abs(ss * e1) <= 1.0e-3
        converged = (err1 <= tol1) & (err2 <= tol2) | smalle2
        result = np.where(converged, e2 * 1.0, e1 + 1.0 / ss)
    abserr = err1 + err2 + np.where(converged, tol2 * 10, np.abs(result - e2))
    if symmetric and len(result) > 1:
        return result[:-1], abserr[1:]
    return result, abserr","Extrapolate a slowly convergent sequence

    Parameters
    ----------
    v0, v1, v2 : array-like
        3 values of a convergent sequence to extrapolate

    Returns
    -------
    result : array-like
        extrapolated value
    abserr : array-like
        absolute error estimate

    Notes
    -----
    DEA3 attempts to extrapolate nonlinearly to a better estimate
    of the sequence's limiting value, thus improving the rate of
    convergence. The routine is based on the epsilon algorithm of
    P. Wynn, see [1]_.

     Examples
     --------
     # integrate sin(x) from 0 to pi/2

     >>> import numpy as np
     >>> import numdifftools as nd
     >>> Ei= np.zeros(3)
     >>> linfun = lambda i : np.linspace(0, np.pi/2., 2**(i+5)+1)
     >>> for k in np.arange(3):
     ...    x = linfun(k)
     ...    Ei[k] = np.trapz(np.sin(x),x)
     >>> [En, err] = nd.dea3(Ei[0], Ei[1], Ei[2])
     >>> truErr = np.abs(En-1.)
     >>> np.all(truErr < err)
     True
     >>> np.allclose(En, 1)
     True
     >>> np.all(np.abs(Ei-1)<1e-3)
     True

     See also
     --------
     dea

     References
     ----------
     .. [1] C. Brezinski and M. Redivo Zaglia (1991)
            ""Extrapolation Methods. Theory and Practice"", North-Holland.

    ..  [2] C. Brezinski (1977)
            ""Acceleration de la convergence en analyse numerique"",
            ""Lecture Notes in Math."", vol. 584,
            Springer-Verlag, New York, 1977.

    ..  [3] E. J. Weniger (1989)
            ""Nonlinear sequence transformations for the acceleration of
            convergence and the summation of divergent series""
            Computer Physics Reports Vol. 10, 189 - 371
            http://arxiv.org/abs/math/0306302v1",0,0,1,1
"def debug(function):
    

    @wraps(function)
    def _wrapper(*args, **kwargs):
        result = function(*args, **kwargs)
        for key, value in kwargs.items():
            args += tuple(['{}={!r}'.format(key, value)])
        if len(args) == 1:
            args = '({})'.format(args[0])
        print('@{0}{1} -> {2}'.format(function.__name__, args, result))
        _wrapper.last_output = [function.__name__, str(args), result]
        return result
    _wrapper.last_output = []
    return _wrapper","Function: debug
    Summary: decorator to debug a function
    Examples: at the execution of the function wrapped,
              the decorator will allows to print the
              input and output of each execution
    Attributes:
        @param (function): function
    Returns: wrapped function",0,0,1,1
"def decode(self, binary, url, encoding=None, errors=""strict""):
        
        if encoding is None:
            domain = util.get_domain(url)
            if domain in self.domain_encoding_table:
                encoding = self.domain_encoding_table[domain]
                html = binary.decode(encoding, errors=errors)
            else:
                html, encoding, confidence = smart_decode(
                    binary, errors=errors)
                
                self.domain_encoding_table[domain] = encoding
        else:
            html = binary.decode(encoding, errors=errors)

        return html","Decode binary to string.

        :param binary: binary content of a http request.
        :param url: endpoint of the request.
        :param encoding: manually specify the encoding.
        :param errors: errors handle method.

        :return: str",0,0,1,1
"def decode_body(cls, header, f):
        
        assert header.packet_type == MqttControlPacketType.publish

        dupe = bool(header.flags & 0x08)
        retain = bool(header.flags & 0x01)
        qos = ((header.flags & 0x06) >> 1)

        if qos == 0 and dupe:
            
            
            raise DecodeError(""Unexpected dupe=True for qos==0 message [MQTT-3.3.1-2]."")

        decoder = mqtt_io.FileDecoder(mqtt_io.LimitReader(f, header.remaining_len))
        num_bytes_consumed, topic_name = decoder.unpack_utf8()

        if qos != 0:
            
            
            packet_id, = decoder.unpack(mqtt_io.FIELD_PACKET_ID)
        else:
            packet_id = 0

        payload_len = header.remaining_len - decoder.num_bytes_consumed
        payload = decoder.read(payload_len)

        return decoder.num_bytes_consumed, MqttPublish(packet_id, topic_name, payload, dupe, qos, retain)","Generates a `MqttPublish` packet given a
        `MqttFixedHeader`.  This method asserts that header.packet_type
        is `publish`.

        Parameters
        ----------
        header: MqttFixedHeader
        f: file
            Object with a read method.

        Raises
        ------
        DecodeError
            When there are extra bytes at the end of the packet.

        Returns
        -------
        int
            Number of bytes consumed from ``f``.
        MqttPublish
            Object extracted from ``f``.",2,1,2,5
"def decode_cert(cert):
    

    ret_dict = {}
    subject_xname = X509_get_subject_name(cert.value)
    ret_dict[""subject""] = _create_tuple_for_X509_NAME(subject_xname)

    notAfter = X509_get_notAfter(cert.value)
    ret_dict[""notAfter""] = ASN1_TIME_print(notAfter)

    peer_alt_names = _get_peer_alt_names(cert)
    if peer_alt_names is not None:
        ret_dict[""subjectAltName""] = peer_alt_names

    return ret_dict","Convert an X509 certificate into a Python dictionary

    This function converts the given X509 certificate into a Python dictionary
    in the manner established by the Python standard library's ssl module.",0,0,1,1
"def decode_offset_fetch_response(cls, data):
        

        ((correlation_id,), cur) = relative_unpack('>i', data, 0)
        ((num_topics,), cur) = relative_unpack('>i', data, cur)

        for _i in range(num_topics):
            (topic, cur) = read_short_ascii(data, cur)
            ((num_partitions,), cur) = relative_unpack('>i', data, cur)

            for _i in range(num_partitions):
                ((partition, offset), cur) = relative_unpack('>iq', data, cur)
                (metadata, cur) = read_short_bytes(data, cur)
                ((error,), cur) = relative_unpack('>h', data, cur)

                yield OffsetFetchResponse(topic, partition, offset,
                                          metadata, error)","Decode bytes to an OffsetFetchResponse

        :param bytes data: bytes to decode",0,0,1,1
"def decode_payload_as(self,cls):
        
        s = bytes(self.payload)
        self.payload = cls(s, _internal=1, _underlayer=self)
        pp = self
        while pp.underlayer is not None:
            pp = pp.underlayer
        self.payload.dissection_done(pp)",Reassembles the payload and decode it using another packet class,0,0,2,2
"def decorate(self, record):
        
        color = 'gray'
        if record.levelno == logging.WARNING:
            color = 'yellow'
        if record.levelno == logging.INFO:
            color = 'green'
        if record.levelno == logging.DEBUG:
            color = 'gray'
        if record.levelno >= logging.ERROR:
            color = 'red'

        notify = False
        if record.levelno >= logging.ERROR:
            nofiy = True

        payload = {
            'color': color,
            'notify': notify,
            'message_format': 'text'
        }

        return payload","Build up HipChat specific values for log record

        Args:
            record (:obj:`logging.record`): log message object

        Returns:
            dict: params for POST request",0,0,1,1
"def decrypt_file(self,
                     path,
                     output_path=None,
                     overwrite=False,
                     stream=True,
                     enable_verbose=True,
                     **kwargs):
        
        path, output_path = files.process_dst_overwrite_args(
            src=path, dst=output_path, overwrite=overwrite,
            src_to_dst_func=files.get_decrpyted_path,
        )

        self._show(""Decrypt '%s' ..."" % path, enable_verbose=enable_verbose)
        st = time.clock()
        files.transform(path, output_path, converter=self.decrypt,
                        overwrite=overwrite, stream=stream,
                        chunksize=self._decrypt_chunk_size)
        self._show(""    Finished! Elapse %.6f seconds"" % (time.clock() - st,),
                   enable_verbose=enable_verbose)

        return output_path","Decrypt a file. If output_path are not given, then try to use the
        path with a surfix appended. The default automatical file path handling
        is defined here :meth:`windtalker.files.recover_path`

        :param path: path of the file you need to decrypt
        :param output_path: decrypted file output path
        :param overwrite: if True, then silently overwrite output file if exists
        :param stream: if it is a very big file, stream mode can avoid using
          too much memory
        :param enable_verbose: boolean, trigger on/off the help information",1,0,1,2
"def decrypt_with_key(encrypted_token, key):
        
        try:
            jwe_token = jwe.JWE(algs=['RSA-OAEP', 'A256GCM'])
            jwe_token.deserialize(encrypted_token)

            jwe_token.decrypt(key)

            return jwe_token.payload.decode()
        except (ValueError, InvalidJWEData) as e:
            raise InvalidTokenException(str(e)) from e","Decrypts JWE token with supplied key
        :param encrypted_token:
        :param key: A (:class:`jwcrypto.jwk.JWK`) decryption key or a password
        :returns: The payload of the decrypted token",1,0,2,3
"def define_charset(self, code, mode):
        
        if code in cs.MAPS:
            if mode == ""("":
                self.g0_charset = cs.MAPS[code]
            elif mode == "")"":
                self.g1_charset = cs.MAPS[code]","Define ``G0`` or ``G1`` charset.

        :param str code: character set code, should be a character
                         from ``""B0UK""``, otherwise ignored.
        :param str mode: if ``""(""`` ``G0`` charset is defined, if
                         ``"")""`` -- we operate on ``G1``.

        .. warning:: User-defined charsets are currently not supported.",0,0,6,6
"def delay(self, params, now=None):
        

        if now is None:
            now = time.time()

        
        if not self.last:
            self.last = now
        elif now < self.last:
            now = self.last

        
        leaked = now - self.last

        
        self.last = now

        
        self.level = max(self.level - leaked, 0)

        
        difference = self.level + self.limit.cost - self.limit.unit_value
        if difference >= self.eps:
            self.next = now + difference
            return difference

        
        
        self.level += self.limit.cost
        self.next = now

        return None",Determine delay until next request.,0,0,1,1
"def delete(self, *objs, condition=None, atomic=False):
        
        objs = set(objs)
        validate_not_abstract(*objs)
        for obj in objs:
            self.session.delete_item({
                ""TableName"": self._compute_table_name(obj.__class__),
                ""Key"": dump_key(self, obj),
                **render(self, obj=obj, atomic=atomic, condition=condition)
            })
            object_deleted.send(self, engine=self, obj=obj)
        logger.info(""successfully deleted {} objects"".format(len(objs)))","Delete one or more objects.

        :param objs: objects to delete.
        :param condition: only perform each delete if this condition holds.
        :param bool atomic: only perform each delete if the local and DynamoDB versions of the object match.
        :raises bloop.exceptions.ConstraintViolation: if the condition (or atomic) is not met.",0,2,0,2
"def delete(self, id):
        

        refs = self.db.execute(
            'SELECT * FROM comments WHERE parent=?', (id, )).fetchone()

        if refs is None:
            self.db.execute('DELETE FROM comments WHERE id=?', (id, ))
            self._remove_stale()
            return None

        self.db.execute('UPDATE comments SET text=? WHERE id=?', ('', id))
        self.db.execute('UPDATE comments SET mode=? WHERE id=?', (4, id))
        for field in ('author', 'website'):
            self.db.execute('UPDATE comments SET %s=? WHERE id=?' %
                            field, (None, id))

        self._remove_stale()
        return self.get(id)","Delete a comment. There are two distinctions: a comment is referenced
        by another valid comment's parent attribute or stand-a-lone. In this
        case the comment can't be removed without losing depending comments.
        Hence, delete removes all visible data such as text, author, email,
        website sets the mode field to 4.

        In the second case this comment can be safely removed without any side
        effects.",5,0,1,6
"def delete(self, key, noreply=None):
        
        if noreply is None:
            noreply = self.default_noreply
        cmd = b'delete ' + self.check_key(key)
        if noreply:
            cmd += b' noreply'
        cmd += b'\r\n'
        results = self._misc_cmd([cmd], b'delete', noreply)
        if noreply:
            return True
        return results[0] == b'DELETED'","The memcached ""delete"" command.

        Args:
          key: str, see class docs for details.
          noreply: optional bool, True to not wait for the reply (defaults to
                   self.default_noreply).

        Returns:
          If noreply is True, always returns True. Otherwise returns True if
          the key was deleted, and False if it wasn't found.",0,1,5,6
"def delete(self, password, message=""""):
        
        data = {'user': self.user.name,
                'passwd': password,
                'delete_message': message,
                'confirm': True}
        return self.request_json(self.config['delete_redditor'], data=data)","Delete the currently authenticated redditor.

        WARNING!

        This action is IRREVERSIBLE. Use only if you're okay with NEVER
        accessing this reddit account again.

        :param password: password for currently authenticated account
        :param message: optional 'reason for deletion' message.
        :returns: json response from the server.",0,1,0,1
"def delete(self, queue='', if_unused=False, if_empty=False):
        
        if not compatibility.is_string(queue):
            raise AMQPInvalidArgument('queue should be a string')
        elif not isinstance(if_unused, bool):
            raise AMQPInvalidArgument('if_unused should be a boolean')
        elif not isinstance(if_empty, bool):
            raise AMQPInvalidArgument('if_empty should be a boolean')

        delete_frame = pamqp_queue.Delete(queue=queue, if_unused=if_unused,
                                          if_empty=if_empty)
        return self._channel.rpc_request(delete_frame)","Delete a Queue.

        :param str queue: Queue name
        :param bool if_unused: Delete only if unused
        :param bool if_empty: Delete only if empty

        :raises AMQPInvalidArgument: Invalid Parameters
        :raises AMQPChannelError: Raises if the channel encountered an error.
        :raises AMQPConnectionError: Raises if the connection
                                     encountered an error.

        :rtype: dict",3,1,0,4
"def delete(self, version):
        
        
        if version.state != version.DRAFT and \
                      version.state != version.PUBLISHED:
            version.delete()
            message = ""%s version deleted."" % version.date_published
            return self.write_message(message=message)","Deletes the given version, not the object itself.
        No log entry is generated but the user is notified
        with a message.",0,1,1,2
"def delete_attachment(request, link_field=None, uri=None):
    
    if link_field is None:
        link_field = ""record_uri""
    if uri is None:
        uri = record_uri(request)

    
    filters = [Filter(link_field, uri, core_utils.COMPARISON.EQ)]
    storage = request.registry.storage
    file_links, _ = storage.get_all("""", FILE_LINKS, filters=filters)
    for link in file_links:
        request.attachment.delete(link['location'])

    
    storage.delete_all("""", FILE_LINKS, filters=filters, with_deleted=False)",Delete existing file and link.,1,0,1,2
"def delete_cfg_template(template_name, auth, url):
    
    file_id = get_template_id(template_name, auth, url)
    delete_cfg_template_url = ""/imcrs/icc/confFile/""+str(file_id)
    f_url = url + delete_cfg_template_url
    
    r = requests.delete(f_url, auth=auth, headers=HEADERS)
    
    try:
        if r.status_code == 204:
            return True
    except requests.exceptions.RequestException as e:
            return ""Error:\n"" + str(e) + "" delete_cfg_template: An Error has occured""","Uses the get_template_id() funct to gather the template_id to craft a url which is sent to the IMC server using
    a Delete Method
    :param template_name: str containing the entire contents of the configuration segment

    :param auth: requests auth object #usually auth.creds from auth pyhpeimc.auth.class

    :param url: base url of IMC RS interface #usually auth.url from pyhpeimc.auth.authclass

    :return: If successful, Boolean of type True

    :rtype: Boolean

    >>> from pyhpeimc.auth import *

    >>> from pyhpeimc.plat.icc import *

    >>> auth = IMCAuth(""http://"", ""10.101.0.203"", ""8080"", ""admin"", ""admin"")

    >>> delete_cfg_template('CW7SNMP.cfg', auth.creds, auth.url)
    True

    >>> get_template_id('CW7SNMP.cfg', auth.creds, auth.url)
    'template not found'",1,1,1,3
"def delete_collection_by_id(cls, collection_id, **kwargs):
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async'):
            return cls._delete_collection_by_id_with_http_info(collection_id, **kwargs)
        else:
            (data) = cls._delete_collection_by_id_with_http_info(collection_id, **kwargs)
            return data","Delete Collection

        Delete an instance of Collection by its ID.
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async=True
        >>> thread = api.delete_collection_by_id(collection_id, async=True)
        >>> result = thread.get()

        :param async bool
        :param str collection_id: ID of collection to delete. (required)
        :return: None
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def delete_deployment_target(self, project, deployment_group_id, target_id):
        
        route_values = {}
        if project is not None:
            route_values['project'] = self._serialize.url('project', project, 'str')
        if deployment_group_id is not None:
            route_values['deploymentGroupId'] = self._serialize.url('deployment_group_id', deployment_group_id, 'int')
        if target_id is not None:
            route_values['targetId'] = self._serialize.url('target_id', target_id, 'int')
        self._send(http_method='DELETE',
                   location_id='2f0aa599-c121-4256-a5fd-ba370e0ae7b6',
                   version='5.1-preview.1',
                   route_values=route_values)","DeleteDeploymentTarget.
        [Preview API] Delete a deployment target in a deployment group. This deletes the agent from associated deployment pool too.
        :param str project: Project ID or project name
        :param int deployment_group_id: ID of the deployment group in which deployment target is deleted.
        :param int target_id: ID of the deployment target to delete.",0,1,1,2
"def delete_device(name, safety_on=True):
    

    config = _get_vistara_configuration()
    if not config:
        return False

    access_token = _get_oath2_access_token(config['client_key'], config['client_secret'])

    if not access_token:
        return 'Vistara access token not available'

    query_string = 'dnsName:{0}'.format(name)

    devices = _search_devices(query_string, config['client_id'], access_token)

    if not devices:
        return ""No devices found""

    device_count = len(devices)

    if safety_on and device_count != 1:
        return ""Expected to delete 1 device and found {0}. ""\
            ""Set safety_on=False to override."".format(device_count)

    delete_responses = []
    for device in devices:
        device_id = device['id']
        log.debug(device_id)
        delete_response = _delete_resource(device_id, config['client_id'], access_token)
        if not delete_response:
            return False
        delete_responses.append(delete_response)

    return delete_responses","Deletes a device from Vistara based on DNS name or partial name. By default,
    delete_device will only perform the delete if a single host is returned. Set
    safety_on=False to delete all matches (up to default API search page size)

    CLI Example:

    .. code-block:: bash

        salt-run vistara.delete_device 'hostname-101.mycompany.com'
        salt-run vistara.delete_device 'hostname-101'
        salt-run vistara.delete_device 'hostname-1' safety_on=False",0,4,1,5
"def delete_device(self, rid):
        
        headers = {
                'User-Agent': self.user_agent(),
                'Content-Type': self.content_type()
            }
        headers.update(self.headers())
        r = requests.delete(    self.portals_url()+'/devices/'+rid, 
                                headers=headers,
                                auth=self.auth())
        if HTTP_STATUS.NO_CONTENT == r.status_code:
            print(""Successfully deleted device with rid: {0}"".format(rid))
            return True
        else:
            print(""Something went wrong: <{0}>: {1}"".format(
                        r.status_code, r.reason))
            r.raise_for_status()
            return False","Deletes device object with given rid

            http://docs.exosite.com/portals/#delete-device",1,1,2,4
"def delete_device_enrollment(self, id, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('asynchronous'):
            return self.delete_device_enrollment_with_http_info(id, **kwargs)  
        else:
            (data) = self.delete_device_enrollment_with_http_info(id, **kwargs)  
            return data","Delete an enrollment by ID.  # noqa: E501

        To free a device from your account you can delete the enrollment claim. To bypass the device ownership, you need to delete the enrollment and do a factory reset for the device. For more information, see [Transferring the ownership using First-to-Claim](/docs/current/connecting/device-ownership.html). <br> **Example usage:** ``` curl -X DELETE \\ -H 'Authorization: Bearer <valid access token>' \\ https://api.us-east-1.mbedcloud.com/v3/device-enrollments/{id} ```   # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass asynchronous=True
        >>> thread = api.delete_device_enrollment(id, asynchronous=True)
        >>> result = thread.get()

        :param asynchronous bool
        :param str id: Enrollment identity. (required)
        :return: None
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def delete_group(self, group_id, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('asynchronous'):
            return self.delete_group_with_http_info(group_id, **kwargs)  
        else:
            (data) = self.delete_group_with_http_info(group_id, **kwargs)  
            return data","Delete a group.  # noqa: E501

        An endpoint for deleting a group.   **Example usage:** `curl -X DELETE https://api.us-east-1.mbedcloud.com/v3/policy-groups/{group-id} -H 'Authorization: Bearer API_KEY'`  # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass asynchronous=True
        >>> thread = api.delete_group(group_id, asynchronous=True)
        >>> result = thread.get()

        :param asynchronous bool
        :param str group_id: The ID of the group to be deleted. (required)
        :return: None
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def delete_group_action(model, request):
    
    try:
        groups = model.parent.backend
        uid = model.model.name
        del groups[uid]
        groups()
        model.parent.invalidate()
    except Exception as e:
        return {
            'success': False,
            'message': str(e)
        }
    localizer = get_localizer(request)
    message = localizer.translate(_(
        'deleted_group',
        default='Deleted group from database'
    ))
    return {
        'success': True,
        'message': message
    }",Delete group from database.,1,0,0,1
"def delete_key(self):
        
        print(""This command will delete an API key."")
        apiKeyID = input(""API Key ID: "")
        try:
            self._curl_bitmex(""/apiKey/"",
                              postdict={""apiKeyID"": apiKeyID}, verb='DELETE')
            print(""Key with ID %s disabled."" % apiKeyID)
        except:
            print(""Unable to delete key, please try again."")
            self.delete_key()",Delete an existing API Key.,0,1,0,1
"def delete_long_poll_channel(self, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('asynchronous'):
            return self.delete_long_poll_channel_with_http_info(**kwargs)  
        else:
            (data) = self.delete_long_poll_channel_with_http_info(**kwargs)  
            return data","Delete notification Long Poll channel  # noqa: E501

        To delete a notification Long Poll channel. This is required to change the channel from Long Poll to a callback. You should not make a GET `/v2/notification/pull` call for 2 minutes after channel was deleted, because it can implicitly recreate the pull channel. You can also have some random responses with payload or 410 GONE with \""CHANNEL_DELETED\"" as a payload or 200/204 until the old channel is purged.  **Example usage:**     curl -X DELETE https://api.us-east-1.mbedcloud.com/v2/notification/pull -H 'authorization: Bearer {api-key}'   # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass asynchronous=True
        >>> thread = api.delete_long_poll_channel(asynchronous=True)
        >>> result = thread.get()

        :param asynchronous bool
        :return: None
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def delete_namespaced_pod_disruption_budget(self, name, namespace, **kwargs):
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.delete_namespaced_pod_disruption_budget_with_http_info(name, namespace, **kwargs)
        else:
            (data) = self.delete_namespaced_pod_disruption_budget_with_http_info(name, namespace, **kwargs)
            return data","delete a PodDisruptionBudget
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.delete_namespaced_pod_disruption_budget(name, namespace, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the PodDisruptionBudget (required)
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param V1DeleteOptions body:
        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed
        :param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.
        :param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \""orphan\"" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both.
        :param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground.
        :return: V1Status
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def delete_namespaced_service_account(self, name, namespace, **kwargs):
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.delete_namespaced_service_account_with_http_info(name, namespace, **kwargs)
        else:
            (data) = self.delete_namespaced_service_account_with_http_info(name, namespace, **kwargs)
            return data","delete a ServiceAccount
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.delete_namespaced_service_account(name, namespace, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the ServiceAccount (required)
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param V1DeleteOptions body:
        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed
        :param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.
        :param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \""orphan\"" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both.
        :param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground.
        :return: V1Status
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def delete_network(self, name, tenant_id, subnet_id, net_id):
        
        try:
            self.neutronclient.delete_subnet(subnet_id)
        except Exception as exc:
            LOG.error(""Failed to delete subnet %(sub)s exc %(exc)s"",
                      {'sub': subnet_id, 'exc': str(exc)})
            return
        try:
            self.neutronclient.delete_network(net_id)
        except Exception as exc:
            LOG.error(""Failed to delete network %(name)s exc %(exc)s"",
                      {'name': name, 'exc': str(exc)})",Delete the openstack subnet and network.,0,4,0,4
"def delete_node(self, node_id):
        

        node = None
        try:
            node = self.get_node(node_id)
            yield from self.close_node(node_id)
        finally:
            if node:
                node.project.emit(""node.deleted"", node)
                yield from node.project.remove_node(node)
        if node.id in self._nodes:
            del self._nodes[node.id]
        return node","Delete a node. The node working directory will be destroyed when a commit is received.

        :param node_id: Node identifier
        :returns: Node instance",1,1,0,2
"def delete_order_line_item_by_id(cls, order_line_item_id, **kwargs):
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async'):
            return cls._delete_order_line_item_by_id_with_http_info(order_line_item_id, **kwargs)
        else:
            (data) = cls._delete_order_line_item_by_id_with_http_info(order_line_item_id, **kwargs)
            return data","Delete OrderLineItem

        Delete an instance of OrderLineItem by its ID.
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async=True
        >>> thread = api.delete_order_line_item_by_id(order_line_item_id, async=True)
        >>> result = thread.get()

        :param async bool
        :param str order_line_item_id: ID of orderLineItem to delete. (required)
        :return: None
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def delete_response(self, publisher_name, extension_name, question_id, response_id):
        
        route_values = {}
        if publisher_name is not None:
            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')
        if extension_name is not None:
            route_values['extensionName'] = self._serialize.url('extension_name', extension_name, 'str')
        if question_id is not None:
            route_values['questionId'] = self._serialize.url('question_id', question_id, 'long')
        if response_id is not None:
            route_values['responseId'] = self._serialize.url('response_id', response_id, 'long')
        self._send(http_method='DELETE',
                   location_id='7f8ae5e0-46b0-438f-b2e8-13e8513517bd',
                   version='5.1-preview.1',
                   route_values=route_values)","DeleteResponse.
        [Preview API] Deletes a response for an extension. (soft delete)
        :param str publisher_name: Name of the publisher who published the extension.
        :param str extension_name: Name of the extension.
        :param long question_id: Identifies the question whose response is to be deleted.
        :param long response_id: Identifies the response to be deleted.",0,1,1,2
"def delete_service(name, namespace='default', **kwargs):
    
    cfg = _setup_conn(**kwargs)

    try:
        api_instance = kubernetes.client.CoreV1Api()
        api_response = api_instance.delete_namespaced_service(
            name=name,
            namespace=namespace)

        return api_response.to_dict()
    except (ApiException, HTTPError) as exc:
        if isinstance(exc, ApiException) and exc.status == 404:
            return None
        else:
            log.exception(
                'Exception when calling CoreV1Api->delete_namespaced_service'
            )
            raise CommandExecutionError(exc)
    finally:
        _cleanup(**cfg)","Deletes the kubernetes service defined by name and namespace

    CLI Examples::

        salt '*' kubernetes.delete_service my-nginx default
        salt '*' kubernetes.delete_service name=my-nginx namespace=default",1,2,0,3
"def delete_store_credit_payment_by_id(cls, store_credit_payment_id, **kwargs):
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async'):
            return cls._delete_store_credit_payment_by_id_with_http_info(store_credit_payment_id, **kwargs)
        else:
            (data) = cls._delete_store_credit_payment_by_id_with_http_info(store_credit_payment_id, **kwargs)
            return data","Delete StoreCreditPayment

        Delete an instance of StoreCreditPayment by its ID.
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async=True
        >>> thread = api.delete_store_credit_payment_by_id(store_credit_payment_id, async=True)
        >>> result = thread.get()

        :param async bool
        :param str store_credit_payment_id: ID of storeCreditPayment to delete. (required)
        :return: None
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def delete_tag(self, key, update_session=True):
        
        existing_tags = {x.key: x for x in self.tags}
        if key in existing_tags:
            if update_session:
                db.session.delete(existing_tags[key])

            self.tags.remove(existing_tags[key])
            return True

        return False","Removes a tag from a resource based on the tag key. Returns `True` if the tag was removed or `False` if the
        tag didn't exist

        Args:
            key (str): Key of the tag to delete
            update_session (bool): Automatically add the change to the SQLAlchemy session. Default: True

        Returns:",1,0,2,3
"def delete_tenant_if_removed(self, tenant_id):
        
        if not db_lib.tenant_provisioned(tenant_id):
            t_res = MechResource(tenant_id, a_const.TENANT_RESOURCE,
                                 a_const.DELETE)
            self.provision_queue.put(t_res)",Enqueue tenant delete if it's no longer in the db,1,1,0,2
"def delete_user(self, user_id, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('asynchronous'):
            return self.delete_user_with_http_info(user_id, **kwargs)  
        else:
            (data) = self.delete_user_with_http_info(user_id, **kwargs)  
            return data","Delete a user.  # noqa: E501

        An endpoint for deleting a user.   **Example usage:** `curl -X DELETE https://api.us-east-1.mbedcloud.com/v3/users/{user-id} -H 'Authorization: Bearer API_KEY'`  # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass asynchronous=True
        >>> thread = api.delete_user(user_id, asynchronous=True)
        >>> result = thread.get()

        :param asynchronous bool
        :param str user_id: The ID of the user to be deleted. (required)
        :return: None
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def delete_version(self, version_id=None):
        
        if not version_id:
            version_id = self.version.id

        target_url = self._client.get_url('VERSION', 'DELETE', 'single', {'layer_id': self.id, 'version_id': version_id})
        r = self._client.request('DELETE', target_url)
        logger.info(""delete_version(): %s"", r.status_code)","Deletes this draft version (revert to published)

        :raises NotAllowed: if this version is already published.
        :raises Conflict: if this version is already deleted.",0,2,0,2
"def deletegitlabciservice(self, project_id, token, project_url):
        
        request = requests.delete(
            '{0}/{1}/services/gitlab-ci'.format(self.projects_url, project_id),
            headers=self.headers, verify=self.verify_ssl, auth=self.auth, timeout=self.timeout)

        return request.status_code == 200","Delete GitLab CI service settings

        :param project_id: Project ID
        :param token: Token
        :param project_url: Project URL
        :return: true if success, false if not",0,1,1,2
"def denormalize_attachment(attachment):
        
        res = dict()
        ext = ['id', 'url']
        for k in ext:
            if k in attachment['metadata']:
                raise ValueError(""metadata section could not contain special key '{}'"".format(k))
            res[k] = attachment[k]
        res.update(attachment['metadata'])
        return res",convert attachment metadata from archivant to es format,1,0,2,3
"def describeTable(TableName):
    
    print('-----------------------------------------')
    print(TableName+' summary:')
    try:
       print('-----------------------------------------')
       print('Comment: \n'+LOCAL_TABLE_CACHE[TableName]['header']['comment'])
    except:
       pass
    print('Number of rows: '+str(LOCAL_TABLE_CACHE[TableName]['header']['number_of_rows']))
    print('Table type: '+str(LOCAL_TABLE_CACHE[TableName]['header']['table_type']))
    print('-----------------------------------------')
    print('            PAR_NAME           PAR_FORMAT')
    print('')
    for par_name in LOCAL_TABLE_CACHE[TableName]['header']['order']:
        par_format = LOCAL_TABLE_CACHE[TableName]['header']['format'][par_name]
        print('%20s %20s' % (par_name,par_format))
    print('-----------------------------------------')","INPUT PARAMETERS: 
        TableName: name of the table to describe
    OUTPUT PARAMETERS: 
        none
    ---
    DESCRIPTION:
        Print information about table, including 
        parameter names, formats and wavenumber range.
    ---
    EXAMPLE OF USAGE:
        describeTable('sampletab')
    ---",1,0,0,1
"def descriptor_to_generator(cls_descriptor, cls, limit=0):
    'Convert a protobuf descriptor to a protofuzz generator for same type'

    generators = []
    for descriptor in cls_descriptor.fields_by_name.values():
        generator = _prototype_to_generator(descriptor, cls)

        if limit != 0:
            generator.set_limit(limit)

        generators.append(generator)

    obj = cls(cls_descriptor.name, *generators)
    return obj",Convert a protobuf descriptor to a protofuzz generator for same type,0,0,1,1
"def deseq2_size_factors(counts, meta, design):
    
    import rpy2.robjects as r
    from rpy2.robjects import pandas2ri
    pandas2ri.activate()
    r.r('suppressMessages(library(DESeq2))')
    r.globalenv['counts'] = counts
    r.globalenv['meta'] = meta
    r.r('dds = DESeqDataSetFromMatrix(countData=counts, colData=meta, '
        'design={})'.format(design))
    r.r('dds = estimateSizeFactors(dds)')
    r.r('sf = sizeFactors(dds)')
    sf = r.globalenv['sf']
    return pd.Series(sf, index=counts.columns)","Get size factors for counts using DESeq2.

    Parameters
    ----------
    counts : pandas.DataFrame
        Counts to pass to DESeq2.

    meta : pandas.DataFrame
        Pandas dataframe whose index matches the columns of counts. This is
        passed to DESeq2's colData.

    design : str
        Design like ~subject_id that will be passed to DESeq2. The design
        variables should match columns in meta.

    Returns
    -------
    sf : pandas.Series
        Series whose index matches the columns of counts and whose values are
        the size factors from DESeq2. Divide each column by its size factor to
        obtain normalized counts.",0,0,4,4
"def design_create(self, name, ddoc, use_devmode=True, syncwait=0):
        
        name = self._cb._mk_devmode(name, use_devmode)

        fqname = ""_design/{0}"".format(name)
        if not isinstance(ddoc, dict):
            ddoc = json.loads(ddoc)

        ddoc = ddoc.copy()
        ddoc['_id'] = fqname
        ddoc = json.dumps(ddoc)

        existing = None
        if syncwait:
            try:
                existing = self.design_get(name, use_devmode=False)
            except CouchbaseError:
                pass

        ret = self._cb._http_request(
            type=_LCB.LCB_HTTP_TYPE_VIEW, path=fqname,
            method=_LCB.LCB_HTTP_METHOD_PUT, post_data=ddoc,
            content_type=""application/json"")

        self._design_poll(name, 'add', existing, syncwait,
                          use_devmode=use_devmode)
        return ret","Store a design document

        :param string name: The name of the design
        :param ddoc: The actual contents of the design document

        :type ddoc: string or dict
            If ``ddoc`` is a string, it is passed, as-is, to the server.
            Otherwise it is serialized as JSON, and its ``_id`` field is set to
            ``_design/{name}``.

        :param bool use_devmode:
            Whether a *development* mode view should be used. Development-mode
            views are less resource demanding with the caveat that by default
            they only operate on a subset of the data. Normally a view will
            initially be created in 'development mode', and then published
            using :meth:`design_publish`

        :param float syncwait:
            How long to poll for the action to complete. Server side design
            operations are scheduled and thus this function may return before
            the operation is actually completed. Specifying the timeout here
            ensures the client polls during this interval to ensure the
            operation has completed.

        :raise: :exc:`couchbase.exceptions.TimeoutError` if ``syncwait`` was
            specified and the operation could not be verified within the
            interval specified.

        :return: An :class:`~couchbase.result.HttpResult` object.

        .. seealso:: :meth:`design_get`, :meth:`design_delete`,
            :meth:`design_publish`",0,2,1,3
"def destroy_digidoc_session(self):
        

        
        self.destroy_digidoc_session_data()

        try:
            session = self.request.session[self.DIGIDOC_SESSION_KEY]

            if session:
                try:
                    service = self.flat_service()
                    service.session_code = session
                    service.close_session()

                except DigiDocError:
                    pass

            del self.request.session[self.DIGIDOC_SESSION_KEY]

        except KeyError:
            pass",Closes DigiDocService session and clears request.session[I{DIGIDOC_SESSION_KEY}],0,2,2,4
"def destroy_template(name=None, call=None, kwargs=None):
    
    if call == 'action':
        raise SaltCloudSystemExit(
            'The destroy_template function must be called with  -f.'
        )
    if kwargs is None:
        kwargs = {}
    name = kwargs.get('name', None)
    session = _get_session()
    vms = session.xenapi.VM.get_all_records()
    ret = {}
    found = False
    for vm in vms:
        record = session.xenapi.VM.get_record(vm)
        if record['is_a_template']:
            if record['name_label'] == name:
                found = True
                
                session.xenapi.VM.destroy(vm)
                ret[name] = {'status': 'destroyed'}
    if not found:
        ret[name] = {'status': 'not found'}
    return ret","Destroy Xen VM or template instance

        .. code-block:: bash

            salt-cloud -f destroy_template myxen name=testvm2",1,2,1,4
"def detect_rowspans(self, use_actual=1):
        
        
        vals = self.actual_values if use_actual else self.formatted_values
        if self.is_contiguous_rows:
            for cidx in range(self.ncols):
                for r0, r1 in span_iter(vals.iloc[:, cidx]):
                    actual_idx = self.col_ilocs[cidx]
                    self.style_cmds.append(['SPAN', (actual_idx, r0), (actual_idx, r1)])
        return self","Determine if any row spans are present in the values.
        :param use_actual:  if True, check actual_values for span. if False, use the formatted_values
        :return: self",0,0,3,3
"def detect_state_variable_shadowing(contracts):
    
    results = set()
    for contract in contracts:
        variables_declared = {variable.name: variable for variable in contract.variables
                              if variable.contract == contract}
        for immediate_base_contract in contract.immediate_inheritance:
            for variable in immediate_base_contract.variables:
                if variable.name in variables_declared:
                    results.add((contract, variables_declared[variable.name], immediate_base_contract, variable))
    return results","Detects all overshadowing and overshadowed state variables in the provided contracts.
    :param contracts: The contracts to detect shadowing within.
    :return: Returns a set of tuples (overshadowing_contract, overshadowing_state_var, overshadowed_contract,
    overshadowed_state_var).
    The contract-variable pair's variable does not need to be defined in its paired contract, it may have been
    inherited. The contracts are simply included to denote the immediate inheritance path from which the shadowed
    variable originates.",0,0,2,2
"def determine_alert(self, action_schedule, issue_creation_time, last_alert):
        
        issue_age = time.time() - issue_creation_time
        alert_schedule_lookup = {pytimeparse.parse(action_time): action_time for action_time in action_schedule}
        alert_schedule = sorted(alert_schedule_lookup.keys())
        last_alert_time = pytimeparse.parse(last_alert)

        for alert_time in alert_schedule:
            if last_alert_time < alert_time <= issue_age and last_alert_time != alert_time:
                return alert_schedule_lookup[alert_time]
        else:
            return None","Determine if we need to trigger an alert

        Args:
            action_schedule (`list`): A list contains the alert schedule
            issue_creation_time (`int`): Time we create the issue
            last_alert (`str`): Time we sent the last alert

        Returns:
            (`None` or `str`)
            None if no alert should be sent. Otherwise return the alert we should send",0,0,1,1
"def device_event_retrieve(self, device_event_id, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('asynchronous'):
            return self.device_event_retrieve_with_http_info(device_event_id, **kwargs)  
        else:
            (data) = self.device_event_retrieve_with_http_info(device_event_id, **kwargs)  
            return data","Retrieve a device event.  # noqa: E501

        Retrieve a specific device event.  # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass asynchronous=True
        >>> thread = api.device_event_retrieve(device_event_id, asynchronous=True)
        >>> result = thread.get()

        :param asynchronous bool
        :param str device_event_id: (required)
        :return: DeviceEventData
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def device_measurement(device,
                       ts=None,
                       part=None,
                       result=None,
                       code=None,
                       **kwargs):
    
    if ts is None:
        ts = local_now()

    payload = MeasurementPayload(device=device, part=part)
    m = Measurement(ts, result, code, list(kwargs))
    payload.measurements.append(m)
    m.add_sample(ts, **kwargs)
    return dumps(payload)","Returns a JSON MeasurementPayload ready to be send through a
    transport.

    If `ts` is not given, the current time is used. `part` is an
    optional `Part` object, and `result` and `code` are the respective
    fields of the `Measurement` object. All other arguments are
    interpreted as dimensions.

    Minimal example, using a `Device` object to send two
    measurements:

    >>> d = Device(""12345"")
    >>> def publish(msg):
    ...     pass
    >>> publish(d.measurement(temperature=22.8))
    >>> publish(d.measurement(pressure=4.1))",0,0,1,1
"def device_message(device,
                   code,
                   ts=None,
                   origin=None,
                   type=None,
                   severity=None,
                   title=None,
                   description=None,
                   hint=None,
                   **metaData):
    
    
    if ts is None:
        ts = local_now()
    payload = MessagePayload(device=device)
    payload.messages.append(
        Message(
            code=code,
            ts=ts,
            origin=origin,
            type=type,
            severity=severity,
            title=title,
            description=description,
            hint=hint,
            **metaData))
    return dumps(payload)","This quickly builds a time-stamped message. If `ts` is None, the
    current time is used.",0,0,1,1
"def device_query_retrieve(self, query_id, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('asynchronous'):
            return self.device_query_retrieve_with_http_info(query_id, **kwargs)  
        else:
            (data) = self.device_query_retrieve_with_http_info(query_id, **kwargs)  
            return data","Retrieve a device query.  # noqa: E501

        Retrieve a specific device query.  # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass asynchronous=True
        >>> thread = api.device_query_retrieve(query_id, asynchronous=True)
        >>> result = thread.get()

        :param asynchronous bool
        :param str query_id: (required)
        :return: DeviceQuery
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def df_query(self, query, with_labels=False):
        
        import pandas as pd

        if with_labels:
            query = query.with_labels()

        
        statement = query.statement.compile(self.engine)

        
        return pd.read_sql_query(sql=statement, con=self.engine)","Run a :mod:`sqlalchemy` query and return result as a :class:`pandas.DataFrame`

        Args:

            query (sqlalchemy.orm.query.Query): query object, usually generated by :func:`session.query()` in
                an :class:`sqlalchemy.orm.session.Session`

            with_labels (bool): A query for fields with the same name from different tables will cause problems
                when converting it to a :class:`pandas.DataFrame`, because there will be duplicate column names.
                When setting `with_labels=True`, disambiguation labels are assigned to all (!)
                fields in the query - the field name is prefixed with the column name. This enables
                querying fields with identical names from multiple tables but getting unique column names in the output.

        :return: query result as :class:`pandas.DataFrame`",1,0,0,1
"def dict_deep_merge(tgt, src):
    
    for k, v in src.items():
        if k in tgt:
            if isinstance(tgt[k], dict) and isinstance(v, dict):
                dict_deep_merge(tgt[k], v)
            else:
                tgt[k].extend(deepcopy(v))
        else:
            tgt[k] = deepcopy(v)","Utility function to merge the source dictionary `src` to the target
    dictionary recursively

    Note:
        The type of the values in the dictionary can only be `dict` or `list`

    Parameters:
        tgt (dict): The target dictionary
        src (dict): The source dictionary",0,0,1,1
"def dict_from_file(filename, key_type=str):
    
    mapping = {}
    with open(filename, 'r') as f:
        for line in f:
            items = line.rstrip('\n').split()
            assert len(items) >= 2
            key = key_type(items[0])
            val = items[1:] if len(items) > 2 else items[1]
            mapping[key] = val
    return mapping","Load a text file and parse the content as a dict.

    Each line of the text file will be two or more columns splited by
    whitespaces or tabs. The first column will be parsed as dict keys, and
    the following columns will be parsed as dict values.

    Args:
        filename(str): Filename.
        key_type(type): Type of the dict's keys. str is user by default and
            type conversion will be performed if specified.

    Returns:
        dict: The parsed contents.",1,0,2,3
"def discard(self, pid=None):
        
        pid = pid or self.pid

        with db.session.begin_nested():
            before_record_update.send(
                current_app._get_current_object(), record=self)

            _, record = self.fetch_published()
            self.model.json = deepcopy(record.model.json)
            self.model.json['$schema'] = self.build_deposit_schema(record)

            flag_modified(self.model, 'json')
            db.session.merge(self.model)

        after_record_update.send(
            current_app._get_current_object(), record=self)
        return self.__class__(self.model.json, model=self.model)","Discard deposit changes.

        #. The signal :data:`invenio_records.signals.before_record_update` is
            sent before the edit execution.

        #. It restores the last published version.

        #. The following meta information are saved inside the deposit:

        .. code-block:: python

            deposit['$schema'] = deposit_schema_from_record_schema

        #. The signal :data:`invenio_records.signals.after_record_update` is
            sent after the edit execution.

        #. The deposit index is updated.

        Status required: ``'draft'``.

        :param pid: Force a pid object. (Default: ``None``)
        :returns: A new Deposit object.",1,0,0,1
"def discharge_required_response(macaroon, path, cookie_suffix_name,
                                message=None):
    
    if message is None:
        message = 'discharge required'
    content = json.dumps(
        {
            'Code': 'macaroon discharge required',
            'Message': message,
            'Info': {
                'Macaroon': macaroon.to_dict(),
                'MacaroonPath': path,
                'CookieNameSuffix': cookie_suffix_name
            },
        }
    ).encode('utf-8')
    return content, {
        'WWW-Authenticate': 'Macaroon',
        'Content-Type': 'application/json'
    }","Get response content and headers from a discharge macaroons error.

    @param macaroon may hold a macaroon that, when discharged, may
    allow access to a service.
    @param path holds the URL path to be associated with the macaroon.
    The macaroon is potentially valid for all URLs under the given path.
    @param cookie_suffix_name holds the desired cookie name suffix to be
    associated with the macaroon. The actual name used will be
    (""macaroon-"" + CookieName). Clients may ignore this field -
    older clients will always use (""macaroon-"" + macaroon.signature() in hex)
    @return content(bytes) and the headers to set on the response(dict).",0,0,1,1
"def dispatch(self, inp):
    
    dispatched = self._dp(lambda a, b: a.dispatch(b), self._dispatchers, inp)
    ret = self._ep(tf.concat, transpose_list_of_lists(dispatched), 0)
    if ret[0].dtype == tf.float32:
      
      ret = self._ep(common_layers.convert_gradient_to_tensor, ret)
    return ret","Create one input Tensor for each expert.

    Args:
      inp: a list of length num_datashards `Tensor`s with shapes
        `[batch_size[d], <extra_input_dims>]`.
    Returns:
      a list of `num_experts` `Tensor`s with shapes
        `[num_examples[i], <extra_input_dims>]`.",0,0,1,1
"def displacements(self):
        

        disps = []
        if 'first_atoms' in self._displacement_dataset:
            for disp in self._displacement_dataset['first_atoms']:
                x = disp['displacement']
                disps.append([disp['number'], x[0], x[1], x[2]])
        elif 'displacements' in self._displacement_dataset:
            disps = self._displacement_dataset['displacements']

        return disps","Return displacements

        Returns
        -------
        There are two types of displacement dataset. See the docstring
        of set_displacement_dataset about types 1 and 2 for displacement
        dataset format.

        Type-1, List of list
            The internal list has 4 elements such as [32, 0.01, 0.0, 0.0]].
            The first element is the supercell atom index starting with 0.
            The remaining three elements give the displacement in Cartesian
            coordinates.
        Type-2, array_like
            Displacements of all atoms of all supercells in Cartesian
            coordinates.
            shape=(supercells, natom, 3)
            dtype='double'",0,0,1,1
"def display_eventtype(self):
        
        if self.annot is not None:
            event_types = sorted(self.annot.event_types, key=str.lower)
        else:
            event_types = []

        self.idx_eventtype.clear()

        evttype_group = QGroupBox('Event Types')
        layout = QVBoxLayout()
        evttype_group.setLayout(layout)
        
        self.check_all_eventtype = check_all = QCheckBox('All event types')
        check_all.setCheckState(Qt.Checked)
        check_all.clicked.connect(self.toggle_eventtype)
        layout.addWidget(check_all)

        self.idx_eventtype_list = []
        for one_eventtype in event_types:
            self.idx_eventtype.addItem(one_eventtype)
            item = QCheckBox(one_eventtype)
            layout.addWidget(item)
            item.setCheckState(Qt.Checked)
            item.stateChanged.connect(self.update_annotations)
            item.stateChanged.connect(self.toggle_check_all_eventtype)
            self.idx_eventtype_list.append(item)

        self.idx_eventtype_scroll.setWidget(evttype_group)",Read the list of event types in the annotations and update widgets.,0,0,2,2
"def do(self, changes, task_handle=taskhandle.NullTaskHandle()):
        
        try:
            self.current_change = changes
            changes.do(change.create_job_set(task_handle, changes))
        finally:
            self.current_change = None
        if self._is_change_interesting(changes):
            self.undo_list.append(changes)
            self._remove_extra_items()
        del self.redo_list[:]","Perform the change and add it to the `self.undo_list`

        Note that uninteresting changes (changes to ignored files)
        will not be appended to `self.undo_list`.",1,0,2,3
"def do_ams_get_url(endpoint, access_token, flag=True):
    
    headers = {""Content-Type"": json_acceptformat,
               ""DataServiceVersion"": dsversion_min,
               ""MaxDataServiceVersion"": dsversion_max,
               ""Accept"": json_acceptformat,
               ""Accept-Charset"" : charset,
               ""Authorization"": ""Bearer "" + access_token,
               ""x-ms-version"" : xmsversion}
    body = ''
    response = requests.get(endpoint, headers=headers, allow_redirects=flag)
    if flag:
        if response.status_code == 301:
            response = requests.get(response.headers['location'], data=body, headers=headers)
    return response","Do an AMS GET request to retrieve the Final AMS Endpoint and return JSON.
    Args:
        endpoint (str): Azure Media Services Initial Endpoint.
        access_token (str): A valid Azure authentication token.
        flag  (str): A Flag to follow the redirect or not.

    Returns:
        HTTP response. JSON body.",1,1,2,4
"def do_email_notification(self, comment, entry, site):
        
        if not self.mail_comment_notification_recipients:
            return

        template = loader.get_template(
            'comments/zinnia/entry/email/notification.txt')
        context = {
            'comment': comment,
            'entry': entry,
            'site': site,
            'protocol': PROTOCOL
        }
        subject = _('[%(site)s] New comment posted on ""%(title)s""') % \
            {'site': site.name, 'title': entry.title}
        message = template.render(context)

        send_mail(
            subject, message,
            settings.DEFAULT_FROM_EMAIL,
            self.mail_comment_notification_recipients,
            fail_silently=not settings.DEBUG
        )",Send email notification of a new comment to site staff.,1,1,0,2
"def do_truncate(parser, token):
    
    _, count = token.split_contents()

    nodelist = parser.parse(('more', 'endtruncate',))
    token = parser.next_token()
    if token.contents == 'more':
        nodelist_more = parser.parse(('endtruncate',))
        parser.delete_first_token()
    else:
        nodelist_more = None
    return TruncateNode(count, nodelist, nodelist_more)","Truncates given text (html-aware)
    Sample usage::

        {% truncate 60 %}
             {{ some_text }}
        {% more %}
             <a href=""asdasd"">Read more..</a>
        {% endtruncate %}",0,0,1,1
"def do_until(lambda_expr, timeout=WTF_TIMEOUT_MANAGER.NORMAL, sleep=0.5, message=None):
    
    __check_condition_parameter_is_function(lambda_expr)

    end_time = datetime.now() + timedelta(seconds=timeout)
    last_exception = None
    while datetime.now() < end_time:
        try:
            return lambda_expr()
        except Exception as e:
            last_exception = e
            time.sleep(sleep)

    if message:
        raise OperationTimeoutError(message, last_exception)
    else:
        raise OperationTimeoutError(""Operation timed out."", last_exception)","A retry wrapper that'll keep performing the action until it succeeds.
    (main differnce between do_until and wait_until is do_until will keep trying 
    until a value is returned, while wait until will wait until the function 
    evaluates True.)

    Args:
        lambda_expr (lambda) : Expression to evaluate.

    Kwargs: 
        timeout (number): Timeout period in seconds.
        sleep (number) : Sleep time to wait between iterations
        message (str) : Provide a message for TimeoutError raised.

    Returns:
        The value of the evaluated lambda expression.

    Usage::

        do_until(lambda: driver.find_element_by_id(""save"").click(),
                 timeout=30,
                 sleep=0.5)

    Is equivalent to:

        end_time = datetime.now() + timedelta(seconds=30)
        while datetime.now() < end_time:
            try:
                return driver.find_element_by_id(""save"").click()
            except:
                pass
            time.sleep(0.5)
        raise OperationTimeoutError()",1,0,1,2
"def domain_urlize(value):
    
    parsed_uri = urlparse(value)
    domain = '{uri.netloc}'.format(uri=parsed_uri)

    if domain.startswith('www.'):
        domain = domain[4:]

    return format_html('<a href=""{}"" rel=""nofollow"">{}</a>',
            value,
            domain
        )","Returns an HTML link to the supplied URL, but only using the domain as the
    text. Strips 'www.' from the start of the domain, if present.

    e.g. if `my_url` is 'http://www.example.org/foo/' then:

        {{ my_url|domain_urlize }}

    returns:
        <a href=""http://www.example.org/foo/"" rel=""nofollow"">example.org</a>",0,0,1,1
"def doublegauss(x,p):
    
    mu,sig1,sig2 = p
    x = np.atleast_1d(x)
    A = 1./(np.sqrt(2*np.pi)*(sig1+sig2)/2.)
    ylo = A*np.exp(-(x-mu)**2/(2*sig1**2))
    yhi = A*np.exp(-(x-mu)**2/(2*sig2**2))
    y = x*0
    wlo = np.where(x < mu)
    whi = np.where(x >= mu)
    y[wlo] = ylo[wlo]
    y[whi] = yhi[whi]
    if np.size(x)==1:
        return y[0]
    else:
        return y","Evaluates normalized two-sided Gaussian distribution

    Parameters
    ----------
    x : float or array-like
        Value(s) at which to evaluate distribution

    p : array-like
        Parameters of distribution: (mu: mode of distribution,
                                     sig1: LH width,
                                     sig2: RH width)

    Returns
    -------
    value : float or array-like
        Distribution evaluated at input value(s).  If single value provided,
        single value returned.",0,0,1,1
"def download(self, file_name, save_as=None):
        
        self._check_session()
        try:
            if save_as:
                save_as = os.path.normpath(save_as)
                save_dir = os.path.dirname(save_as)
                if save_dir:
                    if not os.path.exists(save_dir):
                        os.makedirs(save_dir)
                    elif not os.path.isdir(save_dir):
                        raise RuntimeError(save_dir + "" is not a directory"")

            status, save_path, bytes = self._rest.download_file(
                'files', file_name, save_as, 'application/octet-stream')
        except resthttp.RestHttpError as e:
            raise RuntimeError('failed to download ""%s"": %s' % (file_name, e))
        return save_path, bytes","Download the specified file from the server.

        Arguments:
        file_name -- Name of file resource to save.
        save_as   -- Optional path name to write file to.  If not specified,
                     then file named by the last part of the resource path is
                     downloaded to current directory.

        Return: (save_path, bytes)
        save_path -- Path where downloaded file was saved.
        bytes     -- Bytes downloaded.",3,1,2,6
"def download_as_file(self, url: str, folder: Path, name: str, delay: float = 0) -> str:
        
        while folder.joinpath(name).exists():  
            self.log.warning('already exists: ' + name)
            name = name + '_d'

        with self._downloader.request('GET', url, preload_content=False, retries=urllib3.util.retry.Retry(3)) as reader:
            if reader.status == 200:
                with folder.joinpath(name).open(mode='wb') as out_file:
                    out_file.write(reader.data)
            else:
                raise HTTPError(f""{url} | {reader.status}"")

        if delay > 0:
            time.sleep(delay)

        return url","Download the given url to the given target folder.

        :param url: link
        :type url: str
        :param folder: target folder
        :type folder: ~pathlib.Path
        :param name: target file name
        :type name: str
        :param delay: after download wait in seconds
        :type delay: float
        :return: url
        :rtype: str
        :raises ~urllib3.exceptions.HTTPError: if the connection has an error",1,2,1,4
"def download_user_playlists_by_id(self, user_id):
        

        try:
            playlist = self.crawler.get_user_playlists(user_id)
        except RequestException as exception:
            click.echo(exception)
        else:
            self.download_playlist_by_id(
                playlist.playlist_id, playlist.playlist_name)","Download user's playlists by his/her id.

        :params user_id: user id.",1,2,1,4
"def drawDiscrete(N,P=[1.0],X=[0.0],exact_match=False,seed=0):
    
    
    RNG = np.random.RandomState(seed)

    if exact_match:
        events = np.arange(P.size) 
        cutoffs = np.round(np.cumsum(P)*N).astype(int) 
        top = 0
        
        event_list        = []
        for j in range(events.size):
            bot = top
            top = cutoffs[j]
            event_list += (top-bot)*[events[j]]
        
        event_draws = RNG.permutation(event_list)
        draws = X[event_draws]
    else:
        
        base_draws = RNG.uniform(size=N)
        cum_dist = np.cumsum(P)

        
        indices = cum_dist.searchsorted(base_draws)
        draws = np.asarray(X)[indices]
    return draws","Simulates N draws from a discrete distribution with probabilities P and outcomes X.

    Parameters
    ----------
    P : np.array
        A list of probabilities of outcomes.
    X : np.array
        A list of discrete outcomes.
    N : int
        Number of draws to simulate.
    exact_match : boolean
        Whether the draws should ""exactly"" match the discrete distribution (as
        closely as possible given finite draws).  When True, returned draws are
        a random permutation of the N-length list that best fits the discrete
        distribution.  When False (default), each draw is independent from the
        others and the result could deviate from the input.
    seed : int
        Seed for random number generator.

    Returns
    -------
    draws : np.array
        An array draws from the discrete distribution; each element is a value in X.",1,0,3,4
"def drawdown_details(drawdown, index_type=pd.DatetimeIndex):
    

    is_zero = drawdown == 0
    
    start = ~is_zero & is_zero.shift(1)
    start = list(start[start == True].index)  

    
    end = is_zero & (~is_zero).shift(1)
    end = list(end[end == True].index)  

    if len(start) is 0:
        return None

    
    if len(end) is 0:
        end.append(drawdown.index[-1])

    
    
    
    if start[0] > end[0]:
        start.insert(0, drawdown.index[0])

    
    
    if start[-1] > end[-1]:
        end.append(drawdown.index[-1])

    result = pd.DataFrame(
        columns=('Start', 'End', 'Length', 'drawdown'),
        index=range(0, len(start))
    )

    for i in range(0, len(start)):
        dd = drawdown[start[i]:end[i]].min()

        if index_type is pd.DatetimeIndex:
            result.iloc[i] = (start[i], end[i], (end[i] - start[i]).days, dd)
        else:
            result.iloc[i] = (start[i], end[i], (end[i] - start[i]), dd)

    return result","Returns a data frame with start, end, days (duration) and
    drawdown for each drawdown in a drawdown series.

    .. note::

        days are actual calendar days, not trading days

    Args:
        * drawdown (pandas.Series): A drawdown Series
            (can be obtained w/ drawdown(prices).
    Returns:
        * pandas.DataFrame -- A data frame with the following
            columns: start, end, days, drawdown.",0,0,1,1
"def drel(simulated_array, observed_array, replace_nan=None, replace_inf=None,
         remove_neg=False, remove_zero=False):
    

    
    simulated_array, observed_array = treat_values(
        simulated_array,
        observed_array,
        replace_nan=replace_nan,
        replace_inf=replace_inf,
        remove_neg=remove_neg,
        remove_zero=remove_zero
    )

    a = ((simulated_array - observed_array) / observed_array) ** 2
    b = np.abs(simulated_array - np.mean(observed_array))
    c = np.abs(observed_array - np.mean(observed_array))
    e = ((b + c) / np.mean(observed_array)) ** 2
    return 1 - (np.sum(a) / np.sum(e))","Compute the the relative index of agreement (drel).

    .. image:: /pictures/drel.png

    **Range:** 0 ≤ drel < 1, does not indicate bias, larger is better.

    **Notes:** Instead of absolute differences, this metric uses relative differences.

    Parameters
    ----------
    simulated_array: one dimensional ndarray
        An array of simulated data from the time series.

    observed_array: one dimensional ndarray
        An array of observed data from the time series.

    replace_nan: float, optional
        If given, indicates which value to replace NaN values with in the two arrays. If None, when
        a NaN value is found at the i-th position in the observed OR simulated array, the i-th value
        of the observed and simulated array are removed before the computation.

    replace_inf: float, optional
        If given, indicates which value to replace Inf values with in the two arrays. If None, when
        an inf value is found at the i-th position in the observed OR simulated array, the i-th
        value of the observed and simulated array are removed before the computation.

    remove_neg: boolean, optional
        If True, when a negative value is found at the i-th position in the observed OR simulated
        array, the i-th value of the observed AND simulated array are removed before the
        computation.

    remove_zero: boolean, optional
        If true, when a zero value is found at the i-th position in the observed OR simulated
        array, the i-th value of the observed AND simulated array are removed before the
        computation.

    Returns
    -------
    float
        The relative index of agreement.

    Examples
    --------

    >>> import HydroErr as he
    >>> import numpy as np

    >>> sim = np.array([5, 7, 9, 2, 4.5, 6.7])
    >>> obs = np.array([4.7, 6, 10, 2.5, 4, 7])
    >>> he.drel(sim, obs)
    0.9740868625579597

    References
    ----------
    - Krause, P., Boyle, D., Bäse, F., 2005. Comparison of different efficiency criteria for
      hydrological model assessment. Advances in geosciences 5 89-97.",0,0,1,1
"def ds_discrete(self, d_min=None, d_max=None, pts=20, limit=1e-9, 
                    method='logarithmic'):
        r
        if method[0] not in ('R', 'r'):
            if d_min is None:
                d_min = self.dn(limit)
            if d_max is None:
                d_max = self.dn(1.0 - limit)
        return psd_spacing(d_min=d_min, d_max=d_max, pts=pts, method=method)","r'''Create a particle spacing mesh to perform calculations with, 
        according to one of several ways. The allowable meshes are
        'linear', 'logarithmic', a geometric series specified by a Renard 
        number such as 'R10', or the meshes available in one of several sieve 
        standards.
        
        Parameters
        ----------
        d_min : float, optional
            The minimum diameter at which the mesh starts, [m]
        d_max : float, optional
            The maximum diameter at which the mesh ends, [m]
        pts : int, optional
            The number of points to return for the mesh (note this is not 
            respected by sieve meshes), [-]
        limit : float
            If `d_min` or `d_max` is not specified, it will be calculated as the
            `dn` at which this limit or 1-limit exists (this is ignored for 
            Renard numbers), [-]
        method : str, optional
            Either 'linear', 'logarithmic', a Renard number like 'R10' or 'R5' 
            or'R2.5', or one of the sieve standards 'ISO 3310-1 R40/3', 
            'ISO 3310-1 R20', 'ISO 3310-1 R20/3', 'ISO 3310-1', 
            'ISO 3310-1 R10', 'ASTM E11', [-]
    
        Returns
        -------
        ds : list[float]
            The generated mesh diameters, [m]
    
        Notes
        -----
        Note that when specifying a Renard series, only one of `d_min` or `d_max` can
        be respected! Provide only one of those numbers. 
        
        Note that when specifying a sieve standard the number of points is not
        respected!
    
        References
        ----------
        .. [1] ASTM E11 - 17 - Standard Specification for Woven Wire Test Sieve 
           Cloth and Test Sieves.
        .. [2] ISO 3310-1:2016 - Test Sieves -- Technical Requirements and Testing
           -- Part 1: Test Sieves of Metal Wire Cloth.",0,0,1,1
"def dt_to_struct_time(dt):
    
    if isinstance(dt, datetime):
        return struct_time(
            [dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second] +
            TIME_EMPTY_EXTRAS
        )
    elif isinstance(dt, date):
        return struct_time(
            [dt.year, dt.month, dt.day] + TIME_EMPTY_TIME + TIME_EMPTY_EXTRAS
        )
    else:
        raise NotImplementedError(
            ""Cannot convert %s to `struct_time`"" % type(dt))","Convert a `datetime.date` or `datetime.datetime` to a `struct_time`
    representation *with zero values* for data fields that we cannot always
    rely on for ancient or far-future dates: tm_wday, tm_yday, tm_isdst

    NOTE: If it wasn't for the requirement that the extra fields are unset
    we could use the `timetuple()` method instead of this function.",1,0,2,3
"def dump_json(json_info, json_file, overwrite=True):
    
    if overwrite:
        mode = ""w""
    else:
        mode = ""w+""

    try:
        with open(json_file, mode) as f:
            f.write(json.dumps(json_info))
    except BaseException as e:
        logging.error(e.message)","Dump a whole json record into the given file.

    Overwrite the file if the overwrite flag set.

    Args:
        json_info (dict): Information dict to be dumped.
        json_file (str): File path to be dumped to.
        overwrite(boolean)",2,0,1,3
"def dump_object(self, obj):
        
        if isinstance(obj, uuid.UUID):
            return str(obj)
        if hasattr(obj, 'isoformat'):
            return obj.isoformat()
        if isinstance(obj, (bytes, bytearray, memoryview)):
            return base64.b64encode(obj).decode('ASCII')
        raise TypeError('{!r} is not JSON serializable'.format(obj))","Called to encode unrecognized object.

        :param object obj: the object to encode
        :return: the encoded object
        :raises TypeError: when `obj` cannot be encoded

        This method is passed as the ``default`` keyword parameter
        to :func:`json.dumps`.  It provides default representations for
        a number of Python language/standard library types.

        +----------------------------+---------------------------------------+
        | Python Type                | String Format                         |
        +----------------------------+---------------------------------------+
        | :class:`bytes`,            | Base64 encoded string.                |
        | :class:`bytearray`,        |                                       |
        | :class:`memoryview`        |                                       |
        +----------------------------+---------------------------------------+
        | :class:`datetime.datetime` | ISO8601 formatted timestamp in the    |
        |                            | extended format including separators, |
        |                            | milliseconds, and the timezone        |
        |                            | designator.                           |
        +----------------------------+---------------------------------------+
        | :class:`uuid.UUID`         | Same as ``str(value)``                |
        +----------------------------+---------------------------------------+",1,0,2,3
"def dump_public_key(public_key, encoding='pem'):
    

    if encoding not in set(['pem', 'der']):
        raise ValueError(pretty_message(
            ,
            repr(encoding)
        ))

    is_oscrypto = isinstance(public_key, PublicKey)
    if not isinstance(public_key, keys.PublicKeyInfo) and not is_oscrypto:
        raise TypeError(pretty_message(
            ,
            type_name(public_key)
        ))

    if is_oscrypto:
        public_key = public_key.asn1

    output = public_key.dump()
    if encoding == 'pem':
        output = pem.armor('PUBLIC KEY', output)
    return output","Serializes a public key object into a byte string

    :param public_key:
        An oscrypto.asymmetric.PublicKey or asn1crypto.keys.PublicKeyInfo object

    :param encoding:
        A unicode string of ""pem"" or ""der""

    :return:
        A byte string of the encoded public key",2,0,3,5
"def dumps(voevent, pretty_print=False, xml_declaration=True, encoding='UTF-8'):
    
    vcopy = copy.deepcopy(voevent)
    _return_to_standard_xml(vcopy)
    s = etree.tostring(vcopy, pretty_print=pretty_print,
                       xml_declaration=xml_declaration,
                       encoding=encoding)
    return s","Converts voevent to string.

    .. note:: Default encoding is UTF-8, in line with VOE2.0 schema.
        Declaring the encoding can cause diffs with the original loaded VOEvent,
        but I think it's probably the right thing to do (and lxml doesn't
        really give you a choice anyway).

    Args:
        voevent (:class:`Voevent`): Root node of the VOevent etree.
        pretty_print (bool): indent the output for improved human-legibility
            when possible. See also:
            http://lxml.de/FAQ.html#why-doesn-t-the-pretty-print-option-reformat-my-xml-output
        xml_declaration (bool): Prepends a doctype tag to the string output,
            i.e. something like ``<?xml version='1.0' encoding='UTF-8'?>``
    Returns:
        bytes: Bytestring containing raw XML representation of VOEvent.",0,0,1,1
"def during(rrule, duration=None, timestamp=None, **kwargs):
    

    result = False

    
    if isinstance(rrule, string_types):
        rrule_object = rrule_class.rrulestr(rrule)

    else:
        rrule_object = rrule_class(**rrule)

    
    if timestamp is None:
        timestamp = time()

    
    now = datetime.fromtimestamp(timestamp)

    
    duration_delta = now if duration is None else relativedelta(**duration)

    
    last_date = rrule_object.before(now, inc=True)

    
    if last_date is not None:
        next_date = last_date + duration_delta

        
        result = last_date <= now <= next_date

    return result","Check if input timestamp is in rrule+duration period

    :param rrule: rrule to check
    :type rrule: str or dict
        (freq, dtstart, interval, count, wkst, until, bymonth, byminute, etc.)
    :param dict duration: time duration from rrule step. Ex:{'minutes': 60}
    :param float timestamp: timestamp to check between rrule+duration. If None,
        use now",0,0,1,1
"def eat_config(self, conf_file=None):
        

        chroot = os.path.dirname(self.filename)  
        chroot = os.path.abspath(chroot)

        
        if hasattr(self, 'conf_file') and not conf_file:
            cfgfn = self.conf_file
        elif conf_file:
            cfgfn = conf_file
        else:
            cfgfn = os.path.join(chroot, CONFIG_FILE)

        with open(cfgfn, 'r') as fo:
            self.conconf.eat_config(fo)

        
        if not self.no_auto:
            self.make_mask()
        else:
            self.make_mask(dry=True)","Read the the conf_file and update this instance accordingly.

        conf_file: str or Falseish
            If conf_file is Falseish, look in the directory where
            self.filename sits if self is not already associated with a
            conf_file. If associated, and conf_file arg is Falseish,
            read self.conf_file. If conf_file arg is a file name, read
            from that file, but do not update self.conf_file
            accordingly. An Implicit IOError is raised if no conf_file
            was found.

        See spit_config for documentation on the file layout.

        .. note::
           Updates the mask if not no_auto.

        .. note::
           If the config_file exist because of an earlier spit, and
           custom channel names was not available, channels are listed as the
           fallback names in the file. Then after this eat, self.chnames
           will be set to the list in the conf_file section 'channels'. The
           result can be that self.chnames and self.chnames_0 will be
           equal.

        The message then is that, if channel names are updated, you
        should spit before you eat.",2,0,0,2
"def ecdsa_sign_compact(msg32, seckey):
    
    
    output64 = ffi.new(""unsigned char[65]"")
    
    recid = ffi.new(""int *"")

    lib.secp256k1_ecdsa_recoverable_signature_serialize_compact(
        ctx,
        output64,
        recid,
        _ecdsa_sign_recoverable(msg32, seckey)
    )

    
    r = ffi.buffer(output64)[:64] + struct.pack(""B"", recid[0])
    assert len(r) == 65, len(r)
    return r","Takes the same message and seckey as _ecdsa_sign_recoverable
        Returns an unsigned char array of length 65 containing the signed message",0,0,2,2
"def edit(self,
             name,
             description=None,
             homepage=None,
             private=None,
             has_issues=None,
             has_wiki=None,
             has_downloads=None,
             default_branch=None):
        
        edit = {'name': name, 'description': description, 'homepage': homepage,
                'private': private, 'has_issues': has_issues,
                'has_wiki': has_wiki, 'has_downloads': has_downloads,
                'default_branch': default_branch}
        self._remove_none(edit)
        json = None
        if edit:
            json = self._json(self._patch(self._api, data=dumps(edit)), 200)
            self._update_(json)
            return True
        return False","Edit this repository.

        :param str name: (required), name of the repository
        :param str description: (optional), If not ``None``, change the
            description for this repository. API default: ``None`` - leave
            value unchanged.
        :param str homepage: (optional), If not ``None``, change the homepage
            for this repository. API default: ``None`` - leave value unchanged.
        :param bool private: (optional), If ``True``, make the repository
            private. If ``False``, make the repository public. API default:
            ``None`` - leave value unchanged.
        :param bool has_issues: (optional), If ``True``, enable issues for
            this repository. If ``False``, disable issues for this repository.
            API default: ``None`` - leave value unchanged.
        :param bool has_wiki: (optional), If ``True``, enable the wiki for
            this repository. If ``False``, disable the wiki for this
            repository. API default: ``None`` - leave value unchanged.
        :param bool has_downloads: (optional), If ``True``, enable downloads
            for this repository. If ``False``, disable downloads for this
            repository. API default: ``None`` - leave value unchanged.
        :param str default_branch: (optional), If not ``None``, change the
            default branch for this repository. API default: ``None`` - leave
            value unchanged.
        :returns: bool -- True if successful, False otherwise",0,1,1,2
"def editItemInfo(self, json_dict):
        
        url = self._url + ""/iteminfo/edit""
        params = {
            ""f"" : ""json"",
            ""serviceItemInfo"" : json.dumps(json_dict)
        }
        return self._post(url=url,
                             param_dict=params,
                             securityHandler=self._securityHandler,
                             proxy_url=self._proxy_url,
                             proxy_port=self._proxy_port)","Allows for the direct edit of the service's item's information.
        To get the current item information, pull the data by calling
        iteminfo property.  This will return the default template then pass
        this object back into the editItemInfo() as a dictionary.

        Inputs:
           json_dict - iteminfo dictionary.
        Output:
           json as dictionary",0,1,1,2
"def editable_loader(context):
    
    user = context[""request""].user
    template_vars = {
        ""has_site_permission"": has_site_permission(user),
        ""request"": context[""request""],
    }
    if (settings.INLINE_EDITING_ENABLED and
            template_vars[""has_site_permission""]):
        t = get_template(""includes/editable_toolbar.html"")
        template_vars[""REDIRECT_FIELD_NAME""] = REDIRECT_FIELD_NAME
        template_vars[""editable_obj""] = context.get(""editable_obj"",
                                        context.get(""page"", None))
        template_vars[""accounts_logout_url""] = context.get(
            ""accounts_logout_url"", None)
        template_vars[""toolbar""] = t.render(Context(template_vars))
        template_vars[""richtext_media""] = RichTextField().formfield(
            ).widget.media
    return template_vars",Set up the required JS/CSS for the in-line editing toolbar and controls.,0,0,2,2
"def eeg_add_channel(raw, channel, sync_index_eeg=0, sync_index_channel=0, channel_type=None, channel_name=None):
    
    if channel_name is None:
        if isinstance(channel, pd.core.series.Series):
            if channel.name is not None:
                channel_name = channel.name
            else:
                channel_name = ""Added_Channel""
        else:
            channel_name = ""Added_Channel""

    
    diff = sync_index_channel - sync_index_eeg
    if diff > 0:
        channel = list(channel)[diff:len(channel)]
        channel = channel + [np.nan]*diff
    if diff < 0:
        channel = [np.nan]*diff + list(channel)
        channel = list(channel)[0:len(channel)]

    
    if len(channel) < len(raw):
        channel = list(channel) + [np.nan]*(len(raw)-len(channel))
    else:
        channel = list(channel)[0:len(raw)]  

    info = mne.create_info([channel_name], raw.info[""sfreq""], ch_types=channel_type)
    channel = mne.io.RawArray([channel], info)

    raw.add_channels([channel], force_update_info=True)

    return(raw)","Add a channel to a mne's Raw m/eeg file. It will basically synchronize the channel to the eeg data following a particular index and add it.

    Parameters
    ----------
    raw : mne.io.Raw
        Raw EEG data.
    channel : list or numpy.array
        The channel to be added.
    sync_index_eeg : int or list
        An index, in the raw data, by which to align the two inputs.
    sync_index_channel : int or list
        An index, in the channel to add, by which to align the two inputs.
    channel_type : str
        Channel type. Currently supported fields are 'ecg', 'bio', 'stim', 'eog', 'misc', 'seeg', 'ecog', 'mag', 'eeg', 'ref_meg', 'grad', 'emg', 'hbr' or 'hbo'.

    Returns
    ----------
    raw : mne.io.Raw
        Raw data in FIF format.

    Example
    ----------
    >>> import neurokit as nk
    >>> event_index_in_eeg = 42
    >>> event_index_in_ecg = 666
    >>> raw = nk.eeg_add_channel(raw, ecg, sync_index_raw=event_index_in_eeg, sync_index_channel=event_index_in_ecg, channel_type=""ecg"")

    Notes
    ----------
    *Authors*

    - `Dominique Makowski <https://dominiquemakowski.github.io/>`_

    *Dependencies*

    - mne

    *See Also*

    - mne: http://martinos.org/mne/dev/index.html",1,0,0,1
"def eigenMethod(self, tol = 1e-8, maxiter = 1e5):
        
        Q = self.getIrreducibleTransitionMatrix(probabilities=False)
        
        if Q.shape == (1, 1):
            self.pi = np.array([1.0]) 
            return          
        
        if Q.shape == (2, 2):
            self.pi= np.array([Q[1,0],Q[0,1]]/(Q[0,1]+Q[1,0]))
            return                 
        
        size = Q.shape[0]
        guess = np.ones(size,dtype=float)
        w, v = eigs(Q.T, k=1, v0=guess, sigma=1e-6, which='LM',tol=tol, maxiter=maxiter)
        pi = v[:, 0].real
        pi /= pi.sum()
        
        self.pi = pi","Determines ``pi`` by searching for the eigenvector corresponding to the first eigenvalue, using the :func:`eigs` function. 
        The result is stored in the class attribute ``pi``.         
        
        Parameters
        ----------
        tol : float, optional(default=1e-8)
            Tolerance level for the precision of the end result. A lower tolerance leads to more accurate estimate of ``pi``.
        maxiter : int, optional(default=1e5)
            The maximum number of iterations to be carried out.            
        
        Example
        -------
        >>> P = np.array([[0.5,0.5],[0.6,0.4]])
        >>> mc = markovChain(P)
        >>> mc.eigenMethod()
        >>> print(mc.pi) 
        [ 0.54545455  0.45454545]
        
        Remarks
        -------
        The speed of convergence depends heavily on the choice of the initial guess for ``pi``.
        Here we let the initial ``pi`` be a vector of ones. 
        For large state spaces, this method may not work well.
        At the moment, we call :func:`powerMethod` if the number of states is 2.
        Code is due to a colleague: http://nicky.vanforeest.com/probability/markovChains/markovChain.html",0,0,1,1
"def element_resolver(elements, do_raise=True):
    
    if isinstance(elements, list):
        e = []
        for element in elements:
            try:
                e.append(element.href)
            except AttributeError:
                e.append(element)
            except smc.api.exceptions.ElementNotFound:
                if do_raise:
                    raise
        return e
    try:
        return elements.href
    except AttributeError:
        return elements
    except smc.api.exceptions.ElementNotFound:
        if do_raise:
            raise","Element resolver takes either a single class instance
    or a list of elements to resolve the href. It does
    not assume a specific interface, instead if it's
    a class, it just needs an 'href' attribute that should
    hold the http url for the resource. If a list is
    provided, a list is returned. If you want to suppress
    raising an exception and just return None or [] instead,
    set do_raise=False.

    :raises ElementNotFound: if this is of type Element,
        ElementLocator will attempt to retrieve meta if it
        doesn't already exist but the element was not found.",1,0,2,3
"def elementwise_cdf(self, p):
        r
        p = scipy.atleast_1d(p)
        if len(p) != len(self.sigma):
            raise ValueError(""length of p must equal the number of parameters!"")
        if p.ndim != 1:
            raise ValueError(""p must be one-dimensional!"")
        return scipy.asarray([scipy.stats.lognorm.cdf(v, s, loc=0, scale=em) for v, s, em in zip(p, self.sigma, self.emu)])","r""""""Convert a sample to random variates uniform on :math:`[0, 1]`.
        
        For a univariate distribution, this is simply evaluating the CDF. To
        facilitate efficient sampling, this function returns a *vector* of CDF
        values, one value for each variable. Basically, the idea is that, given
        a vector :math:`q` of `num_params` values each of which is distributed
        according to the prior, this function will return variables uniform on
        :math:`[0, 1]` corresponding to each variable. This is the inverse
        operation to :py:meth:`sample_u`.
        
        Parameters
        ----------
        p : array-like, (`num_params`,)
            Values to evaluate CDF at.",2,0,3,5
"def elevation_profile(request, format=None):
    
    if format is None:
        format = 'json'

    path = request.query_params.get('path')
    if not path:
        return Response({'detail': _('missing required path argument')}, status=400)

    return Response(elevation(path,
                              api_key=ELEVATION_API_KEY,
                              sampling=ELEVATION_DEFAULT_SAMPLING))","Proxy to google elevation API but returns GeoJSON
    (unless ""original"" parameter is passed, in which case the original response is returned).

    For input parameters read:
    https://developers.google.com/maps/documentation/elevation/",0,2,1,3
"def email(self, repo, event, fields, dryrun=False):
        
        tcontents = self._get_template(event, ""txt"", fields)
        hcontents = self._get_template(event, ""html"", fields)
        if tcontents is not None and hcontents is not None:
            return Email(self.server, repo, self.settings[repo], tcontents, hcontents, dryrun)","Sends an email to the configured recipients for the specified event.

        :arg repo: the name of the repository to include in the email subject.
        :arg event: one of [""start"", ""success"", ""failure"", ""timeout"", ""error""].
        :arg fields: a dictionary of field values to replace into the email template
          contents to specialize them.
        :arg dryrun: when true, the email object and contents are initialized, but
          the request is never sent to the SMTP server.",0,1,0,1
"def enable_passive_svc_checks(self, service):
        
        if not service.passive_checks_enabled:
            service.modified_attributes |= \
                DICT_MODATTR[""MODATTR_PASSIVE_CHECKS_ENABLED""].value
            service.passive_checks_enabled = True
            self.send_an_element(service.get_update_status_brok())","Enable passive checks for a service
        Format of the line that triggers function call::

        ENABLE_PASSIVE_SVC_CHECKS;<host_name>;<service_description>

        :param service: service to edit
        :type service: alignak.objects.service.Service
        :return: None",0,0,1,1
"def enable_snapshots(self, volume_id, schedule_type, retention_count,
                         minute, hour, day_of_week, **kwargs):
        

        return self.client.call('Network_Storage', 'enableSnapshots',
                                schedule_type,
                                retention_count,
                                minute,
                                hour,
                                day_of_week,
                                id=volume_id,
                                **kwargs)","Enables snapshots for a specific block volume at a given schedule

        :param integer volume_id: The id of the volume
        :param string schedule_type: 'HOURLY'|'DAILY'|'WEEKLY'
        :param integer retention_count: Number of snapshots to be kept
        :param integer minute: Minute when to take snapshot
        :param integer hour: Hour when to take snapshot
        :param string day_of_week: Day when to take snapshot
        :return: Returns whether successfully scheduled or not",0,1,0,1
"def encode(self):
        
        
        first_line = self.first_line()
        if self.body and self.wait_continue:
            self.headers['expect'] = '100-continue'
        headers = self.headers
        if self.unredirected_headers:
            headers = self.unredirected_headers.copy()
            headers.update(self.headers)
        buffer = [first_line.encode('ascii'), b'\r\n']
        buffer.extend((('%s: %s\r\n' % (name, value)).encode(CHARSET)
                      for name, value in headers.items()))
        buffer.append(b'\r\n')
        return b''.join(buffer)","The bytes representation of this :class:`HttpRequest`.

        Called by :class:`HttpResponse` when it needs to encode this
        :class:`HttpRequest` before sending it to the HTTP resource.",0,0,1,1
"def encode_command(*args, buf=None):
    
    if buf is None:
        buf = bytearray()
    buf.extend(b'*%d\r\n' % len(args))

    try:
        for arg in args:
            barg = _converters[type(arg)](arg)
            buf.extend(b'$%d\r\n%s\r\n' % (len(barg), barg))
    except KeyError:
        raise TypeError(""Argument {!r} expected to be of bytearray, bytes,""
                        "" float, int, or str type"".format(arg))
    return buf","Encodes arguments into redis bulk-strings array.

    Raises TypeError if any of args not of bytearray, bytes, float, int, or str
    type.",1,0,1,2
"def encode_filesystem_name(input_str):
    
    if isinstance(input_str, str):
        input_str = unicode(input_str)
    elif not isinstance(input_str, unicode):
        raise TypeError(""input_str must be a basestring"")

    as_is = u'abcdefghijklmnopqrstuvwxyz0123456789.-'
    uppercase = u'ABCDEFGHIJKLMNOPQRSTUVWXYZ'
    result = []
    for char in input_str:
        if char in as_is:
            result.append(char)
        elif char == u'_':
            result.append('__')
        elif char in uppercase:
            result.append('_%s' % char.lower())
        else:
            utf8 = char.encode('utf8')
            N = len(utf8)
            if N == 1:
                N = 0
            HH = ''.join('%x' % ord(c) for c in utf8)
            result.append('_%d%s' % (N, HH))
    return ''.join(result)","Encodes an arbitrary unicode string to a generic filesystem-compatible
    non-unicode filename.

    The result after encoding will only contain the standard ascii lowercase
    letters (a-z), the digits (0-9), or periods, underscores, or dashes
    (""."", ""_"", or ""-"").  No uppercase letters will be used, for
    comaptibility with case-insensitive filesystems.

    The rules for the encoding are:

    1) Any lowercase letter, digit, period, or dash (a-z, 0-9, ., or -) is
    encoded as-is.

    2) Any underscore is encoded as a double-underscore (""__"")

    3) Any uppercase ascii letter (A-Z) is encoded as an underscore followed
    by the corresponding lowercase letter (ie, ""A"" => ""_a"")

    4) All other characters are encoded using their UTF-8 encoded unicode
    representation, in the following format: ""_NHH..., where:
        a) N represents the number of bytes needed for the UTF-8 encoding,
        except with N=0 for one-byte representation (the exception for N=1
        is made both because it means that for ""standard"" ascii characters
        in the range 0-127, their encoding will be _0xx, where xx is their
        ascii hex code; and because it mirrors the ways UTF-8 encoding
        itself works, where the number of bytes needed for the character can
        be determined by counting the number of leading ""1""s in the binary
        representation of the character, except that if it is a 1-byte
        sequence, there are 0 leading 1's).
        b) HH represents the bytes of the corresponding UTF-8 encoding, in
        hexadecimal (using lower-case letters)

        As an example, the character ""*"", whose (hex) UTF-8 representation
        of 2A, would be encoded as ""_02a"", while the ""euro"" symbol, which
        has a UTF-8 representation of E2 82 AC, would be encoded as
        ""_3e282ac"".  (Note that, strictly speaking, the ""N"" part of the
        encoding is redundant information, since it is essentially encoded
        in the UTF-8 representation itself, but it makes the resulting
        string more human-readable, and easier to decode).

    As an example, the string ""Foo_Bar (fun).txt"" would get encoded as:
        _foo___bar_020_028fun_029.txt",1,0,3,4
"def encode_offset_fetch_request(cls, group, payloads, from_kafka=False):
        
        version = 1 if from_kafka else 0
        return kafka.protocol.commit.OffsetFetchRequest[version](
            consumer_group=group,
            topics=[(
                topic,
                list(topic_payloads.keys()))
            for topic, topic_payloads in six.iteritems(group_by_topic_and_partition(payloads))])","Encode an OffsetFetchRequest struct. The request is encoded using
        version 0 if from_kafka is false, indicating a request for Zookeeper
        offsets. It is encoded using version 1 otherwise, indicating a request
        for Kafka offsets.

        Arguments:
            group: string, the consumer group you are fetching offsets for
            payloads: list of OffsetFetchRequestPayload
            from_kafka: bool, default False, set True for Kafka-committed offsets",0,1,1,2
"def encode_string(self, value):
        
        if not isinstance(value, str): return value
        try:
            return unicode(value, 'utf-8')
        except: 
                
                
            arr = []
            for ch in value:
                arr.append(unichr(ord(ch)))
            return u"""".join(arr)","Convert ASCII, Latin-1 or UTF-8 to pure Unicode",0,0,1,1
"def encrypt(self, value):
        
        timestamp = str(int(time.time()))
        value = base64.b64encode(value.encode(self.encoding))
        signature = create_signature(self.secret, value + timestamp.encode(),
                                     encoding=self.encoding)
        return ""|"".join([value.decode(self.encoding), timestamp, signature])",Encrypt session data.,0,0,1,1
"def endElement(self, name):
        
        if self.current:
            
            obj = self.current
        else:
            
            text = ''.join(self.chardata).strip()
            obj = self._parse_node_data(text)
        newcurrent, self.chardata = self.stack.pop()
        self.current = self._element_to_node(newcurrent, name, obj)","End current xml element, parse and add to to parent node.",0,0,3,3
"def end_request(req, collector_addr='tcp://127.0.0.2:2345', prefix='my_app'):
    

    req_end = time()
    hreq = hash(req)

    if hreq in requests:
        req_time = req_end - requests[hreq]
        req_time *= 1000

        del requests[hreq]

        collector = get_context().socket(zmq.PUSH)

        collector.connect(collector_addr)
        collector.send_multipart([prefix, str(req_time)])
        collector.close()

        return req_time","registers the end of a request

    registers the end of a request, computes elapsed time, sends it to the collector

    :param req: request, can be mostly any hash-able object
    :param collector_addr: collector address, in zeromq format (string, default tcp://127.0.0.2:2345)
    :param prefix: label under which to register the request (string, default my_app)",1,1,1,3
"def end_segment(self, end_time=None):
        
        entity = self.get_trace_entity()
        if not entity:
            log.warning(""No segment to end"")
            return
        if self._is_subsegment(entity):
            entity.parent_segment.close(end_time)
        else:
            entity.close(end_time)","End the current active segment.

        :param int end_time: epoch in seconds. If not specified the current
            system time will be used.",0,1,2,3
"def enroll_user_in_course(self, username, course_id, mode, cohort=None):
        
        return self.client.enrollment.post(
            {
                'user': username,
                'course_details': {'course_id': course_id},
                'mode': mode,
                'cohort': cohort,
            }
        )","Call the enrollment API to enroll the user in the course specified by course_id.

        Args:
            username (str): The username by which the user goes on the OpenEdX platform
            course_id (str): The string value of the course's unique identifier
            mode (str): The enrollment mode which should be used for the enrollment
            cohort (str): Add the user to this named cohort

        Returns:
            dict: A dictionary containing details of the enrollment, including course details, mode, username, etc.",0,1,0,1
"def ensure_security_manager(self, subject_context):
        
        if (subject_context.resolve_security_manager() is not None):
            msg = (""Subject Context resolved a security_manager ""
                   ""instance, so not re-assigning.  Returning."")
            logger.debug(msg)
            return subject_context

        msg = (""No security_manager found in context.  Adding self ""
               ""reference."")
        logger.debug(msg)

        subject_context.security_manager = self

        return subject_context","Determines whether there is a ``SecurityManager`` instance in the context,
        and if not, adds 'self' to the context.  This ensures that do_create_subject
        will have access to a ``SecurityManager`` during Subject construction.

        :param subject_context: the subject context data that may contain a
                                SecurityManager instance
        :returns: the SubjectContext",0,2,3,5
"def enterabs(self, time, priority, action, argument=(), kwargs=_sentinel):
        
        if kwargs is _sentinel:
            kwargs = {}
        event = Event(time, priority, action, argument, kwargs)
        with self._lock:
            heapq.heappush(self._queue, event)
        return event","Enter a new event in the queue at an absolute time.

        Returns an ID for the event which can be used to remove it,
        if necessary.",0,1,0,1
"def environ_setenv(self, tag, data):
        
        environ = data.get('environ', None)
        if environ is None:
            return False
        false_unsets = data.get('false_unsets', False)
        clear_all = data.get('clear_all', False)
        import salt.modules.environ as mod_environ
        return mod_environ.setenv(environ, false_unsets, clear_all)","Set the salt-minion main process environment according to
        the data contained in the minion event data",0,0,1,1
"def eq(self, o):
        

        if (self.is_integer
            and o.is_integer
            ):
            
            if self.lower_bound == o.lower_bound:
                
                return TrueResult()
            else:
                
                return FalseResult()

        else:
            if self.name == o.name:
                return TrueResult() 

            si_intersection = self.intersection(o)
            if si_intersection.is_empty:
                return FalseResult()

            else:
                return MaybeResult()","Equal

        :param o: The ohter operand
        :return: TrueResult(), FalseResult(), or MaybeResult()",0,0,1,1
"def error(self):
        
        try:
            errcode = DeviceErrorCode(self.error_code)
            return ""%s (%s): %s"" % (errcode.name, errcode.value, self.error_message)
        except:
            return ""Error %s: %s"" % (self.error_code, self.error_message)",Return user-friendly error message.,2,0,0,2
"def estimateDistance(m, M, Av=0.0):
    
    try:
        m = float(m)  
        M = float(M)
        Av = float(Av)
    except TypeError:
        return np.nan

    d = 10 ** ((m - M + 5 - Av) / 5)

    if math.isnan(d):
        return np.nan
    else:
        return d * aq.pc","estimate the distance to star based on the absolute magnitude, apparent
    magnitude and the absorption / extinction.

    :param m: apparent magnitude
    :param M: absolute magnitude
    :param Av: absorbtion / extinction

    :return: d (distance to object) in parsecs",0,0,1,1
"def estimate_assignment_probs(q, trials, cxn, p0=None):
    
    from pyquil.quil import Program
    if p0 is None:  
        p0 = Program()
    results_i = np.sum(cxn.run(p0 + Program(I(q), MEASURE(q, 0)), [0], trials))
    results_x = np.sum(cxn.run(p0 + Program(X(q), MEASURE(q, 0)), [0], trials))

    p00 = 1. - results_i / float(trials)
    p11 = results_x / float(trials)
    return np.array([[p00, 1 - p11],
                     [1 - p00, p11]])","Estimate the readout assignment probabilities for a given qubit ``q``.
    The returned matrix is of the form::

            [[p00 p01]
             [p10 p11]]

    :param int q: The index of the qubit.
    :param int trials: The number of samples for each state preparation.
    :param Union[QVMConnection,QPUConnection] cxn: The quantum abstract machine to sample from.
    :param Program p0: A header program to prepend to the state preparation programs.
    :return: The assignment probability matrix
    :rtype: np.array",0,0,1,1
"def et2lst(et, body, lon, typein, timlen=_default_len_out, ampmlen=_default_len_out):
    
    et = ctypes.c_double(et)
    body = ctypes.c_int(body)
    lon = ctypes.c_double(lon)
    typein = stypes.stringToCharP(typein)
    timlen = ctypes.c_int(timlen)
    ampmlen = ctypes.c_int(ampmlen)
    hr = ctypes.c_int()
    mn = ctypes.c_int()
    sc = ctypes.c_int()
    time = stypes.stringToCharP(timlen)
    ampm = stypes.stringToCharP(ampmlen)
    libspice.et2lst_c(et, body, lon, typein, timlen, ampmlen,
                      ctypes.byref(hr), ctypes.byref(mn), ctypes.byref(sc),
                      time, ampm)
    return hr.value, mn.value, sc.value, stypes.toPythonString(
            time), stypes.toPythonString(ampm)","Given an ephemeris epoch, compute the local solar time for
    an object on the surface of a body at a specified longitude.

    http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/et2lst_c.html

    :param et: Epoch in seconds past J2000 epoch.
    :type et: float
    :param body: ID-code of the body of interest.
    :type body: int
    :param lon: Longitude of surface point (RADIANS).
    :type lon: float
    :param typein: Type of longitude ""PLANETOCENTRIC"", etc.
    :type typein: str
    :param timlen: Available room in output time string.
    :type timlen: int
    :param ampmlen: Available room in output ampm string.
    :type ampmlen: int
    :return:
            Local hour on a ""24 hour"" clock,
            Minutes past the hour,
            Seconds past the minute,
            String giving local time on 24 hour clock,
            String giving time on A.M. / P.M. scale.
    :rtype: tuple",0,0,1,1
"def eul2m(angle3, angle2, angle1, axis3, axis2, axis1):
    
    angle3 = ctypes.c_double(angle3)
    angle2 = ctypes.c_double(angle2)
    angle1 = ctypes.c_double(angle1)
    axis3 = ctypes.c_int(axis3)
    axis2 = ctypes.c_int(axis2)
    axis1 = ctypes.c_int(axis1)
    r = stypes.emptyDoubleMatrix()
    libspice.eul2m_c(angle3, angle2, angle1, axis3, axis2, axis1, r)
    return stypes.cMatrixToNumpy(r)","Construct a rotation matrix from a set of Euler angles.

    http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/eul2m_c.html

    :param angle3: Rotation angle about third rotation axis (radians).
    :type angle3: float
    :param angle2: Rotation angle about second rotation axis (radians).
    :type angle2: float
    :param angle1: Rotation angle about first rotation axis (radians).
    :type angle1: float
    :param axis3: Axis number of third rotation axis.
    :type axis3: int
    :param axis2: Axis number of second rotation axis.
    :type axis2: int
    :param axis1: Axis number of first rotation axis.]
    :type axis1: int
    :return: Product of the 3 rotations.
    :rtype: 3x3-Element Array of floats",0,0,1,1
"def eval(self, code, *args):
        
        warnings.warn(""Database.eval() is deprecated"",
                      DeprecationWarning, stacklevel=2)

        if not isinstance(code, Code):
            code = Code(code)

        result = self.command(""$eval"", code, args=args)
        return result.get(""retval"", None)","**DEPRECATED**: Evaluate a JavaScript expression in MongoDB.

        :Parameters:
          - `code`: string representation of JavaScript code to be
            evaluated
          - `args` (optional): additional positional arguments are
            passed to the `code` being evaluated

        .. warning:: the eval command is deprecated in MongoDB 3.0 and
          will be removed in a future server version.",1,0,0,1
"def eval_rs(gains, losses):
        
        
        count = len(gains) + len(losses)

        avg_gains = stats.avg(gains, count=count) if gains else 1
        avg_losses = stats.avg(losses,count=count) if losses else 1
        if avg_losses == 0:
            return avg_gains
        else:
            return avg_gains / avg_losses","Evaluates the RS variable in RSI algorithm

        Args:
            gains: List of price gains.
            losses: List of prices losses.

        Returns:
            Float of average gains over average losses.",0,0,1,1
"def evaluate(self, dataset, metric='auto', missing_value_action='auto'):
        r

        _raise_error_evaluation_metric_is_valid(metric,
                                          ['auto', 'rmse', 'max_error'])
        return super(LinearRegression, self).evaluate(dataset, missing_value_action=missing_value_action,
                                                      metric=metric)","Evaluate the model by making target value predictions and comparing
        to actual values.

        Two metrics are used to evaluate linear regression models.  The first
        is root-mean-squared error (RMSE) while the second is the absolute
        value of the maximum error between the actual and predicted values.
        Let :math:`y` and :math:`\hat{y}` denote vectors of length :math:`N`
        (number of examples) with actual and predicted values. The RMSE is
        defined as:

        .. math::

            RMSE = \sqrt{\frac{1}{N} \sum_{i=1}^N (\widehat{y}_i - y_i)^2}

        while the max-error is defined as

        .. math::

            max-error = \max_{i=1}^N \|\widehat{y}_i - y_i\|

        Parameters
        ----------
        dataset : SFrame
            Dataset of new observations. Must include columns with the same
            names as the target and features used for model training. Additional
            columns are ignored.

        metric : str, optional
            Name of the evaluation metric.  Possible values are:
            - 'auto': Compute all metrics.
            - 'rmse': Rooted mean squared error.
            - 'max_error': Maximum error.

        missing_value_action : str, optional
            Action to perform when missing values are encountered. This can be
            one of:

            - 'auto': Default to 'impute'
            - 'impute': Proceed with evaluation by filling in the missing
              values with the mean of the training data. Missing
              values are also imputed if an entire column of data is
              missing during evaluation.
            - 'error': Do not proceed with evaluation and terminate with
              an error message.

        Returns
        -------
        out : dict
            Results from  model evaluation procedure.

        See Also
        ----------
        create, predict

        Examples
        ----------
        >>> data =  turicreate.SFrame('https://static.turi.com/datasets/regression/houses.csv')

        >>> model = turicreate.linear_regression.create(data,
                                             target='price',
                                             features=['bath', 'bedroom', 'size'])
        >>> results = model.evaluate(data)",0,0,3,3
"def evaluate_reader(reader, name, events, aux=0.):
    
    if not isinstance(reader, TMVA.Reader):
        raise TypeError(""reader must be a TMVA.Reader instance"")
    events = np.ascontiguousarray(events, dtype=np.float64)
    if events.ndim == 1:
        
        events = events[:, np.newaxis]
    elif events.ndim != 2:
        raise ValueError(
            ""events must be a two-dimensional array ""
            ""with one event per row"")
    return _libtmvanumpy.evaluate_reader(
        ROOT.AsCObject(reader), name, events, aux)","Evaluate a TMVA::Reader over a NumPy array.

    Parameters
    ----------
    reader : TMVA::Reader
        A TMVA::Factory instance with variables booked in exactly the same
        order as the columns in ``events``.
    name : string
        The method name.
    events : numpy array of shape [n_events, n_variables]
        A two-dimensional NumPy array containing the rows of events and columns
        of variables. The order of the columns must match the order in which
        you called ``AddVariable()`` for each variable.
    aux : float, optional (default=0.)
        Auxiliary value used by MethodCuts to set the desired signal
        efficiency.

    Returns
    -------
    output : numpy array of shape [n_events]
        The method output value for each event

    See Also
    --------
    evaluate_method",2,0,3,5
"def event_handler(self, event_type, src_path):
        
        filename = os.path.relpath(src_path, self.searchpath)
        if self.should_handle(event_type, src_path):
            print(""%s %s"" % (event_type, filename))
            if self.site.is_static(filename):
                files = self.site.get_dependencies(filename)
                self.site.copy_static(files)
            else:
                templates = self.site.get_dependencies(filename)
                self.site.render_templates(templates)","Re-render templates if they are modified.

        :param event_type: a string, representing the type of event

        :param src_path: the path to the file that triggered the event.",1,0,1,2
"def event_key_pressed(self, event):
        
        char = event.char
        if not char:
            return

        if char in string.ascii_letters:
            char = invert_shift(char)

        self.user_input_queue.put(char)

        
        
        return ""break""","So a ""invert shift"" for user inputs:
        Convert all lowercase letters to uppercase and vice versa.",0,0,1,1
"def eventgroup_delete(self, event_group_id=""0.0.0"", account=None, **kwargs):
        
        if not account:
            if ""default_account"" in config:
                account = config[""default_account""]
        if not account:
            raise ValueError(""You need to provide an Account"")
        account = Account(account)
        eventgroup = EventGroup(event_group_id)

        op = operations.Event_group_delete(
            **{
                ""fee"": {""amount"": 0, ""asset_id"": ""1.3.0""},
                ""event_group_id"": eventgroup[""id""],
                ""prefix"": self.prefix,
            }
        )
        return self.finalizeOp(op, account[""name""], ""active"", **kwargs)","Delete an eventgroup. This needs to be **propose**.

            :param str event_group_id: ID of the event group to be deleted

            :param str account: (optional) Account used to verify the operation",1,1,1,3
"def execute(self, program, start=None, stop=None, resolution=None,
                max_delay=None, persistent=False, immediate=False,
                disable_all_metric_publishes=None):
        
        params = self._get_params(start=start, stop=stop,
                                  resolution=resolution,
                                  maxDelay=max_delay,
                                  persistent=persistent,
                                  immediate=immediate,
                                  disableAllMetricPublishes=disable_all_metric_publishes)

        def exec_fn(since=None):
            if since:
                params['start'] = since
            return self._transport.execute(program, params)

        c = computation.Computation(exec_fn)
        self._computations.add(c)
        return c",Execute the given SignalFlow program and stream the output back.,0,1,1,2
"def execute_concurrent(session, statements_and_parameters, concurrency=100, raise_on_first_error=True, results_generator=False):
    
    if concurrency <= 0:
        raise ValueError(""concurrency must be greater than 0"")

    if not statements_and_parameters:
        return []

    executor = ConcurrentExecutorGenResults(session, statements_and_parameters) if results_generator else ConcurrentExecutorListResults(session, statements_and_parameters)
    return executor.execute(concurrency, raise_on_first_error)","Executes a sequence of (statement, parameters) tuples concurrently.  Each
    ``parameters`` item must be a sequence or :const:`None`.

    The `concurrency` parameter controls how many statements will be executed
    concurrently.  When :attr:`.Cluster.protocol_version` is set to 1 or 2,
    it is recommended that this be kept below 100 times the number of
    core connections per host times the number of connected hosts (see
    :meth:`.Cluster.set_core_connections_per_host`).  If that amount is exceeded,
    the event loop thread may attempt to block on new connection creation,
    substantially impacting throughput.  If :attr:`~.Cluster.protocol_version`
    is 3 or higher, you can safely experiment with higher levels of concurrency.

    If `raise_on_first_error` is left as :const:`True`, execution will stop
    after the first failed statement and the corresponding exception will be
    raised.

    `results_generator` controls how the results are returned.

    * If :const:`False`, the results are returned only after all requests have completed.
    * If :const:`True`, a generator expression is returned. Using a generator results in a constrained
      memory footprint when the results set will be large -- results are yielded
      as they return instead of materializing the entire list at once. The trade for lower memory
      footprint is marginal CPU overhead (more thread coordination and sorting out-of-order results
      on-the-fly).

    A sequence of ``ExecutionResult(success, result_or_exc)`` namedtuples is returned
    in the same order that the statements were passed in.  If ``success`` is :const:`False`,
    there was an error executing the statement, and ``result_or_exc`` will be
    an :class:`Exception`.  If ``success`` is :const:`True`, ``result_or_exc``
    will be the query result.

    Example usage::

        select_statement = session.prepare(""SELECT * FROM users WHERE id=?"")

        statements_and_params = []
        for user_id in user_ids:
            params = (user_id, )
            statements_and_params.append((select_statement, params))

        results = execute_concurrent(
            session, statements_and_params, raise_on_first_error=False)

        for (success, result) in results:
            if not success:
                handle_error(result)  # result will be an Exception
            else:
                process_user(result[0])  # result will be a list of rows

    Note: in the case that `generators` are used, it is important to ensure the consumers do not
    block or attempt further synchronous requests, because no further IO will be processed until
    the consumer returns. This may also produce a deadlock in the IO event thread.",1,1,1,3
"def execute_or_create_resource(self, device_id, _resource_path, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('asynchronous'):
            return self.execute_or_create_resource_with_http_info(device_id, _resource_path, **kwargs)  
        else:
            (data) = self.execute_or_create_resource_with_http_info(device_id, _resource_path, **kwargs)  
            return data","Execute a function on a Resource or create new Object instance  # noqa: E501

        With this API, you can [execute a function](/docs/current/connecting/handle-resource-webapp.html#the-execute-operation) on an existing resource and create new Object instance to the device. The resource-path does not have to exist - it can be created by the call. The maximum length of resource-path is 255 characters.  All resource APIs are asynchronous. These APIs respond only if the device is turned on and connected to Device Management Connect and there is an active notification channel.  Supported content types depend on the device and its resource. Device Management translates HTTP to equivalent CoAP content type.  **Example usage:**  This example resets the min and max values of the [temperature sensor](http://www.openmobilealliance.org/tech/profiles/lwm2m/3303.xml) instance 0 by executing the Resource 5605 'Reset Min and Max Measured Values'.      curl -X POST \\       https://api.us-east-1.mbedcloud.com/v2/endpoints/{device-id}/3303/0/5605 \\       -H 'authorization: Bearer {api-key}'   # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass asynchronous=True
        >>> thread = api.execute_or_create_resource(device_id, _resource_path, asynchronous=True)
        >>> result = thread.get()

        :param asynchronous bool
        :param str device_id: A unique Device Management device ID for the endpoint. Note that the ID must be an exact match. You cannot use wildcards here.  (required)
        :param str _resource_path: The URL of the resource. (required)
        :param str resource_function: This value is not needed. Most of the time resources do not accept a function but they have their own functions predefined. You can use this to trigger them.  If a function is included, the body of this request is passed as a char* to the function in Device Management Client. 
        :param bool no_resp: <br/><br/><b>Non-confirmable requests</b><br/>  All resource APIs have the parameter noResp. If you make a request with `noResp=true`, Device Management Connect makes a CoAP non-confirmable request to the device. Such requests are not guaranteed to arrive in the device, and you do not get back an async-response-id.  If calls with this parameter enabled succeed, they return with the status code `204 No Content`. If the underlying protocol does not support non-confirmable requests, or if the endpoint is registered in queue mode, the response is status code `409 Conflict`. 
        :return: AsyncID
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def execute_per_identity_records(
            self,
            identity: str,
            records: List[TimeAndRecord],
            old_state: Optional[Dict[Key, Any]] = None) -> Tuple[str, Tuple[Dict, List]]:
        
        schema_loader = SchemaLoader()
        if records:
            records.sort(key=lambda x: x[0])
        else:
            records = []

        block_data = self._execute_stream_bts(records, identity, schema_loader, old_state)
        window_data = self._execute_window_bts(identity, schema_loader)

        return identity, (block_data, window_data)","Executes the streaming and window BTS on the given records. An option old state can provided
        which initializes the state for execution. This is useful for batch execution where the
        previous state is written out to storage and can be loaded for the next batch run.

        :param identity: Identity of the records.
        :param records: List of TimeAndRecord to be processed.
        :param old_state: Streaming BTS state dictionary from a previous execution.
        :return: Tuple[Identity, Tuple[Identity, Tuple[Streaming BTS state dictionary,
            List of window BTS output]].",0,0,2,2
"def execute_sql(self, statement):
        
        path = '/archive/{}/sql'.format(self._instance)
        req = archive_pb2.ExecuteSqlRequest()
        req.statement = statement

        response = self._client.post_proto(path=path,
                                           data=req.SerializeToString())
        message = archive_pb2.ExecuteSqlResponse()
        message.ParseFromString(response.content)
        if message.HasField('result'):
            return message.result
        return None","Executes a single SQL statement.

        :param statement: SQL string
        :return: String response
        :rtype: str",1,0,0,1
"def exist(self, table: str, libref: str ="""") -> bool:
      
      code  = ""data _null_; e = exist('""
      if len(libref):
         code += libref+"".""
      code += table+""');\n""
      code += ""v = exist('""
      if len(libref):
         code += libref+"".""
      code += table+""', 'VIEW');\n if e or v then e = 1;\n""
      code += ""te='TABLE_EXISTS='; put te e;run;""

      ll = self.submit(code, ""text"")

      l2 = ll['LOG'].rpartition(""TABLE_EXISTS= "")
      l2 = l2[2].partition(""\n"")
      exists = int(l2[0])

      return bool(exists)","table  - the name of the SAS Data Set
      libref - the libref for the Data Set, defaults to WORK, or USER if assigned

      Returns True if the Data Set exists and False if it does not",1,0,1,2
"def exists(instance_id=None, name=None, tags=None, region=None, key=None,
           keyid=None, profile=None, in_states=None, filters=None):
    
    instances = find_instances(instance_id=instance_id, name=name, tags=tags,
                               region=region, key=key, keyid=keyid,
                               profile=profile, in_states=in_states, filters=filters)
    if instances:
        log.info('Instance exists.')
        return True
    else:
        log.warning('Instance does not exist.')
        return False","Given an instance id, check to see if the given instance id exists.

    Returns True if the given instance with the given id, name, or tags
    exists; otherwise, False is returned.

    CLI Example:

    .. code-block:: bash

        salt myminion boto_ec2.exists myinstance",1,2,0,3
"def exists(self):
        
        path = self.path
        if '*' in path or '?' in path or '[' in path or '{' in path:
            logger.warning(""Using wildcards in path %s might lead to processing of an incomplete dataset; ""
                           ""override exists() to suppress the warning."", path)
        return self.fs.exists(path)","Returns ``True`` if the path for this FileSystemTarget exists; ``False`` otherwise.

        This method is implemented by using :py:attr:`fs`.",0,1,2,3
"def exists(self, client=None):
        
        client = self._require_client(client)

        try:
            client.sinks_api.sink_get(self.project, self.name)
        except NotFound:
            return False
        else:
            return True","API call:  test for the existence of the sink via a GET request

        See
        https://cloud.google.com/logging/docs/reference/v2/rest/v2/projects.sinks/get

        :type client: :class:`~google.cloud.logging.client.Client` or
                      ``NoneType``
        :param client: the client to use.  If not passed, falls back to the
                       ``client`` stored on the current sink.

        :rtype: bool
        :returns: Boolean indicating existence of the sink.",0,1,0,1
"def expectation_step(t_table, stanzas, schemes, rprobs):
    
    probs = numpy.zeros((len(stanzas), schemes.num_schemes))
    for i, stanza in enumerate(stanzas):
        scheme_indices = schemes.get_schemes_for_len(len(stanza))
        for scheme_index in scheme_indices:
            scheme = schemes.scheme_list[scheme_index]
            probs[i, scheme_index] = post_prob_scheme(t_table, stanza, scheme)
    probs = numpy.dot(probs, numpy.diag(rprobs))

    
    scheme_sums = numpy.sum(probs, axis=1)
    for i, scheme_sum in enumerate(scheme_sums.tolist()):
        if scheme_sum > 0:
            probs[i, :] /= scheme_sum
    return probs",Compute posterior probability of schemes for each stanza,0,0,1,1
"def export(self, image_path, tmptar=None):
    
    from spython.utils import check_install
    check_install()

    if tmptar is None:
        tmptar = ""/%s/tmptar.tar"" %(tempfile.mkdtemp())
    cmd = ['singularity', 'image.export', '-f', tmptar, image_path]

    output = self.run_command(cmd, sudo=False)
    return tmptar","export will export an image, sudo must be used.

       Parameters
       ==========
   
       image_path: full path to image
       tmptar: if defined, use custom temporary path for tar export",0,1,0,1
"def extend_embedder_vocab(self, embedding_sources_mapping: Dict[str, str] = None) -> None:
        
        
        
        embedding_sources_mapping = embedding_sources_mapping or {}
        for model_path, module in self.named_modules():
            if hasattr(module, 'extend_vocab'):
                pretrained_file = embedding_sources_mapping.get(model_path, None)
                module.extend_vocab(self.vocab,
                                    extension_pretrained_file=pretrained_file,
                                    model_path=model_path)","Iterates through all embedding modules in the model and assures it can embed
        with the extended vocab. This is required in fine-tuning or transfer learning
        scenarios where model was trained with original vocabulary but during
        fine-tuning/tranfer-learning, it will have it work with extended vocabulary
        (original + new-data vocabulary).

        Parameters
        ----------
        embedding_sources_mapping : Dict[str, str], (optional, default=None)
            Mapping from model_path to pretrained-file path of the embedding
            modules. If pretrained-file used at time of embedding initialization
            isn't available now, user should pass this mapping. Model path is
            path traversing the model attributes upto this embedding module.
            Eg. ""_text_field_embedder.token_embedder_tokens"".",0,0,2,2
"def extract(query_dict, prefix=""""):
    

    strs = ['order_by']
    ints = ['per_page', 'page']

    extracted = { }

    for key in (strs + ints):
        if (prefix + key) in query_dict:
            val = query_dict.get(prefix + key)

            extracted[key] = (val
                if not key in ints
                else int(val))

    return extracted","Extract the *order_by*, *per_page*, and *page* parameters from
    `query_dict` (a Django QueryDict), and return a dict suitable for
    instantiating a preconfigured Table object.",0,0,2,2
"def extract(self, other):
        

        
        if type(other) is float or \
           type(other) is numpy.float64 or \
           type(other) is numpy.float32:
            return self._extract_mfr(other)

        
        elif self._is_compound_mfr_tuple(other):
            return self._extract_compound_mfr(other[0], other[1])

        
        elif type(other) is str:
            return self._extract_compound(other)

        
        
        elif type(other) is Material:
            return self._extract_material(other)

        
        else:
            raise TypeError(""Invalid extraction argument."")","Extract 'other' from this stream, modifying this stream and returning
        the extracted material as a new stream.

        :param other: Can be one of the following:

          * float: A mass flow rate equal to other is extracted from self. Self
            is reduced by other and the extracted stream is returned as
            a new stream.
          * tuple (compound, mass): The other tuple specifies the mass flow
            rate of a compound to be extracted. It is extracted from self and
            the extracted mass flow rate is returned as a new stream.
          * string: The 'other' string specifies the compound to be
            extracted. All of the mass flow rate of that compound will be
            removed from self and a new stream created with it.
          * Material: The 'other' material specifies the list of
            compounds to extract.


        :returns: New MaterialStream object.",1,0,4,5
"def extract(self, table: dict) -> List[Extraction]:
        
        if table['features']['max_cols_in_a_row'] != 2 and table['features']['no_of_rows'] < 2:
            return []
        results = list()
        for row in table['rows']:
            if len(row['cells']) != 2:
                continue
            text = [row['cells'][0]['text'], row['cells'][1]['text']]
            for field_name in self.glossaries.keys():
                if self.cell_matches_dict(text[0], self.glossaries[field_name]):
                    results.append(self.wrap_value_with_context(text[1], field_name))
                if self.cell_matches_dict(text[1], self.glossaries[field_name]):
                    results.append(self.wrap_value_with_context(text[0], field_name))
        return results",":param table: a table extracted by table extractor, as a json object
        :return: list of all extractions from the input table",0,0,1,1
"def extract_angular(fileobj, keywords, comment_tags, options):
    

    if keywords:
        logging.debug('Parameter keywords ignored.')

    if comment_tags:
        logging.debug('Parameter comment_tags ignored.')

    if options:
        logging.debug('Parameter options ignored.')

    parser = AngularGettextHTMLParser()

    for line in fileobj:
        if not isinstance(line, str):
            line = line.decode(locale.getpreferredencoding())
        parser.feed(line)

    for string in parser.strings:
        yield(string)","Extract messages from angular template (HTML) files that use the
    angular-gettext translate directive as per
    https://angular-gettext.rocketeer.be/ .

    :param fileobj: the file-like object the messages should be extracted
                    from
    :param keywords: This is a standard parameter so it is accepted but ignored.

    :param comment_tags: This is a standard parameter so it is accepted but
                        ignored.
    :param options: Another standard parameter that is accepted but ignored.
    :return: an iterator over ``(lineno, funcname, message, comments)``
             tuples
    :rtype: ``iterator``

    This particular extractor is quite simple because it is intended to only
    deal with angular templates which do not need comments, or the more
    complicated forms of translations.

    A later version will address pluralization.",0,3,1,4
"def extract_aws_metadata(wrapped, instance, args, kwargs, return_value):
    
    response = return_value
    LOGGER.debug(
        'Extracting AWS metadata',
        args=args,
        kwargs=kwargs,
    )
    if 'operation_name' in kwargs:
        operation_name = kwargs['operation_name']
    else:
        operation_name = args[0]

    
    
    if len(kwargs) == 0 and len(args) == 2:
        kwargs = args[1]

    region_name = instance._client_config.region_name

    response_metadata = response.get('ResponseMetadata')

    metadata = {
        'aws': {
            'operation': operation_name,
            'region': region_name,
        }
    }

    if 'TableName' in kwargs:
        metadata['aws']['table_name'] = kwargs['TableName']
    if 'QueueUrl' in kwargs:
        metadata['aws']['queue_url'] = kwargs['QueueUrl']

    if response_metadata is not None:
        metadata['http'] = {
            'response': {
                'status': response_metadata['HTTPStatusCode'],
            },
        }
        metadata['aws']['request_id'] = response_metadata['RequestId']

    return metadata","Provide AWS metadata for improved visualization.

    See documentation for this data structure:
    http://docs.aws.amazon.com/xray/latest/devguide/xray-api-segmentdocuments.html#api-segmentdocuments-aws",0,1,2,3
"def extract_chain(server_handshake_bytes):
    

    output = []

    chain_bytes = None

    for record_type, _, record_data in parse_tls_records(server_handshake_bytes):
        if record_type != b'\x16':
            continue
        for message_type, message_data in parse_handshake_messages(record_data):
            if message_type == b'\x0b':
                chain_bytes = message_data
                break
        if chain_bytes:
            break

    if chain_bytes:
        
        pointer = 3
        while pointer < len(chain_bytes):
            cert_length = int_from_bytes(chain_bytes[pointer:pointer + 3])
            cert_start = pointer + 3
            cert_end = cert_start + cert_length
            pointer = cert_end
            cert_bytes = chain_bytes[cert_start:cert_end]
            output.append(Certificate.load(cert_bytes))

    return output","Extracts the X.509 certificates from the server handshake bytes for use
    when debugging

    :param server_handshake_bytes:
        A byte string of the handshake data received from the server

    :return:
        A list of asn1crypto.x509.Certificate objects",0,0,1,1
"def extract_keywords_from_text(index_page, no_items=5):
    
    index_page = MLStripper.strip_tags(index_page)
    tokenized_index = TextBlob(index_page).lower()

    def to_str(key):
        if isinstance(key, unicode):
            return key.encode(""utf-8"")

        return key

    present_keywords = [
        KEYWORDS_LOWER[key]
        for key in KEYWORDS_LOWER.keys()
        if len(key) > 3 and key in tokenized_index
    ]

    def to_source_string(key):
        source = ""Keyword analysis""
        try:
            return SourceString(key, source)
        except UnicodeEncodeError:
            return SourceString(key.encode(""utf-8""), source)

    multi_keywords = [
        to_source_string(key)
        for key in present_keywords
        if tokenized_index.words.count(key) >= 1
    ]

    multi_keywords = sorted(multi_keywords, key=lambda x: len(x), reverse=True)

    if len(multi_keywords) > no_items:
        return multi_keywords[:no_items]

    return multi_keywords","Try to process text on the `index_page` deduce the keywords and then try
    to match them on the Aleph's dataset.

    Function returns maximally `no_items` items, to prevent spamming the user.

    Args:
        index_page (str): Content of the page as UTF-8 string
        no_items (int, default 5): Number of items to return.

    Returns:
        list: List of :class:`.SourceString` objects.",0,0,3,3
"def extract_kwargs(docstring):
    
    lines = inspect.cleandoc(docstring).split('\n')
    retval = []

    
    
    
    
    while lines[0] != 'Parameters':
        lines.pop(0)
    lines.pop(0)
    lines.pop(0)

    while lines and lines[0]:
        name, type_ = lines.pop(0).split(':', 1)
        description = []
        while lines and lines[0].startswith('    '):
            description.append(lines.pop(0).strip())
        if 'optional' in type_:
            retval.append((name.strip(), type_.strip(), description))

    return retval","Extract keyword argument documentation from a function's docstring.

    Parameters
    ----------
    docstring: str
        The docstring to extract keyword arguments from.

    Returns
    -------
    list of (str, str, list str)

    str
        The name of the keyword argument.
    str
        Its type.
    str
        Its documentation as a list of lines.

    Notes
    -----
    The implementation is rather fragile.  It expects the following:

    1. The parameters are under an underlined Parameters section
    2. Keyword parameters have the literal "", optional"" after the type
    3. Names and types are not indented
    4. Descriptions are indented with 4 spaces
    5. The Parameters section ends with an empty line.

    Examples
    --------

    >>> docstring = '''The foo function.
    ... Parameters
    ... ----------
    ... bar: str, optional
    ...     This parameter is the bar.
    ... baz: int, optional
    ...     This parameter is the baz.
    ...
    ... '''
    >>> kwargs = extract_kwargs(docstring)
    >>> kwargs[0]
    ('bar', 'str, optional', ['This parameter is the bar.'])",0,0,1,1
"def extract_plain_text(self, reduce: bool = False) -> str:
        
        if reduce:
            self.reduce()

        result = ''
        for seg in self:
            if seg.type == 'text':
                result += ' ' + seg.data['text']
        if result:
            result = result[1:]
        return result","Extract text segments from the message, joined by single space.

        :param reduce: reduce the message before extracting
        :return: the joined string",0,0,2,2
"def extract_random_video_patch(videos, num_frames=-1):
  
  if num_frames == -1:
    return videos
  batch_size, num_total_frames, h, w, c = common_layers.shape_list(videos)
  if num_total_frames < num_frames:
    raise ValueError(""Expected num_frames <= %d, got %d"" %
                     (num_total_frames, num_frames))

  
  frame_start = tf.random_uniform(
      shape=(batch_size,), minval=0, maxval=num_total_frames - num_frames + 1,
      dtype=tf.int32)

  
  
  range_inds = tf.expand_dims(tf.range(num_frames), axis=0)
  frame_inds = range_inds + tf.expand_dims(frame_start, axis=1)
  frame_inds = tf.reshape(frame_inds, [-1])

  
  batch_inds = tf.expand_dims(tf.range(batch_size), axis=1)
  batch_inds = tf.tile(batch_inds, [1, num_frames])
  batch_inds = tf.reshape(batch_inds, [-1])

  gather_inds = tf.stack((batch_inds, frame_inds), axis=1)
  video_patches = tf.gather_nd(videos, gather_inds)
  return tf.reshape(video_patches, (batch_size, num_frames, h, w, c))","For every video, extract a random consecutive patch of num_frames.

  Args:
    videos: 5-D Tensor, (NTHWC)
    num_frames: Integer, if -1 then the entire video is returned.
  Returns:
    video_patch: 5-D Tensor, (NTHWC) with T = num_frames.
  Raises:
    ValueError: If num_frames is greater than the number of total frames in
                the video.",1,0,2,3
"def extract_session_details(request_headers, session_header, secret_key):

    

    session_details = {
        'error': '',
        'code': 200,
        'session': {}
    }

    if not session_header in request_headers.keys():
        session_details['error'] = '%s is missing.' % session_header
        session_details['code'] = 400
    else:
        import jwt
        session_token = request_headers[session_header]
        try:
            session_details['session'] = jwt.decode(session_token, secret_key)
        except jwt.DecodeError as err:
            session_details['error'] = 'Session token decoding error.'
            session_details['code'] = 400
        except jwt.ExpiredSignatureError as err:
            session_details['error'] = 'Session token has expired.'
            session_details['code'] = 400
        except Exception:
            session_details['error'] = 'Session token is invalid.'
            session_details['code'] = 400

    return session_details","a method to extract and validate jwt session token from request headers

    :param request_headers: dictionary with header fields from request
    :param session_header: string with name of session token header key
    :param secret_key: string with secret key to json web token encryption
    :return: dictionary with request details with session details or error coding",0,0,2,2
"def f_val_to_str(self):
        

        resstrlist = []
        strlen = 0

        for key in self._data:
            val = self._data[key]
            resstr = '%s=%s, ' % (key, repr(val))
            resstrlist.append(resstr)

            strlen += len(resstr)
            if strlen > pypetconstants.HDF5_STRCOL_MAX_VALUE_LENGTH:
                break

        return_string = """".join(resstrlist)
        if len(return_string) > pypetconstants.HDF5_STRCOL_MAX_VALUE_LENGTH:
            return_string =\
                return_string[0:pypetconstants.HDF5_STRCOL_MAX_VALUE_LENGTH - 3] + '...'
        else:
            return_string = return_string[0:-2] 

        return return_string","Summarizes data handled by the result as a string.

        Calls `__repr__` on all handled data. Data is NOT ordered.

        Truncates the string if it is longer than
        :const:`pypetconstants.HDF5_STRCOL_MAX_VALUE_LENGTH`

        :return: string",0,0,3,3
"def factory(
        name,
        desc,
        type,
        subtypes=None,
        required=True,
        default=None,
        ctor=None,
        hide=False,
    ):
        
        if ctor:
            type_assert(ctor, str)
            module_name, cls_name, method_name = ctor.rsplit('.', 2)
            module = importlib.import_module(module_name)
            cls = getattr(module, cls_name)
            method = getattr(cls, method_name)
            docstring = DocString.from_ctor(method)
        else:
            docstring = None

        return DocStringArg(
            name,
            desc,
            type,
            subtypes,
            required,
            default,
            docstring=docstring,
            hide=hide,
        )","desc: >
            Creates a DocStringArg and recursively includes child
            docstrings if they are not JSON types.
        args:
            -   name: name
                desc: The name of the argument
                type: str
            -   name: desc
                desc: A description of the argument
                type: str
            -   name: type
                desc: The type of the argument
                type: str
            -   name: subtypes
                desc: >
                    If @type is a list, dict or other data structure,
                    the contained objects should be of this type.
                    This should be a list of one item for lists, or a list
                    of 2 items for a dict.  More than 2 items is not
                    supported.
                type: list
                required: false
                default: None
            -   name: required
                desc: >
                    True if this is a mandatory argument, False if it
                    has a default value.  If False, @default should be
                    set appropriately.
                type: bool
                required: false
                default: true
            -   name: default
                desc: >
                    The default value for this argument.  This is ignored
                    if @required == True
                type: any
            -   name: ctor
                desc: >
                    Only use if @type is a class instance and not a JSON type
                    The constructor that the JSON object will be
                    unmarshalled to.  Either the Class.__init__, or a
                    factory function or factory static method.
                    Use the full path: module.submodule.Class.__init__
                type: str
                required: false
                default: None
            -   name: hide
                desc: >
                    Don't display this argument to the user.  Useful for
                    not showing arguments that are excluded from marshalling.
                    Note that this is only a hint to the client that will
                    render the JSON, the argument will still be sent
                required: false
                type: bool
                default: false
        returns:
            desc: A DocStringArg instance with recursively populated
                  @docstring attributes for child arguments
            type: pymarshal.api_docs.docstring.DocStringArg",0,0,3,3
"def fallback_to_default_project_id(func):
        
        @functools.wraps(func)
        def inner_wrapper(self, *args, **kwargs):
            if len(args) > 0:
                raise AirflowException(
                    ""You must use keyword arguments in this methods rather than""
                    "" positional"")
            if 'project_id' in kwargs:
                kwargs['project_id'] = self._get_project_id(kwargs['project_id'])
            else:
                kwargs['project_id'] = self._get_project_id(None)
            if not kwargs['project_id']:
                raise AirflowException(""The project id must be passed either as ""
                                       ""keyword project_id parameter or as project_id extra ""
                                       ""in GCP connection definition. Both are not set!"")
            return func(self, *args, **kwargs)
        return inner_wrapper","Decorator that provides fallback for Google Cloud Platform project id. If
        the project is None it will be replaced with the project_id from the
        service account the Hook is authenticated with. Project id can be specified
        either via project_id kwarg or via first parameter in positional args.

        :param func: function to wrap
        :return: result of the function call",2,0,5,7
"def fast_gradient_method(model_fn, x, eps, ord, clip_min=None, clip_max=None, y=None,
                         targeted=False, sanity_checks=False):
  
  if ord not in [np.inf, 1, 2]:
    raise ValueError(""Norm order must be either np.inf, 1, or 2."")

  asserts = []

  
  if clip_min is not None:
    asserts.append(tf.math.greater_equal(x, clip_min))

  if clip_max is not None:
    asserts.append(tf.math.less_equal(x, clip_max))

  if y is None:
    
    y = tf.argmax(model_fn(x), 1)

  grad = compute_gradient(model_fn, x, y, targeted)

  optimal_perturbation = optimize_linear(grad, eps, ord)
  
  adv_x = x + optimal_perturbation

  
  if (clip_min is not None) or (clip_max is not None):
    
    assert clip_min is not None and clip_max is not None
    adv_x = tf.clip_by_value(adv_x, clip_min, clip_max)

  if sanity_checks:
    assert np.all(asserts)
  return adv_x","Tensorflow 2.0 implementation of the Fast Gradient Method.
  :param model_fn: a callable that takes an input tensor and returns the model logits.
  :param x: input tensor.
  :param eps: epsilon (input variation parameter); see https://arxiv.org/abs/1412.6572.
  :param ord: Order of the norm (mimics NumPy). Possible values: np.inf, 1 or 2.
  :param clip_min: (optional) float. Minimum float value for adversarial example components.
  :param clip_max: (optional) float. Maximum float value for adversarial example components.
  :param y: (optional) Tensor with true labels. If targeted is true, then provide the
            target label. Otherwise, only provide this parameter if you'd like to use true
            labels when crafting adversarial samples. Otherwise, model predictions are used
            as labels to avoid the ""label leaking"" effect (explained in this paper:
            https://arxiv.org/abs/1611.01236). Default is None.
  :param targeted: (optional) bool. Is the attack targeted or untargeted?
            Untargeted, the default, will try to make the label incorrect.
            Targeted will instead try to move in the direction of being more like y.
  :param sanity_checks: bool, if True, include asserts (Turn them off to use less runtime /
            memory or for unit tests that intentionally pass strange input)
  :return: a tensor for the adversarial example",5,0,6,11
"def fbeta(log_preds, targs, beta, thresh=0.5, epsilon=1e-8):
    
    assert beta > 0, 'beta needs to be greater than 0'
    beta2 = beta ** 2
    rec = recall(log_preds, targs, thresh)
    prec = precision(log_preds, targs, thresh)
    return (1 + beta2) * prec * rec / (beta2 * prec + rec + epsilon)","Calculates the F-beta score (the weighted harmonic mean of precision and recall).
    This is the micro averaged version where the true positives, false negatives and
    false positives are calculated globally (as opposed to on a per label basis).

    beta == 1 places equal weight on precision and recall, b < 1 emphasizes precision and
    beta > 1 favors recall.",0,0,1,1
"def fcm_send_single_device_data_message(
        registration_id,
        condition=None,
        collapse_key=None,
        delay_while_idle=False,
        time_to_live=None,
        restricted_package_name=None,
        low_priority=False,
        dry_run=False,
        data_message=None,
        content_available=None,
        api_key=None,
        timeout=5,
        json_encoder=None):
    
    push_service = FCMNotification(
        api_key=SETTINGS.get(""FCM_SERVER_KEY"") if api_key is None else api_key,
        json_encoder=json_encoder,
    )
    return push_service.single_device_data_message(
        registration_id=registration_id,
        condition=condition,
        collapse_key=collapse_key,
        delay_while_idle=delay_while_idle,
        time_to_live=time_to_live,
        restricted_package_name=restricted_package_name,
        low_priority=low_priority,
        dry_run=dry_run,
        data_message=data_message,
        content_available=content_available,
        timeout=timeout
    )","Send push message to a single device
    All arguments correspond to that defined in pyfcm/fcm.py.

    Args:
        registration_id (str): FCM device registration IDs.
        data_message (dict): Data message payload to send alone or with the
            notification message

    Keyword Args:
        collapse_key (str, optional): Identifier for a group of messages
            that can be collapsed so that only the last message gets sent
            when delivery can be resumed. Defaults to ``None``.
        delay_while_idle (bool, optional): If ``True`` indicates that the
            message should not be sent until the device becomes active.
        time_to_live (int, optional): How long (in seconds) the message
            should be kept in FCM storage if the device is offline. The
            maximum time to live supported is 4 weeks. Defaults to ``None``
            which uses the FCM default of 4 weeks.
        low_priority (boolean, optional): Whether to send notification with
            the low priority flag. Defaults to ``False``.
        restricted_package_name (str, optional): Package name of the
            application where the registration IDs must match in order to
            receive the message. Defaults to ``None``.
        dry_run (bool, optional): If ``True`` no message will be sent but
            request will be tested.
        timeout (int, optional): set time limit for the request
    Returns:
        :dict:`multicast_id(long), success(int), failure(int),
            canonical_ids(int), results(list)`:
        Response from FCM server.

    Raises:
        AuthenticationError: If :attr:`api_key` is not set or provided or there
            is an error authenticating the sender.
        FCMServerError: Internal server error or timeout error on Firebase cloud
            messaging server
        InvalidDataError: Invalid data provided
        InternalPackageError: Mostly from changes in the response of FCM,
            contact the project owner to resolve the issue",0,1,0,1
"def feat(self,k,v):
                


                if (not hasattr(self,'feats')):
                        self.feats = {}
                        if (not hasattr(self,'featpaths')):
                                self.featpaths={}
                if (not k in self.feats):
                        self.feats[k] = v
                else:
                        if type(self.feats[k])==type([]):
                                self.feats[k].append(v)
                        else:
                                obj=self.feats[k]
                                self.feats[k]=[obj,v]","Store value 'v' as a feature name 'k' for this object.
                Features are stored in the dictionary self.feats.
                [IMPORTANT NOTE:]
                If the feature 'k' is not yet defined, then:
                        self.feats[k]=v
                OTHERWISE:
                        self.feats[k] becomes a list (if not already)
                        'v' is added to this list",0,0,3,3
"def feature_selection(df, labels, n_features, method='chi2'):
        
        fs_obj = None
        if method == 'chi2':
            fs_obj = chi2
        elif method == 'ANOVA':
            fs_obj = f_classif
        elif method == 'mi':
            fs_obj = mutual_info_classif
        else:
            raise ValueError('The method is not recognized')
    
        fs = SelectKBest(fs_obj, k=n_features)
        fs.fit_transform(df, labels)
        df_reduced = df.loc[:, fs.get_support()]
        return df_reduced","Reduces the number of features in the input dataframe.
        Ex: labels = gs.meta['biospecimen_sample__sample_type_id'].apply(int).apply(lambda x: 0 if x < 10 else 1)
            chi2_fs(gs.data, labels, 50)

        :param df: The input dataframe
        :param labels: Labels for each row in the df. Type: Pandas.Series
        :param no_features: The desired number of features
        :param method: The feature selection method to be employed. It is set to 'chi2' by default
        To select the features using mutual information, the method value should be set to 'mi'
        To select the features using ANOVA, the method value should be set to 'ANOVA'
        :return: Returns the dataframe with the selected features",0,0,1,1
"def fetch(self, category=CATEGORY_ISSUE, from_date=DEFAULT_DATETIME, to_date=DEFAULT_LAST_DATETIME):
        
        if not from_date:
            from_date = DEFAULT_DATETIME
        if not to_date:
            to_date = DEFAULT_LAST_DATETIME

        from_date = datetime_to_utc(from_date)
        to_date = datetime_to_utc(to_date)

        kwargs = {
            'from_date': from_date,
            'to_date': to_date
        }
        items = super().fetch(category, **kwargs)

        return items","Fetch the issues/pull requests from the repository.

        The method retrieves, from a GitHub repository, the issues/pull requests
        updated since the given date.

        :param category: the category of items to fetch
        :param from_date: obtain issues/pull requests updated since this date
        :param to_date: obtain issues/pull requests until a specific date (included)

        :returns: a generator of issues",0,1,1,2
"def fetch(self, url, path, filename):
        
        logger.debug('initializing download in ', url)
        remote_file_size = self.get_remote_file_size(url)

        if exists(join(path, filename)):
            size = getsize(join(path, filename))
            if size == remote_file_size:
                logger.error('%s already exists on your system' % filename)
                print('%s already exists on your system' % filename)
                return [join(path, filename), size]

        logger.debug('Downloading: %s' % filename)
        print('Downloading: %s' % filename)
        fetch(url, path)
        print('stored at %s' % path)
        logger.debug('stored at %s' % path)
        return [join(path, filename), remote_file_size]","Verify if the file is already downloaded and complete. If they don't
        exists or if are not complete, use homura download function to fetch
        files. Return a list with the path of the downloaded file and the size
        of the remote file.",0,4,1,5
"def fetch(self, x, y, w, h):
        

        if not at_least_libvips(8, 8):
            raise Error('libvips too old')

        psize = ffi.new('size_t *')
        pointer = vips_lib.vips_region_fetch(self.pointer, x, y, w, h, psize)
        if pointer == ffi.NULL:
            raise Error('unable to fetch from region')

        pointer = ffi.gc(pointer, glib_lib.g_free)
        return ffi.buffer(pointer, psize[0])","Fill a region with pixel data.

        Pixels are filled with data!

        Returns:
            Pixel data.

        Raises:
            :class:`.Error`",1,0,2,3
"def fetch_entries(self):
        
        data = []
        for row in self.get_rows():
            
            if exceeded_limit(self.limit, len(data)):
                break

            entry = row.find_all('td')
            entry_dict = {}

            show = entry[0].string
            net = entry[1].string
            if not self._match_query(show, net):
                continue

            entry_dict['show'] = show
            entry_dict['net'] = net
            entry_dict['time'] = entry[2].string

            if ',' in entry[3].string:
                entry_dict['viewers'] = entry[3].string.replace(',', '.')
            else:
                entry_dict['viewers'] = '0.' + entry[3].string
            entry_dict['rating'] = entry[4].string

            
            data.append(Entry(**entry_dict))

        return data",Fetch data and parse it to build a list of cable entries.,0,1,1,2
"def fetch_next_block(self):
        
        if not self._has_more_pages():
            return []
        
        if len(self._buffer):
            
            res = list(self._buffer)
            self._buffer.clear()
            return res
        else:
            
            return self._fetch_next_block()","Returns a block of results with respecting retry policy.
        
        This method only exists for backward compatibility reasons. (Because QueryIterable
        has exposed fetch_next_block api).
        
        :return:
            List of results.
        :rtype: list",0,1,0,1
"def fetch_sampling_target(self, rules):
        
        now = int(time.time())
        report_docs = self._generate_reporting_docs(rules, now)
        resp = self._xray_client.get_sampling_targets(
            SamplingStatisticsDocuments=report_docs
        )
        new_docs = resp['SamplingTargetDocuments']

        targets_mapping = {}
        for doc in new_docs:
            TTL = self._dt_to_epoch(doc['ReservoirQuotaTTL']) if doc.get('ReservoirQuotaTTL', None) else None
            target = {
                'rate': doc['FixedRate'],
                'quota': doc.get('ReservoirQuota', None),
                'TTL': TTL,
                'interval': doc.get('Interval', None),
            }
            targets_mapping[doc['RuleName']] = target

        return targets_mapping, self._dt_to_epoch(resp['LastRuleModification'])","Report the current statistics of sampling rules and
        get back the new assgiend quota/TTL froom the X-Ray service.
        The call is proxied and signed via X-Ray Daemon.",1,1,0,2
"def fetch_table_names(self, include_system_table=False):
        

        result = self.__cur.execute(""SELECT name FROM sqlite_master WHERE TYPE='table'"")
        if result is None:
            return []

        table_names = [record[0] for record in result.fetchall()]

        if include_system_table:
            return table_names

        return [table for table in table_names if table not in SQLITE_SYSTEM_TABLES]",":return: List of table names in the database.
        :rtype: list",1,0,0,1
"def fields(self):
        
        process_markdown = (""fields"" in self._raw.get(""mrkdwn_in"", []))
        fields = self._raw.get(""fields"", [])
        if fields:
            logging.debug(""Rendering with markdown markdown %s for %s"", process_markdown, fields)
        return [
            {""title"": e[""title""], ""short"": e[""short""], ""value"": self._formatter.render_text(e[""value""], process_markdown)}
            for e in fields
        ]","Fetch the ""fields"" list, and process the text within each field, including markdown
        processing if the message indicates that the fields contain markdown.

        Only present on attachments, not files--this abstraction isn't 100% awesome.'",0,2,1,3
"def file_exists(path, saltenv=None):
    
    pillar_roots = __opts__.get('pillar_roots')
    if not pillar_roots:
        raise CommandExecutionError('No pillar_roots found. Are you running '
                                    'this on the master?')

    if saltenv:
        if saltenv in pillar_roots:
            pillar_roots = {saltenv: pillar_roots[saltenv]}
        else:
            return False

    for env in pillar_roots:
        for pillar_dir in pillar_roots[env]:
            full_path = os.path.join(pillar_dir, path)
            if __salt__['file.file_exists'](full_path):
                return True

    return False",".. versionadded:: 2016.3.0

    This is a master-only function. Calling from the minion is not supported.

    Use the given path and search relative to the pillar environments to see if
    a file exists at that path.

    If the ``saltenv`` argument is given, restrict search to that environment
    only.

    Will only work with ``pillar_roots``, not external pillars.

    Returns True if the file is found, and False otherwise.

    path
        The path to the file in question. Will be treated as a relative path

    saltenv
        Optional argument to restrict the search to a specific saltenv

    CLI Example:

    .. code-block:: bash

        salt '*' pillar.file_exists foo/bar.sls",1,0,2,3
"def file_path(fobj):
    
    if isinstance(fobj, string_types) and fobj.startswith(""file:""):
        return urlparse(fobj).path
    if isinstance(fobj, string_types):
        return fobj
    if (isinstance(fobj, FILE_LIKE) and hasattr(fobj, ""name"")):
        return fobj.name
    try:
        return fobj.path
    except AttributeError:
        raise ValueError(""Cannot parse file name for {!r}"".format(fobj))","Determine the path of a file.

    This doesn't do any sanity checking to check that the file
    actually exists, or is readable.

    Parameters
    ----------
    fobj : `file`, `str`, `CacheEntry`, ...
        the file object or path to parse

    Returns
    -------
    path : `str`
        the path of the underlying file

    Raises
    ------
    ValueError
        if a file path cannnot be determined

    Examples
    --------
    >>> from gwpy.io.utils import file_path
    >>> file_path(""test.txt"")
    'test.txt'
    >>> file_path(open(""test.txt"", ""r""))
    'test.txt'
    >>> file_path(""file:///home/user/test.txt"")
    '/home/user/test.txt'",1,0,2,3
"def filter(self, **kwargs):
    
    
    f_field = kwargs.keys()[0]
    f_value = kwargs[f_field]
    _newset = []
    
    for m in self._dataset:
      if hasattr(m, f_field):
        if getattr(m, f_field) == f_value:
          _newset.append(m)
    self._dataset = _newset
    return self","filter results of dataset eg.
    
    Query('Posts').filter(post_type='post')",0,0,1,1
"def filter(self, **kwargs):
        
        if kwargs:
            q = self._clone()
            if self.fargs:
                kwargs = update_dictionary(self.fargs.copy(), kwargs)
            q.fargs = kwargs
            return q
        else:
            return self","Create a new :class:`Query` with additional clauses corresponding to
``where`` or ``limit`` in a ``SQL SELECT`` statement.

:parameter kwargs: dictionary of limiting clauses.
:rtype: a new :class:`Query` instance.

For example::

    qs = session.query(MyModel)
    result = qs.filter(group = 'planet')",0,0,2,2
"def filter_basis_sets(substr=None, family=None, role=None, data_dir=None):
    

    data_dir = fix_data_dir(data_dir)
    metadata = get_metadata(data_dir)

    

    if family:
        family = family.lower()
        if not family in get_families(data_dir):
            raise RuntimeError(""Family '{}' is not a valid family"".format(family))
        metadata = {k: v for k, v in metadata.items() if v['family'] == family}
    if role:
        role = role.lower()
        if not role in get_roles():
            raise RuntimeError(""Role '{}' is not a valid role"".format(role))
        metadata = {k: v for k, v in metadata.items() if v['role'] == role}
    if substr:
        substr = substr.lower()
        metadata = {k: v for k, v in metadata.items() if substr in k or substr in v['display_name']}

    return metadata","Filter basis sets by some criteria

    All parameters are ANDed together and are not case sensitive.

    Parameters
    ----------
    substr : str
        Substring to search for in the basis set name
    family : str
        Family the basis set belongs to
    role : str
        Role of the basis set
    data_dir : str
        Data directory with all the basis set information. By default,
        it is in the 'data' subdirectory of this project.

    Returns
    -------
    dict
        Basis set metadata that matches the search criteria",2,0,4,6
"def filter_like(self, **filters):
        
        Query = {}
        for name, value in filters.items():
            name = resolve_name(self.type, name)
            Query[name] = re_compile(value, IGNORECASE)
        self.filter(QueryExpression(Query))
        return self","Filter query using re.compile().

            **Examples**: ``query.filter_like(Name=""andi"")``",0,0,1,1
"def filter_magic(source, magic, strip=True):
    
    filtered, magic_lines=[],[]
    for line in source.splitlines():
        if line.strip().startswith(magic):
            magic_lines.append(line)
        else:
            filtered.append(line)
    if strip:
        magic_lines = [el.replace(magic,'') for el in magic_lines]
    return '\n'.join(filtered), magic_lines","Given the source of a cell, filter out the given magic and collect
    the lines using the magic into a list.

    If strip is True, the IPython syntax part of the magic (e.g %magic
    or %%magic) is stripped from the returned lines.",0,0,4,4
"def filter_taxa(records, taxids, unclassified=False, discard=False):
    
    taxids = set(taxids)
    kept_records = []
    ncbi = ete3.NCBITaxa()
    unclassified_taxids = {0,1}
    for r in records:
        taxid, rank, sciname, lineage = r.description.strip().partition(' ')[2].split('|')
        taxid = int(taxid)
        lineage = set(ncbi.get_lineage(taxid) or [0])  
        intersection = lineage & taxids
        if taxid in unclassified_taxids and unclassified:
            kept_records.append(r)
        elif intersection and not discard:
            kept_records.append(r)
        elif not intersection and discard and taxid not in unclassified_taxids:
            kept_records.append(r)

    return kept_records","Selectively include or discard specified taxon IDs from tictax annotated FASTA/Qs
    Filters all children of specified taxon IDs
    Returns subset of input SeqRecords
    Taxon IDs of 1 and 2 are considered unclassified",1,0,2,3
"def filtered(self, efilter):
        
        if not self.params:
            self.params={'filter' : efilter}
            return self
        if not self.params.has_key('filter'):
            self.params['filter'] = efilter
            return self
        self.params['filter'].update(efilter)
        return self",Applies a filter to the search,0,0,1,1
"def filterwarnings(action, message="""", category=Warning, module="""", lineno=0,
                   append=False, emodule=""""):
    
    assert action in (""error"", ""ignore"", ""always"", ""default"", ""module"",
                      ""once""), ""invalid action: %r"" % (action,)
    assert isinstance(message, str), ""message must be a string""
    assert isinstance(category, type), ""category must be a class""
    assert issubclass(category, Warning), ""category must be a Warning subclass""
    assert isinstance(module, str), ""module must be a string""
    assert isinstance(lineno, int) and lineno >= 0, \
        ""lineno must be an int >= 0""
    if emodule:
        item = (action, re.compile(message, re.I), category,
            re.compile(module), lineno, re.compile(emodule, ))
    else:
        item = (action, re.compile(message, re.I), category,
            re.compile(module), lineno)
    if append:
        warnings.filters.append(_set_proxy_filter(item))
    else:
        warnings.filters.insert(0, _set_proxy_filter(item))
    warnings._filters_mutated()","Insert an entry into the list of warnings filters (at the front).

    'action' -- one of ""error"", ""ignore"", ""always"", ""default"", ""module"",
                or ""once""
    'message' -- a regex that the warning message must match
    'category' -- a class that the warning must be a subclass of
    'module' -- a regex that the module name must match
    'lineno' -- an integer line number, 0 matches all warnings
    'append' -- if true, append to the list of filters
    'emodule' -- a regex that the module emitting the warning must match.",1,0,6,7
"def find(cls, *args, **kwargs):
        
        seconds = 60 * 60 * 24 * 30  
        until = kwargs.pop('until', None)
        since = kwargs.pop('since', None)

        if until is None:
            until = datetime.datetime.now()

        if since is None:
            since = until - datetime.timedelta(seconds=seconds)

        dt = until - since
        if dt > datetime.timedelta(seconds=seconds):
            raise InvalidArguments(until, since)

        kwargs['since'] = since.isoformat()
        kwargs['until'] = until.isoformat()

        return getattr(Entity, 'find').__func__(cls, *args, **kwargs)","Find notifications.

        Optional kwargs are:
            since:
                datetime instance
            until:
                datetime instance

        If not specified, until will default to now(), and since will default
        to 30 days prior to until.

        As per PD spec, date range must not exceed 1 month.",0,0,3,3
"def findPrefix(self, uri, default=None):
        
        for item in self.nsprefixes.items():
            if item[1] == uri:
                prefix = item[0]
                return prefix
        for item in self.specialprefixes.items():
            if item[1] == uri:
                prefix = item[0]
                return prefix
        if self.parent is not None:
            return self.parent.findPrefix(uri, default)
        else:
            return default","Find the first prefix that has been mapped to a namespace URI.
        The local mapping is searched, then it walks up the tree until
        it reaches the top or finds a match.
        @param uri: A namespace URI.
        @type uri: basestring
        @param default: A default prefix when not found.
        @type default: basestring
        @return: A mapped prefix.
        @rtype: basestring",0,0,3,3
"def find_actions(results, t, N_matrix=8, use_box=False, ifloop=False, ifprint = True):
    

    
    loop = assess_angmom(results)
    arethereloops = np.any(loop>0)
    if(arethereloops and not use_box):
        L = loop_actions(flip_coords(results,loop),t,N_matrix, ifprint)
        if(L==None):
            if(ifprint):
                print(""Failed to find actions for this orbit"")
            return
        
        
        
        
        
        
        
    else:
        L = box_actions(results,t,N_matrix, ifprint)
        if(L==None):
            if(ifprint):
                print(""Failed to find actions for this orbit"")
            return
    if(ifloop):
        return L,loop
    else:
        return L","Main routine:
        Takes a series of phase-space points from an orbit integration at times t and returns
        L = (act,ang,n_vec,toy_aa, pars) where act is the actions, ang the initial angles and
        frequencies, n_vec the n vectors of the Fourier modes, toy_aa the toy action-angle
        coords, and pars are the toy potential parameters
        N_matrix sets the maximum |n| of the Fourier modes used,
        use_box forces the routine to use the triaxial harmonic oscillator as the toy potential,
        ifloop=True returns orbit classification,
        ifprint=True prints progress messages.",0,0,1,1
"def find_all_iter( source, substring, start=None, end=None, overlap=False ):
    
    data = source
    base = 0
    if end is not None:
        data = data[:end]
    if start is not None:
        data = data[start:]
        base = start
    pointer = 0
    increment = 1 if overlap else (len( substring ) or 1)
    while True:
        pointer = data.find( substring, pointer )
        if pointer == -1:
            return
        yield base+pointer
        pointer += increment","Iterate through every location a substring can be found in a source string.

    source
        The source string to search.

    start
        Start offset to read from (default: start)

    end
        End offset to stop reading at (default: end)

    overlap
        Whether to return overlapping matches (default: false)",0,0,1,1
"def find_attr_start_line(self, lines, min_line=4, max_line=9):
        
        for idx, line in enumerate(lines[min_line:max_line]):
            col = line.split()
            if len(col) > 1 and col[1] == 'ATTRIBUTE_NAME':
                return idx + min_line + 1

        self.log.warn('ATTRIBUTE_NAME not found in second column of'
                      ' smartctl output between lines %d and %d.'
                      % (min_line, max_line))

        return max_line + 1","Return line number of the first real attribute and value.
        The first line is 0.  If the 'ATTRIBUTE_NAME' header is not
        found, return the index after max_line.",0,1,1,2
"def find_bin(self, x):
        
        x = np.asarray(x).flatten()
        right = np.digitize(x, self.bins, right=True)
        if right.max() == len(self.bins):
            right[right == len(self.bins)] = len(self.bins) - 1
        return right","Sort input or inputs according to the current bin estimate

        Parameters
        ----------
        x       :   array or numeric
                    a value or array of values to fit within the estimated
                    bins

        Returns
        -------
        a bin index or array of bin indices that classify the input into one of
        the classifiers' bins.

        Note that this differs from similar functionality in
        numpy.digitize(x, classi.bins, right=True).

        This will always provide the closest bin, so data ""outside"" the classifier,
        above and below the max/min breaks, will be classified into the nearest bin.

        numpy.digitize returns k+1 for data greater than the greatest bin, but retains 0
        for data below the lowest bin.",0,0,1,1
"def find_frequencies(data, freq=44100, bits=16):
    
    
    n = len(data)
    p = _fft(data)
    uniquePts = numpy.ceil((n + 1) / 2.0)

    
    p = [(abs(x) / float(n)) ** 2 * 2 for x in p[0:uniquePts]]
    p[0] = p[0] / 2
    if n % 2 == 0:
        p[-1] = p[-1] / 2

    
    s = freq / float(n)
    freqArray = numpy.arange(0, uniquePts * s, s)
    return zip(freqArray, p)","Convert audio data into a frequency-amplitude table using fast fourier
    transformation.

    Return a list of tuples (frequency, amplitude).

    Data should only contain one channel of audio.",0,0,1,1
"def find_l90(contig_lengths_dict, genome_length_dict):
    
    
    l90_dict = dict()
    for file_name, contig_lengths in contig_lengths_dict.items():
        currentlength = 0
        
        currentcontig = 0
        for contig_length in contig_lengths:
            currentlength += contig_length
            
            currentcontig += 1
            
            if currentlength >= genome_length_dict[file_name] * 0.9:
                l90_dict[file_name] = currentcontig
                break
    return l90_dict","Calculate the L90 for each strain. L90 is defined as the number of contigs required to achieve the N90
    :param contig_lengths_dict: dictionary of strain name: reverse-sorted list of all contig lengths
    :param genome_length_dict: dictionary of strain name: total genome length
    :return: l90_dict: dictionary of strain name: L90",0,0,1,1
"def find_objects_wo_child(config=None,
                          config_path=None,
                          parent_regex=None,
                          child_regex=None,
                          ignore_ws=False,
                          saltenv='base'):
    
    ccp = _get_ccp(config=config, config_path=config_path, saltenv=saltenv)
    lines = ccp.find_objects_wo_child(parent_regex, child_regex, ignore_ws=ignore_ws)
    return lines","Return a list of parent ``ciscoconfparse.IOSCfgLine`` objects, which matched
    the ``parent_regex`` and whose children did *not* match ``child_regex``.
    Only the parent ``ciscoconfparse.IOSCfgLine`` objects will be returned. For
    simplicity, this method only finds oldest ancestors without immediate
    children that match.

    .. warning::
        This function is mostly valuable when invoked from other Salt
        components (i.e., execution modules, states, templates etc.). For CLI
        usage, please consider using
        :py:func:`ciscoconfparse.find_lines_wo_child <salt.ciscoconfparse_mod.find_lines_wo_child>`

    config
        The configuration sent as text.

        .. note::
            This argument is ignored when ``config_path`` is specified.

    config_path
        The absolute or remote path to the file with the configuration to be
        parsed. This argument supports the usual Salt filesystem URIs, e.g.,
        ``salt://``, ``https://``, ``ftp://``, ``s3://``, etc.

    parent_regex
        The regular expression to match the parent lines against.

    child_regex
        The regular expression to match the child lines against.

    ignore_ws: ``False``
        Whether to ignore the white spaces.

    saltenv: ``base``
        Salt fileserver environment from which to retrieve the file. This
        argument is ignored when ``config_path`` is not a ``salt://`` URL.

    Usage example:

    .. code-block:: python

        objects = __salt__['ciscoconfparse.find_objects_wo_child'](config_path='https://bit.ly/2mAdq7z',
                                                                   parent_regex='line con',
                                                                   child_regex='stopbits')
        for obj in objects:
            print(obj.text)",0,0,2,2
"def find_playlists_by_ids(self, playlist_ids):
        
        url = 'https://openapi.youku.com/v2/playlists/show_batch.json'
        params = {
            'client_id': self.client_id,
            'playlist_ids': playlist_ids
        }
        r = requests.get(url, params=params)
        check_error(r)
        return r.json()",doc: http://open.youku.com/docs/doc?id=67,0,1,0,1
"def find_tag(__matcher: str = 'v[0-9]*', *, strict: bool = True,
             git_dir: str = '.') -> str:
    
    command = 'git describe --abbrev=12 --dirty'.split()
    with chdir(git_dir):
        try:
            stdout = check_output(command + ['--match={}'.format(__matcher), ])
        except CalledProcessError:
            if strict:
                raise
            stdout = check_output(command + ['--always', ])

        stdout = stdout.decode('ascii', 'replace')
    return stdout.strip()","Find closest tag for a git repository.

    Note:
        This defaults to `Semantic Version`_ tag matching.

    Args:
        __matcher: Glob-style tag pattern to match
        strict: Allow commit-ish, if no tag found
        git_dir: Repository to search
    Returns:
        Matching tag name

    .. _Semantic Version: http://semver.org/",1,0,2,3
"def firmware_manifest_list(self, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('asynchronous'):
            return self.firmware_manifest_list_with_http_info(**kwargs)  
        else:
            (data) = self.firmware_manifest_list_with_http_info(**kwargs)  
            return data","List manifests  # noqa: E501

        List firmware manifests.  # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass asynchronous=True
        >>> thread = api.firmware_manifest_list(asynchronous=True)
        >>> result = thread.get()

        :param asynchronous bool
        :param int limit: How many firmware manifests to retrieve
        :param str order: ASC or DESC
        :param str after: The ID of the the item after which to retrieve the next page
        :param str include: A comma-separated list of data fields to return. Currently supported: total_count
        :param str filter: URL-encoded query string parameter to filter returned data  `?filter={URL-encoded query string}`  ###### Filterable fields:  The table lists all the fields that can be filtered on with certain filters:  <table>   <thead>     <tr>       <th>Field</th>       <th>= / __eq / __neq</th>       <th>__in /  __nin</th>       <th>__lte / __gte</th>     <tr>   <thead>   <tbody>     <tr>       <td>created_at</td>       <td>✓</td>       <td>✓</td>       <td>✓</td>     </tr>     <tr>       <td>datafile</td>       <td>✓</td>       <td>✓</td>       <td>&nbsp;</td>     </tr>     <tr>       <td>datafile_size</td>       <td>✓</td>       <td>✓</td>       <td>&nbsp;</td>     </tr>     <tr>       <td>description</td>       <td>✓</td>       <td>✓</td>       <td>&nbsp;</td>     </tr>     <tr>       <td>device_class</td>       <td>✓</td>       <td>✓</td>       <td>&nbsp;</td>     </tr>     <tr>       <td>etag</td>       <td>✓</td>       <td>✓</td>       <td>✓</td>     </tr>     <tr>       <td>id</td>       <td>✓</td>       <td>✓</td>       <td>&nbsp;</td>     </tr>     <tr>       <td>name</td>       <td>✓</td>       <td>✓</td>       <td>&nbsp;</td>     </tr>     <tr>       <td>timestamp</td>       <td>✓</td>       <td>✓</td>       <td>✓</td>     </tr>     <tr>       <td>updated_at</td>       <td>✓</td>       <td>✓</td>       <td>✓</td>     </tr>   </tbody> </table> &nbsp;  The query string is made up of key-value pairs separated by ampersands. For example, this query: `key1__eq=value1&key2__eq=value2&key3__eq=value3`  would be URL-encoded as: `?filter=key1__eq%3Dvalue1%26key2__eq%3Dvalue2%26key3__eq%3Dvalue3`   **Filtering by properties** `name__eq=mymanifest`  **Filtering on date-time fields**  Date-time fields should be specified in UTC RFC3339 format, `YYYY-MM-DDThh:mm:ss.msZ`. There are three permitted variations:  * UTC RFC3339 with milliseconds. Example: `2016-11-30T16:25:12.1234Z` * UTC RFC3339 without milliseconds. Example: `2016-11-30T16:25:12Z` * UTC RFC3339 shortened without milliseconds and punctuation. Example: `20161130T162512Z`  Date-time filtering supports three operators:  * equality by appending `__eq` to the field name * greater than or equal to by appending `__gte` to the field name * less than or equal to by appending `__lte` to the field name  `{field name}[|__eq|__lte|__gte]={UTC RFC3339 date-time}`  Time ranges may be specified by including both the `__gte` and `__lte` forms in the filter. For example:  `created_at__gte=2016-11-30T16:25:12.1234Z&created_at__lte=2016-12-30T00:00:00Z`  **Filtering on multiple fields**  `name__eq=mymanifest&created_at__gte=2016-11-30T16:25:12.1234Z&created_at__lte=2016-12-30T00:00:00Z`  **Filtering with filter operators**  String field filtering supports the following operators:  * equality: `__eq` * non-equality: `__neq` * in : `__in` * not in: `__nin`  For `__in` and `__nin` filters list of parameters must be comma-separated:  `name__in=fw-manifest1,fw-manifest2`
        :return: FirmwareManifestPage
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def fit(self, struct1, struct2):
        
        struct1, struct2 = self._process_species([struct1, struct2])

        if not self._subset and self._comparator.get_hash(struct1.composition) \
                != self._comparator.get_hash(struct2.composition):
            return None

        struct1, struct2, fu, s1_supercell = self._preprocess(struct1, struct2)
        match = self._match(struct1, struct2, fu, s1_supercell,
                            break_on_match=True)

        if match is None:
            return False
        else:
            return match[0] <= self.stol","Fit two structures.

        Args:
            struct1 (Structure): 1st structure
            struct2 (Structure): 2nd structure

        Returns:
            True or False.",0,0,1,1
"def fit(self, times, losses, configs=None):
        

        assert np.all(times > 0) and np.all(times <= self.max_num_epochs)

        train = None
        targets = None

        for i in range(len(configs)):

            t_idx = times[i] / self.max_num_epochs

            x = np.repeat(np.array(configs[i])[None, :], t_idx.shape[0], axis=0)
            x = np.concatenate((x, t_idx[:, None]), axis=1)

            
            lc = [1 - l for l in losses[i]]

            if train is None:
                train = x
                targets = lc
            else:
                train = np.concatenate((train, x), 0)
                targets = np.concatenate((targets, lc), 0)

        self.model.train(train, targets)","function to train the model on the observed data

            Parameters:
            -----------

            times: list
                list of numpy arrays of the timesteps for each curve
            losses: list
                list of numpy arrays of the loss (the actual learning curve)
            configs: list or None
                list of the configurations for each sample. Each element
                has to be a numpy array. Set to None, if no configuration
                information is available.",0,0,2,2
"def fit(self, users, items, user_features=None, item_features=None):
        

        self._user_id_mapping = {}
        self._item_id_mapping = {}
        self._user_feature_mapping = {}
        self._item_feature_mapping = {}

        return self.fit_partial(users, items, user_features, item_features)","Fit the user/item id and feature name mappings.

        Calling fit the second time will reset existing mappings.

        Parameters
        ----------

        users: iterable of user ids
        items: iterable of item ids
        user_features: iterable of user features, optional
        item_features: iterable of item features, optional",0,0,1,1
"def fit_ahrs(A, H, Aoff, Arot, Hoff, Hrot):
    
    Acal = np.dot(A - Aoff, Arot)
    Hcal = np.dot(H - Hoff, Hrot)

    
    for i in (1, 2):
        Acal[i] = -Acal[i]
        Hcal[i] = -Hcal[i]

    roll = arctan2(-Acal[1], -Acal[2])
    pitch = arctan2(Acal[0], np.sqrt(Acal[1] * Acal[1] + Acal[2] * Acal[2]))
    yaw = arctan2(
        Hcal[2] * sin(roll) - Hcal[1] * cos(roll),
        sum((
            Hcal[0] * cos(pitch), Hcal[1] * sin(pitch) * sin(roll),
            Hcal[2] * sin(pitch) * cos(roll)
        ))
    )

    yaw = np.degrees(yaw)
    while yaw < 0:
        yaw += 360
    
    roll = np.degrees(roll)
    pitch = np.degrees(pitch)
    return yaw, pitch, roll","Calculate yaw, pitch and roll for given A/H and calibration set.

    Author: Vladimir Kulikovsky

    Parameters
    ----------
    A: list, tuple or numpy.array of shape (3,)
    H: list, tuple or numpy.array of shape (3,)
    Aoff: numpy.array of shape(3,)
    Arot: numpy.array of shape(3, 3)
    Hoff: numpy.array of shape(3,)
    Hrot: numpy.array of shape(3, 3)

    Returns
    -------
    yaw, pitch, roll",0,0,3,3
"def fit_freq_min_max(self, training_signal):
        

        window_length = len(self.window)
        window_weight = sum(self.window)
        max_mask = np.zeros(int(window_length / 2) + 1)
        min_mask = np.zeros(int(window_length / 2) + 1)

        for i in range(0, len(training_signal) - window_length - 1):
            rfft = np.fft.rfft(training_signal[i:i + window_length] * self.window)
            temp = np.abs(rfft) / window_weight
            max_mask = np.maximum(max_mask, temp)
            min_mask = np.minimum(min_mask, temp)

        self.mask_top = self.gain * max_mask
        self.mask_bottom = min_mask / self.gain","Defines a spectral mask based on training data using min and max values of each
             frequency component

        Args:
            training_signal: Training data",0,0,1,1
"def fit_quadrature(orth, nodes, weights, solves, retall=False, norms=None, **kws):
    
    orth = chaospy.poly.Poly(orth)
    nodes = numpy.asfarray(nodes)
    weights = numpy.asfarray(weights)

    if callable(solves):
        solves = [solves(node) for node in nodes.T]
    solves = numpy.asfarray(solves)

    shape = solves.shape
    solves = solves.reshape(weights.size, int(solves.size/weights.size))

    ovals = orth(*nodes)
    vals1 = [(val*solves.T*weights).T for val in ovals]

    if norms is None:
        norms = numpy.sum(ovals**2*weights, -1)
    else:
        norms = numpy.array(norms).flatten()
        assert len(norms) == len(orth)

    coefs = (numpy.sum(vals1, 1).T/norms).T
    coefs = coefs.reshape(len(coefs), *shape[1:])
    approx_model = chaospy.poly.transpose(chaospy.poly.sum(orth*coefs.T, -1))

    if retall:
        return approx_model, coefs
    return approx_model","Using spectral projection to create a polynomial approximation over
    distribution space.

    Args:
        orth (chaospy.poly.base.Poly):
            Orthogonal polynomial expansion. Must be orthogonal for the
            approximation to be accurate.
        nodes (numpy.ndarray):
            Where to evaluate the polynomial expansion and model to
            approximate. ``nodes.shape==(D,K)`` where ``D`` is the number of
            dimensions and ``K`` is the number of nodes.
        weights (numpy.ndarray):
            Weights when doing numerical integration. ``weights.shape == (K,)``
            must hold.
        solves (numpy.ndarray):
            The model evaluation to approximate. If `numpy.ndarray` is
            provided, it must have ``len(solves) == K``. If callable, it must
            take a single argument X with ``len(X) == D``, and return
            a consistent numpy compatible shape.
        norms (numpy.ndarray):
            In the of TTR using coefficients to estimate the polynomial norm is
            more stable than manual calculation. Calculated using quadrature if
            no provided. ``norms.shape == (len(orth),)`` must hold.

    Returns:
        (chaospy.poly.base.Poly):
            Fitted model approximation in the form of an polynomial.",0,0,6,6
"def fit_shown_data(f=""a*x+b"", p=""a=1, b=2"", axes=""gca"", verbose=True, **kwargs):
    

    
    if axes==""gca"": axes = _pylab.gca()

    
    _pylab.sca(axes)
    xmin,xmax = axes.get_xlim()
    ymin,ymax = axes.get_ylim()

    
    if 'first_figure' not in kwargs: kwargs['first_figure'] = axes.figure.number+1

    
    fitters = []
    for l in axes.lines:

        
        x,y = l.get_data()
        x,y = _s.fun.trim_data_uber([x,y],[xmin<x,x<xmax,ymin<y,y<ymax])

        
        fitters.append(_s.data.fitter(**kwargs).set_functions(f=f, p=p))
        fitters[-1].set_data(x,y)
        fitters[-1].fit()
        fitters[-1].autoscale_eydata().fit()
        if verbose:
            print(fitters[-1])
            print(""<click the graph to continue>"")
        if not axes.lines[-1] == l: fitters[-1].ginput(timeout=0)

    return fitters","Fast-and-loos quick fit:

    Loops over each line of the supplied axes and fits with the supplied
    function (f) and parameters (p). Assumes uniform error and scales this
    such that the reduced chi^2 is 1.

    Returns a list of fitter objects

    **kwargs are sent to _s.data.fitter()",0,0,1,1
"def flatten(d, reducer='tuple', inverse=False):
    
    if isinstance(reducer, str):
        reducer = REDUCER_DICT[reducer]
    flat_dict = {}

    def _flatten(d, parent=None):
        for key, value in six.viewitems(d):
            flat_key = reducer(parent, key)
            if isinstance(value, Mapping):
                _flatten(value, flat_key)
            else:
                if inverse:
                    flat_key, value = value, flat_key
                if flat_key in flat_dict:
                    raise ValueError(""duplicated key '{}'"".format(flat_key))
                flat_dict[flat_key] = value

    _flatten(d)
    return flat_dict","Flatten dict-like object.

    Parameters
    ----------
    d: dict-like object
        The dict that will be flattened.
    reducer: {'tuple', 'path', function} (default: 'tuple')
        The key joining method. If a function is given, the function will be
        used to reduce.
        'tuple': The resulting key will be tuple of the original keys
        'path': Use ``os.path.join`` to join keys.
    inverse: bool (default: False)
        Whether you want invert the resulting key and value.

    Returns
    -------
    flat_dict: dict",1,0,3,4
"def flatten(dic, keep_iter=False, position=None):
    
    child = {}
    if not dic:
        return {}

    for k, v in get_iter(dic):
        if isstr(k):
            k = k.replace('.', '_')
        if position:
            item_position = '%s.%s' % (position, k)
        else:
            item_position = '%s' % k

        if is_iter(v):
            child.update(flatten(dic[k], keep_iter, item_position))
            if keep_iter:
                child[item_position] = v
        else:
            child[item_position] = v

    return child","Returns a flattened dictionary from a dictionary of nested dictionaries and lists.
    `keep_iter` will treat iterables as valid values, while also flattening them.",0,0,1,1
"def flatten(self):
        

        for key in self.keys:
            try:
                arr = self.__dict__[key]
                shape = arr.shape
                if shape[2] == 1:
                    self.__dict__[key] = arr.reshape(shape[0], shape[1])
            except:
                
                pass","Flattens any np.array of column vectors into 1D arrays. Basically,
        this makes data readable for humans if you are just inspecting via
        the REPL. For example, if you have saved a KalmanFilter object with 89
        epochs, self.x will be shape (89, 9, 1) (for example). After flatten
        is run, self.x.shape == (89, 9), which displays nicely from the REPL.

        There is no way to unflatten, so it's a one way trip.",0,0,1,1
"def flatten_until(is_leaf, xs):
    

    def _flatten_until(items):
        if isinstance(Iterable, items) and not is_leaf(items):
            for item in items:
                for i in _flatten_until(item):
                    yield i
        else:
            yield items

    return list(_flatten_until(xs))","Flatten a nested  sequence. A sequence could be a nested list of lists
    or tuples or a combination of both

    :param is_leaf: Predicate. Predicate to  determine whether an item
                    in the iterable `xs` is a leaf node or not.
    :param xs: Iterable. Nested lists or tuples
    :return: list.",0,0,3,3
"def flattencopy(lst):
    
    
    
    thelist = copy.deepcopy(lst)
    list_is_nested = True
    while list_is_nested:  
        keepchecking = False
        atemp = []
        for element in thelist:  
            if isinstance(element, list):
                atemp.extend(element)
                keepchecking = True
            else:
                atemp.append(element)
        list_is_nested = keepchecking  
        thelist = atemp[:]
    return thelist","flatten and return a copy of the list
    indefficient on large lists",0,0,2,2
"def flattened(self):
        
        flatx = {}
        flatf = {}
        for i in self.res:
            if isinstance(i, int):
                flatx[i] = []
                flatf[i] = []
                for x in sorted(self.res[i]):
                    for d in sorted(self.res[i][x]):
                        flatx[i].append(x)
                        flatf[i].append(d)
        return flatx, flatf","return flattened data ``(x, f)`` such that for the sweep through
        coordinate ``i`` we have for data point ``j`` that ``f[i][j] == func(x[i][j])``",0,0,1,1
"def flush_queue(self):
        
        records = []

        while not self.queue.empty() and len(records) < self.batch_size:
            records.append(self.queue.get())

        if records:
            self.send_records(records)
            self.last_flush = time.time()",Grab all the current records in the queue and send them.,0,1,1,2
"def fork(self, **args):
		
		if 'name' in args:
			self.gist_name = args['name']
			self.gist_id = self.getMyID(self.gist_name)
		elif 'id' in args:
			self.gist_id = args['id']
		else:
			raise Exception('Either provide authenticated user\'s Unambigious Gistname or any unique Gistid to be forked')

		r = requests.post(
			'%s'%BASE_URL+'/gists/%s/forks' % self.gist_id,
			headers=self.gist.header
			)
		if (r.status_code == 201):
			response = {
				'id': self.gist_id,
				'description': r.json()['description'],
				'public': r.json()['public'],
				'comments': r.json()['comments']
			}
			return response

		raise Exception('Gist can\'t be forked')",fork any gist by providing gistID or gistname(for authenticated user),2,1,2,5
"def format(self, format_str):
        
        return format_str.format(**{
            ""num"": self.group_num,
            ""name"": self.group_name,
            ""symbol"": self.group_symbol,
            ""variant"": self.group_variant,
            ""current_data"": self.group_data,
            ""count"": self.groups_count,
            ""names"": self.groups_names,
            ""symbols"": self.groups_symbols,
            ""variants"": self.groups_variants,
            ""all_data"": self.groups_data})","Returns a formatted version of format_str.
        The only named replacement fields supported by this method and
        their corresponding API calls are:

        * {num}           group_num
        * {name}          group_name
        * {symbol}        group_symbol
        * {variant}       group_variant
        * {current_data}  group_data
        * {nums}          groups_nums
        * {names}         groups_names
        * {symbols}       groups_symbols
        * {variants}      groups_variants
        * {all_data}      groups_data

        Passing other replacement fields will result in raising exceptions.

        :param format_str: a new style format string
        :rtype: str",1,0,2,3
"def format(self, record):
        

        record_fields = record.__dict__.copy()
        self._set_exc_info(record_fields)

        event_name = 'default'
        if record_fields.get('event_name'):
            event_name = record_fields.pop('event_name')

        log_level = 'INFO'
        if record_fields.get('log_level'):
            log_level = record_fields.pop('log_level')

        [record_fields.pop(k) for k in record_fields.keys()
         if k not in self.fields]

        defaults = self.defaults.copy()
        fields = self.fields.copy()
        fields.update(record_fields)
        filtered_fields = {}
        for k, v in fields.iteritems():
            if v is not None:
                filtered_fields[k] = v

        defaults.update({
            'event_timestamp': self._get_now(),
            'event_name': event_name,
            'log_level': log_level,
            'fields': filtered_fields})

        return json.dumps(defaults, default=self.json_default)","formats a logging.Record into a standard json log entry

        :param record: record to be formatted
        :type record: logging.Record
        :return: the formatted json string
        :rtype: string",0,0,1,1
"def format(self, record):
        
        
        if record.levelno >= logging.ERROR:
            color = colorama.Fore.RED
        elif record.levelno >= logging.WARNING:
            color = colorama.Fore.YELLOW
        elif record.levelno >= logging.INFO:
            color = colorama.Fore.RESET
        else:
            color = colorama.Fore.CYAN
        format_template = (
            '{}{}%(levelname)s{} [%(asctime)s][%(name)s]{} %(message)s')
        if sys.stdout.isatty():
            self._fmt = format_template.format(
                colorama.Style.BRIGHT,
                color,
                colorama.Fore.RESET,
                colorama.Style.RESET_ALL
            )
        else:
            self._fmt = format_template.format(*[''] * 4)
        if six.PY3:
            self._style._fmt = self._fmt  
        return super(_LogColorFormatter, self).format(record)","Format the log record with timestamps and level based colors.

        Args:
            record: The log record to format.

        Returns:
            The formatted log record.",0,0,2,2
"def format(self, status, headers, environ, bucket, delay):
        

        
        
        entity = (""This request was rate-limited.  ""
                  ""Please retry your request after %s."" %
                  time.strftime(""%Y-%m-%dT%H:%M:%SZ"",
                                time.gmtime(bucket.next)))
        headers['Content-Type'] = 'text/plain'

        return status, entity","Formats a response entity.  Returns a tuple of the desired
        status code and the formatted entity.  The default status code
        is passed in, as is a dictionary of headers.

        :param status: The default status code.  Should be returned to
                       the caller, or an alternate selected.  The
                       status code should include both the number and
                       the message, separated by a single space.
        :param headers: A dictionary of headers for the response.
                        Should update the 'Content-Type' header at a
                        minimum.
        :param environ: The WSGI environment for the request.
        :param bucket: The bucket containing the data which caused the
                       delay decision to be made.  This can be used to
                       obtain such information as the next time the
                       request can be made.
        :param delay: The number of seconds by which the request
                      should be delayed.",0,0,1,1
"def format_datetime(dt, usegmt=False):
    
    now = dt.timetuple()
    if usegmt:
        if dt.tzinfo is None or dt.tzinfo != datetime.timezone.utc:
            raise ValueError(""usegmt option requires a UTC datetime"")
        zone = 'GMT'
    elif dt.tzinfo is None:
        zone = '-0000'
    else:
        zone = dt.strftime(""%z"")
    return _format_timetuple_and_zone(now, zone)","Turn a datetime into a date string as specified in RFC 2822.

    If usegmt is True, dt must be an aware datetime with an offset of zero.  In
    this case 'GMT' will be rendered instead of the normal +0000 required by
    RFC2822.  This is to support HTTP headers involving date stamps.",1,0,2,3
"def format_props(props, prop_template=""{{k}} = { {{v}} }"", delim=""\n""):
    
    vars_ = []
    props_ = []
    for k, v in list(props.items()):
        vars_.append(Template(""var {{k}} = {{v}};"").render(k=k,v=json.dumps(v)))
        props_.append(Template(prop_template).render(k=k, v=k))
    return ""\n"".join(vars_), delim.join(props_)","Formats props for the React template.

        Args:
            props (dict): properties to be written to the template.

        Returns:
            Two lists, one containing variable names and the other
            containing a list of props to be fed to the React template.",0,0,1,1
"def forward_message(self, chat_id, from_chat_id, message_id, disable_notification=False):
        
        assert_type_or_raise(chat_id, (int, unicode_type), parameter_name=""chat_id"")

        assert_type_or_raise(from_chat_id, (int, unicode_type), parameter_name=""from_chat_id"")

        assert_type_or_raise(message_id, int, parameter_name=""message_id"")

        assert_type_or_raise(disable_notification, None, bool, parameter_name=""disable_notification"")

        result = self.do(
            ""forwardMessage"", chat_id=chat_id, from_chat_id=from_chat_id, message_id=message_id,
            disable_notification=disable_notification
        )
        if self.return_python_objects:
            logger.debug(""Trying to parse {data}"".format(data=repr(result)))
            from pytgbot.api_types.receivable.updates import Message
            try:
                return Message.from_array(result)
            except TgApiParseException:
                logger.debug(""Failed parsing as api_type Message"", exc_info=True)
            
            
            raise TgApiParseException(""Could not parse result."")  
        
        return result","Use this method to forward messages of any kind. On success, the sent Message is returned.

        https://core.telegram.org/bots/api#forwardmessage

        Parameters:

        :param chat_id: Unique identifier for the target chat (chat id of user chat or group chat) or username of the
                        target channel (in the format @channelusername)
        :type  chat_id: int | str|unicode

        :param from_chat_id: Unique identifier for the chat where the original message was sent
                             (id for chats or the channel's username in the format @channelusername)
        :type  from_chat_id: int | str|unicode

        :param message_id: Message identifier in the chat specified in from_chat_id
        :type  message_id: int


        Optional keyword parameters:

        :param disable_notification: Sends the message silently. Users will receive a notification with no sound.
        :type  disable_notification: bool


        Returns:

        :return: On success, the sent Message is returned
        :rtype:  pytgbot.api_types.receivable.updates.Message",1,3,0,4
"def fprint(fmt, *args, **kwargs):
    
    if not fmt:
        return

    hascolor = False
    target = kwargs.get(""target"", sys.stdout)

    
    fmt = fmt.format(*args, **kwargs)

    for txt, markups in _color_format_parser.parse(fmt):
        if markups != (None, None):
            _color_manager.set_color(*markups)
            hascolor = True
        else:
            if hascolor:
                _color_manager.set_defaults()
                hascolor = False

        target.write(txt)
        target.flush()  

    _color_manager.set_defaults()
    target.write(kwargs.get('end', '\n'))
    _color_manager.set_defaults()","Parse and print a colored and perhaps formatted string.

    The remaining keyword arguments are the same as for Python's built-in print
    function. Colors are returning to their defaults before the function
    returns.",0,0,3,3
"def fromSuccessResponse(cls, success_response):
        
        self = cls()

        
        args = success_response.getSignedNS(self.ns_uri)

        
        
        if args is not None:
            self.parseExtensionArgs(args)
            return self
        else:
            return None","Create a C{L{Response}} object from a successful OpenID
        library response
        (C{L{openid.consumer.consumer.SuccessResponse}}) response
        message

        @param success_response: A SuccessResponse from consumer.complete()
        @type success_response: C{L{openid.consumer.consumer.SuccessResponse}}

        @rtype: Response or None
        @returns: A provider authentication policy response from the
            data that was supplied with the C{id_res} response or None
            if the provider sent no signed PAPE response arguments.",0,0,1,1
"def from_ascii_hex(text: str) -> int:
    
    value = 0
    for index in range(0, len(text)):
        char_ord = ord(text[index:index + 1])
        if char_ord in range(ord('0'), ord('?') + 1):
            digit = char_ord - ord('0')
        elif char_ord in range(ord('a'), ord('f') + 1):
            digit = 0xa + (char_ord - ord('a'))
        else:
            raise ValueError(
                ""Response contains invalid character."")
        value = (value * 0x10) + digit
    return value","Converts to an int value from both ASCII and regular hex.
       The format used appears to vary based on whether the command was to
       get an existing value (regular hex) or set a new value (ASCII hex
       mirrored back from original command).

       Regular hex: 0123456789abcdef
       ASCII hex:   0123456789:;<=>?",1,0,2,3
"def from_cdms2(variable):
    
    values = np.asarray(variable)
    name = variable.id
    dims = variable.getAxisIds()
    coords = {}
    for axis in variable.getAxisList():
        coords[axis.id] = DataArray(
            np.asarray(axis), dims=[axis.id],
            attrs=_filter_attrs(axis.attributes, cdms2_ignored_attrs))
    grid = variable.getGrid()
    if grid is not None:
        ids = [a.id for a in grid.getAxisList()]
        for axis in grid.getLongitude(), grid.getLatitude():
            if axis.id not in variable.getAxisIds():
                coords[axis.id] = DataArray(
                    np.asarray(axis[:]), dims=ids,
                    attrs=_filter_attrs(axis.attributes,
                                        cdms2_ignored_attrs))
    attrs = _filter_attrs(variable.attributes, cdms2_ignored_attrs)
    dataarray = DataArray(values, dims=dims, coords=coords, name=name,
                          attrs=attrs)
    return decode_cf(dataarray.to_dataset())[dataarray.name]",Convert a cdms2 variable into an DataArray,0,0,1,1
"def from_csv(self, input_data):
        
        reformatted_data = []
        for (i,row) in enumerate(input_data):
            if i==0:
                headers = row
            else:
                data_row = {}
                for (j,h) in enumerate(headers):
                    data_row.update({h : row[j]})
                reformatted_data.append(data_row)
        return reformatted_data",Reads csv format input data and converts to json.,1,0,1,2
"def from_dict(self, dictionary):
        

        for remote_name, remote_value in dictionary.items():
            
            
            local_name = next((name for name, attribute in self._attributes.items() if attribute.remote_name == remote_name), None)

            if local_name:
                setattr(self, local_name, remote_value)
            else:
                
                pass","Sets all the exposed ReST attribues from the given dictionary

            Args:
                dictionary (dict): dictionnary containing the raw object attributes and their values.

            Example:
                >>> info = {""name"": ""my group"", ""private"": False}
                >>> group = NUGroup()
                >>> group.from_dict(info)
                >>> print ""name: %s - private: %s"" % (group.name, group.private)
                ""name: my group - private: False""",0,0,1,1
"def from_http(
        cls,
        raw_body: MutableMapping,
        verification_token: Optional[str] = None,
        team_id: Optional[str] = None,
    ) -> ""Event"":
        
        if verification_token and raw_body[""token""] != verification_token:
            raise exceptions.FailedVerification(raw_body[""token""], raw_body[""team_id""])

        if team_id and raw_body[""team_id""] != team_id:
            raise exceptions.FailedVerification(raw_body[""token""], raw_body[""team_id""])

        if raw_body[""event""][""type""].startswith(""message""):
            return Message(raw_body[""event""], metadata=raw_body)
        else:
            return Event(raw_body[""event""], metadata=raw_body)","Create an event with data coming from the HTTP Event API.

        If the event type is a message a :class:`slack.events.Message` is returned.

        Args:
            raw_body: Decoded body of the Event API request
            verification_token: Slack verification token used to verify the request came from slack
            team_id: Verify the event is for the correct team

        Returns:
            :class:`slack.events.Event` or :class:`slack.events.Message`

        Raises:
            :class:`slack.exceptions.FailedVerification`: when `verification_token` or `team_id` does not match the
                                                          incoming event's.",2,0,5,7
"def from_inline(cls: Type[BMAEndpointType], inline: str) -> BMAEndpointType:
        
        m = BMAEndpoint.re_inline.match(inline)
        if m is None:
            raise MalformedDocumentError(BMAEndpoint.API)
        server = m.group(1)
        ipv4 = m.group(2)
        ipv6 = m.group(3)
        port = int(m.group(4))
        return cls(server, ipv4, ipv6, port)","Return BMAEndpoint instance from endpoint string

        :param inline: Endpoint string
        :return:",1,0,2,3
"def from_json(cls, json_obj):
        
        if isinstance(json_obj, str):
            json_obj = json.loads(json_obj)

        type = None
        time = None
        data = None
        if cls.TYPE_FIELD_NAME in json_obj:
            type = json_obj[cls.TYPE_FIELD_NAME]

        if cls.TIME_FIELD_NAME in json_obj:
            time = json_obj[cls.TIME_FIELD_NAME]
        else:
            raise InvalidEventError(""{field} must be present!"".format(
                field=cls.TIME_FIELD_NAME))

        if cls.DATA_FIELD_NAME in json_obj:
            data = json_obj[cls.DATA_FIELD_NAME]

        return cls(type, time, data)","Build an Event from JSON.

        :param json_obj: JSON data representing a Cube Event
        :type json_obj: `String` or `json`
        :throws: `InvalidEventError` when any of time field is not present
        in json_obj.",2,0,2,4
"def from_meta(node):
    
    
    if 'related_element_type' not in node.data:
        return Element.from_href(
            node.data.get('href'))
    
    
    
    return Element.from_meta(
        name=node.data.get('name'),
        type=node.related_element_type,
        href=node.data.get('href'))","Helper method that reolves a routing node to element. Rather than doing
    a lookup and fetch, the routing node provides the information to
    build the element from meta alone.
    
    :rtype: Element",0,0,2,2
"def from_raw_message(cls, rawmessage):
        
        userdata_dict = Userdata(rawmessage[8:22])
        return ExtendedSend(rawmessage[2:5],
                            {'cmd1': rawmessage[6],
                             'cmd2': rawmessage[7]},
                            userdata_dict,
                            flags=rawmessage[5],
                            acknak=rawmessage[22:23])",Create a message from a raw byte stream.,0,0,1,1
"def gcs_get_url(url,
                altexts=None,
                client=None,
                service_account_json=None,
                raiseonfail=False):
    
    bucket_item = url.replace('gs://','')
    bucket_item = bucket_item.split('/')
    bucket = bucket_item[0]
    filekey = '/'.join(bucket_item[1:])

    return gcs_get_file(bucket,
                        filekey,
                        bucket_item[-1],
                        altexts=altexts,
                        client=client,
                        service_account_json=service_account_json,
                        raiseonfail=raiseonfail)","This gets a single file from a Google Cloud Storage bucket.

    This uses the gs:// URL instead of a bucket name and key.

    Parameters
    ----------

    url : str
        GCS URL to download. This should begin with 'gs://'.

    altexts : None or list of str
        If not None, this is a list of alternate extensions to try for the file
        other than the one provided in `filename`. For example, to get anything
        that's an .sqlite where .sqlite.gz is expected, use altexts=[''] to
        strip the .gz.

    client : google.cloud.storage.Client instance
        The instance of the Client to use to perform the download operation. If
        this is None, a new Client will be used. If this is None and
        `service_account_json` points to a downloaded JSON file with GCS
        credentials, a new Client with the provided credentials will be used. If
        this is not None, the existing Client instance will be used.

    service_account_json : str
        Path to a downloaded GCS credentials JSON file.

    raiseonfail : bool
        If True, will re-raise whatever Exception caused the operation to fail
        and break out immediately.

    Returns
    -------

    str
        Path to the downloaded filename or None if the download was
        unsuccessful.",0,1,1,2
"def gdpool(name, start, room):
    
    name = stypes.stringToCharP(name)
    start = ctypes.c_int(start)
    values = stypes.emptyDoubleVector(room)
    room = ctypes.c_int(room)
    n = ctypes.c_int()
    found = ctypes.c_int()
    libspice.gdpool_c(name, start, room, ctypes.byref(n),
                      ctypes.cast(values, ctypes.POINTER(ctypes.c_double)),
                      ctypes.byref(found))
    return stypes.cVectorToPython(values)[0:n.value], bool(found.value)","Return the d.p. value of a kernel variable from the kernel pool.

    http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/gdpool_c.html

    :param name: Name of the variable whose value is to be returned.
    :type name: str
    :param start: Which component to start retrieving for name.
    :type start: int
    :param room: The largest number of values to return.
    :type room: int
    :return: Values associated with name.
    :rtype: list of float",0,0,1,1
"def genMeme(template_id, text0, text1):
    

    username = 'blag'
    password = 'blag'

    api_url = 'https://api.imgflip.com/caption_image'

    payload = {
        'template_id': template_id,
        'username': username,
        'password': password,
        'text0': text0,
    }
    
    if text1 != '':
        payload['text1'] = text1

    try:
        r = requests.get(api_url, params=payload)
    except ConnectionError:
        time.sleep(1)
        r = requests.get(api_url, params=payload)

    
    parsed_json = json.loads(r.text)

    request_status = parsed_json['success']

    if request_status != True:
        error_msg = parsed_json['error_message']
        print(error_msg)
        return None

    else:
        imgURL = parsed_json['data']['url']
        
        return imgURL","This function returns the url of the meme with the given
    template, upper text, and lower text using the ImgFlip
    meme generation API. Thanks!

    Returns None if it is unable to generate the meme.",0,1,1,2
"def gen_references(self) -> str:
        
        rval = []
        for cls in self.schema.classes.values():
            pkeys = self.primary_keys_for(cls)
            for pk in pkeys:
                pk_slot = self.schema.slots[pk]
                classname = camelcase(cls.name) + camelcase(pk)
                if cls.is_a and getattr(self.schema.classes[cls.is_a], pk, None):
                    parent = self.range_type_name(pk_slot, cls.is_a)
                else:
                    parent = self.python_name_for(pk_slot.range)
                rval.append(f'class {classname}({parent}):\n\tpass')
        return '\n\n\n'.join(rval)",Generate python type declarations for all identifiers (primary keys),0,0,1,1
"def gen_schlumberger(self, M, N, a=None):
        
        if a is None:
            a = np.abs(M - N)

        nr_of_steps_left = int(min(M, N) - 1 / a)
        nr_of_steps_right = int((self.nr_electrodes - max(M, N)) / a)
        configs = []
        for i in range(0, min(nr_of_steps_left, nr_of_steps_right)):
            A = min(M, N) - (i + 1) * a
            B = max(M, N) + (i + 1) * a
            configs.append((A, B, M, N))
        configs = np.array(configs)
        self.add_to_configs(configs)
        return configs","generate one Schlumberger sounding configuration, that is, one set
        of configurations for one potential dipole MN.

        Parameters
        ----------
        M: int
            electrode number for the first potential electrode
        N: int
            electrode number for the second potential electrode
        a: int, optional
            stepping between subsequent voltage electrodes. If not set,
            determine it as a = abs(M - N)

        Returns
        -------
        configs: Kx4 numpy.ndarray
            array holding the configurations

        Examples
        --------

            import reda.configs.configManager as CRconfig
            config = CRconfig.ConfigManager(nr_of_electrodes=40)
            config.gen_schlumberger(M=20, N=21)",0,0,1,1
"def gene_panel(self, panel_id, version=None):
        
        query = {'panel_name': panel_id}
        if version:
            LOG.info(""Fetch gene panel {0}, version {1} from database"".format(
                panel_id, version
            ))
            query['version'] = version
            return self.panel_collection.find_one(query)
        else:
            LOG.info(""Fetching gene panels %s from database"", panel_id)
            res = self.panel_collection.find(query).sort('version', -1)
            if res.count() > 0:
                return res[0]
            else:
                LOG.info(""No gene panel found"")
                return None","Fetch a gene panel.

        If no panel is sent return all panels

        Args:
            panel_id (str): unique id for the panel
            version (str): version of the panel. If 'None' latest version will be returned

        Returns:
            gene_panel: gene panel object",2,3,0,5
"def generate(env):
    

    disttar_action=SCons.Action.Action(disttar, disttar_string)
    env['BUILDERS']['DistTar'] =  Builder(
            action=disttar_action
            , emitter=disttar_emitter
            , suffix = disttar_suffix
            , target_factory = env.fs.Entry
    )

    env.AppendUnique(
        DISTTAR_FORMAT = 'gz'
    )",Add builders and construction variables for the DistTar builder.,0,0,1,1
"def generate(self, num_to_generate, starting_place):
        
        res = []
        activ = starting_place[None, :]
        index = activ.__getattribute__(self.argfunc)(1)
        item = self.weights[index]
        for x in range(num_to_generate):
            activ = self.forward(item, prev_activation=activ)[0]
            index = activ.__getattribute__(self.argfunc)(1)
            res.append(index)
            item = self.weights[index]

        return res",Generate data based on some initial position.,0,0,1,1
"def generate(tagGroups, terms):
    

    rv = []
    for pid in tagGroups:
        
        if pid not in terms.keys():
            continue

        groupData = terms[pid]
        groupName = ""[%s] %s"" % (pid, groupData['name'])
        groupDesc = groupData['desc']
        children = []
        group = dict(name=groupName, desc=groupDesc, set=children)
        rv.append(group)

        for cid in groupData['children']:
            cData = terms[cid]
            cName = ""[%s] %s"" % (cid, cData['name'])
            cDesc = cData['desc']
            child = dict(name=cName, desc=cDesc)
            children.append(child)

    return json.dumps(rv, indent=2)",create Tag Groups and Child Tags using data from terms dict,0,0,2,2
"def generateKML(self, save_location, docName, layers, layerOptions=""composite""):
        
        kmlURL = self._url + ""/generateKml""
        params= {
            ""f"" : ""json"",
            'docName' : docName,
            'layers' : layers,
            'layerOptions': layerOptions}
        return self._get(url=kmlURL,
                         out_folder=save_location,
                         param_dict=params,
                         securityHandler=self._securityHandler,
                         proxy_url=self._proxy_url,
                         proxy_port=self._proxy_port
                         )","The generateKml operation is performed on a map service resource.
           The result of this operation is a KML document wrapped in a KMZ
           file. The document contains a network link to the KML Service
           endpoint with properties and parameters you specify.
           Inputs:
              docName - The name of the resulting KML document. This is the
                        name that appears in the Places panel of Google
                        Earth.
              layers - the layers to perform the generateKML operation on.
                       The layers are specified as a comma-separated list
                       of layer ids.
              layerOptions - The layer drawing options. Based on the option
                             chosen, the layers are drawn as one composite
                             image, as separate images, or as vectors. When
                             the KML capability is enabled, the ArcGIS
                             Server administrator has the option of setting
                             the layer operations allowed. If vectors are
                             not allowed, then the caller will not be able
                             to get vectors. Instead, the caller receives a
                             single composite image.
                             values: composite | separateImage |
                                     nonComposite",0,1,0,1
"def generate_annotation(result, settings):
    
    if result.status:
        return """"

    header = (""\n\n""
              ""({label}   ; MUK_ANNOTATION\n""
              ""    (Color {color})   ; MUK_ANNOTATION\n""
              ""    (Name \""{name}\"")   ; MUK_ANNOTATION"").format(**settings)

    points = [p for _, _points in result.info for p in _points]

    annotations = (""    ({0} {1} {2} 0.50)   ; MUK_ANNOTATION"".format(
        p[COLS.X], p[COLS.Y], p[COLS.Z]) for p in points)
    footer = "")   ; MUK_ANNOTATION\n""

    return '\n'.join(chain.from_iterable(([header], annotations, [footer])))","Generate the annotation for a given checker

    Arguments
        neuron(Neuron): The neuron object
        checker: A tuple where the first item is the checking function (usually from neuron_checks)
                 and the second item is a dictionary of settings for the annotation. It must
                 contain the keys name, label and color
    Returns
        An S-expression-like string representing the annotation",0,0,2,2
"def generate_dataset(number_items=1000):
    
    data = []
    names = get_names()
    totalnames = len(names)
    
    random.seed()
    

    for i in range(number_items):
        data.append({""name"":names[random.randint(0,totalnames-1)],
                     ""age"":random.randint(1,100),
                     ""description"":li_words(50, False)})
    return data",Generate a dataset with number_items elements.,0,0,1,1
"def generate_env(self):
        
        for key in sorted(list(self.spec.keys())):
            if self.spec[key]['type'] in (dict, list):
                value = f""\'{json.dumps(self.spec[key].get('example', ''))}\'""
            else:
                value = f""{self.spec[key].get('example', '')}""
            print(f""export {self.env_prefix}_{key.upper()}={value}"")",Generate sample environment variables,0,0,2,2
"def generate_key(key_length=64):
    
    if hasattr(random, 'SystemRandom'):
        logging.info('Generating a secure random key using SystemRandom.')
        choice = random.SystemRandom().choice
    else:
        msg = ""WARNING: SystemRandom not present. Generating a random ""\
              ""key using random.choice (NOT CRYPTOGRAPHICALLY SECURE).""
        logging.warning(msg)
        choice = random.choice
    return ''.join(map(lambda x: choice(string.digits + string.ascii_letters),
                   range(key_length)))","Secret key generator.

    The quality of randomness depends on operating system support,
    see http://docs.python.org/library/random.html#random.SystemRandom.",0,2,1,3
"def generate_lifetime_subparser(subparsers):
    
    parser = subparsers.add_parser(
        'lifetime', description=constants.LIFETIME_DESCRIPTION,
        epilog=constants.LIFETIME_EPILOG, formatter_class=ParagraphFormatter,
        help=constants.LIFETIME_HELP)
    parser.set_defaults(func=lifetime_report)
    utils.add_tokenizer_argument(parser)
    utils.add_common_arguments(parser)
    utils.add_query_arguments(parser)
    parser.add_argument('results', help=constants.LIFETIME_RESULTS_HELP,
                        metavar='RESULTS')
    parser.add_argument('label', help=constants.LIFETIME_LABEL_HELP,
                        metavar='LABEL')
    parser.add_argument('output', help=constants.REPORT_OUTPUT_HELP,
                        metavar='OUTPUT')",Adds a sub-command parser to `subparsers` to make a lifetime report.,0,0,1,1
"def generate_look_up_table():
    
    poly = 0xA001
    table = []

    for index in range(256):

        data = index << 1
        crc = 0
        for _ in range(8, 0, -1):
            data >>= 1
            if (data ^ crc) & 0x0001:
                crc = (crc >> 1) ^ poly
            else:
                crc >>= 1
        table.append(crc)

    return table","Generate look up table.

    :return: List",0,0,1,1
"def generate_mechanism(graph: BELGraph, node: BaseEntity, key: Optional[str] = None) -> BELGraph:
    
    subgraph = get_upstream_causal_subgraph(graph, node)
    expand_upstream_causal(graph, subgraph)
    remove_inconsistent_edges(subgraph)
    collapse_consistent_edges(subgraph)

    if key is not None:  
        prune_mechanism_by_data(subgraph, key)

    return subgraph","Generate a mechanistic sub-graph upstream of the given node.

    :param graph: A BEL graph
    :param node: A BEL node
    :param key: The key in the node data dictionary representing the experimental data.
    :return: A sub-graph grown around the target BEL node",0,0,1,1
"def generate_operators(name, n_vars=1, hermitian=None, commutative=False):
    

    variables = []
    for i in range(n_vars):
        if n_vars > 1:
            var_name = '%s%s' % (name, i)
        else:
            var_name = '%s' % name
        if hermitian is not None and hermitian:
            variables.append(HermitianOperator(var_name))
        else:
            variables.append(Operator(var_name))
        variables[-1].is_commutative = commutative
    return variables","Generates a number of commutative or noncommutative operators

    :param name: The prefix in the symbolic representation of the noncommuting
                 variables. This will be suffixed by a number from 0 to
                 n_vars-1 if n_vars > 1.
    :type name: str.
    :param n_vars: The number of variables.
    :type n_vars: int.
    :param hermitian: Optional parameter to request Hermitian variables .
    :type hermitian: bool.
    :param commutative: Optional parameter to request commutative variables.
                        Commutative variables are Hermitian by default.
    :type commutative: bool.

    :returns: list of :class:`sympy.physics.quantum.operator.Operator` or
              :class:`sympy.physics.quantum.operator.HermitianOperator`
              variables

    :Example:

    >>> generate_variables('y', 2, commutative=True)
    ￼[y0, y1]",1,0,2,3
"def get(interface, method, version=1,
        apihost=DEFAULT_PARAMS['apihost'], https=DEFAULT_PARAMS['https'],
        caller=None, session=None, params=None):
    
    url = u""%s://%s/%s/%s/v%s/"" % (
        'https' if https else 'http', apihost, interface, method, version)
    return webapi_request(url, 'GET', caller=caller, session=session, params=params)","Send GET request to an API endpoint

    .. versionadded:: 0.8.3

    :param interface: interface name
    :type interface: str
    :param method: method name
    :type method: str
    :param version: method version
    :type version: int
    :param apihost: API hostname
    :type apihost: str
    :param https: whether to use HTTPS
    :type https: bool
    :param params: parameters for endpoint
    :type params: dict
    :return: endpoint response
    :rtype: :class:`dict`, :class:`lxml.etree.Element`, :class:`str`",0,1,0,1
"def get(self, *args, **kwargs):
        
        if self:
            if args:
                index = args[0]
                if index <= len(self) -1:
                    return self[args[0]]
                return None
            elif kwargs:
                key, value = kwargs.popitem()
                for item in self.items:
                    if getattr(item, key, None) == value:
                        return item
            else:
                raise ValueError('Missing argument. You must provide an '
                    'arg or kwarg to fetch an element from the collection.')","Get an element from the iterable by an arg or kwarg.
        Args can be a single positional argument that is an index
        value to retrieve. If the specified index is out of range,
        None is returned. Otherwise use kwargs to provide a  key/value.
        The key is expected to be a valid attribute of the iterated class.
        For example, to get an element that has a attribute name of 'foo',
        pass name='foo'.
        
        :raises ValueError: An argument was missing
        :return: the specified item, type is based on what is
            returned by this iterable, may be None",1,0,2,3
"def get(self, *args, **kwargs):
        
        self.session.username = 'gmr'

        session = self.session.as_dict()
        if session['last_request_at']:
            session['last_request_at'] = str(date.fromtimestamp(
                                             session['last_request_at']))

        
        self.write({'message': 'Hello World',
                    'request': {'method': self.request.method,
                                'protocol': self.request.protocol,
                                'path': self.request.path,
                                'query': self.request.query,
                                'remote_ip': self.request.remote_ip,
                                'version': self.request.version},
                    'session': session,
                    'tinman': {'version':  __version__}})
        self.finish()","Example HTTP Get response method.

        :param args: positional args
        :param kwargs: keyword args",0,1,0,1
"def get(self, id, lazy=False, **kwargs):
        
        if not isinstance(id, int):
            id = id.replace('/', '%2F')
        path = '%s/%s' % (self.path, id)
        if lazy is True:
            return self._obj_cls(self, {self._obj_cls._id_attr: id})
        server_data = self.gitlab.http_get(path, **kwargs)
        return self._obj_cls(self, server_data)","Retrieve a single object.

        Args:
            id (int or str): ID of the object to retrieve
            lazy (bool): If True, don't request the server, but create a
                         shallow object giving access to the managers. This is
                         useful if you want to avoid useless calls to the API.
            **kwargs: Extra options to send to the server (e.g. sudo)

        Returns:
            object: The generated RESTObject.

        Raises:
            GitlabAuthenticationError: If authentication is not correct
            GitlabGetError: If the server cannot perform the request",0,1,2,3
"def get(self, id=None, slug=None, **kwargs):
        

        if id:
            items = self.ghost.execute_get('%s/%s/' % (self._type_name, id), **kwargs)

        elif slug:
            items = self.ghost.execute_get('%s/slug/%s/' % (self._type_name, slug), **kwargs)

        else:
            raise GhostException(
                500, 'Either the ID or the Slug of the resource needs to be specified'
            )

        return self._model_type(items[self._type_name][0])","Fetch a resource from the API.
        Either the `id` or the `slug` has to be present.

        :param id: The ID of the resource
        :param slug: The slug of the resource
        :param kwargs: Parameters for the request
            (see from and below https://api.ghost.org/docs/limit)
        :return: The item returned by the API
            wrapped as a `Model` object",1,2,0,3
"def get(self, key):
        

        if isinstance(key, tuple):
            
            try:
                x, y = float(key[0]), float(key[1])
            except IndexError:
                raise ValueError(""Two values are required for a coordinate pair"")

            except ValueError:
                raise ValueError(""Only float or float-coercable values can be passed"")

            key = ""{0},{1}"".format(x, y)

        return self[self.lookups[key]]","Returns an individual Location by query lookup, e.g. address or point.",2,0,3,5
"def get(self, key, default=CACHE_MISS):
        
        if not self.options.enabled:
            return CACHE_DISABLED
        ret = default
        if self.has(key):
            ret = self._dict[key].value
        logger.debug('get({}, default={}) == {}'.format(repr(key), repr(default), repr(ret)))
        return ret","Get a value out of the cache

        Returns CACHE_DISABLED if the cache is disabled

        :param key: key to search for
        :param default: value to return if the key is not found (defaults to CACHE_MISS)",0,1,1,2
"def get(self, keys, default=None):
        r
        
        if isinstance(keys, list):
            ret = {}
            for k in keys:
                ret[k] = super().get(k, default)
        else:  
            ret = super().get(keys, default)
        return ret","r""""""
        This subclassed method can be used to obtain a dictionary containing
        subset of data on the object

        Parameters
        ----------
        keys : string or list of strings
            The item or items to retrieve.

        default : any object
            The value to return in the event that the requested key(s) is not
            found.  The default is ``None``.

        Returns
        -------
        If a single string is given in ``keys``, this method behaves exactly
        as the ``dict's`` native ``get`` method and returns just the item
        requested (or the ``default`` if not found).  If, however, a list of
        strings is received, then a dictionary containing each of the
        requested items is returned.

        Notes
        -----
        This is useful for creating Pandas Dataframes of a specific subset of
        data.  Note that a Dataframe can be initialized with a ``dict``, but
        all columns must be the same length.  (e.g. ``df = pd.Dataframe(d)``)

        Examples
        --------
        >>> import openpnm as op
        >>> pn = op.network.Cubic(shape=[5, 5, 5])
        >>> pore_props = pn.props(element='pore')
        >>> subset = pn.get(keys=pore_props)
        >>> print(len(subset))  # Only pore.coords, so gives dict with 1 array
        1
        >>> subset = pn.get(['pore.top', 'pore.bottom'])
        >>> print(len(subset))  # Returns a dict with the 2 requested array
        2

        It behaves exactly as normal with a dict key string is supplied:

        >>> array = pn.get('pore.coords')
        >>> print(array.shape)  # Returns requested array
        (125, 3)",0,0,3,3
"def getAdditionalImages(self, setID):
        

        params = {
            'apiKey': self.apiKey,
            'setID': setID
        }
        url = Client.ENDPOINT.format('getAdditionalImages')
        returned = get(url, params=params)
        self.checkResponse(returned)

        
        root = ET.fromstring(returned.text)
        urlList = []

        for imageHolder in root:
            urlList.append(imageHolder[-1].text)
        return urlList","Gets a list of URLs containing images of the set.

        :param str setID: The ID of the set you want to grab the images for.
        :returns: A list of URL strings.
        :rtype: list
        .. warning:: An empty list will be returned if there are no additional images, or if the set ID is invalid.",0,1,1,2
"def getAttribute(self, name, ns=None, default=None):
        
        if ns is None:
            prefix, name = splitPrefix(name)
            if prefix is None:
                ns = None
            else:
                ns = self.resolvePrefix(prefix)
        for a in self.attributes:
            if a.match(name, ns):
                return a
        return default","Get an attribute by name and (optional) namespace
        @param name: The name of a contained attribute (may contain prefix).
        @type name: basestring
        @param ns: An optional namespace
        @type ns: (I{prefix}, I{name})
        @param default: Returned when attribute not-found.
        @type default: L{Attribute}
        @return: The requested attribute object.
        @rtype: L{Attribute}",0,0,1,1
"def getAugYbus(self, U0, gbus):
        
        j = 0 + 1j
        buses = self.case.connected_buses
        nb = len(buses)

        Ybus, _, _ = self.case.getYbus()

        

        
        Sd = array([self.case.s_demand(bus) for bus in buses])
        Yd = conj(Sd) / abs(U0)**2

        xd_tr = array([g.xd_tr for g in self.dyn_generators])

        
        Yg = zeros(nb)
        Yg[gbus] = 1 / (j * xd_tr)

        
        for i in range(nb):
            Ybus[i, i] = Ybus[i, i] + Yg[i] + Yd[i]

        return Ybus","Based on AugYbus.m from MatDyn by Stijn Cole, developed at
        Katholieke Universiteit Leuven. See U{http://www.esat.kuleuven.be/
        electa/teaching/matdyn/} for more information.

        @rtype: csr_matrix
        @return: The augmented bus admittance matrix.",0,0,1,1
"def getCandScoresMapFromSamplesFile(self, profile, sampleFileName):
        

        wmg = profile.getWmg(True)

        
        utilities = dict()
        for cand in wmg.keys():
            utilities[cand] = 0.0

        
        sampleFile = open(sampleFileName)
        for i in range(0, SAMPLESFILEMETADATALINECOUNT):
            sampleFile.readline()
        for i in range(0, self.burnIn):
            sampleFile.readline()

        
        numSamples = 0
        for i in range(0, self.n2*self.n1):
            line = sampleFile.readline()
            if i % self.n1 != 0: continue
            sample = json.loads(line)
            for cand in wmg.keys():
                utilities[cand] += self.utilityFunction.getUtility([cand], sample)
            numSamples += 1
        sampleFile.close()
        for key in utilities.keys():
            utilities[key] = utilities[key]/numSamples

        return utilities","Returns a dictonary that associates the integer representation of each candidate with the 
        Bayesian utilities we approximate from the samples we generated into a file.

        :ivar Profile profile: A Profile object that represents an election profile.
        :ivar str sampleFileName: The name of the input file containing the sample data.",1,0,1,2
"def getClusterPrototypes(self, numClusters, numPrototypes=1):
    
    linkage = self.getLinkageMatrix()
    linkage[:, 2] -= linkage[:, 2].min()

    clusters = scipy.cluster.hierarchy.fcluster(
      linkage, numClusters, criterion=""maxclust"")
    prototypes = []
    clusterSizes = []

    for cluster_id in numpy.unique(clusters):
      ids = numpy.arange(len(clusters))[clusters == cluster_id]
      clusterSizes.append(len(ids))
      if len(ids) > numPrototypes:
        cluster_prototypes = HierarchicalClustering._getPrototypes(
          ids, self._overlaps, numPrototypes)
      else:
        cluster_prototypes = numpy.ones(numPrototypes) * -1
        cluster_prototypes[:len(ids)] = ids
      prototypes.append(cluster_prototypes)

    return numpy.vstack(prototypes).astype(int), numpy.array(clusterSizes)","Create numClusters flat clusters and find approximately numPrototypes
    prototypes per flat cluster. Returns an array with each row containing the
    indices of the prototypes for a single flat cluster.

    @param numClusters (int) Number of flat clusters to return (approximate).

    @param numPrototypes (int) Number of prototypes to return per cluster.

    @returns (tuple of numpy.ndarray) The first element is an array with rows
        containing the indices of the prototypes for a single flat cluster.
        If a cluster has less than numPrototypes members, missing indices are
        filled in with -1. The second element is an array of number of elements
        in each cluster.",0,0,4,4
"def getElementsBy(start_node: ParentNode,
                  cond: Callable[['Element'], bool]) -> NodeList:
    
    elements = []
    for child in start_node.children:
        if cond(child):
            elements.append(child)
        elements.extend(child.getElementsBy(cond))
    return NodeList(elements)","Return list of child elements of start_node which matches ``cond``.

    ``cond`` must be a function which gets a single argument ``Element``,
    and returns boolean. If the node matches requested condition, ``cond``
    should return True.
    This searches all child elements recursively.

    :arg ParentNode start_node:
    :arg cond: Callable[[Element], bool]
    :rtype: NodeList[Element]",0,0,3,3
"def getFailureMessage(failure):
    
    str(failure.type)
    failure.getErrorMessage()
    if len(failure.frames) == 0:
        return ""failure %(exc)s: %(msg)s"" % locals()

    (func, filename, line, some, other) = failure.frames[-1]
    filename = scrubFilename(filename)
    return ""failure %(exc)s at %(filename)s:%(line)s: %(func)s(): %(msg)s"" \
        % locals()","Return a short message based on L{twisted.python.failure.Failure}.
    Tries to find where the exception was triggered.",0,0,1,1
"def getFeature(self, compoundId):
        
        featureId = long(compoundId.featureId)
        with self._db as dataSource:
            featureReturned = dataSource.getFeatureById(featureId)

        if featureReturned is None:
            raise exceptions.ObjectWithIdNotFoundException(compoundId)
        else:
            gaFeature = self._gaFeatureForFeatureDbRecord(featureReturned)
            return gaFeature","Returns a protocol.Feature object corresponding to a compoundId
        :param compoundId: a datamodel.FeatureCompoundId object
        :return: a Feature object.
        :raises: exceptions.ObjectWithIdNotFoundException if invalid
            compoundId is provided.",2,0,2,4
"def getFilename(name):
    
    
    name = re.sub(r""[^0-9a-zA-Z_\-\.]"", ""_"", name)
    
    while "".."" in name:
        name = name.replace('..', '.')
    while ""__"" in name:
        name = name.replace('__', '_')
    
    if name.startswith((""."", ""-"")):
        name = name[1:]
    return name",Get a filename from given name without dangerous or incompatible characters.,0,0,1,1
"def getGerritChanges(props):
        
        if 'gerrit_changes' in props:
            return props.getProperty('gerrit_changes')

        if 'event.change.number' in props:
            return [{
                'change_id': props.getProperty('event.change.number'),
                'revision_id': props.getProperty('event.patchSet.number')
            }]
        return []","Get the gerrit changes

            This method could be overridden if really needed to accommodate for other
            custom steps method for fetching gerrit changes.

            :param props: an IProperty

            :return: (optionally via deferred) a list of dictionary with at list
                change_id, and revision_id,
                which format is the one accepted by the gerrit REST API as of
                /changes/:change_id/revision/:revision_id paths (see gerrit doc)",0,0,2,2
"def getHostDetailsByMACAddress(self, macAddress, lanInterfaceId=1, timeout=1):
        
        namespace = Lan.getServiceType(""getHostDetailsByMACAddress"") + str(lanInterfaceId)
        uri = self.getControlURL(namespace)

        results = self.execute(uri, namespace, ""GetSpecificHostEntry"", timeout=timeout, NewMACAddress=macAddress)

        return HostDetails(results, macAddress=macAddress)","Get host details for a host specified by its MAC address.

        :param str macAddress: MAC address in the form ``38:C9:86:26:7E:38``; be aware that the MAC address might
            be case sensitive, depending on the router
        :param int lanInterfaceId: the id of the LAN interface
        :param float timeout: the timeout to wait for the action to be executed
        :return: return the host details if found otherwise an Exception will be raised
        :rtype: HostDetails",0,1,1,2
"def getResponse(self, http_request, request):
        
        response = remoting.Envelope(request.amfVersion)

        for name, message in request:
            http_request.amf_request = message

            processor = self.getProcessor(message)
            response[name] = processor(message, http_request=http_request)

        return response","Processes the AMF request, returning an AMF response.

        @param http_request: The underlying HTTP Request.
        @type http_request: U{HTTPRequest<http://docs.djangoproject.com
            /en/dev/ref/request-response/#httprequest-objects>}
        @param request: The AMF Request.
        @type request: L{Envelope<pyamf.remoting.Envelope>}
        @rtype: L{Envelope<pyamf.remoting.Envelope>}",0,2,1,3
"def getStats(self):
        
        url = ""%s://%s:%d/%s"" % (self._proto, self._host, self._port, 
                                 self._monpath)
        response = util.get_url(url, self._user, self._password)
        stats = {}
        for line in response.splitlines():
            mobj = re.match('([\w\s]+):\s+(\w+)$', line)
            if mobj:
                stats[mobj.group(1)] = util.parse_value(mobj.group(2))
        return stats",Query and parse Web Server Status Page.,0,1,1,2
"def getTableMisnestedNodePosition(self):
        
        
        
        
        lastTable = None
        fosterParent = None
        insertBefore = None
        for elm in self.openElements[::-1]:
            if elm.name == ""table"":
                lastTable = elm
                break
        if lastTable:
            
            
            if lastTable.parent:
                fosterParent = lastTable.parent
                insertBefore = lastTable
            else:
                fosterParent = self.openElements[
                    self.openElements.index(lastTable) - 1]
        else:
            fosterParent = self.openElements[0]
        return fosterParent, insertBefore","Get the foster parent element, and sibling to insert before
        (or None) when inserting a misnested table node",0,0,1,1
"def getUpdaters(self):
        
        how_many_update = int(round(self.UpdatePrb*self.AgentCount))
        base_bool = np.zeros(self.AgentCount,dtype=bool)
        base_bool[0:how_many_update] = True
        self.update = self.RNG.permutation(base_bool)
        self.dont = np.logical_not(self.update)","Determine which agents update this period vs which don't.  Fills in the
        attributes update and dont as boolean arrays of size AgentCount.

        Parameters
        ----------
        None

        Returns
        -------
        None",0,0,2,2
"def get_Note(self, string=0, fret=0, maxfret=24):
        
        if 0 <= string < self.count_strings():
            if 0 <= fret <= maxfret:
                s = self.tuning[string]
                if type(s) == list:
                    s = s[0]
                n = Note(int(s) + fret)
                n.string = string
                n.fret = fret
                return n
            else:
                raise RangeError(""Fret '%d' on string '%d' is out of range""
                        % (string, fret))
        else:
            raise RangeError(""String '%d' out of range"" % string)","Return the Note on 'string', 'fret'.

        Throw a RangeError if either the fret or string is unplayable.

        Examples:
        >>> t = tunings.StringTuning('test', 'test', ['A-3', 'A-4'])
        >>> t,get_Note(0, 0)
        'A-3'
        >>> t.get_Note(0, 1)
        'A#-3'
        >>> t.get_Note(1, 0)
        'A-4'",2,0,3,5
"def get_absolute_url(self):
        
        url_name = ""blog_post_detail""
        kwargs = {""slug"": self.slug}
        date_parts = (""year"", ""month"", ""day"")
        if settings.BLOG_URLS_DATE_FORMAT in date_parts:
            url_name = ""blog_post_detail_%s"" % settings.BLOG_URLS_DATE_FORMAT
            for date_part in date_parts:
                date_value = str(getattr(self.publish_date, date_part))
                if len(date_value) == 1:
                    date_value = ""0%s"" % date_value
                kwargs[date_part] = date_value
                if date_part == settings.BLOG_URLS_DATE_FORMAT:
                    break
        return reverse(url_name, kwargs=kwargs)","URLs for blog posts can either be just their slug, or prefixed
        with a portion of the post's publish date, controlled by the
        setting ``BLOG_URLS_DATE_FORMAT``, which can contain the value
        ``year``, ``month``, or ``day``. Each of these maps to the name
        of the corresponding urlpattern, and if defined, we loop through
        each of these and build up the kwargs for the correct urlpattern.
        The order which we loop through them is important, since the
        order goes from least granular (just year) to most granular
        (year/month/day).",0,0,1,1
"def get_access_info(self, unscoped_auth):
        
        try:
            unscoped_auth_ref = base.BasePlugin.get_access_info(
                self, unscoped_auth)
        except exceptions.KeystoneAuthException as excp:
            msg = _('Service provider authentication failed. %s')
            raise exceptions.KeystoneAuthException(msg % str(excp))
        return unscoped_auth_ref","Get the access info object

        We attempt to get the auth ref. If it fails and if the K2K auth plugin
        was being used then we will prepend a message saying that the error was
        on the service provider side.
        :param: unscoped_auth: Keystone auth plugin for unscoped user
        :returns: keystoneclient.access.AccessInfo object",1,0,2,3
"def get_all(self, page=None, per_page=None, include_totals=False):
        

        params = {
            'page': page,
            'per_page': per_page,
            'include_totals': str(include_totals).lower()
        }

        return self.client.get(self._url(), params=params)","Retrieves all resource servers

        Args:
            page (int, optional): The result's page number (zero based).

            per_page (int, optional): The amount of entries per page.

            include_totals (bool, optional): True if the query summary is
                to be included in the result, False otherwise.

        See: https://auth0.com/docs/api/management/v2#!/Resource_Servers/get_resource_servers",0,1,0,1
"def get_all(self, start=0, count=-1, filter='', query='', sort='', view='', fields='', uri=None, scope_uris=''):
        

        uri = self.build_query_uri(start=start, count=count, filter=filter,
                                   query=query, sort=sort, view=view, fields=fields, uri=uri, scope_uris=scope_uris)

        logger.debug('Getting all resources with uri: {0}'.format(uri))

        result = self.__do_requests_to_getall(uri, count)

        return result","Gets all items according with the given arguments.

        Args:
            start:
                The first item to return, using 0-based indexing.
                If not specified, the default is 0 - start with the first available item.
            count:
                The number of resources to return. A count of -1 requests all items (default).
            filter (list or str):
                A general filter/query string to narrow the list of items returned. The default is no
                filter; all resources are returned.
            query:
                A single query parameter can do what would take multiple parameters or multiple GET requests using
                filter. Use query for more complex queries. NOTE: This parameter is experimental for OneView 2.0.
            sort:
                The sort order of the returned data set. By default, the sort order is based on create time with the
                oldest entry first.
            view:
                Returns a specific subset of the attributes of the resource or collection by specifying the name of a
                predefined view. The default view is expand (show all attributes of the resource and all elements of
                the collections or resources).
            fields:
                Name of the fields.
            uri:
                A specific URI (optional)
            scope_uris:
                An expression to restrict the resources returned according to the scopes to
                which they are assigned.

        Returns:
            list: A list of items matching the specified filter.",0,1,0,1
"def get_all_account_users(self, account_id, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('asynchronous'):
            return self.get_all_account_users_with_http_info(account_id, **kwargs)  
        else:
            (data) = self.get_all_account_users_with_http_info(account_id, **kwargs)  
            return data","Get all user details.  # noqa: E501

        An endpoint for retrieving details of all users.   **Example usage:** `curl https://api.us-east-1.mbedcloud.com/v3/accounts/{accountID}/users -H 'Authorization: Bearer API_KEY'`  # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass asynchronous=True
        >>> thread = api.get_all_account_users(account_id, asynchronous=True)
        >>> result = thread.get()

        :param asynchronous bool
        :param str account_id: Account ID. (required)
        :param int limit: The number of results to return (2-1000), default is 50.
        :param str after: The entity ID to fetch after the given one.
        :param str order: The order of the records based on creation time, ASC or DESC; by default ASC
        :param str include: Comma separated additional data to return. Currently supported: total_count
        :param str email__eq: Filter for email address
        :param str status__eq: Filter for status
        :param str status__in: An optional filter for getting users with a specified set of statuses.
        :param str status__nin: An optional filter for excluding users with a specified set of statuses.
        :return: UserInfoRespList
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def get_all_bundle_tasks(self, bundle_ids=None, filters=None):
        

        params = {}
        if bundle_ids:
            self.build_list_params(params, bundle_ids, 'BundleId')
        if filters:
            self.build_filter_params(params, filters)
        return self.get_list('DescribeBundleTasks', params,
                             [('item', BundleInstanceTask)], verb='POST')","Retrieve current bundling tasks. If no bundle id is specified, all
        tasks are retrieved.

        :type bundle_ids: list
        :param bundle_ids: A list of strings containing identifiers for
                           previously created bundling tasks.

        :type filters: dict
        :param filters: Optional filters that can be used to limit
                        the results returned.  Filters are provided
                        in the form of a dictionary consisting of
                        filter names as the key and filter values
                        as the value.  The set of allowable filter
                        names/values is dependent on the request
                        being performed.  Check the EC2 API guide
                        for details.",0,0,2,2
"def get_all_dashboard(self, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.get_all_dashboard_with_http_info(**kwargs)  
        else:
            (data) = self.get_all_dashboard_with_http_info(**kwargs)  
            return data","Get all dashboards for a customer  # noqa: E501

          # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.get_all_dashboard(async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param int offset:
        :param int limit:
        :return: ResponseContainerPagedDashboard
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def get_all_hep(self):
        
        return dict(
            before_request_hook=[plugin[""plugin_hep""][""before_request_hook""] for plugin in self.get_enabled_plugins if plugin[""plugin_hep""].get(""before_request_hook"")],
            before_request_top_hook=[plugin[""plugin_hep""][""before_request_top_hook""] for plugin in self.get_enabled_plugins if plugin[""plugin_hep""].get(""before_request_top_hook"")],
            after_request_hook=[plugin[""plugin_hep""][""after_request_hook""] for plugin in self.get_enabled_plugins if plugin[""plugin_hep""].get(""after_request_hook"")],
            teardown_request_hook=[plugin[""plugin_hep""][""teardown_request_hook""] for plugin in self.get_enabled_plugins if plugin[""plugin_hep""].get(""teardown_request_hook"")],
        )","Hook extension point.

        * before_request_hook, Before request (intercept requests are allowed)

        * before_request_top_hook, Before top request (Put it first)

        * after_request_hook, After request (no exception before return)

        * teardown_request_hook, After request (before return, with or without exception)",0,0,1,1
"def get_all_ids(idVendor = GrizzlyUSB.ID_VENDOR, idProduct=GrizzlyUSB.ID_PRODUCT):
        
        all_dev = GrizzlyUSB.get_all_usb_devices(idVendor, idProduct)
            
        if len(all_dev) <= 0:
            raise usb.USBError(""Could not find any GrizzlyBear device (idVendor=%d, idProduct=%d)"" % (idVendor, idProduct))
        else:
            all_addresses = []

            
            bound_devices = []
            for device in all_dev:
                internal_addr = GrizzlyUSB.get_device_address(device)

                if internal_addr == GrizzlyUSB.USB_DEVICE_ERROR: 
                    bound_devices.append(device)
                else:
                    all_addresses.append(internal_addr)

            
            for device in all_dev:
                if device not in bound_devices:
                    usb.util.dispose_resources(device)
            return map(addr_to_id, all_addresses)","Scans for grizzlies that have not been bound, or constructed, and returns a list
        of their id's, or motor number.",1,0,2,3
"def get_all_images_count(self):
        
        self_imgs = self.image_set.count()
        update_ids = self.update_set.values_list('id', flat=True)
        u_images = UpdateImage.objects.filter(update__id__in=update_ids).count()
        count = self_imgs + u_images

        return count",Gets count of all images from both event and updates.,0,0,1,1
"def get_all_load_balancers(self, load_balancer_names=None):
        
        params = {}
        if load_balancer_names:
            self.build_list_params(params, load_balancer_names,
                                   'LoadBalancerNames.member.%d')
        return self.get_list('DescribeLoadBalancers', params,
                             [('member', LoadBalancer)])","Retrieve all load balancers associated with your account.

        :type load_balancer_names: list
        :keyword load_balancer_names: An optional list of load balancer names.

        :rtype: :py:class:`boto.resultset.ResultSet`
        :return: A ResultSet containing instances of
            :class:`boto.ec2.elb.loadbalancer.LoadBalancer`",0,0,2,2
"def get_all_results(starting_page):
    
    logging.info('Retrieving all results for {}'.format(starting_page))
    page = starting_page
    results = []

    while True:
        logging.debug('Getting data from: {}'.format(page))
        data = get_page(page)
        logging.debug('JSON data: {}'.format(data))
        results = results + data['results']

        if data['next']:
            page = data['next']
        else:
            break

    return results","Given starting API query for Open Humans, iterate to get all results.

    :param starting page: This field is the first page, starting from which
        results will be obtained.",0,2,1,3
"def get_all_server_credentials(self, authorization, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('asynchronous'):
            return self.get_all_server_credentials_with_http_info(authorization, **kwargs)  
        else:
            (data) = self.get_all_server_credentials_with_http_info(authorization, **kwargs)  
            return data","Fetch all (Bootstrap and LwM2M) server credentials.  # noqa: E501

        This REST API is intended to be used by customers to fetch all (Bootstrap and LwM2M) server credentials that they will need to use with their clients to connect to bootstrap or LwM2M server.  **Example usage:** curl -X GET \""http://api.us-east-1.mbedcloud.com/v3/server-credentials\"" -H \""accept: application/json\"" -H \""Authorization: Bearer THE_ACCESS_TOKEN\""   # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass asynchronous=True
        >>> thread = api.get_all_server_credentials(authorization, asynchronous=True)
        >>> result = thread.get()

        :param asynchronous bool
        :param str authorization: Bearer {Access Token}.  (required)
        :return: AllServerCredentialsResponseData
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def get_all_usb_devices(idVendor, idProduct):
        
        all_dev = list(usb.core.find(find_all = True, idVendor = idVendor, idProduct = idProduct))
        for dev in all_dev:
            try:
                dev.detach_kernel_driver(0)
            except usb.USBError:
                pass
        return all_dev",Returns a list of all the usb devices matching the provided vendor ID and product ID.,0,0,1,1
"def get_all_users(path_prefix='/', region=None, key=None, keyid=None,
                 profile=None):
    
    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)
    if not conn:
        return None
    _users = conn.get_all_users(path_prefix=path_prefix)
    users = _users.list_users_response.list_users_result.users
    marker = getattr(
        _users.list_users_response.list_users_result, 'marker', None
    )
    while marker:
        _users = conn.get_all_users(path_prefix=path_prefix, marker=marker)
        users = users + _users.list_users_response.list_users_result.users
        marker = getattr(
            _users.list_users_response.list_users_result, 'marker', None
        )
    return users","Get and return all IAM user details, starting at the optional path.

    .. versionadded:: 2016.3.0

    CLI Example:

        salt-call boto_iam.get_all_users",1,0,1,2
"def get_api_version(base_url, api_version=None, timeout=10, verify=True):
    
    versions = available_api_versions(base_url, timeout, verify)
    
    newest_version = max([float(i) for i in versions])
    if api_version is None:  
        api_version = newest_version
    else:
        if api_version not in versions:
            api_version = newest_version
    
    return api_version","Get the API version specified or resolve the latest version

    :return api version
    :rtype: float",0,1,1,2
"def get_appointment_group(self, appointment_group):
        
        from canvasapi.appointment_group import AppointmentGroup

        appointment_group_id = obj_or_id(
            appointment_group, ""appointment_group"", (AppointmentGroup,)
        )

        response = self.__requester.request(
            'GET',
            'appointment_groups/{}'.format(appointment_group_id)
        )
        return AppointmentGroup(self.__requester, response.json())","Return single Appointment Group by id

        :calls: `GET /api/v1/appointment_groups/:id \
        <https://canvas.instructure.com/doc/api/appointment_groups.html#method.appointment_groups.show>`_

        :param appointment_group: The ID of the appointment group.
        :type appointment_group: :class:`canvasapi.appointment_group.AppointmentGroup` or int

        :rtype: :class:`canvasapi.appointment_group.AppointmentGroup`",0,1,0,1
"def get_argument(self, name, default=_ARG_DEFAULT, strip=True):
        
        args = self.get_arguments(name, strip=strip)
        if not args:
            if default is self._ARG_DEFAULT:
                raise HTTPError(400, ""Missing argument %s"" % name)
            return default
        return args[-1]","Returns the value of the argument with the given name.

        If default is not provided, the argument is considered to be
        required, and we throw an HTTP 400 exception if it is missing.

        If the argument appears in the url more than once, we return the
        last value.

        The returned value is always unicode.",1,0,2,3
"def get_artists_hot_songs(self, artist_id):
        
        url = 'http://music.163.com/api/artist/{}'.format(artist_id)
        result = self.get_request(url)

        hot_songs = result['hotSongs']
        songs = [Song(song['id'], song['name']) for song in hot_songs]
        return songs","Get a artist's top50 songs.

        warning: use old api.
        :params artist_id: artist id.
        :return: a list of Song object.",0,1,0,1
"def get_as_csv(self, output_file_path: Optional[str] = None) -> str:
        
        output = StringIO() if not output_file_path else open(output_file_path, 'w')
        try:
            csv_writer = csv.writer(output)

            csv_writer.writerow(self.columns)
            for row in self.rows:
                csv_writer.writerow(row)
            output.seek(0)
            return output.read()
        finally:
            output.close()","Returns the table object as a CSV string.

        :param output_file_path: The output file to save the CSV to, or None.
        :return: CSV representation of the table.",1,0,0,1
"def get_assessment_offered_admin_session_for_bank(self, bank_id, proxy):
        
        if not self.supports_assessment_offered_admin():
            raise errors.Unimplemented()
        
        
        
        
        return sessions.AssessmentOfferedAdminSession(bank_id, proxy, self._runtime)","Gets the ``OsidSession`` associated with the assessment offered admin service for the given bank.

        arg:    bank_id (osid.id.Id): the ``Id`` of the bank
        arg:    proxy (osid.proxy.Proxy): a proxy
        return: (osid.assessment.AssessmentOfferedAdminSession) - an
                ``AssessmentOfferedAdminSession``
        raise:  NotFound - ``bank_id`` not found
        raise:  NullArgument - ``bank_id`` or ``proxy`` is ``null``
        raise:  OperationFailed - ``unable to complete request``
        raise:  Unimplemented - ``supports_assessment_offered_admin()``
                or ``supports_visible_federation()`` is ``false``
        *compliance: optional -- This method must be implemented if
        ``supports_assessment_offered_admin()`` and
        ``supports_visible_federation()`` are ``true``.*",1,1,1,3
"def get_assessment_offered_lookup_session_for_bank(self, bank_id):
        
        if not self.supports_assessment_offered_lookup():
            raise errors.Unimplemented()
        
        
        
        
        return sessions.AssessmentOfferedLookupSession(bank_id, runtime=self._runtime)","Gets the ``OsidSession`` associated with the assessment offered lookup service for the given bank.

        arg:    bank_id (osid.id.Id): the ``Id`` of the bank
        return: (osid.assessment.AssessmentOfferedLookupSession) - an
                ``AssessmentOfferedLookupSession``
        raise:  NotFound - ``bank_id`` not found
        raise:  NullArgument - ``bank_id`` is ``null``
        raise:  OperationFailed - ``unable to complete request``
        raise:  Unimplemented - ``supports_assessment_offered_lookup()``
                or ``supports_visible_federation()`` is ``false``
        *compliance: optional -- This method must be implemented if
        ``supports_assessment_offered_lookup()`` and
        ``supports_visible_federation()`` are ``true``.*",1,1,1,3
"def get_asset_lookup_session(self, proxy, *args, **kwargs):
        
        if not self.supports_asset_lookup():
            raise Unimplemented()
        try:
            from . import sessions
        except ImportError:
            raise  
        proxy = self._convert_proxy(proxy)
        try:
            session = sessions.AssetLookupSession(proxy=proxy, runtime=self._runtime, **kwargs)
        except AttributeError:
            raise  
        return session","Gets the OsidSession associated with the asset lookup service.

        arg     proxy (osid.proxy.Proxy): a proxy
        return: (osid.repository.AssetLookupSession) - the new
                AssetLookupSession
        raise:  OperationFailed - unable to complete request
        raise:  Unimplemented - supports_asset_lookup() is false
        compliance: optional - This method must be implemented if
                    supports_asset_lookup() is true.",3,1,0,4
"def get_assets(self):
        
        
        
        
        collection = JSONClientValidated('repository',
                                         collection='Asset',
                                         runtime=self._runtime)
        result = collection.find(self._view_filter()).sort('_id', DESCENDING)
        return objects.AssetList(result, runtime=self._runtime, proxy=self._proxy)","Gets all ``Assets``.

        In plenary mode, the returned list contains all known assets or
        an error results. Otherwise, the returned list may contain only
        those assets that are accessible through this session.

        return: (osid.repository.AssetList) - a list of ``Assets``
        raise:  OperationFailed - unable to complete request
        raise:  PermissionDenied - authorization failure
        *compliance: mandatory -- This method must be implemented.*",0,1,1,2
"def get_authenticate_kwargs(oauth_credentials=None, http_=None):
    
    if oauth_credentials:
        authenticate_kwargs = {
            ""credentials"": oauth_credentials
        }
    elif http_:
        authenticate_kwargs = {
            ""http"": http_
        }
    else:
        
        try:
            
            credentials, _ = google.auth.default()
            authenticate_kwargs = {
                ""credentials"": credentials
            }
        except google.auth.exceptions.DefaultCredentialsError:
            
            authenticate_kwargs = {
                ""http"": httplib2.Http()
            }

    return authenticate_kwargs","Returns a dictionary with keyword arguments for use with discovery

    Prioritizes oauth_credentials or a http client provided by the user
    If none provided, falls back to default credentials provided by google's command line
    utilities. If that also fails, tries using httplib2.Http()

    Used by `gcs.GCSClient` and `bigquery.BigQueryClient` to initiate the API Client",0,1,2,3
"def get_authorization_header():
    
    header = request.environ.get('HTTP_AUTHORIZATION')

    if not header:
        return None

    header = wsgi_to_bytes(header)

    try:
        auth_type, auth_info = header.split(None, 1)
        auth_type = auth_type.lower()
    except ValueError:
        return None

    return auth_type, auth_info","Return request's 'Authorization:' header as
    a two-tuple of (type, info).",0,0,2,2
"def get_banks_by_assessment_taken(self, assessment_taken_id):
        
        
        
        mgr = self._get_provider_manager('ASSESSMENT', local=True)
        lookup_session = mgr.get_bank_lookup_session(proxy=self._proxy)
        return lookup_session.get_banks_by_ids(
            self.get_bank_ids_by_assessment_taken(assessment_taken_id))","Gets the list of ``Banks`` mapped to an ``AssessmentTaken``.

        arg:    assessment_taken_id (osid.id.Id): ``Id`` of an
                ``AssessmentTaken``
        return: (osid.assessment.BankList) - list of banks
        raise:  NotFound - ``assessment_taken_id`` is not found
        raise:  NullArgument - ``assessment_taken_id`` is ``null``
        raise:  OperationFailed - unable to complete request
        raise:  PermissionDenied - authorization failure occurred
        *compliance: mandatory -- This method must be implemented.*",0,2,1,3
"def get_billing_report_firmware_updates(self, month, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('asynchronous'):
            return self.get_billing_report_firmware_updates_with_http_info(month, **kwargs)  
        else:
            (data) = self.get_billing_report_firmware_updates_with_http_info(month, **kwargs)  
            return data","Get raw billing data of the firmware updates for the month.  # noqa: E501

        Fetch raw billing data of the firmware updates for the currently authenticated commercial non-subtenant account. This is supplementary data for the billing report. The raw billing data of the firmware updates for subtenant accounts are included in their aggregator's raw billing data of the firmware updates. The endpoint returns the URL to download the gzipped CSV file. The first line is the header providing information on the firmware updates. For example, the ID of an firmware update.  **Example usage:**      curl -X GET https://api.us-east-1.mbedcloud.com/v3/billing-report-firmware-updates?month=2018-07 -H 'authorization: Bearer {api-key}'  # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass asynchronous=True
        >>> thread = api.get_billing_report_firmware_updates(month, asynchronous=True)
        >>> result = thread.get()

        :param asynchronous bool
        :param str month: Queried year and month of billing report. (required)
        :return: BillingReportRawDataResponse
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def get_bounding_box(df_points):
    
    xy_min = df_points[['x', 'y']].min()
    xy_max = df_points[['x', 'y']].max()

    wh = xy_max - xy_min
    wh.index = 'width', 'height'
    bbox = pd.concat([xy_min, wh])
    bbox.name = 'bounding_box'
    return bbox",Calculate the bounding box of all points in a data frame.,0,0,1,1
"def get_bounds(self):
        
        start, end = None, None
        if len(self._weather_series) == 0:
            return start, end

        for i in (0, -1):
            
            if self.has_tuple_instants:
                row = self._weather_series.iloc[i, :]
                instant = dt.datetime(row[""year""], row[""month""], row[""day""], row[""hour""], row[""minute""])
            else:
                instant = self._weather_series.index[i].to_pydatetime()

            
            if i == 0:
                start = instant
            else:
                end = instant

        return start, end","Returns
        -------
        (start, end)

        Datetime instants of beginning and end of data. If no data, will be: (None, None).",0,0,2,2
"def get_bounds(self, R):
        

        max_sigma = self._get_max_sigma(R)
        final_lower = np.zeros(self.K * (self.n_dim + 1))
        final_lower[0:self.K * self.n_dim] =\
            np.tile(np.nanmin(R, axis=0), self.K)
        final_lower[self.K * self.n_dim:] =\
            np.repeat(self.lower_ratio * max_sigma, self.K)
        final_upper = np.zeros(self.K * (self.n_dim + 1))
        final_upper[0:self.K * self.n_dim] =\
            np.tile(np.nanmax(R, axis=0), self.K)
        final_upper[self.K * self.n_dim:] =\
            np.repeat(self.upper_ratio * max_sigma, self.K)
        bounds = (final_lower, final_upper)
        return bounds","Calculate lower and upper bounds for centers and widths

        Parameters
        ----------

        R : 2D array, with shape [n_voxel, n_dim]
            The coordinate matrix of fMRI data from one subject


        Returns
        -------

        bounds : 2-tuple of array_like, default: None
            The lower and upper bounds on factor's centers and widths.",0,0,2,2
"def get_brain(brain_or_object):
    
    if is_brain(brain_or_object):
        return brain_or_object
    if is_root(brain_or_object):
        return brain_or_object

    
    uid = get_uid(brain_or_object)
    uc = get_tool(""uid_catalog"")
    results = uc({""UID"": uid}) or search(query={'UID': uid})
    if len(results) == 0:
        return None
    if len(results) > 1:
        fail(500, ""More than one object with UID={} found in portal_catalog"".format(uid))
    return results[0]","Return a ZCatalog brain for the object

    :param brain_or_object: A single catalog brain or content object
    :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain
    :returns: True if the object is a catalog brain
    :rtype: bool",0,0,2,2
"def get_branch_refs(self, scope_path, project=None, include_deleted=None, include_links=None):
        
        route_values = {}
        if project is not None:
            route_values['project'] = self._serialize.url('project', project, 'str')
        query_parameters = {}
        if scope_path is not None:
            query_parameters['scopePath'] = self._serialize.query('scope_path', scope_path, 'str')
        if include_deleted is not None:
            query_parameters['includeDeleted'] = self._serialize.query('include_deleted', include_deleted, 'bool')
        if include_links is not None:
            query_parameters['includeLinks'] = self._serialize.query('include_links', include_links, 'bool')
        response = self._send(http_method='GET',
                              location_id='bc1f417e-239d-42e7-85e1-76e80cb2d6eb',
                              version='5.0',
                              route_values=route_values,
                              query_parameters=query_parameters)
        return self._deserialize('[TfvcBranchRef]', self._unwrap_collection(response))","GetBranchRefs.
        Get branch hierarchies below the specified scopePath
        :param str scope_path: Full path to the branch.  Default: $/ Examples: $/, $/MyProject, $/MyProject/SomeFolder.
        :param str project: Project ID or project name
        :param bool include_deleted: Return deleted branches. Default: False
        :param bool include_links: Return links. Default: False
        :rtype: [TfvcBranchRef]",0,1,1,2
"def get_bulk_device_enrollment(self, id, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('asynchronous'):
            return self.get_bulk_device_enrollment_with_http_info(id, **kwargs)  
        else:
            (data) = self.get_bulk_device_enrollment_with_http_info(id, **kwargs)  
            return data","Get bulk upload entity  # noqa: E501

        Provides information on bulk upload for the given ID. For example, the bulk status and the number of processed enrollment identities. Also links to the bulk upload reports are provided. **Example usage:** ``` curl -X GET \\ -H 'Authorization: Bearer <valid access token>' \\ https://api.us-east-1.mbedcloud.com/v3/device-enrollments-bulk-uploads/{id} ```   # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass asynchronous=True
        >>> thread = api.get_bulk_device_enrollment(id, asynchronous=True)
        >>> result = thread.get()

        :param asynchronous bool
        :param str id: Bulk create task entity ID (required)
        :return: BulkResponse
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def get_by_page(query, page, page_size):
    
    pager = Paginator(query, page_size)

    try:
        models = pager.page(page)
    except PageNotAnInteger:
        
        models = pager.page(1)
    except EmptyPage:
        
        models = pager.page(pager.num_pages)

    return models","Performs pagination
:param query: query
:param page: page number
:param page_size: number of objects on the page
:return:",0,0,1,1
"def get_chat_server(self, channel):
        
        r = self.oldapi_request(
            'GET', 'channels/%s/chat_properties' % channel.name)
        json = r.json()
        servers = json['chat_servers']

        try:
            r = self.get(TWITCH_STATUSURL)
        except requests.HTTPError:
            log.debug('Error getting chat server status. Using random one.')
            address = servers[0]
        else:
            stats = [client.ChatServerStatus(**d) for d in r.json()]
            address = self._find_best_chat_server(servers, stats)

        server, port = address.split(':')
        return server, int(port)","Get an appropriate chat server for the given channel

        Usually the server is irc.twitch.tv. But because of the delicate
        twitch chat, they use a lot of servers. Big events are on special
        event servers. This method tries to find a good one.

        :param channel: the channel with the chat
        :type channel: :class:`models.Channel`
        :returns: the server address and port
        :rtype: (:class:`str`, :class:`int`)
        :raises: None",0,3,1,4
"def get_cli_string(path=None, action=None, key=None, value=None, quote=None):
    
    command = ['dotenv']
    if quote:
        command.append('-q %s' % quote)
    if path:
        command.append('-f %s' % path)
    if action:
        command.append(action)
        if key:
            command.append(key)
            if value:
                if ' ' in value:
                    command.append('""%s""' % value)
                else:
                    command.append(value)

    return ' '.join(command).strip()","Returns a string suitable for running as a shell script.

    Useful for converting a arguments passed to a fabric task
    to be passed to a `local` or `run` command.",0,0,1,1
"def get_client(self, client_type):
        
        route_values = {}
        if client_type is not None:
            route_values['clientType'] = self._serialize.url('client_type', client_type, 'str')
        response = self._send(http_method='GET',
                              location_id='79c83865-4de3-460c-8a16-01be238e0818',
                              version='5.0-preview.1',
                              route_values=route_values)
        return self._deserialize('object', response)","GetClient.
        [Preview API] Get the client package.
        :param str client_type: Either ""EXE"" for a zip file containing a Windows symbol client (a.k.a. symbol.exe) along with dependencies, or ""TASK"" for a VSTS task that can be run on a VSTS build agent. All the other values are invalid. The parameter is case-insensitive.
        :rtype: object",0,1,0,1
"def get_client_ip(request):
    
    _SECURE_PROXY_ADDR_HEADER = getattr(
        settings, 'SECURE_PROXY_ADDR_HEADER', False
    )
    if _SECURE_PROXY_ADDR_HEADER:
        return request.META.get(
            _SECURE_PROXY_ADDR_HEADER,
            request.META.get('REMOTE_ADDR')
        )
    return request.META.get('REMOTE_ADDR')","Return client ip address using SECURE_PROXY_ADDR_HEADER variable.

    If not present or not defined on settings then REMOTE_ADDR is used.

    :param request: Django http request object.
    :type request: django.http.HttpRequest

    :returns: Possible client ip address
    :rtype: string",0,0,3,3
"def get_cloud_integration(self, id, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.get_cloud_integration_with_http_info(id, **kwargs)  
        else:
            (data) = self.get_cloud_integration_with_http_info(id, **kwargs)  
            return data","Get a specific cloud integration  # noqa: E501

          # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.get_cloud_integration(id, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str id: (required)
        :return: ResponseContainerCloudIntegration
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def get_collection(self, collection, filter=None, fields=None,
            page_size=None):
        
        params = {}
        if filter:
            params['filter'] = filter
        if fields:
            params['fields'] = fields
        if page_size:
            params['pageSize'] = page_size

        uri = self.uri + '/v1' + collection
        return self.service._get(uri, params=params)","Returns a specific collection from the asset service with
        the given collection endpoint.

        Supports passing through parameters such as...
        - filters such as ""name=Vesuvius"" following GEL spec
        - fields such as ""uri,description"" comma delimited
        - page_size such as ""100"" (the default)",0,1,1,2
"def get_column_name(self, column_name):
        
        name = pretty_name(column_name)
        if column_name in self._meta.columns:
            column_cls = self._meta.columns[column_name]
            if column_cls.verbose_name:
                name = column_cls.verbose_name
        return name",Get a column for given column name from META api.,0,0,1,1
"def get_composition_lookup_session_for_repository(self, repository_id, proxy):
        
        if not self.supports_composition_lookup():
            raise errors.Unimplemented()
        
        
        
        
        return sessions.CompositionLookupSession(repository_id, proxy, self._runtime)","Gets the ``OsidSession`` associated with the composition lookup service for the given repository.

        arg:    repository_id (osid.id.Id): the ``Id`` of the repository
        arg:    proxy (osid.proxy.Proxy): a proxy
        return: (osid.repository.CompositionLookupSession) - the new
                ``CompositionLookupSession``
        raise:  NotFound - ``repository_id`` not found
        raise:  NullArgument - ``repository_id`` or ``proxy`` is
                ``null``
        raise:  OperationFailed - ``unable to complete request``
        raise:  Unimplemented - ``supports_composition_lookup()`` or
                ``supports_visible_federation()`` is ``false``
        *compliance: optional -- This method must be implemented if
        ``supports_composition_lookup()`` and
        ``supports_visible_federation()`` are ``true``.*",1,1,1,3
"def get_composition_repository_assignment_session(self, proxy):
        
        if not self.supports_composition_repository_assignment():
            raise Unimplemented()
        try:
            from . import sessions
        except ImportError:
            raise  
        proxy = self._convert_proxy(proxy)
        try:
            session = sessions.CompositionRepositoryAssignmentSession(proxy, runtime=self._runtime)
        except AttributeError:
            raise  
        return session","Gets the session for assigning composition to repository
        mappings.

        arg     proxy (osid.proxy.Proxy): a proxy
        return: (osid.repository.CompositionRepositoryAssignmentSession)
                - a CompositionRepositoryAssignmentSession
        raise:  OperationFailed - unable to complete request
        raise:  Unimplemented -
                supports_composition_repository_assignment() is false
        compliance: optional - This method must be implemented if
                    supports_composition_repository_assignment() is
                    true.",3,1,1,5
"def get_config_nodes(self):
        

        from .config import Config

        config = Config(self.device, config=None)
        for notification in self.reply.notification:
            updates = []
            for update in notification.update:
                config += self.build_config_node(Config(self.device, config=None),
                                                 notification.prefix,
                                                 update.path, update.val)
        return config","get_config_nodes

        High-level api: get_config_nodes returns a list of config nodes. Each
        config node is an Element node in the config tree, which is
        corresponding to one 'update' in the gNMI GetResponse.

        Returns
        -------

        list
            A list of config nodes.
        Config
            A Config object.",0,0,1,1
"def get_configspec_str(self, index):
        
        p = index.internalPointer()
        if p is None:
            return
        spec = p.configspec
        if spec is None:
            return None
        k = self.get_key(p, index.row())
        try:
            return spec[k]
        except KeyError:
            return None","Return the config spec string of the given index

        The index stores the section as internal pointer.
        The row of the index determines the key.
        The section stores the spec in its configspec attribute
        The key is used on the configspec attribute to return the spec

        :param index: The QModelIndex
        :type index: QModelIndex
        :returns: The spec for the given index or None",0,0,1,1
"def get_conns(cred, providers):
    
    cld_svc_map = {""aws"": conn_aws,
                   ""azure"": conn_az,
                   ""gcp"": conn_gcp,
                   ""alicloud"": conn_ali}
    sys.stdout.write(""\rEstablishing Connections:  "")
    sys.stdout.flush()
    busy_obj = busy_disp_on()
    conn_fn = [[cld_svc_map[x.rstrip('1234567890')], cred[x], x]
               for x in providers]
    cgroup = Group()
    conn_res = []
    conn_res = cgroup.map(get_conn, conn_fn)
    cgroup.join()
    conn_objs = {}
    for item in conn_res:
        conn_objs.update(item)
    busy_disp_off(dobj=busy_obj)
    sys.stdout.write(""\r                                                 \r"")
    sys.stdout.write(""\033[?25h"")  
    sys.stdout.flush()
    return conn_objs",Collect node data asynchronously using gevent lib.,0,3,1,4
"def get_container_object(container_name, object_name, profile, **libcloud_kwargs):
    
    conn = _get_driver(profile=profile)
    libcloud_kwargs = salt.utils.args.clean_kwargs(**libcloud_kwargs)
    obj = conn.get_container_object(container_name, object_name, **libcloud_kwargs)
    return {
        'name': obj.name,
        'size': obj.size,
        'hash': obj.hash,
        'container': obj.container.name,
        'extra': obj.extra,
        'meta_data': obj.meta_data}","Get the details for a container object (file or object in the cloud)

    :param container_name: Container name
    :type  container_name: ``str``

    :param object_name: Object name
    :type  object_name: ``str``

    :param profile: The profile key
    :type  profile: ``str``

    :param libcloud_kwargs: Extra arguments for the driver's get_container_object method
    :type  libcloud_kwargs: ``dict``

    CLI Example:

    .. code-block:: bash

        salt myminion libcloud_storage.get_container_object MyFolder MyFile.xyz profile1",0,1,1,2
"def get_containers(self, scope=None, artifact_uris=None):
        
        query_parameters = {}
        if scope is not None:
            query_parameters['scope'] = self._serialize.query('scope', scope, 'str')
        if artifact_uris is not None:
            query_parameters['artifactUris'] = self._serialize.query('artifact_uris', artifact_uris, 'str')
        response = self._send(http_method='GET',
                              location_id='e4f5c81e-e250-447b-9fef-bd48471bea5e',
                              version='5.0-preview.4',
                              query_parameters=query_parameters)
        return self._deserialize('[FileContainer]', self._unwrap_collection(response))","GetContainers.
        [Preview API] Gets containers filtered by a comma separated list of artifact uris within the same scope, if not specified returns all containers
        :param str scope: A guid representing the scope of the container. This is often the project id.
        :param str artifact_uris:
        :rtype: [FileContainer]",0,1,1,2
"def get_content_type(self):
        
        mime_type, encoding = mimetypes.guess_type(self.filepath)
        if encoding == ""gzip"":
            return ""application/gzip""
        elif encoding is not None:
            return ""application/octet-stream""
        elif mime_type is not None:
            return mime_type
        
        else:
            return ""application/octet-stream""",Returns the ``Content-Type`` header to be used for this request.,0,0,1,1
"def get_cookie_domain(self, app):
        
        if app.config['SESSION_COOKIE_DOMAIN'] is not None:
            return app.config['SESSION_COOKIE_DOMAIN']
        if app.config['SERVER_NAME'] is not None:
            
            rv = '.' + app.config['SERVER_NAME'].rsplit(':', 1)[0]

            
            
            
            if rv == '.localhost':
                rv = None

            
            
            
            if rv is not None:
                path = self.get_cookie_path(app)
                if path != '/':
                    rv = rv.lstrip('.')

            return rv","Helpful helper method that returns the cookie domain that should
        be used for the session cookie if session cookies are used.",0,0,1,1
"def get_cosmology(cosmology=None, **kwargs):
    r
    if kwargs and cosmology is not None:
        raise ValueError(""if providing custom cosmological parameters, do ""
                         ""not provide a `cosmology` argument"")
    if isinstance(cosmology, astropy.cosmology.FlatLambdaCDM):
        
        return cosmology
    if kwargs:
        cosmology = astropy.cosmology.FlatLambdaCDM(**kwargs)
    else:
        if cosmology is None:
            cosmology = DEFAULT_COSMOLOGY
        if cosmology not in astropy.cosmology.parameters.available:
            raise ValueError(""unrecognized cosmology {}"".format(cosmology))
        cosmology = getattr(astropy.cosmology, cosmology)
    return cosmology","r""""""Gets an astropy cosmology class.

    Parameters
    ----------
    cosmology : str or astropy.cosmology.FlatLambdaCDM, optional
        The name of the cosmology to use. For the list of options, see
        :py:attr:`astropy.cosmology.parameters.available`. If None, and no
        other keyword arguments are provided, will default to
        :py:attr:`DEFAULT_COSMOLOGY`. If an instance of
        :py:class:`astropy.cosmology.FlatLambdaCDM`, will just return that.
    \**kwargs :
        If any other keyword arguments are provided they will be passed to
        :py:attr:`astropy.cosmology.FlatLambdaCDM` to create a custom
        cosmology.

    Returns
    -------
    astropy.cosmology.FlatLambdaCDM
        The cosmology to use.

    Examples
    --------
    Use the default:

    >>> from pycbc.cosmology import get_cosmology
    >>> get_cosmology()
    FlatLambdaCDM(name=""Planck15"", H0=67.7 km / (Mpc s), Om0=0.307,
                  Tcmb0=2.725 K, Neff=3.05, m_nu=[0.   0.   0.06] eV,
                  Ob0=0.0486)

    Use properties measured by WMAP instead:

    >>> get_cosmology(""WMAP9"")
    FlatLambdaCDM(name=""WMAP9"", H0=69.3 km / (Mpc s), Om0=0.286, Tcmb0=2.725 K,
                  Neff=3.04, m_nu=[0. 0. 0.] eV, Ob0=0.0463)

    Create your own cosmology (see :py:class:`astropy.cosmology.FlatLambdaCDM`
    for details on the default values used):

    >>> get_cosmology(H0=70., Om0=0.3)
    FlatLambdaCDM(H0=70 km / (Mpc s), Om0=0.3, Tcmb0=0 K, Neff=3.04, m_nu=None,
                  Ob0=None)",2,2,3,7
"def get_create_index_sql(self, index, table):
        
        if isinstance(table, Table):
            table = table.get_quoted_name(self)

        name = index.get_quoted_name(self)
        columns = index.get_quoted_columns(self)

        if not columns:
            raise DBALException('Incomplete definition. ""columns"" required.')

        if index.is_primary():
            return self.get_create_primary_key_sql(index, table)

        query = ""CREATE %sINDEX %s ON %s"" % (
            self.get_create_index_sql_flags(index),
            name,
            table,
        )
        query += "" (%s)%s"" % (
            self.get_index_field_declaration_list_sql(columns),
            self.get_partial_index_sql(index),
        )

        return query","Returns the SQL to create an index on a table on this platform.

        :param index: The index
        :type index: Index

        :param table: The table
        :type table: Table or str

        :rtype: str",0,0,1,1
"def get_creation_datetime(filepath):
    
    if platform.system() == 'Windows':
        return datetime.fromtimestamp(os.path.getctime(filepath))
    else:
        stat = os.stat(filepath)
        try:
            return datetime.fromtimestamp(stat.st_birthtime)
        except AttributeError:
            
            
            return None","Get the date that a file was created.

    Parameters
    ----------
    filepath : str

    Returns
    -------
    creation_datetime : datetime.datetime or None",0,0,1,1
"def get_cso_dataframe(self):
        
        assert self.jco is not None
        assert self.pst is not None
        weights = self.pst.observation_data.loc[self.jco.to_dataframe().index,""weight""].copy().values
        cso = np.diag(np.sqrt((self.qhalfx.x.dot(self.qhalfx.x.T))))/(float(self.pst.npar-1))
        cso_df = pd.DataFrame.from_dict({'obnme':self.jco.to_dataframe().index,'cso':cso})
        cso_df.index=cso_df['obnme']
        cso_df.drop('obnme', axis=1, inplace=True)
        return cso_df","get a dataframe of composite observation sensitivity, as returned by PEST in the
        seo file.

        Note that this formulation deviates slightly from the PEST documentation in that the
        values are divided by (npar-1) rather than by (npar).

        The equation is cso_j = ((Q^1/2*J*J^T*Q^1/2)^1/2)_jj/(NPAR-1)
        Returns:
        cso : pandas.DataFrame",0,0,2,2
"def get_current_course_run(course, users_active_course_runs):
    
    current_course_run = None
    filtered_course_runs = []
    all_course_runs = course['course_runs']

    if users_active_course_runs:
        current_course_run = get_closest_course_run(users_active_course_runs)
    else:
        for course_run in all_course_runs:
            if is_course_run_enrollable(course_run) and is_course_run_upgradeable(course_run):
                filtered_course_runs.append(course_run)

        if not filtered_course_runs:
            
            filtered_course_runs = all_course_runs

        if filtered_course_runs:
            current_course_run = get_closest_course_run(filtered_course_runs)
    return current_course_run","Return the current course run on the following conditions.

    - If user has active course runs (already enrolled) then return course run with closest start date
    Otherwise it will check the following logic:
    - Course run is enrollable (see is_course_run_enrollable)
    - Course run has a verified seat and the upgrade deadline has not expired.
    - Course run start date is closer to now than any other enrollable/upgradeable course runs.
    - If no enrollable/upgradeable course runs, return course run with most recent start date.",0,0,3,3
"def get_dashboard_tags(self, id, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.get_dashboard_tags_with_http_info(id, **kwargs)  
        else:
            (data) = self.get_dashboard_tags_with_http_info(id, **kwargs)  
            return data","Get all tags associated with a specific dashboard  # noqa: E501

          # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.get_dashboard_tags(id, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str id: (required)
        :return: ResponseContainerTagsResponse
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def get_data_item_for_hardware_source(self, hardware_source, channel_id: str=None, processor_id: str=None, create_if_needed: bool=False, large_format: bool=False) -> DataItem:
        
        assert hardware_source is not None
        hardware_source_id = hardware_source._hardware_source.hardware_source_id
        document_model = self._document_model
        data_item_reference_key = document_model.make_data_item_reference_key(hardware_source_id, channel_id, processor_id)
        return self.get_data_item_for_reference_key(data_item_reference_key, create_if_needed=create_if_needed, large_format=large_format)","Get the data item associated with hardware source and (optional) channel id and processor_id. Optionally create if missing.

        :param hardware_source: The hardware_source.
        :param channel_id: The (optional) channel id.
        :param processor_id: The (optional) processor id for the channel.
        :param create_if_needed: Whether to create a new data item if none is found.
        :return: The associated data item. May be None.

        .. versionadded:: 1.0

        Status: Provisional
        Scriptable: Yes",0,1,3,4
"def get_data_object(data_id, use_data_config=True):
    
    normalized_data_reference = normalize_data_name(data_id, use_data_config=use_data_config)
    client = DataClient()
    data_obj = client.get(normalized_data_reference)

    
    if not data_obj and data_id != normalized_data_reference:
        data_obj = client.get(data_id)

    return data_obj","Normalize the data_id and query the server.
    If that is unavailable try the raw ID",2,0,2,4
"def get_datacenters(service_instance, datacenter_names=None,
                    get_all_datacenters=False):
    
    items = [i['object'] for i in
             get_mors_with_properties(service_instance,
                                      vim.Datacenter,
                                      property_list=['name'])
             if get_all_datacenters or
             (datacenter_names and i['name'] in datacenter_names)]
    return items","Returns all datacenters in a vCenter.

    service_instance
        The Service Instance Object from which to obtain cluster.

    datacenter_names
        List of datacenter names to filter by. Default value is None.

    get_all_datacenters
        Flag specifying whether to retrieve all datacenters.
        Default value is None.",0,1,2,3
"def get_dataset(self, dataset_ref, retry=DEFAULT_RETRY):
        
        if isinstance(dataset_ref, str):
            dataset_ref = DatasetReference.from_string(
                dataset_ref, default_project=self.project
            )

        api_response = self._call_api(retry, method=""GET"", path=dataset_ref.path)
        return Dataset.from_api_repr(api_response)","Fetch the dataset referenced by ``dataset_ref``

        Args:
            dataset_ref (Union[ \
                :class:`~google.cloud.bigquery.dataset.DatasetReference`, \
                str, \
            ]):
                A reference to the dataset to fetch from the BigQuery API.
                If a string is passed in, this method attempts to create a
                dataset reference from a string using
                :func:`~google.cloud.bigquery.dataset.DatasetReference.from_string`.
            retry (:class:`google.api_core.retry.Retry`):
                (Optional) How to retry the RPC.

        Returns:
            google.cloud.bigquery.dataset.Dataset:
                A ``Dataset`` instance.",0,1,0,1
"def get_default_save_path():
    
    macro = '%{_topdir}'
    if rpm:
        save_path = rpm.expandMacro(macro)
    else:
        save_path = rpm_eval(macro)
        if not save_path:
            logger.warn(""rpm tools are missing, using default save path ""
                        ""~/rpmbuild/."")
            save_path = os.path.expanduser('~/rpmbuild')
    return save_path",Return default save path for the packages,0,1,1,2
"def get_definition_tags(self, project, definition_id, revision=None):
        
        route_values = {}
        if project is not None:
            route_values['project'] = self._serialize.url('project', project, 'str')
        if definition_id is not None:
            route_values['definitionId'] = self._serialize.url('definition_id', definition_id, 'int')
        query_parameters = {}
        if revision is not None:
            query_parameters['revision'] = self._serialize.query('revision', revision, 'int')
        response = self._send(http_method='GET',
                              location_id='cb894432-134a-4d31-a839-83beceaace4b',
                              version='5.0-preview.2',
                              route_values=route_values,
                              query_parameters=query_parameters)
        return self._deserialize('[str]', self._unwrap_collection(response))","GetDefinitionTags.
        [Preview API] Gets the tags for a definition.
        :param str project: Project ID or project name
        :param int definition_id: The ID of the definition.
        :param int revision: The definition revision number. If not specified, uses the latest revision of the definition.
        :rtype: [str]",0,1,1,2
"def get_deployment_target(self, project, deployment_group_id, target_id, expand=None):
        
        route_values = {}
        if project is not None:
            route_values['project'] = self._serialize.url('project', project, 'str')
        if deployment_group_id is not None:
            route_values['deploymentGroupId'] = self._serialize.url('deployment_group_id', deployment_group_id, 'int')
        if target_id is not None:
            route_values['targetId'] = self._serialize.url('target_id', target_id, 'int')
        query_parameters = {}
        if expand is not None:
            query_parameters['$expand'] = self._serialize.query('expand', expand, 'str')
        response = self._send(http_method='GET',
                              location_id='2f0aa599-c121-4256-a5fd-ba370e0ae7b6',
                              version='5.1-preview.1',
                              route_values=route_values,
                              query_parameters=query_parameters)
        return self._deserialize('DeploymentMachine', response)","GetDeploymentTarget.
        [Preview API] Get a deployment target by its ID in a deployment group
        :param str project: Project ID or project name
        :param int deployment_group_id: ID of the deployment group to which deployment target belongs.
        :param int target_id: ID of the deployment target to return.
        :param str expand: Include these additional details in the returned objects.
        :rtype: :class:`<DeploymentMachine> <azure.devops.v5_1.task-agent.models.DeploymentMachine>`",0,1,2,3
"def get_description(self, path, method, view):
        
        description = super(WaldurSchemaGenerator, self).get_description(path, method, view)

        permissions_description = get_permissions_description(view, method)
        if permissions_description:
            description += '\n\n' + permissions_description if description else permissions_description

        if isinstance(view, core_views.ActionsViewSet):
            validators_description = get_validators_description(view)
            if validators_description:
                description += '\n\n' + validators_description if description else validators_description

        validation_description = get_validation_description(view, method)
        if validation_description:
            description += '\n\n' + validation_description if description else validation_description

        return description","Determine a link description.

        This will be based on the method docstring if one exists,
        or else the class docstring.",0,0,3,3
"def get_detail(self):
        
        self.size_total = 0
        self.num_folder_total = 0
        self.num_file_total = 0
        
        self.size_current = 0
        self.num_folder_current = 0
        self.num_file_current = 0
        
        for current_dir, folderlist, fnamelist in os.walk(self.abspath):
            self.num_folder_total += len(folderlist)
            self.num_file_total += len(fnamelist)
            for fname in fnamelist:
                self.size_total += os.path.getsize(os.path.join(current_dir, fname))
                
        current_dir, folderlist, fnamelist = next(os.walk(self.abspath))
        self.num_folder_current = len(folderlist)
        self.num_file_current = len(fnamelist)
        for fname in fnamelist:
            self.size_current += os.path.getsize(os.path.join(current_dir, fname))","Get general stats information.
        
        Includes:
        
        - size_total: total size on disk
        - num_folder_total: how many subfolders
        - num_file_total: how many files
        - size_current: total size of files on this folder. file in subfolders 
            doesn't count
        - num_folder_current: how many files, subfolders doens't count
        - num_file_current: how many files, file in subfolders doens't count",0,0,1,1
"def get_developer_certificate(self, developer_certificate_id, authorization, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('asynchronous'):
            return self.get_developer_certificate_with_http_info(developer_certificate_id, authorization, **kwargs)  
        else:
            (data) = self.get_developer_certificate_with_http_info(developer_certificate_id, authorization, **kwargs)  
            return data","Fetch an existing developer certificate to connect to the bootstrap server.  # noqa: E501

        This REST API is intended to be used by customers to fetch an existing developer certificate (a certificate that can be flashed into multiple devices to connect to bootstrap server).  **Example usage:** curl -X GET \""http://api.us-east-1.mbedcloud.com/v3/developer-certificates/THE_CERTIFICATE_ID\"" -H \""accept: application/json\"" -H \""Authorization: Bearer THE_ACCESS_TOKEN\""   # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass asynchronous=True
        >>> thread = api.get_developer_certificate(developer_certificate_id, authorization, asynchronous=True)
        >>> result = thread.get()

        :param asynchronous bool
        :param str developer_certificate_id: A unique identifier for the developer certificate.  (required)
        :param str authorization: Bearer {Access Token}.  (required)
        :return: DeveloperCertificateResponseData
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def get_device(_id):
        
        url = DEVICE_URL % _id
        arequest = requests.get(url, headers=HEADERS)
        status_code = str(arequest.status_code)
        if status_code == '401':
            _LOGGER.error(""Token expired."")
            return False
        return arequest.json()",Pull a device from the API.,0,2,0,2
"def get_discipline(cls, name, ignore='', min_length=3):
        
        for discipline in cls.CONFIG_DISCIPLINES:
            re_abbr = '({RECURSE}(?=[0-9]|[A-Z]|{SEPARATORS}))'.format(
                RECURSE=cls._build_abbreviation_regex(discipline),
                SEPARATORS=cls.REGEX_SEPARATORS)
            matches = cls._get_regex_search(name, re_abbr, ignore=ignore)
            if matches:
                matches = [m for m in matches if
                           re.findall('([a-z]{%d,})' % min_length, m['match'], flags=re.IGNORECASE)]
                if matches:
                    return matches[-1]
        return None","Checks a string for a possible discipline string token, this assumes its on its own
            and is not part of or camel cased and combined with a word.  Returns first found match to reduce duplicates.
            We can be safe to assume the abbreviation for the discipline does not have camel casing within its own word.

        :param name: str, the string based object name
        :param ignore: str, specific ignore string for the search to avoid
        :param min_length: int, minimum length for possible abbreviations of disciplines. Lower = more wrong guesses.
        :return: dict, match dictionary",0,0,2,2
"def get_discount_promotion_by_id(cls, discount_promotion_id, **kwargs):
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async'):
            return cls._get_discount_promotion_by_id_with_http_info(discount_promotion_id, **kwargs)
        else:
            (data) = cls._get_discount_promotion_by_id_with_http_info(discount_promotion_id, **kwargs)
            return data","Find DiscountPromotion

        Return single instance of DiscountPromotion by its ID.
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async=True
        >>> thread = api.get_discount_promotion_by_id(discount_promotion_id, async=True)
        >>> result = thread.get()

        :param async bool
        :param str discount_promotion_id: ID of discountPromotion to return (required)
        :return: DiscountPromotion
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def get_document_summary(self, N=None, cite_sort=True, refresh=True):
        
        abstracts = self.get_abstracts(refresh=refresh)

        if cite_sort:
            counts = [(a, int(a.citedby_count)) for a in abstracts]
            counts.sort(reverse=True, key=itemgetter(1))
            abstracts = [a[0] for a in counts]

        if N is None:
            N = len(abstracts)

        s = [u'{0} of {1} documents'.format(N, len(abstracts))]

        for i in range(N):
            s += ['{0:2d}. {1}\n'.format(i + 1, str(abstracts[i]))]

        return '\n'.join(s)","Return a summary string of documents.

        Parameters
        ----------
        N : int or None (optional, default=None)
            Maximum number of documents to include in the summary.
            If None, return all documents.

        cite_sort : bool (optional, default=True)
            Whether to sort xml by number of citations, in decreasing order,
            or not.

        refresh : bool (optional, default=True)
            Whether to refresh the cached abstract file (if it exists) or not.

        Returns
        -------
        s : str
            Text summarizing an author's documents.",0,0,4,4
"def get_documentation(self, element, namespace=None, schema_str=None):
        
        if namespace is None:
            namespace = {'xs': 'http://www.w3.org/2001/XMLSchema'}
        schema_root = etree.parse(StringIO(self.schema))
        document = schema_root.xpath(self.get_element_from_clark(element),
                                     namespaces=namespace)
        return document and document[0].text or ''","**Helper method:** should return an schema specific documentation
        given an element parsing or getting the `Clark's Notation`_
        `{url:schema}Element` from the message error on validate method.

        :param str element: Element string following the Clark's Notation
        :param dict namespace: Element string following the Clark's Notation

        :returns: The documentation text if exists
        :rtype: unicode

        .. _`Clark's Notation`: http://effbot.org/zone/element-namespaces.htm",1,0,1,2
"def get_duration(y=None, sr=22050, S=None, n_fft=2048, hop_length=512,
                 center=True, filename=None):
    

    if filename is not None:
        try:
            return sf.info(filename).duration
        except:
            with audioread.audio_open(filename) as fdesc:
                return fdesc.duration

    if y is None:
        if S is None:
            raise ParameterError('At least one of (y, sr), S, or filename must be provided')

        n_frames = S.shape[1]
        n_samples = n_fft + hop_length * (n_frames - 1)

        
        if center:
            n_samples = n_samples - 2 * int(n_fft / 2)

    else:
        
        util.valid_audio(y, mono=False)
        if y.ndim == 1:
            n_samples = len(y)
        else:
            n_samples = y.shape[-1]

    return float(n_samples) / sr","Compute the duration (in seconds) of an audio time series,
    feature matrix, or filename.

    Examples
    --------
    >>> # Load the example audio file
    >>> y, sr = librosa.load(librosa.util.example_audio_file())
    >>> librosa.get_duration(y=y, sr=sr)
    61.45886621315193

    >>> # Or directly from an audio file
    >>> librosa.get_duration(filename=librosa.util.example_audio_file())
    61.4

    >>> # Or compute duration from an STFT matrix
    >>> y, sr = librosa.load(librosa.util.example_audio_file())
    >>> S = librosa.stft(y)
    >>> librosa.get_duration(S=S, sr=sr)
    61.44

    >>> # Or a non-centered STFT matrix
    >>> S_left = librosa.stft(y, center=False)
    >>> librosa.get_duration(S=S_left, sr=sr)
    61.3471201814059

    Parameters
    ----------
    y : np.ndarray [shape=(n,), (2, n)] or None
        audio time series

    sr : number > 0 [scalar]
        audio sampling rate of `y`

    S : np.ndarray [shape=(d, t)] or None
        STFT matrix, or any STFT-derived matrix (e.g., chromagram
        or mel spectrogram).
        Durations calculated from spectrogram inputs are only accurate
        up to the frame resolution. If high precision is required,
        it is better to use the audio time series directly.

    n_fft       : int > 0 [scalar]
        FFT window size for `S`

    hop_length  : int > 0 [ scalar]
        number of audio samples between columns of `S`

    center  : boolean
        - If `True`, `S[:, t]` is centered at `y[t * hop_length]`
        - If `False`, then `S[:, t]` begins at `y[t * hop_length]`

    filename : str
        If provided, all other parameters are ignored, and the
        duration is calculated directly from the audio file.
        Note that this avoids loading the contents into memory,
        and is therefore useful for querying the duration of
        long files.

    Returns
    -------
    d : float >= 0
        Duration (in seconds) of the input time series or spectrogram.

    Raises
    ------
    ParameterError
        if none of `y`, `S`, or `filename` are provided.

    Notes
    -----
    `get_duration` can be applied to a file (`filename`), a spectrogram (`S`),
    or audio buffer (`y, sr`).  Only one of these three options should be
    provided.  If you do provide multiple options (e.g., `filename` and `S`),
    then `filename` takes precedence over `S`, and `S` takes precedence over
    `(y, sr)`.",1,0,5,6
"def get_edges(self, indexed=None):
        
        
        if indexed is None:
            if self._edges is None:
                self._compute_edges(indexed=None)
            return self._edges
        elif indexed == 'faces':
            if self._edges_indexed_by_faces is None:
                self._compute_edges(indexed='faces')
            return self._edges_indexed_by_faces
        else:
            raise Exception(""Invalid indexing mode. Accepts: None, 'faces'"")","Edges of the mesh
        
        Parameters
        ----------
        indexed : str | None
           If indexed is None, return (Nf, 3) array of vertex indices,
           two per edge in the mesh.
           If indexed is 'faces', then return (Nf, 3, 2) array of vertex
           indices with 3 edges per face, and two vertices per edge.

        Returns
        -------
        edges : ndarray
            The edges.",1,0,3,4
"def get_edges(self, name: str, *, axis: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
		
		deprecated(""'get_edges' is deprecated. Use 'ds.row_graphs[name]' or 'ds.col_graphs[name]' instead"")
		if axis == 0:
			g = self.row_graphs[name]
			return (g.row, g.col, g.data)
		if axis == 1:
			g = self.col_graphs[name]
			return (g.row, g.col, g.data)
		raise ValueError(""Axis must be 0 or 1"")",**DEPRECATED** - Use `ds.row_graphs[name]` or `ds.col_graphs[name]` instead,1,0,2,3
"def get_editor_widget(self, request, plugins, plugin):
        
        cancel_url_name = self.get_admin_url_name('delete_on_cancel')
        cancel_url = reverse('admin:%s' % cancel_url_name)

        render_plugin_url_name = self.get_admin_url_name('render_plugin')
        render_plugin_url = reverse('admin:%s' % render_plugin_url_name)

        action_token = self.get_action_token(request, plugin)

        
        
        delete_text_on_cancel = (
            'delete-on-cancel' in request.GET and  
            not plugin.get_plugin_instance()[0]
        )

        widget = TextEditorWidget(
            installed_plugins=plugins, pk=plugin.pk,
            placeholder=plugin.placeholder,
            plugin_language=plugin.language,
            configuration=self.ckeditor_configuration,
            render_plugin_url=render_plugin_url,
            cancel_url=cancel_url,
            action_token=action_token,
            delete_on_cancel=delete_text_on_cancel,
        )
        return widget","Returns the Django form Widget to be used for
        the text area",1,1,0,2
"def get_eip_address_info(addresses=None, allocation_ids=None, region=None, key=None,
                         keyid=None, profile=None):
    
    if type(addresses) == (type('string')):
        addresses = [addresses]
    if type(allocation_ids) == (type('string')):
        allocation_ids = [allocation_ids]

    ret = _get_all_eip_addresses(addresses=addresses, allocation_ids=allocation_ids,
                       region=region, key=key, keyid=keyid, profile=profile)

    interesting = ['allocation_id', 'association_id', 'domain', 'instance_id',
                   'network_interface_id', 'network_interface_owner_id', 'public_ip',
                   'private_ip_address']

    return [dict([(x, getattr(address, x)) for x in interesting]) for address in ret]","Get 'interesting' info about some, or all EIPs associated with the current account.

    addresses
        (list) - Optional list of addresses.  If provided, only the addresses
        associated with those in the list will be returned.
    allocation_ids
        (list) - Optional list of allocation IDs.  If provided, only the
        addresses associated with the given allocation IDs will be returned.

    returns
        (list of dicts) - A list of dicts, each containing the info for one of the requested EIPs.

    CLI Example:

    .. code-block:: bash

        salt-call boto_ec2.get_eip_address_info addresses=52.4.2.15

    .. versionadded:: 2016.3.0",1,0,1,2
"def get_element_ids(self, prefix_id):
        
        if isinstance(self.widget, widgets.MultiWidget):
            ids = ['{0}_{1}_{2}'.format(prefix_id, self.name, field_name) for field_name in self.widget]
        elif isinstance(self.widget, (widgets.SelectMultiple, widgets.RadioSelect)):
            ids = ['{0}_{1}_{2}'.format(prefix_id, self.name, k) for k in range(len(self.widget.choices))]
        else:
            ids = ['{0}_{1}'.format(prefix_id, self.name)]
        return ids","Returns a single or a list of element ids, one for each input widget of this field",1,0,1,2
"def get_endpoints(self, endpoints=[]):
        
        if isinstance(endpoints, str) and endpoints in self._ENDPOINTS:
            endpoints = list(endpoints)
        if not endpoints or not set(endpoints).issubset(self._ENDPOINTS):
            raise IEXEndpointError(""Please provide a valid list of endpoints"")
        elif len(endpoints) > 10:
            raise ValueError(""Please input up to 10 valid endpoints"")
        self.optional_params = {}
        self.endpoints = endpoints
        json_data = self.fetch(fmt_p=no_pandas)
        for symbol in self.symbols:
            if symbol not in json_data:
                raise IEXSymbolError(symbol)
        return json_data[self.symbols[0]] if self.n_symbols == 1 else json_data","Universal selector method to obtain specific endpoints from the
        data set.

        Parameters
        ----------
        endpoints: str or list
            Desired valid endpoints for retrieval

        Notes
        -----
        Only allows JSON format (pandas not supported).

        Raises
        ------
        IEXEndpointError
            If an invalid endpoint is specified
        IEXSymbolError
            If a symbol is invalid
        IEXQueryError
            If issues arise during query",3,0,4,7
"def get_entity_by_query(self, uuid=None, path=None, metadata=None):
        
        if not (uuid or path or metadata):
            raise StorageArgumentException('No parameter given for the query.')
        if uuid and not is_valid_uuid(uuid):
            raise StorageArgumentException(
                'Invalid UUID for uuid: {0}'.format(uuid))
        params = locals().copy()
        if metadata:
            if not isinstance(metadata, dict):
                raise StorageArgumentException('The metadata needs to be provided'
                                               ' as a dictionary.')
            key, value = next(iter(metadata.items()))
            params[key] = value
            del params['metadata']
        params = self._prep_params(params)

        return self._authenticated_request \
            .to_endpoint('entity/') \
            .with_params(params) \
            .return_body() \
            .get()","Retrieve entity by query param which can be either uuid/path/metadata.

        Args:
            uuid (str): The UUID of the requested entity.
            path (str): The path of the requested entity.
            metadata (dict): A dictionary of one metadata {key: value} of the
                requested entitity.

        Returns:
            The details of the entity, if found::

                {
                    u'content_type': u'plain/text',
                    u'created_by': u'303447',
                    u'created_on': u'2017-03-13T10:52:23.275087Z',
                    u'description': u'',
                    u'entity_type': u'file',
                    u'modified_by': u'303447',
                    u'modified_on': u'2017-03-13T10:52:23.275126Z',
                    u'name': u'myfile',
                    u'parent': u'3abd8742-d069-44cf-a66b-2370df74a682',
                    u'uuid': u'e2c25c1b-f6a9-4cf6-b8d2-271e628a9a56'
                }

        Raises:
            StorageArgumentException: Invalid arguments
            StorageForbiddenException: Server response code 403
            StorageNotFoundException: Server response code 404
            StorageException: other 400-600 error codes",3,1,4,8
"def get_entity_details(self, entity_id):
        
        if not is_valid_uuid(entity_id):
            raise StorageArgumentException(
                'Invalid UUID for entity_id: {0}'.format(entity_id))
        return self._authenticated_request \
            .to_endpoint('entity/{}/'.format(entity_id)) \
            .return_body() \
            .get()","Get generic entity by UUID.

        Args:
            entity_id (str): The UUID of the requested entity.

        Returns:
            A dictionary describing the entity::

                {
                     u'collab_id': 2271,
                     u'created_by': u'303447',
                     u'created_on': u'2017-03-10T12:50:06.077891Z',
                     u'description': u'',
                     u'entity_type': u'project',
                     u'modified_by': u'303447',
                     u'modified_on': u'2017-03-10T12:50:06.077946Z',
                     u'name': u'2271',
                     u'uuid': u'3abd8742-d069-44cf-a66b-2370df74a682'
                 }

        Raises:
            StorageArgumentException: Invalid arguments
            StorageForbiddenException: Server response code 403
            StorageNotFoundException: Server response code 404
            StorageException: other 400-600 error codes",1,1,1,3
"def get_error_code_msg(cls, full_error_message):
        
        for pattern in cls.ERROR_PATTERNS:
            match = pattern.match(full_error_message)
            if match:
                
                return int(match.group('code')), match.group('msg').strip()

        return 0, full_error_message","Extract the code and message of the exception that clickhouse-server generated.

        See the list of error codes here:
        https://github.com/yandex/ClickHouse/blob/master/dbms/src/Common/ErrorCodes.cpp",0,0,1,1
"def get_event(self, listener, timeout):
        
        if not isinstance(listener, IEventListener):
            raise TypeError(""listener can only be an instance of type IEventListener"")
        if not isinstance(timeout, baseinteger):
            raise TypeError(""timeout can only be an instance of type baseinteger"")
        event = self._call(""getEvent"",
                     in_p=[listener, timeout])
        event = IEvent(event)
        return event","Get events from this peer's event queue (for passive mode). Calling this method
        regularly is required for passive event listeners to avoid system overload;
        see :py:func:`IEventSource.register_listener`  for details.

        in listener of type :class:`IEventListener`
            Which listener to get data for.

        in timeout of type int
            Maximum time to wait for events, in ms;
            0 = no wait, -1 = indefinite wait.

        return event of type :class:`IEvent`
            Event retrieved, or null if none available.

        raises :class:`VBoxErrorObjectNotFound`
            Listener is not registered, or autounregistered.",2,1,2,5
"def get_event_sort_key(self, event_type):
        
        
        if event_type.startswith(""MEM""):
            
            ret = 'memory_percent'
        elif event_type.startswith(""CPU_IOWAIT""):
            
            ret = 'io_counters'
        else:
            
            ret = 'cpu_percent'
        return ret",Return the process sort key,0,0,1,1
"def get_executing_jobs(db):
    
    fields = 'id,pid,user_name,start_time'
    running = List()
    running._fields = fields.split(',')

    query = ( % fields)
    rows = db(query)
    for r in rows:
        
        
        if r.pid and psutil.pid_exists(r.pid):
            running.append(r)
    return running",":param db:
        a :class:`openquake.server.dbapi.Db` instance
    :returns:
        (id, pid, user_name, start_time) tuples",1,0,1,2
"def get_execution_role(sagemaker_session=None):
    
    if not sagemaker_session:
        sagemaker_session = Session()
    arn = sagemaker_session.get_caller_identity_arn()

    if ':role/' in arn:
        return arn
    message = 'The current AWS identity is not a role: {}, therefore it cannot be used as a SageMaker execution role'
    raise ValueError(message.format(arn))","Return the role ARN whose credentials are used to call the API.
    Throws an exception if
    Args:
        sagemaker_session(Session): Current sagemaker session
    Returns:
        (str): The role ARN",1,1,1,3
"def get_external_link(self, id, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.get_external_link_with_http_info(id, **kwargs)  
        else:
            (data) = self.get_external_link_with_http_info(id, **kwargs)  
            return data","Get a specific external link  # noqa: E501

          # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.get_external_link(id, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str id: (required)
        :return: ResponseContainerExternalLink
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def get_f_clvd(self):
        
        if not self.principal_axes:
            
            raise ValueError('Principal Axes not defined!')

        denominator = np.max(np.array([
            fabs(self.principal_axes.t_axis['eigenvalue']),
            fabs(self.principal_axes.p_axis['eigenvalue'])
        ]))
        self.f_clvd = -self.principal_axes.b_axis['eigenvalue'] / denominator
        return self.f_clvd","Returns the statistic f_clvd: the signed ratio of the sizes of the
        intermediate and largest principal moments::

         f_clvd = -b_axis_eigenvalue / max(|t_axis_eigenvalue|,|p_axis_eigenvalue|)",0,0,1,1
"def get_feature_flag_by_name(self, name, check_feature_exists=None):
        
        route_values = {}
        if name is not None:
            route_values['name'] = self._serialize.url('name', name, 'str')
        query_parameters = {}
        if check_feature_exists is not None:
            query_parameters['checkFeatureExists'] = self._serialize.query('check_feature_exists', check_feature_exists, 'bool')
        response = self._send(http_method='GET',
                              location_id='3e2b80f8-9e6f-441e-8393-005610692d9c',
                              version='5.0-preview.1',
                              route_values=route_values,
                              query_parameters=query_parameters)
        return self._deserialize('FeatureFlag', response)","GetFeatureFlagByName.
        [Preview API] Retrieve information on a single feature flag and its current states
        :param str name: The name of the feature to retrieve
        :param bool check_feature_exists: Check if feature exists
        :rtype: :class:`<FeatureFlag> <azure.devops.v5_0.feature_availability.models.FeatureFlag>`",0,1,0,1
"def get_feed(self, buckets=None, since=None, results=15, start=0):
        
        kwargs = {}
        kwargs['bucket'] = buckets or []
	if since:
		kwargs['since']=since
        response = self.get_attribute(""feed"", results=results, start=start, **kwargs)
        rval = ResultList(response['feed'])
        return rval","Returns feed (news, blogs, reviews, audio, video) for the catalog artists; response depends on requested buckets

        Args:

        Kwargs:
            buckets (list): A list of strings specifying which feed items to retrieve

            results (int): An integer number of results to return

            start (int): An integer starting value for the result set

        Returns:
            A list of news, blogs, reviews, audio or video document dicts;

        Example:

        >>> c
        <catalog - my_artists>
        >>> c.get_feed(results=15)
	{u'date_found': u'2011-02-06T07:50:25',
	 u'date_posted': u'2011-02-06T07:50:23',
 	 u'id': u'caec686c0dff361e4c53dceb58fb9d2f',
 	 u'name': u'Linkin Park \u2013 \u201cWaiting For The End\u201d + \u201cWhen They Come For Me\u201d 2/5 SNL',
 	 u'references': [{u'artist_id': u'ARQUMH41187B9AF699',
        	          u'artist_name': u'Linkin Park'}],
	 u'summary': u'<span>Linkin</span> <span>Park</span> performed ""Waiting For The End"" and ""When They Come For Me"" on Saturday Night Live. Watch the videos below and pick up their album A Thousand Suns on iTunes, Amazon MP3, CD    Social Bookmarking ... ',
	 u'type': u'blogs',
	 u'url': u'http://theaudioperv.com/2011/02/06/linkin-park-waiting-for-the-end-when-they-come-for-me-25-snl/'}
        >>>",0,1,0,1
"def get_field_mapping(self, fields, index=None, doc_type=None, params=None):
        
        if fields in SKIP_IN_PATH:
            raise ValueError(""Empty value passed for a required argument 'fields'."")
        return self.transport.perform_request(
            ""GET"",
            _make_path(index, ""_mapping"", doc_type, ""field"", fields),
            params=params,
        )","Retrieve mapping definition of a specific field.
        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-field-mapping.html>`_

        :arg fields: A comma-separated list of fields
        :arg index: A comma-separated list of index names
        :arg doc_type: A comma-separated list of document types
        :arg allow_no_indices: Whether to ignore if a wildcard indices
            expression resolves into no concrete indices. (This includes `_all`
            string or when no indices have been specified)
        :arg expand_wildcards: Whether to expand wildcard expression to concrete
            indices that are open, closed or both., default 'open', valid
            choices are: 'open', 'closed', 'none', 'all'
        :arg ignore_unavailable: Whether specified concrete indices should be
            ignored when unavailable (missing or closed)
        :arg include_defaults: Whether the default mapping values should be
            returned as well
        :arg local: Return local information, do not retrieve the state from
            master node (default: false)
        :arg include_type_name: Specify whether requests and responses should include a
            type name (default: depends on Elasticsearch version).",1,1,1,3
"def get_field_names(obj, ignore_auto=True, ignore_relations=True, 
        exclude=[]):
    

    from django.db.models import (AutoField, ForeignKey, ManyToManyField, 
        ManyToOneRel, OneToOneField, OneToOneRel)

    for field in obj._meta.get_fields():
        if ignore_auto and isinstance(field, AutoField):
            continue

        if ignore_relations and (isinstance(field, ForeignKey) or
                isinstance(field, ManyToManyField) or
                isinstance(field, ManyToOneRel) or
                isinstance(field, OneToOneRel) or
                isinstance(field, OneToOneField)):
            
            
            a = 1; a
            continue

        if field.name in exclude:
            continue

        yield field.name","Returns the field names of a Django model object.

    :param obj: the Django model class or object instance to get the fields
        from
    :param ignore_auto: ignore any fields of type AutoField. Defaults to True
    :param ignore_relations: ignore any fields that involve relations such as
        the ForeignKey or ManyToManyField
    :param exclude: exclude anything in this list from the results

    :returns: generator of found field names",1,0,2,3
"def get_file_service_properties(self, timeout=None):
        
        request = HTTPRequest()
        request.method = 'GET'
        request.host_locations = self._get_host_locations()
        request.path = _get_path()
        request.query = {
            'restype': 'service',
            'comp': 'properties',
            'timeout': _int_to_str(timeout),
        }

        return self._perform_request(request, _convert_xml_to_service_properties)","Gets the properties of a storage account's File service, including
        Azure Storage Analytics.

        :param int timeout:
            The timeout parameter is expressed in seconds.
        :return: The file service properties.
        :rtype:
            :class:`~azure.storage.common.models.ServiceProperties`",0,1,0,1
"def get_filedata(self, condition=None, page_size=1000):
        

        condition = validate_type(condition, type(None), Expression, *six.string_types)
        page_size = validate_type(page_size, *six.integer_types)
        if condition is None:
            condition = (fd_path == ""~/"")  

        params = {""embed"": ""true"", ""condition"": condition.compile()}
        for fd_json in self._conn.iter_json_pages(""/ws/FileData"", page_size=page_size, **params):
            yield FileDataObject.from_json(self, fd_json)","Return a generator over all results matching the provided condition

        :param condition: An :class:`.Expression` which defines the condition
            which must be matched on the filedata that will be retrieved from
            file data store. If a condition is unspecified, the following condition
            will be used ``fd_path == '~/'``.  This condition will match all file
            data in this accounts ""home"" directory (a sensible root).
        :type condition: :class:`.Expression` or None
        :param int page_size: The number of results to fetch in a single page.  Regardless
            of the size specified, :meth:`.get_filedata` will continue to fetch pages
            and yield results until all items have been fetched.
        :return: Generator yielding :class:`.FileDataObject` instances matching the
            provided conditions.",1,0,2,3
"def get_floatingip(context, id, fields=None):
    
    LOG.info('get_floatingip %s for tenant %s' % (id, context.tenant_id))

    filters = {'address_type': ip_types.FLOATING, '_deallocated': False}

    floating_ip = db_api.floating_ip_find(context, id=id, scope=db_api.ONE,
                                          **filters)

    if not floating_ip:
        raise q_exc.FloatingIpNotFound(id=id)

    return v._make_floating_ip_dict(floating_ip)","Retrieve a floating IP.

    :param context: neutron api request context.
    :param id: The UUID of the floating IP.
    :param fields: a list of strings that are valid keys in a
        floating IP dictionary as listed in the RESOURCE_ATTRIBUTE_MAP
        object in neutron/api/v2/attributes.py. Only these fields
        will be returned.

    :returns: Dictionary containing details for the floating IP.  If values
        are declared in the fields parameter, then only those keys will be
        present.",2,1,1,4
"def get_font(self, face, bold=False, italic=False):
        
        key = '%s-%s-%s' % (face, bold, italic)
        if key not in self._fonts:
            font = dict(face=face, bold=bold, italic=italic)
            self._fonts[key] = TextureFont(font, self._renderer)
        return self._fonts[key]",Get a font described by face and size,0,0,1,1
"def get_gain(data, attr, class_attr,
    method=DEFAULT_DISCRETE_METRIC,
    only_sub=0, prefer_fewer_values=False, entropy_func=None):
    
    entropy_func = entropy_func or entropy
    val_freq = defaultdict(float)
    subset_entropy = 0.0

    
    for record in data:
        val_freq[record.get(attr)] += 1.0

    
    
    for val in val_freq.keys():
        val_prob = val_freq[val] / sum(val_freq.values())
        data_subset = [record for record in data if record.get(attr) == val]
        e = entropy_func(data_subset, class_attr, method=method)
        subset_entropy += val_prob * e
        
    if only_sub:
        return subset_entropy

    
    
    main_entropy = entropy_func(data, class_attr, method=method)
    
    
    if prefer_fewer_values:


        
        return ((main_entropy - subset_entropy), 1./len(val_freq))
    else:
        return (main_entropy - subset_entropy)","Calculates the information gain (reduction in entropy) that would
    result by splitting the data on the chosen attribute (attr).
    
    Parameters:
    
    prefer_fewer_values := Weights the gain by the count of the attribute's
        unique values. If multiple attributes have the same gain, but one has
        slightly fewer attributes, this will cause the one with fewer
        attributes to be preferred.",0,0,3,3
"def get_gatk_annotations(config, include_depth=True, include_baseqranksum=True,
                         gatk_input=True):
    
    broad_runner = broad.runner_from_config(config)
    anns = [""MappingQualityRankSumTest"", ""MappingQualityZero"",
            ""QualByDepth"", ""ReadPosRankSumTest"", ""RMSMappingQuality""]
    if include_baseqranksum:
        anns += [""BaseQualityRankSumTest""]
    
    if gatk_input or broad_runner.gatk_type() == ""gatk4"":
        anns += [""FisherStrand""]
    if broad_runner.gatk_type() == ""gatk4"":
        anns += [""MappingQuality""]
    else:
        anns += [""GCContent"", ""HaplotypeScore"", ""HomopolymerRun""]
    if include_depth:
        anns += [""DepthPerAlleleBySample""]
        if broad_runner.gatk_type() in [""restricted"", ""gatk4""]:
            anns += [""Coverage""]
        else:
            anns += [""DepthOfCoverage""]
    return anns","Retrieve annotations to use for GATK VariantAnnotator.

    If include_depth is false, we'll skip annotating DP. Since GATK downsamples
    this will undercount on high depth sequencing and the standard outputs
    from the original callers may be preferable.

    BaseQRankSum can cause issues with some MuTect2 and other runs, so we
    provide option to skip it.",0,0,5,5
"def get_gcloud_pricelist():
    
    try:
        r = requests.get('http://cloudpricingcalculator.appspot.com'
                         '/static/data/pricelist.json')
        content = json.loads(r.content)
    except ConnectionError:
        logger.warning(
            ""Couldn't get updated pricelist from ""
            ""http://cloudpricingcalculator.appspot.com""
            ""/static/data/pricelist.json. Falling back to cached ""
            ""copy, but prices may be out of date."")
        with open('gcloudpricelist.json') as infile:
            content = json.load(infile)

    pricelist = content['gcp_price_list']
    return pricelist","Retrieve latest pricelist from Google Cloud, or use
    cached copy if not reachable.",1,2,0,3
"def get_gene_associations(model):
    

    for reaction in model.reactions:
        assoc = None
        if reaction.genes is None:
            continue
        elif isinstance(reaction.genes, string_types):
            assoc = boolean.Expression(reaction.genes)
        else:
            variables = [boolean.Variable(g) for g in reaction.genes]
            assoc = boolean.Expression(boolean.And(*variables))
        yield reaction.id, assoc","Create gene association for class :class:`.GeneDeletionStrategy`.

    Return a dict mapping reaction IDs to
    :class:`psamm.expression.boolean.Expression` objects,
    representing relationships between reactions and related genes. This helper
    function should be called when creating :class:`.GeneDeletionStrategy`
    objects.

    Args:
        model: :class:`psamm.datasource.native.NativeModel`.",0,0,1,1
"def get_global_dist_packages_dir():
    
    import utool as ut
    if not ut.in_virtual_env():
        
        return get_site_packages_dir()
    else:
        candidates = []
        if ut.LINUX:
            import sys
            candidates += [
                '/usr/lib/python%s/dist-packages' % (sys.version[0:3],),
                '/usr/lib/python%s/dist-packages' % (sys.version[0:1],),
            ]
        else:
            raise NotImplementedError()
        for path in candidates:
            if ut.checkpath(path):
                return path","Attempts to work around virtualenvs and find the system dist_pacakges.
    Essentially this is implmenented as a lookuptable",1,0,3,4
"def get_global_register_objects(self, do_sort=None, reverse=False, **kwargs):
        
        
        try:
            names = iterable(kwargs.pop('name'))
        except KeyError:
            register_objects = []
        else:
            register_objects = [self.global_registers[reg] for reg in names]
        for keyword in kwargs.iterkeys():
            allowed_values = iterable(kwargs[keyword])
            register_objects.extend(filter(lambda global_register: set(iterable(global_register[keyword])).intersection(allowed_values), self.global_registers.itervalues()))
        if not register_objects and filter(None, kwargs.itervalues()):
            raise ValueError('Global register objects empty')
        if do_sort:
            return sorted(register_objects, key=itemgetter(*do_sort), reverse=reverse)
        else:
            return register_objects","Generate register objects (list) from register name list

        Usage: get_global_register_objects(name = [""Amp2Vbn"", ""GateHitOr"", ""DisableColumnCnfg""], address = [2, 3])
        Receives: keyword lists of register names, addresses,... for making cuts
        Returns: list of register objects",1,0,2,3
"def get_group(self, group, use_sis_id=False, **kwargs):
        

        if use_sis_id:
            group_id = group
            uri_str = 'groups/sis_group_id:{}'
        else:
            group_id = obj_or_id(group, ""group"", (Group,))
            uri_str = 'groups/{}'

        response = self.__requester.request(
            'GET',
            uri_str.format(group_id),
            _kwargs=combine_kwargs(**kwargs)
        )
        return Group(self.__requester, response.json())","Return the data for a single group. If the caller does not
        have permission to view the group a 401 will be returned.

        :calls: `GET /api/v1/groups/:group_id \
        <https://canvas.instructure.com/doc/api/groups.html#method.groups.show>`_

        :param group: The object or ID of the group to get.
        :type group: :class:`canvasapi.group.Group` or int

        :param use_sis_id: Whether or not group_id is an sis ID.
            Defaults to `False`.
        :type use_sis_id: bool

        :rtype: :class:`canvasapi.group.Group`",0,1,3,4
"def get_group_category(self, category):
        
        category_id = obj_or_id(category, ""category"", (GroupCategory,))

        response = self.__requester.request(
            'GET',
            'group_categories/{}'.format(category_id)
        )
        return GroupCategory(self.__requester, response.json())","Get a single group category.

        :calls: `GET /api/v1/group_categories/:group_category_id \
        <https://canvas.instructure.com/doc/api/group_categories.html#method.group_categories.show>`_

        :param category: The object or ID of the category.
        :type category: :class:`canvasapi.group.GroupCategory` or int

        :rtype: :class:`canvasapi.group.GroupCategory`",0,1,0,1
"def get_groups(self, **kwargs):
        
        
        params = {
            'cultureInfo': util.language_code(kwargs.get('lang'))
        }

        
        result = self.make_request('geo', 'get_groups', **params)

        if not util.check_result(result):
            return False, result.get('resultDescription', 'UNKNOWN ERROR')

        
        values = util.response_list(result, 'resultValues')
        return True, [emtype.GeoGroupItem(**a) for a in values]","Obtain line types and details.

        Args:
            lang (str): Language code (*es* or *en*).

        Returns:
            Status boolean and parsed response (list[GeoGroupItem]), or message
            string in case of error.",1,1,1,3
"def get_heron_options_from_env():
    
    heron_options_raw = os.environ.get(""HERON_OPTIONS"")
    if heron_options_raw is None:
      raise RuntimeError(""HERON_OPTIONS environment variable not found"")

    options = {}
    for option_line in heron_options_raw.replace(""%%%%"", "" "").split(','):
      key, sep, value = option_line.partition(""="")
      if sep:
        options[key] = value
      else:
        raise ValueError(""Invalid HERON_OPTIONS part %r"" % option_line)
    return options","Retrieves heron options from the `HERON_OPTIONS` environment variable.

    Heron options have the following format:

        cmdline.topologydefn.tmpdirectory=/var/folders/tmpdir
        cmdline.topology.initial.state=PAUSED

    In this case, the returned map will contain:

        #!json
        {
          ""cmdline.topologydefn.tmpdirectory"": ""/var/folders/tmpdir"",
          ""cmdline.topology.initial.state"": ""PAUSED""
        }

    Currently supports the following options natively:

    - `cmdline.topologydefn.tmpdirectory`: (required) the directory to which this
    topology's defn file is written
    - `cmdline.topology.initial.state`: (default: ""RUNNING"") the initial state of the topology
    - `cmdline.topology.name`: (default: class name) topology name on deployment

    Returns: map mapping from key to value",2,0,3,5
"def get_hosts_from_file(filename,
                        default_protocol='telnet',
                        default_domain='',
                        remove_duplicates=False,
                        encoding='utf-8'):
    
    
    if not os.path.exists(filename):
        raise IOError('No such file: %s' % filename)

    
    have = set()
    hosts = []
    with codecs.open(filename, 'r', encoding) as file_handle:
        for line in file_handle:
            hostname = line.split('
            if hostname == '':
                continue
            if remove_duplicates and hostname in have:
                continue
            have.add(hostname)
            hosts.append(to_host(hostname, default_protocol, default_domain))

    return hosts","Reads a list of hostnames from the file with the given name.

    :type  filename: string
    :param filename: A full filename.
    :type  default_protocol: str
    :param default_protocol: Passed to the Host constructor.
    :type  default_domain: str
    :param default_domain: Appended to each hostname that has no domain.
    :type  remove_duplicates: bool
    :param remove_duplicates: Whether duplicates are removed.
    :type  encoding: str
    :param encoding: The encoding of the file.
    :rtype:  list[Host]
    :return: The newly created host instances.",2,0,1,3
"def get_human_readable_bytes(size):
    
    if size == 0: return ""0""
    if size is None: return """"
    assert_is_type(size, int)
    assert size >= 0, ""`size` cannot be negative, got %d"" % size
    suffixes = ""PTGMk""
    maxl = len(suffixes)
    for i in range(maxl + 1):
        shift = (maxl - i) * 10
        if size >> shift == 0: continue
        ndigits = 0
        for nd in [3, 2, 1]:
            if size >> (shift + 12 - nd * 3) == 0:
                ndigits = nd
                break
        if ndigits == 0 or size == (size >> shift) << shift:
            rounded_val = str(size >> shift)
        else:
            rounded_val = ""%.*f"" % (ndigits, size / (1 << shift))
        return ""%s %sb"" % (rounded_val, suffixes[i] if i < maxl else """")","Convert given number of bytes into a human readable representation, i.e. add prefix such as kb, Mb, Gb,
    etc. The `size` argument must be a non-negative integer.

    :param size: integer representing byte size of something
    :return: string representation of the size, in human-readable form",0,0,2,2
"def get_iam_policy(self, client=None):
        
        client = self._require_client(client)
        query_params = {}

        if self.user_project is not None:
            query_params[""userProject""] = self.user_project

        info = client._connection.api_request(
            method=""GET"",
            path=""%s/iam"" % (self.path,),
            query_params=query_params,
            _target_object=None,
        )
        return Policy.from_api_repr(info)","Retrieve the IAM policy for the bucket.

        See
        https://cloud.google.com/storage/docs/json_api/v1/buckets/getIamPolicy

        If :attr:`user_project` is set, bills the API request to that project.

        :type client: :class:`~google.cloud.storage.client.Client` or
                      ``NoneType``
        :param client: Optional. The client to use.  If not passed, falls back
                       to the ``client`` stored on the current bucket.

        :rtype: :class:`google.api_core.iam.Policy`
        :returns: the policy instance, based on the resource returned from
                  the ``getIamPolicy`` API request.",0,1,0,1
"def get_img_data(image, copy=True):
    
    try:
        img = check_img(image)
        if copy:
            return get_data(img)
        else:
            return img.get_data()
    except Exception as exc:
        raise Exception('Error when reading file {0}.'.format(repr_imgs(image))) from exc","Return the voxel matrix of the Nifti file.
    If safe_mode will make a copy of the img before returning the data, so the input image is not modified.

    Parameters
    ----------
    image: img-like object or str
        Can either be:
        - a file path to a Nifti image
        - any object with get_data() and get_affine() methods, e.g., nibabel.Nifti1Image.
        If niimg is a string, consider it as a path to Nifti image and
        call nibabel.load on it. If it is an object, check if get_data()
        and get_affine() methods are present, raise TypeError otherwise.

    copy: bool
    If safe_mode will make a copy of the img before returning the data, so the input image is not modified.

    Returns
    -------
    array_like",2,0,3,5
"def get_imports(filename, source=None):
  

  if source is None:
    with open(filename, 'rb') as fp:
      source = fp.read()

  module = ast.parse(source, filename)
  result = []

  for node in _find_nodes(module, lambda x: isinstance(x, ast.Import)):
    for alias in node.names:
      result.append(ImportInfo(alias.name, filename, node.lineno))
  for node in _find_nodes(module, lambda x: isinstance(x, ast.ImportFrom)):
    import_name = '.' * node.level + (node.module or '')
    result.append(ImportInfo(import_name, filename, node.lineno))

  result.sort(key=lambda x: x.lineno)
  return result","Returns a list of #ImportInfo tuples for all module imports in the specified
  Python source file or the *source* string.",1,0,1,2
"def get_input_var_names(self):
        
        in_vars = copy.copy(self.input_vars)
        for idx, var in enumerate(in_vars):
            if self._map_in_out(var) is not None:
                in_vars[idx] = self._map_in_out(var)
        return in_vars","Return a list of variables names that can be set as input.

        Returns
        -------
        var_names : list[str]
            A list of variable names that can be set from the outside",0,0,1,1
"def get_installed_extension_by_name(self, publisher_name, extension_name, asset_types=None):
        
        route_values = {}
        if publisher_name is not None:
            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')
        if extension_name is not None:
            route_values['extensionName'] = self._serialize.url('extension_name', extension_name, 'str')
        query_parameters = {}
        if asset_types is not None:
            asset_types = "":"".join(asset_types)
            query_parameters['assetTypes'] = self._serialize.query('asset_types', asset_types, 'str')
        response = self._send(http_method='GET',
                              location_id='fb0da285-f23e-4b56-8b53-3ef5f9f6de66',
                              version='5.0-preview.1',
                              route_values=route_values,
                              query_parameters=query_parameters)
        return self._deserialize('InstalledExtension', response)","GetInstalledExtensionByName.
        [Preview API] Get an installed extension by its publisher and extension name.
        :param str publisher_name: Name of the publisher. Example: ""fabrikam"".
        :param str extension_name: Name of the extension. Example: ""ops-tools"".
        :param [str] asset_types:
        :rtype: :class:`<InstalledExtension> <azure.devops.v5_0.extension_management.models.InstalledExtension>`",0,1,1,2
"def get_instance():
    
    resources = []
    env_resource = resource.get_from_env()
    if env_resource is not None:
        resources.append(env_resource)

    if k8s_utils.is_k8s_environment():
        resources.append(resource.Resource(
            _K8S_CONTAINER, k8s_utils.get_k8s_metadata()))

    if is_gce_environment():
        resources.append(resource.Resource(
            _GCE_INSTANCE,
            gcp_metadata_config.GcpMetadataConfig().get_gce_metadata()))
    elif is_aws_environment():
        resources.append(resource.Resource(
            _AWS_EC2_INSTANCE,
            (aws_identity_doc_utils.AwsIdentityDocumentUtils()
             .get_aws_metadata())))

    if not resources:
        return None
    return resource.merge_resources(resources)","Get a resource based on the application environment.

    Returns a `Resource` configured for the current environment, or None if the
    environment is unknown or unsupported.

    :rtype: :class:`opencensus.common.resource.Resource` or None
    :return: A `Resource` configured for the current environment.",0,3,1,4
"def get_instance(self, payload):
        
        return IpAddressInstance(
            self._version,
            payload,
            account_sid=self._solution['account_sid'],
            ip_access_control_list_sid=self._solution['ip_access_control_list_sid'],
        )","Build an instance of IpAddressInstance

        :param dict payload: Payload response from the API

        :returns: twilio.rest.api.v2010.account.sip.ip_access_control_list.ip_address.IpAddressInstance
        :rtype: twilio.rest.api.v2010.account.sip.ip_access_control_list.ip_address.IpAddressInstance",0,0,1,1
"def get_invitation(self, invitation_id, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('asynchronous'):
            return self.get_invitation_with_http_info(invitation_id, **kwargs)  
        else:
            (data) = self.get_invitation_with_http_info(invitation_id, **kwargs)  
            return data","Details of a user invitation.  # noqa: E501

        An endpoint for retrieving the details of an active user invitation sent for a new or an existing user to join the account.   **Example usage:** `curl https://api.us-east-1.mbedcloud.com/v3/user-invitations/{invitation-id} -H 'Authorization: Bearer API_KEY'`  # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass asynchronous=True
        >>> thread = api.get_invitation(invitation_id, asynchronous=True)
        >>> result = thread.get()

        :param asynchronous bool
        :param str invitation_id: The ID of the invitation to be retrieved. (required)
        :return: UserInvitationResp
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def get_ip_by_equip_and_vip(self, equip_name, id_evip):
        

        if not is_valid_int_param(id_evip):
            raise InvalidParameterError(
                u'Vip environment is invalid or was not informed.')

        ip_map = dict()
        ip_map['equip_name'] = equip_name
        ip_map['id_evip'] = id_evip

        url = ""ip/getbyequipandevip/""

        code, xml = self.submit({'ip_map': ip_map}, 'POST', url)

        return self.response(code, xml)","Get a available IP in the Equipment related Environment VIP

        :param equip_name: Equipment Name.
        :param id_evip: Vip environment identifier. Integer value and greater than zero.

        :return: Dictionary with the following structure:

        ::

            { 'ipv4': [ {'id': < id >, 'ip': < ip >, 'network': { 'id': < id >, 'network': < network >, 'mask': < mask >, }} ... ],
            'ipv6': [ {'id': < id >, 'ip': < ip >, 'network': { 'id': < id >, 'network': < network >, 'mask': < mask >, }} ... ] }

        :raise InvalidParameterError: Vip environment identifier or equipment name is none or invalid.
        :raise EquipamentoNotFoundError: Equipment not registered.
        :raise EnvironmentVipNotFoundError: Vip environment not registered.
        :raise UserNotAuthorizedError: User dont have permission to perform operation.
        :raise XMLError: Networkapi failed to generate the XML response.
        :raise DataBaseError: Networkapi failed to access the database.",1,1,1,3
"def get_item_hrefs(result_collection):
    

    
    assert result_collection is not None

    result = []

    links = result_collection.get('_links')
    if links is not None:
        items = links.get('items')
        if items is not None:
            for item in items:
                result.append(item.get('href'))

    return result","Given a result_collection (returned by a previous API call that
    returns a collection, like get_bundle_list() or search()), return a
    list of item hrefs.

    'result_collection' a JSON object returned by a previous API
    call.

    Returns a list, which may be empty if no items were found.",0,0,2,2
"def get_jobs_from_queue(self, queue: str, max_jobs: int) -> List[Job]:
        
        rv = list()
        while len(rv) < max_jobs:
            try:
                job_json_string = self._get_queue(queue).get(block=False)
            except Empty:
                break

            job = Job.deserialize(job_json_string)
            job.status = JobStatus.RUNNING
            rv.append(job)

        return rv",Get jobs from a queue.,0,1,0,1
"def get_jwt(self, request):
        
        try:
            authorization = request.authorization
        except ValueError:
            return None
        if authorization is None:
            return None
        authtype, token = authorization
        if authtype.lower() != self.auth_header_prefix.lower():
            return None
        return token","Extract the JWT token from the authorisation header of the request.

        Returns the JWT token or None, if the token cannot be extracted.

        :param request: request object.
        :type request: :class:`morepath.Request`",0,0,2,2
"def get_kafka_brokers():
    
    
    
    if not os.environ.get('KAFKA_URL'):
        raise RuntimeError('The KAFKA_URL config variable is not set.')

    return ['{}:{}'.format(parsedUrl.hostname, parsedUrl.port) for parsedUrl in
            [urlparse(url) for url in os.environ.get('KAFKA_URL').split(',')]]","Parses the KAKFA_URL and returns a list of hostname:port pairs in the format
    that kafka-python expects.",1,0,2,3
"def get_key_at(self, position):
        
        for row in self.rows:
            if position in row:
                for key in row.keys:
                    if key.is_touched(position):
                        return key
        return None","Retrieves if any key is located at the given position
        
        :param position: Position to check key at.
        :returns: The located key if any at the given position, None otherwise.",0,0,2,2
"def get_latex_nodes(s, pos=0, stop_upon_closing_brace=None, stop_upon_end_environment=None,
                    stop_upon_closing_mathmode=None, **parse_flags):
    

    return LatexWalker(s, **parse_flags).get_latex_nodes(stop_upon_closing_brace=stop_upon_closing_brace,
                                                         stop_upon_end_environment=stop_upon_end_environment,
                                                         stop_upon_closing_mathmode=stop_upon_closing_mathmode)","Parses latex content `s`.

    Returns a tuple `(nodelist, pos, len)` where nodelist is a list of `LatexNode` 's.

    If `stop_upon_closing_brace` is given, then `len` includes the closing brace, but the
    closing brace is not included in any of the nodes in the `nodelist`.

    .. deprecated:: 1.0
       Please use :py:meth:`LatexWalker.get_latex_nodes()` instead.",0,0,3,3
"def get_list(self, list_id):
        
        route_values = {}
        if list_id is not None:
            route_values['listId'] = self._serialize.url('list_id', list_id, 'str')
        response = self._send(http_method='GET',
                              location_id='01e15468-e27c-4e20-a974-bd957dcccebc',
                              version='5.0-preview.1',
                              route_values=route_values)
        return self._deserialize('PickList', response)","GetList.
        [Preview API] Returns a picklist.
        :param str list_id: The ID of the list
        :rtype: :class:`<PickList> <azure.devops.v5_0.work_item_tracking_process.models.PickList>`",0,1,0,1
"def get_live_id(self, username):
        
        params = {
            ""flags"": ""live_onair""
        }
        api_user_videos = USER_INFO_URL.format(username) + ""/videos""
        try:
            res = self.session.http.get(api_user_videos.format(username),
                           params=params)
        except Exception as e:
            self.logger.error(""invalid username"")
            raise NoStreamsError(self.url)

        data = self.session.http.json(res, schema=_live_id_schema)
        if data[""total""] > 0:
            media_id = data[""list""][0][""id""]
            return media_id
        return False","Get the livestream videoid from a username.
           https://developer.dailymotion.com/tools/apiexplorer#/user/videos/list",1,2,2,5
"def get_lock_request(name, version, patch_lock, weak=True):
    
    ch = '~' if weak else ''
    if patch_lock == PatchLock.lock:
        s = ""%s%s==%s"" % (ch, name, str(version))
        return PackageRequest(s)
    elif (patch_lock == PatchLock.no_lock) or (not version):
        return None
    version_ = version.trim(patch_lock.rank)
    s = ""%s%s-%s"" % (ch, name, str(version_))
    return PackageRequest(s)","Given a package and patch lock, return the equivalent request.

    For example, for object 'foo-1.2.1' and lock type 'lock_3', the equivalent
    request is '~foo-1.2'. This restricts updates to foo to patch-or-lower
    version changes only.

    For objects not versioned down to a given lock level, the closest possible
    lock is applied. So 'lock_3' applied to 'foo-1' would give '~foo-1'.

    Args:
        name (str): Package name.
        version (Version): Package version.
        patch_lock (PatchLock): Lock type to apply.

    Returns:
        `PackageRequest` object, or None if there is no equivalent request.",0,0,2,2
"def get_logged_user(self, **kwargs):
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('callback'):
            return self.get_logged_user_with_http_info(**kwargs)
        else:
            (data) = self.get_logged_user_with_http_info(**kwargs)
            return data","Gets logged user and in case not existing creates a new one
        
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please define a `callback` function
        to be invoked when receiving the response.
        >>> def callback_function(response):
        >>>     pprint(response)
        >>>
        >>> thread = api.get_logged_user(callback=callback_function)

        :param callback function: The callback function
            for asynchronous request. (optional)
        :return: UserSingleton
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def get_long_tasks(self, expand=None, start=None, limit=None, callback=None):
        
        params = {}
        if expand:
            params[""expand""] = expand
        if start is not None:
            params[""start""] = int(start)
        if limit is not None:
            params[""limit""] = int(limit)
        return self._service_get_request(""rest/api/longtask"", params=params, callback=callback)","Returns information about all tracked long-running tasks.
        :param expand (string): OPTIONAL: A comma separated list of properties to expand on the tasks.
        :param start (int): OPTIONAL: The pagination start count.
        :param limit (int): OPTIONAL: The pagination return count limit.
        :param callback: OPTIONAL: The callback to execute on the resulting data, before the method returns.
                         Default: None (no callback, raw data returned).
        :return: The JSON data returned from the longtask endpoint,
                 or the results of the callback. Will raise requests.HTTPError on bad input, potentially.",0,1,3,4
"def get_loss_builder(dstore, return_periods=None, loss_dt=None):
    
    oq = dstore['oqparam']
    weights = dstore['weights'][:, 0]
    eff_time = oq.investigation_time * oq.ses_per_logic_tree_path
    num_events = countby(dstore['events'].value, 'rlz')
    periods = return_periods or oq.return_periods or scientific.return_periods(
        eff_time, max(num_events.values()))
    return scientific.LossCurvesMapsBuilder(
        oq.conditional_loss_poes, numpy.array(periods),
        loss_dt or oq.loss_dt(), weights, num_events,
        eff_time, oq.risk_investigation_time)",":param dstore: datastore for an event based risk calculation
    :returns: a LossCurvesMapsBuilder instance",0,0,1,1
"def get_maya_envpath(self):
        
        opj = os.path.join
        ml = self.get_maya_location()
        mb = self.get_maya_bin()
        msp = self.get_maya_sitepackage_dir()
        pyzip = opj(mb, ""python27.zip"")
        pydir = opj(ml, ""Python"")
        pydll = opj(pydir, ""DLLs"")
        pylib = opj(pydir, ""lib"")
        pyplat = opj(pylib, ""plat-win"")
        pytk = opj(pylib, ""lib-tk"")
        path = os.pathsep.join((pyzip, pydll, pylib, pyplat, pytk, mb, pydir, msp))
        return path","Return the PYTHONPATH neccessary for running mayapy

        If you start native mayapy, it will setup these paths.
        You might want to prepend this to your path if running from
        an external intepreter.

        :returns: the PYTHONPATH that is used for running mayapy
        :rtype: str
        :raises: None",0,0,1,1
"def get_members(self, api=None):
        
        api = api or self._API

        response = api.get(url=self._URL['members'].format(id=self.id))

        data = response.json()
        total = response.headers['x-total-matching-query']
        members = [Member(api=api, **member) for member in data['items']]
        links = [Link(**link) for link in data['links']]
        href = data['href']

        return Collection(
            resource=Member,
            href=href,
            total=total,
            items=members,
            links=links,
            api=api
        )","Retrieve dataset members
        :param api: Api instance
        :return: Collection object",0,1,0,1
"def get_mesh_id_name_from_web(mesh_term):
    
    url = MESH_URL + 'sparql'
    query =  % (mesh_term, mesh_term)
    args = {'query': query, 'format': 'JSON', 'inference': 'true'}
    
    
    
    
    query_string = '%s?%s' % (url, urlencode(args))
    resp = requests.get(query_string)
    
    if resp.status_code != 200:
        return None, None

    try:
        
        
        mesh_json = resp.json()

        
        id_uri = mesh_json['results']['bindings'][0]['d']['value']
        name = mesh_json['results']['bindings'][0]['dName']['value']
    except (KeyError, IndexError, json.decoder.JSONDecodeError) as e:
        return None, None

    
    m = re.match('http://id.nlm.nih.gov/mesh/([A-Za-z0-9]*)', id_uri)
    assert m is not None
    id = m.groups()[0]
    return id, name","Get the MESH ID and name for the given MESH term using the NLM REST API.

    Parameters
    ----------
    mesh_term : str
        MESH Descriptor or Concept name, e.g. 'Breast Cancer'.

    Returns
    -------
    tuple of strs
        Returns a 2-tuple of the form `(id, name)` with the ID of the
        descriptor corresponding to the MESH label, and the descriptor name
        (which may not exactly match the name provided as an argument if it is
        a Concept name). If the query failed, or no descriptor corresponding to
        the name was found, returns a tuple of (None, None).",0,1,1,2
"def get_message(self, object_id=None, query=None, *, download_attachments=False):
        
        if object_id is not None and query is not None:
            raise ValueError('Must provide object id or query but not both.')

        if object_id is not None:
            url = self.build_url(self._endpoints.get('message').format(id=object_id))
            response = self.con.get(url)
            if not response:
                return None

            message = response.json()

            return self.message_constructor(parent=self,
                                            download_attachments=download_attachments,
                                            **{self._cloud_data_key: message})

        else:
            messages = list(self.get_messages(limit=1, query=query,
                                              download_attachments=download_attachments))

            return messages[0] if messages else None","Get one message from the query result.
         A shortcut to get_messages with limit=1
        :param object_id: the message id to be retrieved.
        :param query: applies a filter to the request such as
         ""displayName eq 'HelloFolder'""
        :type query: Query or str
        :param bool download_attachments: whether or not to download attachments
        :return: one Message
        :rtype: Message or None",1,2,4,7
"def get_meta(self, **kwargs):
        
        query = kwargs
        
        query = query_params(query, 'productid', None, short_hand='pid')
        query = query_params(query, 'query', 'product')
        query = query_params(query, 'results', 'm')
        query = query_params(query, 'output', 'j')
        return query_ode(self.ode_url, query=query)","Perform a mostly arbitrary meta_data query and dump to std out
        :param kwargs:
        :return:",0,0,1,1
"def get_metadata_count_mat(self):
        
        freq_mat = np.zeros(shape=(self.get_num_metadata(), self.get_num_categories()), dtype=int)
        for cat_i in range(self.get_num_categories()):
            mX = (self._mX[self._y == cat_i, :] > 0).astype(int)
            freq_mat[:, cat_i] = mX.sum(axis=0)
        return freq_mat","Returns
        -------
        np.array with columns as categories and rows as terms",0,0,1,1
"def get_metric_details(self, m, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.get_metric_details_with_http_info(m, **kwargs)  
        else:
            (data) = self.get_metric_details_with_http_info(m, **kwargs)  
            return data","Get more details on a metric, including reporting sources and approximate last time reported  # noqa: E501

          # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.get_metric_details(m, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str m: Metric name (required)
        :param int l: limit
        :param str c: cursor value to continue if the number of results exceeds 1000
        :param list[str] h: glob pattern for sources to include in the query result
        :return: MetricDetailsResponse
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def get_metric_value_by_labels(messages, _metric, _m, metric_suffix):
        
        metric_name = '{}_{}'.format(_m, metric_suffix)
        expected_labels = set(
            [(k, v) for k, v in iteritems(_metric[""labels""]) if k not in PrometheusScraperMixin.UNWANTED_LABELS]
        )
        for elt in messages[metric_name]:
            current_labels = set(
                [(k, v) for k, v in iteritems(elt[""labels""]) if k not in PrometheusScraperMixin.UNWANTED_LABELS]
            )
            
            if current_labels == expected_labels:
                return float(elt[""value""])

        raise AttributeError(""cannot find expected labels for metric %s with suffix %s"" % (metric_name, metric_suffix))",":param messages: dictionary as metric_name: {labels: {}, value: 10}
        :param _metric: dictionary as {labels: {le: '0.001', 'custom': 'value'}}
        :param _m: str as metric name
        :param metric_suffix: str must be in (count or sum)
        :return: value of the metric_name matched by the labels",1,0,2,3
"def get_milestone(self, title):
        
        if not title:
            return GithubObject.NotSet
        if not hasattr(self, '_milestones'):
            self._milestones = {m.title: m for m in self.repo.get_milestones()}

        milestone = self._milestones.get(title)
        if not milestone:
            milestone = self.repo.create_milestone(title=title)
        return milestone","given the title as str, looks for an existing milestone or create a new one,
        and return the object",0,0,3,3
"def get_model_loader(filename):
    
    assert isinstance(filename, six.string_types), filename
    filename = os.path.expanduser(filename)
    if filename.endswith('.npy'):
        assert tf.gfile.Exists(filename), filename
        return DictRestore(np.load(filename, encoding='latin1').item())
    elif filename.endswith('.npz'):
        assert tf.gfile.Exists(filename), filename
        obj = np.load(filename)
        return DictRestore(dict(obj))
    else:
        return SaverRestore(filename)","Get a corresponding model loader by looking at the file name.

    Returns:
        SessInit: either a :class:`DictRestore` (if name ends with 'npy/npz') or
        :class:`SaverRestore` (otherwise).",3,0,2,5
"def get_modifiers(state):
    
    ret = []

    if state & xproto.ModMask.Shift:
        ret.append('Shift')
    if state & xproto.ModMask.Lock:
        ret.append('Lock')
    if state & xproto.ModMask.Control:
        ret.append('Control')
    if state & xproto.ModMask._1:
        ret.append('Mod1')
    if state & xproto.ModMask._2:
        ret.append('Mod2')
    if state & xproto.ModMask._3:
        ret.append('Mod3')
    if state & xproto.ModMask._4:
        ret.append('Mod4')
    if state & xproto.ModMask._5:
        ret.append('Mod5')
    if state & xproto.KeyButMask.Button1:
        ret.append('Button1')
    if state & xproto.KeyButMask.Button2:
        ret.append('Button2')
    if state & xproto.KeyButMask.Button3:
        ret.append('Button3')
    if state & xproto.KeyButMask.Button4:
        ret.append('Button4')
    if state & xproto.KeyButMask.Button5:
        ret.append('Button5')

    return ret","Takes a ``state`` (typically found in key press or button press events)
    and returns a string list representation of the modifiers that were pressed
    when generating the event.

    :param state: Typically from ``some_event.state``.
    :return: List of modifier string representations.
    :rtype: [str]",0,0,1,1
"def get_modules(folder, include_meta=False):
    
    files = [
        file
        for file in _get_modules(folder)
        if is_file(file)  
    ]

    if not include_meta:
        files = [
            file
            for file in files
            if not Document(file).name.startswith(""__"")
        ]

    return files","Finds modules (recursively) in folder

    :param folder: root folder
    :param include_meta: whether include meta files like (__init__ or
        __version__)
    :return: list of modules",0,0,1,1
"def get_most_frequent_response(input_statement, response_list, storage=None):
    
    matching_response = None
    occurrence_count = -1

    logger = logging.getLogger(__name__)
    logger.info('Selecting response with greatest number of occurrences.')

    for statement in response_list:
        count = len(list(storage.filter(
            text=statement.text,
            in_response_to=input_statement.text)
        ))

        
        if count >= occurrence_count:
            matching_response = statement
            occurrence_count = count

    
    return matching_response",":param input_statement: A statement, that closely matches an input to the chat bot.
    :type input_statement: Statement

    :param response_list: A list of statement options to choose a response from.
    :type response_list: list

    :param storage: An instance of a storage adapter to allow the response selection
                    method to access other statements if needed.
    :type storage: StorageAdapter

    :return: The response statement with the greatest number of occurrences.
    :rtype: Statement",0,0,2,2
"def get_namespaced_custom_object_scale(self, group, version, namespace, plural, name, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.get_namespaced_custom_object_scale_with_http_info(group, version, namespace, plural, name, **kwargs)  
        else:
            (data) = self.get_namespaced_custom_object_scale_with_http_info(group, version, namespace, plural, name, **kwargs)  
            return data","get_namespaced_custom_object_scale  # noqa: E501

        read scale of the specified namespace scoped custom object  # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.get_namespaced_custom_object_scale(group, version, namespace, plural, name, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str group: the custom resource's group (required)
        :param str version: the custom resource's version (required)
        :param str namespace: The custom resource's namespace (required)
        :param str plural: the custom resource's plural name. For TPRs this would be lowercase plural kind. (required)
        :param str name: the custom object's name (required)
        :return: object
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def get_negative_cycle(self):
        
        nl = self.get_node_list()
        i = nl[0]
        (valid, distance, nextn) = self.floyd_warshall()
        if not valid:
            cycle = self.floyd_warshall_get_cycle(distance, nextn)
            return cycle
        else:
            return None","API:
            get_negative_cycle(self)
        Description:
            Finds and returns negative cost cycle using 'cost' attribute of
            arcs. Return value is a list of nodes representing cycle it is in
            the following form; n_1-n_2-...-n_k, when the cycle has k nodes.
        Pre:
            Arcs should have 'cost' attribute.
        Return:
            Returns a list of nodes in the cycle if a negative cycle exists,
            returns None otherwise.",0,0,2,2
"def get_neighborhood_overlap(self, node1, node2, connection_type=None):
        
        if connection_type is None or connection_type == ""direct"":
            order = 1
        elif connection_type == ""second-degree"":
            order = 2
        else:
            raise Exception(
                ""Invalid option: {}. Valid options are direct and second-degree"".format(
                    connection_type)
            )

        neighbors1 = self.graph.neighborhood(node1, order=order)
        neighbors2 = self.graph.neighborhood(node2, order=order)
        return set(neighbors1).intersection(neighbors2)","Get the intersection of two nodes's neighborhoods.

        Neighborhood is defined by parameter connection_type.
        :param Vertex node1: First node.
        :param Vertex node2: Second node.
        :param Optional[str] connection_type: One of direct or second-degree. Defaults to direct.
        :return: Overlap of the nodes' neighborhoods.",1,0,2,3
"def get_network(context, id, fields=None):
    
    LOG.info(""get_network %s for tenant %s fields %s"" %
             (id, context.tenant_id, fields))

    network = db_api.network_find(context=context, limit=None, sorts=['id'],
                                  marker=None, page_reverse=False,
                                  id=id, join_subnets=True, scope=db_api.ONE)
    if not network:
        raise n_exc.NetworkNotFound(net_id=id)
    return v._make_network_dict(network, fields=fields)","Retrieve a network.

    : param context: neutron api request context
    : param id: UUID representing the network to fetch.
    : param fields: a list of strings that are valid keys in a
        network dictionary as listed in the RESOURCE_ATTRIBUTE_MAP
        object in neutron/api/v2/attributes.py. Only these fields
        will be returned.",1,2,1,4
"def get_ngrams(self, minimum, maximum, filter_ngrams):
        
        tokens = self.get_tokens()
        filter_pattern = self.get_filter_ngrams_pattern(filter_ngrams)
        for size in range(minimum, maximum + 1):
            ngrams = collections.Counter(
                self._ngrams(tokens, size, filter_pattern))
            yield (size, ngrams)","Returns a generator supplying the n-grams (`minimum` <= n
        <= `maximum`) for this text.

        Each iteration of the generator supplies a tuple consisting of
        the size of the n-grams and a `collections.Counter` of the
        n-grams.

        :param minimum: minimum n-gram size
        :type minimum: `int`
        :param maximum: maximum n-gram size
        :type maximum: `int`
        :param filter_ngrams: n-grams that must be contained by the generated
                              n-grams
        :type filter_ngrams: `list`
        :rtype: `generator`",0,0,1,1
"def get_objective_admin_session_for_objective_bank(self, objective_bank_id):
        
        if not self.supports_objective_admin():
            raise errors.Unimplemented()
        
        
        
        
        return sessions.ObjectiveAdminSession(objective_bank_id, runtime=self._runtime)","Gets the ``OsidSession`` associated with the objective admin service for the given objective bank.

        arg:    objective_bank_id (osid.id.Id): the ``Id`` of the
                objective bank
        return: (osid.learning.ObjectiveAdminSession) - ``an
                _objective_admin_session``
        raise:  NotFound - ``objective_bank_id`` not found
        raise:  NullArgument - ``objective_bank_id`` is ``null``
        raise:  OperationFailed - ``unable to complete request``
        raise:  Unimplemented - ``supports_objective_admin()`` or
                ``supports_visible_federation()`` is ``false``
        *compliance: optional -- This method must be implemented if
        ``supports_objective_admin()`` and
        ``supports_visible_federation()`` are ``true``.*",1,1,1,3
"def get_objects(self, uri, _oid=None, _start=None, _end=None,
                    load_kwargs=None, **kwargs):
        
        load_kwargs = load_kwargs or {}
        objects = load(path=uri, filetype='csv', **load_kwargs)

        k = itertools.count(1)
        now = utcnow()
        __oid = lambda o: k.next()

        _oid = _oid or __oid
        _start = _start or now
        _end = _end or None

        def is_callable(v):
            _v = type(v)
            _ = True if _v is type or hasattr(v, '__call__') else False
            return _

        for obj in objects:
            obj['_oid'] = _oid(obj) if is_callable(_oid) else _oid
            obj['_start'] = _start(obj) if is_callable(_start) else _start
            obj['_end'] = _end(obj) if is_callable(_end) else _end
            self.container.add(obj)

        return super(Rows, self).get_objects(**kwargs)","Load and transform csv data into a list of dictionaries.

        Each row in the csv will result in one dictionary in the list.

        :param uri: uri (file://, http(s)://) of csv file to load
        :param _oid:
            column or func to apply to map _oid in all resulting objects
        :param _start:
            column or func to apply to map _start in all resulting objects
        :param _end:
            column or func to apply to map _end in all resulting objects
        :param kwargs: kwargs to pass to pandas.read_csv method

        _start and _oid arguments can be a column name or a function
        which accepts a single argument -- the row being extracted.

        If either is a column name (string) then that column will be applied
        as _oid for each object generated.

        If either is a function, the function will be applied per each row
        and the result of the function will be assigned to the _start
        or _oid, respectively.",1,0,3,4
"def get_open_fds():
    
    pid = os.getpid()
    procs = subprocess.check_output([""lsof"", '-w', '-Ff', ""-p"", str(pid)])
    procs = procs.decode(""utf-8"")

    return len([s for s in procs.split('\n')
                if s and s[0] == 'f' and s[1:].isdigit()])","Return the number of open file descriptors for current process

    .. warning: will only work on UNIX-like OS-es.",0,0,1,1
"def get_params(self, deep=True):
        

        params = {
            ""loss"": self.loss,
            ""learning_schedule"": self.learning_schedule,
            ""no_components"": self.no_components,
            ""learning_rate"": self.learning_rate,
            ""k"": self.k,
            ""n"": self.n,
            ""rho"": self.rho,
            ""epsilon"": self.epsilon,
            ""max_sampled"": self.max_sampled,
            ""item_alpha"": self.item_alpha,
            ""user_alpha"": self.user_alpha,
            ""random_state"": self.random_state,
        }

        return params","Get parameters for this estimator.

        Arguments
        ---------

        deep: boolean, optional
            If True, will return the parameters for this estimator and
            contained subobjects that are estimators.

        Returns
        -------

        params : mapping of string to any
            Parameter names mapped to their values.",0,0,1,1
"def get_params(self, failobj=None, header='content-type', unquote=True):
        
        missing = object()
        params = self._get_params_preserve(missing, header)
        if params is missing:
            return failobj
        if unquote:
            return [(k, _unquotevalue(v)) for k, v in params]
        else:
            return params","Return the message's Content-Type parameters, as a list.

        The elements of the returned list are 2-tuples of key/value pairs, as
        split on the `=' sign.  The left hand side of the `=' is the key,
        while the right hand side is the value.  If there is no `=' sign in
        the parameter the value is the empty string.  The value is as
        described in the get_param() method.

        Optional failobj is the object to return if there is no Content-Type
        header.  Optional header is the header to search instead of
        Content-Type.  If unquote is True, the value is unquoted.",0,0,1,1
"def get_per_identity_records(self, events: Iterable, data_processor: DataProcessor
                                 ) -> Generator[Tuple[str, TimeAndRecord], None, None]:
        
        schema_loader = SchemaLoader()
        stream_bts_name = schema_loader.add_schema_spec(self._stream_bts)
        stream_transformer_schema: StreamingTransformerSchema = schema_loader.get_schema_object(
            stream_bts_name)
        for event in events:
            try:
                for record in data_processor.process_data(event):
                    try:
                        id = stream_transformer_schema.get_identity(record)
                        time = stream_transformer_schema.get_time(record)
                        yield (id, (time, record))
                    except Exception as err:
                        logging.error('{} in parsing Record {}.'.format(err, record))
            except Exception as err:
                logging.error('{} in parsing Event {}.'.format(err, event))","Uses the given iteratable events and the data processor convert the event into a list of
        Records along with its identity and time.
        :param events: iteratable events.
        :param data_processor: DataProcessor to process each event in events.
        :return: yields Tuple[Identity, TimeAndRecord] for all Records in events,",0,2,2,4
"def get_phonon_frequencies(self):
        
        
        
        frequencies = []
        for k, v0 in self.data.iteritems():
            for v1 in v0.itervalues():
                vec = map(abs, v1['dynmat'][k - 1])
                frequency = math.sqrt(sum(vec)) * 2. * \
                            math.pi * 15.633302  
                frequencies.append(frequency)
        return frequencies",calculate phonon frequencies,0,0,1,1
"def get_pipeline_stage(self, pipeline_key, stage_key = None, sort_by = None):
		
		if not pipeline_key:
			return requests.codes.bad_request, None

		uri = '/'.join([
						self.api_uri,
						self.pipelines_suffix,
						pipeline_key,
						self.stages_suffix
						])
		if stage_key:
			uri = '/'.join([
							uri,
							stage_key
							])
		
		if sort_by:
				if sort_by in ['creationTimestamp', 'lastUpdatedTimestamp']:
					uri += self.sort_by_postfix + sort_by
				else:		
					return requests.codes.bad_request, {'success' : 'False', 
												'error': 'sortBy needs to be \'creationTimestamp\', or \'lastUpdatedTimestamp\''}

		code, data = self._req('get', uri)
		
		
		if stage_key:
			data = list(data.values())
		
		return code, data","Gets a list of one/all stage objects in a pipeline. Performs a single GET.
                Args:
                        pipeline_key        key for pipeline
                        stage_key                 key for stage (default: None i.e. ALL)
                        sort_by                        in desc order by 'creationTimestamp' or 'lastUpdatedTimestamp'
                                                        may or may not be supported
                        returns                 (status code for the GET request, dict of stages)
                                                        It is not a list hence the .values() before return",0,1,1,2
"def get_plink_version():
    
    
    tmp_fn = None
    with tempfile.NamedTemporaryFile(delete=False) as tmpfile:
        tmp_fn = tmpfile.name + ""_pyGenClean""

    
    command = [""plink"", ""--noweb"", ""--out"", tmp_fn]

    output = None
    try:
        proc = Popen(command, stdout=PIPE, stderr=PIPE)
        output = proc.communicate()[0].decode()
    except OSError:
        raise ProgramError(""plink: command not found"")

    
    if os.path.isfile(tmp_fn + "".log""):
        os.remove(tmp_fn + "".log"")

    
    version = re.search(r""\|\s+PLINK!\s+\|\s+(\S+)\s+\|"", output)
    if version is None:
        version = ""unknown""
    else:
        version = version.group(1)

    return version","Gets the Plink version from the binary.

    :returns: the version of the Plink software
    :rtype: str

    This function uses :py:class:`subprocess.Popen` to gather the version of
    the Plink binary. Since executing the software to gather the version
    creates an output file, it is deleted.

    .. warning::
        This function only works as long as the version is returned as
        ``| PLINK! | NNN |`` (where, ``NNN`` is the version), since we use
        regular expresion to extract the version number from the standard
        output of the software.",2,1,4,7
"def get_policy_configuration_revision(self, project, configuration_id, revision_id):
        
        route_values = {}
        if project is not None:
            route_values['project'] = self._serialize.url('project', project, 'str')
        if configuration_id is not None:
            route_values['configurationId'] = self._serialize.url('configuration_id', configuration_id, 'int')
        if revision_id is not None:
            route_values['revisionId'] = self._serialize.url('revision_id', revision_id, 'int')
        response = self._send(http_method='GET',
                              location_id='fe1e68a2-60d3-43cb-855b-85e41ae97c95',
                              version='5.0',
                              route_values=route_values)
        return self._deserialize('PolicyConfiguration', response)","GetPolicyConfigurationRevision.
        Retrieve a specific revision of a given policy by ID.
        :param str project: Project ID or project name
        :param int configuration_id: The policy configuration ID.
        :param int revision_id: The revision ID.
        :rtype: :class:`<PolicyConfiguration> <azure.devops.v5_0.policy.models.PolicyConfiguration>`",0,1,0,1
"def get_prep_lookup(self, lookup_name, rhs):
        
        if lookup_name == 'exact':
            if not isinstance(rhs, Model):
                raise FilteredGenericForeignKeyFilteringException(
                    ""For exact lookup, please pass a single Model instance."")

        elif lookup_name in ['in', 'in_raw']:
            if type(rhs) == QuerySet:
                return rhs, None

            if not is_iterable(rhs):
                raise FilteredGenericForeignKeyFilteringException(
                    ""For 'in' lookup, please pass an iterable or a QuerySet."")

        else:
            raise FilteredGenericForeignKeyFilteringException(
                ""Lookup %s not supported."" % lookup_name)

        return rhs, None",Perform preliminary non-db specific lookup checks and conversions,3,0,5,8
"def get_project(self, owner, id, **kwargs):
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('callback'):
            return self.get_project_with_http_info(owner, id, **kwargs)
        else:
            (data) = self.get_project_with_http_info(owner, id, **kwargs)
            return data","Retrieve a project
        Return details on a project.
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please define a `callback` function
        to be invoked when receiving the response.
        >>> def callback_function(response):
        >>>     pprint(response)
        >>>
        >>> thread = api.get_project(owner, id, callback=callback_function)

        :param callback function: The callback function
            for asynchronous request. (optional)
        :param str owner: User name and unique identifier of the creator of a project. For example, in the URL: [https://data.world/government/how-to-add-depth-to-your-data-with-the-us-census-acs](https://data.world/government/how-to-add-depth-to-your-data-with-the-us-census-acs), government is the unique identifier of the owner. (required)
        :param str id: Project unique identifier. For example, in the URL:[https://data.world/government/how-to-add-depth-to-your-data-with-the-us-census-acs](https://data.world/government/how-to-add-depth-to-your-data-with-the-us-census-acs), how-to-add-depth-to-your-data-with-the-us-census-acs is the unique identifier of the project. (required)
        :return: ProjectSummaryResponse
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def get_queryset(self):
        
        self.init_preferences()
        queryset = super(PreferenceViewSet, self).get_queryset()

        section = self.request.query_params.get('section')
        if section:
            queryset = queryset.filter(section=section)

        return queryset","We just ensure preferences are actually populated before fetching
        from db",0,1,2,3
"def get_rbounds(step):
    
    if step.geom is not None:
        rcmb = step.geom.rcmb
    else:
        rcmb = step.sdat.par['geometry']['r_cmb']
        if step.sdat.par['geometry']['shape'].lower() == 'cartesian':
            rcmb = 0
    rcmb = max(rcmb, 0)
    return rcmb, rcmb + 1","Radial or vertical position of boundaries.

    Args:
        step (:class:`~stagpy.stagyydata._Step`): a step of a StagyyData
            instance.
    Returns:
        tuple of floats: radial or vertical positions of boundaries of the
        domain.",0,0,1,1
"def get_receipt_by_index(self,
                             block_number: BlockNumber,
                             receipt_index: int) -> Receipt:
        
        try:
            block_header = self.get_canonical_block_header_by_number(block_number)
        except HeaderNotFound:
            raise ReceiptNotFound(""Block {} is not in the canonical chain"".format(block_number))

        receipt_db = HexaryTrie(db=self.db, root_hash=block_header.receipt_root)
        receipt_key = rlp.encode(receipt_index)
        if receipt_key in receipt_db:
            receipt_data = receipt_db[receipt_key]
            return rlp.decode(receipt_data, sedes=Receipt)
        else:
            raise ReceiptNotFound(
                ""Receipt with index {} not found in block"".format(receipt_index))","Returns the Receipt of the transaction at specified index
        for the block header obtained by the specified block number",3,0,2,5
"def get_recent_pages(self, namespaces, rccontinue=''):
        

        namespaces.sort()
        params = {
            ""action"": ""query"",
            ""list"": ""recentchanges"",
            ""rclimit"": self.limit,
            ""rcnamespace"": ""|"".join(namespaces),
            ""rcprop"": ""title|timestamp|ids"",
            ""format"": ""json""
        }
        if rccontinue:
            params['rccontinue'] = rccontinue

        return self.call(params)",Retrieve recent pages from all namespaces starting from rccontinue.,0,1,1,2
"def get_ref_annotation_data_between_times(self, id_tier, start, end):
        
        bucket = []
        for aid, (ref, value, _, _) in self.tiers[id_tier][1].items():
            begin, end, rvalue, _ = self.tiers[self.annotations[ref]][0][ref]
            begin = self.timeslots[begin]
            end = self.timeslots[end]
            if begin <= end and end >= begin:
                bucket.append((begin, end, value, rvalue))
        return bucket","Give the ref annotations between times of the form
        ``[(start, end, value, refvalue)]``

        :param str tier: Name of the tier.
        :param int start: End time of the annotation of the parent.
        :param int end: Start time of the annotation of the parent.
        :returns: List of annotations at that time.
        :raises KeyError: If the tier is non existent.",0,0,1,1
"def get_ref_favorites(self, project, repository_id=None, identity_id=None):
        
        route_values = {}
        if project is not None:
            route_values['project'] = self._serialize.url('project', project, 'str')
        query_parameters = {}
        if repository_id is not None:
            query_parameters['repositoryId'] = self._serialize.query('repository_id', repository_id, 'str')
        if identity_id is not None:
            query_parameters['identityId'] = self._serialize.query('identity_id', identity_id, 'str')
        response = self._send(http_method='GET',
                              location_id='876f70af-5792-485a-a1c7-d0a7b2f42bbb',
                              version='5.1-preview.1',
                              route_values=route_values,
                              query_parameters=query_parameters)
        return self._deserialize('[GitRefFavorite]', self._unwrap_collection(response))","GetRefFavorites.
        [Preview API] Gets the refs favorites for a repo and an identity.
        :param str project: Project ID or project name
        :param str repository_id: The id of the repository.
        :param str identity_id: The id of the identity whose favorites are to be retrieved. If null, the requesting identity is used.
        :rtype: [GitRefFavorite]",0,1,1,2
"def get_refs(self, location):
        
        output = call_subprocess([self.cmd, 'show-ref'],
                                 show_stdout=False, cwd=location)
        rv = {}
        for line in output.strip().splitlines():
            commit, ref = line.split(' ', 1)
            ref = ref.strip()
            ref_name = None
            if ref.startswith('refs/remotes/'):
                ref_name = ref[len('refs/remotes/'):]
            elif ref.startswith('refs/heads/'):
                ref_name = ref[len('refs/heads/'):]
            elif ref.startswith('refs/tags/'):
                ref_name = ref[len('refs/tags/'):]
            if ref_name is not None:
                rv[ref_name] = commit.strip()
        return rv",Return map of named refs (branches or tags) to commit hashes.,0,1,1,2
"def get_relationship_form_for_update(self, relationship_id=None):
        
        if relationship_id is None:
            raise NullArgument()
        try:
            url_path = ('/handcar/services/relationship/families/' +
                        self._catalog_idstr + '/relationships/' + str(relationship_id))
            relationship = objects.Relationship(self._get_request(url_path))
        except Exception:
            raise
        relationship_form = objects.RelationshipForm(relationship._my_map)
        self._forms[relationship_form.get_id().get_identifier()] = not UPDATED
        return relationship_form","Gets the relationship form for updating an existing relationship.

        A new relationship form should be requested for each update
        transaction.

        arg:    relationship_id (osid.id.Id): the ``Id`` of the
                ``Relationship``
        return: (osid.relationship.RelationshipForm) - the relationship
                form
        raise:  NotFound - ``relationship_id`` is not found
        raise:  NullArgument - ``relationship_id`` is ``null``
        raise:  OperationFailed - unable to complete request
        raise:  PermissionDenied - authorization failure
        *compliance: mandatory -- This method must be implemented.*",1,1,1,3
"def get_relationship_query_session_for_family(self, family_id):
        
        if not self.supports_relationship_query():
            raise errors.Unimplemented()
        
        
        
        
        return sessions.RelationshipQuerySession(family_id, runtime=self._runtime)","Gets the ``OsidSession`` associated with the relationship query service for the given family.

        arg:    family_id (osid.id.Id): the ``Id`` of the family
        return: (osid.relationship.RelationshipQuerySession) - a
                ``RelationshipQuerySession``
        raise:  NotFound - no ``Family`` found by the given ``Id``
        raise:  NullArgument - ``family_id`` is ``null``
        raise:  OperationFailed - unable to complete request
        raise:  Unimplemented - ``supports_relationship_query()`` or
                ``supports_visible_federation()`` is ``false``
        *compliance: optional -- This method must be implemented if
        ``supports_relationship_query()`` and
        ``supports_visible_federation()`` are ``true``*",1,1,1,3
"def get_rendered_transform_path_relative(self, relative_transform_ref):
        
        path = self.transform_path
        parent = self.parent

        while parent is not None and parent is not relative_transform_ref:
            path = ""{0}/{1}"".format(parent.transform_path, path)
            parent = parent.parent

        return path","Generates a rendered transform path relative to
        parent.
        :param relative_transform_ref:
        :return:",0,0,1,1
"def get_replicas(self, service_id: str) -> str:
        
        
        replicas = []

        
        if not self._manager:
            raise RuntimeError('Only the Swarm manager node can retrieve '
                               'replication level of the service')

        service_tasks = self._client.services.get(service_id).tasks()
        for task in service_tasks:
            if task['Status']['State'] == ""running"":
                replicas.append(task)
        return len(replicas)","Get the replication level of a service.

        Args:
            service_id (str): docker swarm service id

        Returns:
            str, replication level of the service",1,1,2,4
"def get_report_order(self):
        
        order_list = []
        for x in self.__priority:
            order_list.append([x, self[x]])
        for x in sorted(list(self.keys())):
            if x not in self.__priority:
                order_list.append([x, self[x]])
        return order_list","Keys are sorted based on report order (i.e. some keys to be shown first)
            Related: see sorted_by_count",0,0,1,1
"def get_request(self, uuid, raw=False, multiple=False,
                    connection_adapter=None):
        
        if uuid not in self._response:
            return
        self._wait_for_request(
            uuid, connection_adapter or self._default_connection_adapter
        )
        frame = self._get_response_frame(uuid)
        if not multiple:
            self.remove(uuid)
        result = None
        if raw:
            result = frame
        elif frame is not None:
            result = dict(frame)
        return result","Get a RPC request.

        :param str uuid: Rpc Identifier
        :param bool raw: If enabled return the frame as is, else return
                         result as a dictionary.
        :param bool multiple: Are we expecting multiple frames.
        :param obj connection_adapter: Provide custom connection adapter.
        :return:",0,1,3,4
"def get_required_query_params(self, request):
        
        username = get_request_value(request, self.REQUIRED_PARAM_USERNAME, '')
        course_id = get_request_value(request, self.REQUIRED_PARAM_COURSE_ID, '')
        program_uuid = get_request_value(request, self.REQUIRED_PARAM_PROGRAM_UUID, '')
        enterprise_customer_uuid = get_request_value(request, self.REQUIRED_PARAM_ENTERPRISE_CUSTOMER)
        if not (username and (course_id or program_uuid) and enterprise_customer_uuid):
            raise ConsentAPIRequestError(
                self.get_missing_params_message([
                    (""'username'"", bool(username)),
                    (""'enterprise_customer_uuid'"", bool(enterprise_customer_uuid)),
                    (""one of 'course_id' or 'program_uuid'"", bool(course_id or program_uuid)),
                ])
            )
        return username, course_id, program_uuid, enterprise_customer_uuid","Gets ``username``, ``course_id``, and ``enterprise_customer_uuid``,
        which are the relevant query parameters for this API endpoint.

        :param request: The request to this endpoint.
        :return: The ``username``, ``course_id``, and ``enterprise_customer_uuid`` from the request.",1,0,1,2
"def get_resource_attribute(resource_attr_id, **kwargs):
    

    resource_attr_qry = db.DBSession.query(ResourceAttr).filter(
        ResourceAttr.id == resource_attr_id,
        )

    resource_attr = resource_attr_qry.first()

    if resource_attr is None:
        raise ResourceNotFoundError(""Resource attribute %s does not exist"", resource_attr_id)

    return resource_attr","Get a specific resource attribte, by ID
        If type_id is Gspecified, only
        return the resource attributes within the type.",2,0,1,3
"def get_resource_query_session_for_bin(self, bin_id, proxy):
        
        if not self.supports_resource_query():
            raise errors.Unimplemented()
        
        
        
        
        return sessions.ResourceQuerySession(bin_id, proxy, self._runtime)","Gets a resource query session for the given bin.

        arg:    bin_id (osid.id.Id): the ``Id`` of the bin
        arg:    proxy (osid.proxy.Proxy): a proxy
        return: (osid.resource.ResourceQuerySession) - ``a
                ResourceQuerySession``
        raise:  NotFound - ``bin_id`` not found
        raise:  NullArgument - ``bin_id`` or ``proxy`` is ``null``
        raise:  OperationFailed - ``unable to complete request``
        raise:  Unimplemented - ``supports_resource_query()`` or
                ``supports_visible_federation()`` is ``false``
        *compliance: optional -- This method must be implemented if
        ``supports_resource_query()`` and
        ``supports_visible_federation()`` are ``true``.*",1,1,1,3
"def get_resource_value(self, device_id, _resource_path, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('asynchronous'):
            return self.get_resource_value_with_http_info(device_id, _resource_path, **kwargs)  
        else:
            (data) = self.get_resource_value_with_http_info(device_id, _resource_path, **kwargs)  
            return data","Read from a resource  # noqa: E501

        Requests the resource value and when the response is available, an `AsyncIDResponse` json object is received in the notification channel. The preferred way to get resource values is to use the **subscribe** and **callback** methods.  All resource APIs are asynchronous. These APIs only respond if the device is turned on and connected to Device Management.  Please refer to [Lightweight Machine to Machine Technical specification](http://www.openmobilealliance.org/release/LightweightM2M/V1_0-20170208-A/OMA-TS-LightweightM2M-V1_0-20170208-A.pdf) for more inforamtion.  **Example usage:**      curl -X GET \\       https://api.us-east-1.mbedcloud.com/v2/endpoints/{device-id}/{resourcePath} \\       -H 'authorization: Bearer {api-key}'   # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass asynchronous=True
        >>> thread = api.get_resource_value(device_id, _resource_path, asynchronous=True)
        >>> result = thread.get()

        :param asynchronous bool
        :param str device_id: Unique Device Management device ID for the endpoint. Note that the ID needs to be an exact match. You cannot use wildcards here.  (required)
        :param str _resource_path: The URL of the resource.  (required)
        :param bool cache_only: If true, the response comes only from the cache. Default: false. Device Management Connect caches the received resource values for the time of [max_age](/docs/current/connecting/working-with-the-resources.html) defined in the client side. 
        :param bool no_resp: <br/><br/><b>Non-confirmable requests</b><br/>  All resource APIs have the parameter `noResp`. If a request is made with `noResp=true`, Device Management Connect makes a CoAP non-confirmable request to the device. Such requests are not guaranteed to arrive in the device, and you do not get back an async-response-id.  If calls with this parameter enabled succeed, they return with the status code `204 No Content`. If the underlying protocol does not support non-confirmable requests, or if the endpoint is registered in queue mode, the response is status code `409 Conflict`. 
        :return: None
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def get_response(self, **kwargs):
        
        return Response(
            json.dumps({""error"": self.message}),  
            status_code=self.status_code,
            content_type=""application/json"",
            charset=""utf-8"",
            **kwargs
        )","Returns a Response object containing this object's status code and a
        JSON object containing the key ""error"" with the value of this object's
        error message in the body. Keyword args are passed through to
        the Response.",0,0,1,1
"def get_room_member_ids(self, room_id, start=None, timeout=None):
        
        params = None if start is None else {'start': start}

        response = self._get(
            '/v2/bot/room/{room_id}/members/ids'.format(room_id=room_id),
            params=params,
            timeout=timeout
        )

        return MemberIds.new_from_json_dict(response.json)","Call get room member IDs API.

        https://devdocs.line.me/en/#get-group-room-member-ids

        Gets the user IDs of the members of a group that the bot is in.
        This includes the user IDs of users who have not added the bot as a friend
        or has blocked the bot.

        :param str room_id: Room ID
        :param str start: continuationToken
        :param timeout: (optional) How long to wait for the server
            to send data before giving up, as a float,
            or a (connect timeout, read timeout) float tuple.
            Default is self.http_client.timeout
        :type timeout: float | tuple(float, float)
        :rtype: :py:class:`linebot.models.responses.MemberIds`
        :return: MemberIds instance",0,1,0,1
"def get_scalingip(context, id, fields=None):
    
    LOG.info('get_scalingip %s for tenant %s' % (id, context.tenant_id))
    filters = {'address_type': ip_types.SCALING, '_deallocated': False}
    scaling_ip = db_api.floating_ip_find(context, id=id, scope=db_api.ONE,
                                         **filters)
    if not scaling_ip:
        raise q_exc.ScalingIpNotFound(id=id)
    return v._make_scaling_ip_dict(scaling_ip)","Retrieve a scaling IP.

    :param context: neutron api request context.
    :param id: The UUID of the scaling IP.
    :param fields: a list of strings that are valid keys in a
        scaling IP dictionary as listed in the RESOURCE_ATTRIBUTE_MAP
        object in neutron/api/v2/attributes.py. Only these fields
        will be returned.

    :returns: Dictionary containing details for the scaling IP.  If values
        are declared in the fields parameter, then only those keys will be
        present.",2,1,1,4
"def get_schema(self, table_name, database=None):
        
        col_names = []
        col_types = []

        for col in self.con.get_table_details(table_name):
            col_names.append(col.name)
            col_types.append(MapDDataType.parse(col.type))

        return sch.schema(
            [
                (col.name, MapDDataType.parse(col.type))
                for col in self.con.get_table_details(table_name)
            ]
        )","Return a Schema object for the indicated table and database

        Parameters
        ----------
        table_name : string
          May be fully qualified
        database : string, default None

        Returns
        -------
        schema : ibis Schema",1,0,0,1
"def get_screen_settings(self, screen_id):
        
        if not isinstance(screen_id, baseinteger):
            raise TypeError(""screen_id can only be an instance of type baseinteger"")
        record_screen_settings = self._call(""getScreenSettings"",
                     in_p=[screen_id])
        record_screen_settings = IRecordingScreenSettings(record_screen_settings)
        return record_screen_settings","Returns the recording settings for a particular screen.

        in screen_id of type int
            Screen ID to retrieve recording screen settings for.

        return record_screen_settings of type :class:`IRecordingScreenSettings`
            Recording screen settings for the requested screen.",1,0,3,4
"def get_sequences(options, fasta_file_contents):
    
    errors = []
    fasta_files_str = "", "".join(fasta_file_contents.keys())
    fasta_records = None
    reverse_mapping = {}

    try:
        fasta_records, reverse_mapping = parse_FASTA_files(options, fasta_file_contents)
        if not fasta_records:
            errors.append(""No protein sequences found in the FASTA file(s) %s."" % fasta_files_str)
    except Exception, e:
        e = '\n'.join([l for l in traceback.format_exc(), str('e') if l.strip()])
        errors.append(""Error parsing FASTA file(s) %s:\n%s"" % (fasta_files_str, str(e)))

    if not fasta_records:
        return None, {}, errors

    colorprinter.message('Found %d protein sequence(s).' % len(fasta_records))
    return fasta_records, reverse_mapping, errors","This function returns a dict mapping (pdbid, chain, file_name) tuples to sequences:
          - options is the OptionParser member;
          - fasta_file_contents is a map from input filenames to the associated FASTA file contents.",2,0,1,3
"def get_service_categories(self, restricted=True):
        
        bsc = api.get_tool(""bika_setup_catalog"")
        query = {
            ""portal_type"": ""AnalysisCategory"",
            ""is_active"": True,
            ""sort_on"": ""sortable_title"",
        }
        categories = bsc(query)
        client = self.get_client()
        if client and restricted:
            restricted_categories = client.getRestrictedCategories()
            restricted_category_ids = map(
                lambda c: c.getId(), restricted_categories)
            
            if restricted_category_ids:
                categories = filter(
                    lambda c: c.getId in restricted_category_ids, categories)
        return categories","Return all service categories in the right order

        :param restricted: Client settings restrict categories
        :type restricted: bool
        :returns: Category catalog results
        :rtype: brains",0,1,2,3
"def get_service_info(service_instance):
    
    try:
        return service_instance.content.about
    except vim.fault.NoPermission as exc:
        log.exception(exc)
        raise salt.exceptions.VMwareApiError(
            'Not enough permissions. Required privilege: '
            '{0}'.format(exc.privilegeId))
    except vim.fault.VimFault as exc:
        log.exception(exc)
        raise salt.exceptions.VMwareApiError(exc.msg)
    except vmodl.RuntimeFault as exc:
        log.exception(exc)
        raise salt.exceptions.VMwareRuntimeError(exc.msg)","Returns information of the vCenter or ESXi host

    service_instance
        The Service Instance from which to obtain managed object references.",3,3,1,7
"def get_service_name(*args):
    
    raw_services = _get_services()

    services = dict()
    for raw_service in raw_services:
        if args:
            if raw_service['DisplayName'] in args or \
                    raw_service['ServiceName'] in args or \
                    raw_service['ServiceName'].lower() in args:
                services[raw_service['DisplayName']] = raw_service['ServiceName']
        else:
            services[raw_service['DisplayName']] = raw_service['ServiceName']

    return services","The Display Name is what is displayed in Windows when services.msc is
    executed.  Each Display Name has an associated Service Name which is the
    actual name of the service.  This function allows you to discover the
    Service Name by returning a dictionary of Display Names and Service Names,
    or filter by adding arguments of Display Names.

    If no args are passed, return a dict of all services where the keys are the
    service Display Names and the values are the Service Names.

    If arguments are passed, create a dict of Display Names and Service Names

    Returns:
        dict: A dictionary of display names and service names

    CLI Examples:

    .. code-block:: bash

        salt '*' service.get_service_name
        salt '*' service.get_service_name 'Google Update Service (gupdate)' 'DHCP Client'",0,1,3,4
"def get_session_key(
        user,
        password,
        url='https://localhost:8089',
        verify=False,
        ssl_options=None):
    
    servercontent = requests.post(
        '{}/services/auth/login'.format(
            url),
        headers={},
        verify=False,
        data={
            'username': user,
            'password': password
        })
    get_session_key = parseString(
        servercontent.text).getElementsByTagName(
            'sessionKey')[0].childNodes[0].nodeValue
    return get_session_key","get_session_key

    This will get a user session key and throw for any errors

    :param user: username
    :param password: password
    :param url: splunk auth url
    :param verify: verify cert
    :param ssl_options: ssl options dictionary",0,1,1,2
"def get_socket(self, all_credentials, checkout=False):
        
        
        
        sock_info = self._get_socket_no_auth()
        try:
            sock_info.check_auth(all_credentials)
            yield sock_info
        except:
            
            self.return_socket(sock_info)
            raise
        else:
            if not checkout:
                self.return_socket(sock_info)","Get a socket from the pool. Use with a ""with"" statement.

        Returns a :class:`SocketInfo` object wrapping a connected
        :class:`socket.socket`.

        This method should always be used in a with-statement::

            with pool.get_socket(credentials, checkout) as socket_info:
                socket_info.send_message(msg)
                data = socket_info.receive_message(op_code, request_id)

        The socket is logged in or out as needed to match ``all_credentials``
        using the correct authentication mechanism for the server's wire
        protocol version.

        Can raise ConnectionFailure or OperationFailure.

        :Parameters:
          - `all_credentials`: dict, maps auth source to MongoCredential.
          - `checkout` (optional): keep socket checked out.",0,1,1,2
"def get_source_file(filename, include_no_ext=False):
    
    filename = os.path.abspath(_path_from_filename(filename))
    base, orig_ext = os.path.splitext(filename)
    for ext in PY_SOURCE_EXTS:
        source_path = ""%s.%s"" % (base, ext)
        if os.path.exists(source_path):
            return source_path
    if include_no_ext and not orig_ext and os.path.exists(base):
        return base
    raise NoSourceFile(filename)","given a python module's file name return the matching source file
    name (the filename will be returned identically if it's already an
    absolute path to a python source file...)

    :type filename: str
    :param filename: python module's file name


    :raise NoSourceFile: if no source file exists on the file system

    :rtype: str
    :return: the absolute path of the source file if it exists",1,0,1,2
"def get_space_content(self, space_key, depth=None, expand=None, start=None, limit=None, callback=None):
        
        params = {}
        if depth:
            assert depth in {""all"", ""root""}
            params[""depth""] = depth
        if expand:
            params[""expand""] = expand
        if start is not None:
            params[""start""] = int(start)
        if limit is not None:
            params[""limit""] = int(limit)
        return self._service_get_request(""rest/api/space/{key}/content"".format(key=space_key),
                                         params=params, callback=callback)","Returns the content in this given space.
        :param space_key (string): A string containing the key of the space.
        :param depth (string): OPTIONAL: A string indicating if all content, or just the root content of the space is
                               returned. Default: ""all"". Valid values: ""all"", ""root"".
        :param expand (string): OPTIONAL: A comma separated list of properties to expand on each piece of content
                                retrieved. Default: Empty.
        :param start (int): OPTIONAL: The start point of the collection to return. Default: 0.
        :param limit (int): OPTIONAL: The limit of the number of labels to return, this may be restricted by fixed
                            system limits. Default: 25.
        :param callback: OPTIONAL: The callback to execute on the resulting data, before the method returns.
                         Default: None (no callback, raw data returned).
        :return: The JSON data returned from the space/{spaceKey}/content endpoint,
                 or the results of the callback. Will raise requests.HTTPError on bad input, potentially.",0,1,2,3
"def get_space_content_by_type(self, space_key, content_type, depth=None, expand=None, start=None, limit=None,
                                  callback=None):
        
        assert content_type in [""page"", ""blogpost""]
        params = {}
        if depth:
            assert depth in {""all"", ""root""}
            params[""depth""] = depth
        if expand:
            params[""expand""] = expand
        if start is not None:
            params[""start""] = int(start)
        if limit is not None:
            params[""limit""] = int(limit)
        return self._service_get_request(""rest/api/space/{key}/content/{type}"".format(key=space_key, type=content_type),
                                         params=params, callback=callback)","Returns the content in this given space with the given type.
        :param space_key (string): A string containing the key of the space.
        :param content_type (string): The type of content to return with the space. Valid values: ""page"", ""blogpost"".
        :param depth (string): OPTIONAL: A string indicating if all content, or just the root content of the space is
                               returned. Default: ""all"". Valid values: ""all"", ""root"".
        :param expand (string): OPTIONAL: A comma separated list of properties to expand on each piece of content
                                retrieved. Default: Empty.
        :param start (int): OPTIONAL: The start point of the collection to return. Default: 0.
        :param limit (int): OPTIONAL: The limit of the number of labels to return, this may be restricted by fixed
                            system limits. Default: 25.
        :param callback: OPTIONAL: The callback to execute on the resulting data, before the method returns.
                         Default: None (no callback, raw data returned).
        :return: The JSON data returned from the space/{spaceKey}/content/{type} endpoint,
                 or the results of the callback. Will raise requests.HTTPError on bad input, potentially.",0,1,4,5
"def get_sql_insert(table: str,
                   fieldlist: Sequence[str],
                   delims: Tuple[str, str] = ("""", """")) -> str:
    
    return (
        ""INSERT INTO "" + delimit(table, delims) +
        "" ("" +
        "","".join([delimit(x, delims) for x in fieldlist]) +
        "") VALUES ("" +
        "","".join([""?""] * len(fieldlist)) +
        "")""
    )",Returns ?-marked SQL for an INSERT statement.,1,0,0,1
"def get_src_folder(self):
    
    with open('%s/settings.gradle' % self.path) as f:
      for line in f.readlines():
        if line.startswith('include'):
          matches = re.findall(r'\'\:?(.+?)\'', line)
        if len(matches) == 0:
          continue
        for folder in matches:
          if self.is_app_folder(folder):
            return folder
    return 'app'","Gets the app source folder from settings.gradle file.

    Returns:
      A string containing the project source folder name (default is ""app"")",1,0,1,2
"def get_standard_dicomdir_info(self):
        
        dicomdir_filepath = os.path.join(self.dirpath, self.standard_dicomdir_filename)
        if not os.path.exists(dicomdir_filepath):
            self.create_standard_dicomdir()
        return self.read_standard_dicomdir_info()","Read DICOMDIR, crate if necessary.
        :return:",2,0,1,3
"def get_state_transition_function(self):
        
        state_transition_function = []
        for variable in self.network.findall('StateTransitionFunction'):
            for var in variable.findall('CondProb'):
                cond_prob = defaultdict(list)
                cond_prob['Var'] = var.find('Var').text
                cond_prob['Parent'] = var.find('Parent').text.split()
                if not var.find('Parameter').get('type'):
                    cond_prob['Type'] = 'TBL'
                else:
                    cond_prob['Type'] = var.find('Parameter').get('type')
                cond_prob['Parameter'] = self.get_parameter(var)
                state_transition_function.append(cond_prob)

        return state_transition_function","Returns the transition of the state variables as nested dict in the
        case of table type parameter and a nested structure in case of
        decision diagram parameter

        Example
        --------
        >>> reader = PomdpXReader('Test_PomdpX.xml')
        >>> reader.get_state_transition_function()
        [{'Var': 'rover_1',
          'Parent': ['action_rover', 'rover_0'],
          'Type': 'TBL',
          'Parameter': [{'Instance': ['amw', 's0', 's2'],
                         'ProbTable': ['1.0']},
                         {'Instance': ['amw', 's1', 's0'],
                         'ProbTable': ['1.0']},
                         ...
                        ]
        }]",0,0,3,3
"def get_status(self, mxit_id, scope='profile/public'):
        
        status = _get(
            token=self.oauth.get_app_token(scope),
            uri='/user/public/statusmessage/' + urllib.quote(mxit_id)
        )

        if status.startswith('""') and status.endswith('""'):
            status = status[1:-1]

        return status","Retrieve the Mxit user's current status
        No user authentication required",0,1,0,1
"def get_supported_metrics_topic(self, name, topic_name):
        
        response = self._perform_get(
            self._get_get_supported_metrics_topic_path(name, topic_name),
            None)

        return _MinidomXmlToObject.convert_response_to_feeds(
            response,
            partial(
                _ServiceBusManagementXmlSerializer.xml_to_metrics,
                object_type=MetricProperties
            )
        )","Retrieves the list of supported metrics for this namespace and topic

        name:
            Name of the service bus namespace.
        topic_name:
            Name of the service bus queue in this namespace.",0,1,1,2
"def get_system_category(auth, url):
    
    get_system_category_url = '/imcrs/plat/res/category?start=0&size=10000&orderBy=id&desc=false&total=false'
    f_url = url + get_system_category_url
    
    r = requests.get(f_url, auth=auth, headers=HEADERS)
    
    try:
        if r.status_code == 200:
            system_category = (json.loads(r.text))
            return system_category['deviceCategory']
    except requests.exceptions.RequestException as e:
        return ""Error:\n"" + str(e) + "" get_dev_details: An Error has occured""","Takes string no input to issue RESTUL call to HP IMC\n

      :param auth: requests auth object #usually auth.creds from auth pyhpeimc.auth.class

      :param url: base url of IMC RS interface #usually auth.url from pyhpeimc.auth.authclass

      :return: list of dictionaries where each dictionary represents a single device category

      :rtype: list

      >>> from pyhpeimc.auth import *

      >>> from pyhpeimc.plat.device import *

      >>> auth = IMCAuth(""http://"", ""10.101.0.203"", ""8080"", ""admin"", ""admin"")

      >>> categories = get_system_category(auth.creds, auth.url)

      >>> assert type(categories) is list

      >>> assert 'name' in categories[0]",0,1,3,4
"def get_table(self, name='Meta', h5loc='/meta'):
        
        if not self.meta:
            return None

        data = defaultdict(list)
        for entry in self.meta:
            for key, value in entry.items():
                data[key].append(value)
        dtypes = []
        for key, values in data.items():
            max_len = max(map(len, values))
            dtype = 'S{}'.format(max_len)
            dtypes.append((key, dtype))
        tab = Table(
            data, dtype=dtypes, h5loc=h5loc, name='Meta', h5singleton=True
        )
        return tab","Convert metadata to a KM3Pipe Table.

        Returns `None` if there is no data.

        Each column's dtype will be set to a fixed size string (numpy.string_)
        with the length of the longest entry, since writing variable length
        strings does not fit the current scheme.",0,0,1,1
"def get_tenant(self, tenant_id):
        
        route_values = {}
        if tenant_id is not None:
            route_values['tenantId'] = self._serialize.url('tenant_id', tenant_id, 'str')
        response = self._send(http_method='GET',
                              location_id='5f0a1723-2e2c-4c31-8cae-002d01bdd592',
                              version='5.0-preview.1',
                              route_values=route_values)
        return self._deserialize('TenantInfo', response)","GetTenant.
        [Preview API]
        :param str tenant_id:
        :rtype: :class:`<TenantInfo> <azure.devops.v5_0.identity.models.TenantInfo>`",0,1,0,1
"def get_text(self, instance):
        
        instance = instance[0]
        if isinstance(instance, (six.text_type, six.string_types)):
            return serializers.CharField(read_only=True).to_representation(instance)
        elif isinstance(instance, datetime):
            return serializers.DateTimeField(read_only=True).to_representation(instance)
        return instance","Haystack facets are returned as a two-tuple (value, count).
        The text field should contain the faceted value.",0,0,1,1
"def get_tgt(self, temperature=None, structure=None, quad=None):
        
        if temperature and not structure:
            raise ValueError(""If using temperature input, you must also ""
                             ""include structure"")

        quad = quad if quad else DEFAULT_QUAD
        points = quad['points']
        weights = quad['weights']
        num, denom, c = np.zeros((3, 3)), 0, 1
        for p, w in zip(points, weights):
            gk = ElasticTensor(self[0]).green_kristoffel(p)
            rho_wsquareds, us = np.linalg.eigh(gk)
            us = [u / np.linalg.norm(u) for u in np.transpose(us)]
            for u in us:
                
                if temperature:
                    c = self.get_heat_capacity(temperature, structure, p, u)
                num += c*self.get_ggt(p, u) * w
                denom += c * w
        return SquareTensor(num / denom)","Gets the thermodynamic Gruneisen tensor (TGT) by via an
        integration of the GGT weighted by the directional heat
        capacity.

        See refs:
            R. N. Thurston and K. Brugger, Phys. Rev. 113, A1604 (1964).
            K. Brugger Phys. Rev. 137, A1826 (1965).

        Args:
            temperature (float): Temperature in kelvin, if not specified
                will return non-cv-normalized value
            structure (float): Structure to be used in directional heat
                capacity determination, only necessary if temperature
                is specified
            quad (dict): quadrature for integration, should be
                dictionary with ""points"" and ""weights"" keys defaults
                to quadpy.sphere.Lebedev(19) as read from file",1,0,2,3
"def get_threshold_values(self, application_id):
        
        endpoint = ""https://rpm.newrelic.com""
        remote_file = ""threshold_values.xml""
        uri = ""{endpoint}/accounts/{account_id}/applications/{app_id}/{xml}"".format(endpoint=endpoint, account_id=self.account_id, app_id=application_id, xml=remote_file)
        response = self._make_get_request(uri)
        thresholds = []

        for threshold_value in response.findall('.//threshold_value'):
            properties = {}
            
            for tag, text in threshold_value.items():
                properties[tag] = text
            thresholds.append(Threshold(properties))
        return thresholds","Requires: account ID, list of application ID
        Method: Get
        Endpoint: api.newrelic.com
        Restrictions: ???
        Errors: 403 Invalid API key, 422 Invalid Parameters
        Returns: A list of threshold_value objects, each will have information
                 about its start/end time, metric name, metric value, and the
                 current threshold",0,1,1,2
"def get_throttled_by_consumed_read_percent(
        table_name, lookback_window_start=15, lookback_period=5):
    

    try:
        metrics1 = __get_aws_metric(
            table_name,
            lookback_window_start,
            lookback_period,
            'ConsumedReadCapacityUnits')
        metrics2 = __get_aws_metric(
            table_name,
            lookback_window_start,
            lookback_period,
            'ReadThrottleEvents')
    except BotoServerError:
        raise

    if metrics1 and metrics2:
        lookback_seconds = lookback_period * 60
        throttled_by_consumed_read_percent = (
            (
                (float(metrics2[0]['Sum']) / float(lookback_seconds)) /
                (float(metrics1[0]['Sum']) / float(lookback_seconds))
            ) * 100)
    else:
        throttled_by_consumed_read_percent = 0

    logger.info('{0} - Throttled read percent by consumption: {1:.2f}%'.format(
        table_name, throttled_by_consumed_read_percent))
    return throttled_by_consumed_read_percent","Returns the number of throttled read events in percent of consumption

    :type table_name: str
    :param table_name: Name of the DynamoDB table
    :type lookback_window_start: int
    :param lookback_window_start: Relative start time for the CloudWatch metric
    :type lookback_period: int
    :param lookback_period: Number of minutes to look at
    :returns: float -- Percent of throttled read events by consumption",1,3,2,6
"def get_thumbnail(self, mxcurl, width, height, method='scale', allow_remote=True):
        
        if method not in ['scale', 'crop']:
            raise ValueError(
                ""Unsupported thumb method '%s'"" % method
            )
        query_params = {
                    ""width"": width,
                    ""height"": height,
                    ""method"": method
                }
        if not allow_remote:
            query_params[""allow_remote""] = False
        if mxcurl.startswith('mxc://'):
            return self._send(
                ""GET"", mxcurl[6:],
                query_params=query_params,
                api_path=""/_matrix/media/r0/thumbnail/"",
                return_json=False
            )
        else:
            raise ValueError(
                ""MXC URL '%s' did not begin with 'mxc://'"" % mxcurl
            )","Download raw media thumbnail from provided mxc URL.

        Args:
            mxcurl (str): mxc media URL
            width (int): desired thumbnail width
            height (int): desired thumbnail height
            method (str): thumb creation method. Must be
                in ['scale', 'crop']. Default 'scale'.
            allow_remote (bool): indicates to the server that it should not
                attempt to fetch the media if it is deemed remote. Defaults
                to true if not provided.",2,1,2,5
"def get_timeline(self, auth_secret, max_cnt_tweets):
        
        result = {pytwis_constants.ERROR_KEY: None}

        if auth_secret == '':
            
            timeline_key = pytwis_constants.GENERAL_TIMELINE_KEY
        else:
            
            loggedin, userid = self._is_loggedin(auth_secret)
            if not loggedin:
                result[pytwis_constants.ERROR_KEY] = pytwis_constants.ERROR_NOT_LOGGED_IN
                return (False, result)

            
            timeline_key = pytwis_constants.USER_TIMELINE_KEY_FORMAT.format(userid)

        result[pytwis_constants.TWEETS_KEY] = self._get_tweets(timeline_key, max_cnt_tweets)
        return (True, result)","Get the general or user timeline.

        If an empty authentication secret is given, this method returns the general timeline.
        If an authentication secret is given and it is valid, this method returns the user timeline.
        If an authentication secret is given but it is invalid, this method returns an error.

        Parameters
        ----------
        auth_secret: str
            Either the authentication secret of the logged-in user or an empty string.
        max_cnt_tweets: int
            The maximum number of tweets included in the timeline. If it is set to -1,
            then all the available tweets will be included.

        Returns
        -------
        bool
            True if the timeline is successfully retrieved, False otherwise.
        result
            A dict containing a list of tweets with the key TWEETS_KEY if
            the timeline is successfully retrieved, a dict containing
            the error string with the key ERROR_KEY otherwise.

        Note
        ----
        Possible error strings are listed as below:

        -  ERROR_NOT_LOGGED_IN",0,0,5,5
"def get_token(
            self, redirect_uri, client_id, client_secret,
            code, callback_uri=None):
        
        params = {
            'redirect_uri': redirect_uri,
            'client_id': client_id,
            'client_secret': client_secret,
            'code': code,
        }
        if callback_uri:
            params.update({'callback_uri': callback_uri})
        response = self.call('/oauth2/token', params)
      
        
        
        
        
        if 'access_token' not in response:
            raise WePayError(response['error'], response['error_code'], response['error_description'])

        self.access_token = response['access_token']
        return response","Calls wepay.com/v2/oauth2/token to get an access token. Sets the
        access_token for the WePay instance and returns the entire response
        as a dict. Should only be called after the user returns from being
        sent to get_authorization_url.

        :param str redirect_uri: The same URI specified in the
            :py:meth:`get_authorization_url` call that preceeded this.
        :param str client_id: The client ID issued by WePay to your app.
        :param str client_secret: The client secret issued by WePay
            to your app.
        :param str code: The code returned by :py:meth:`get_authorization_url`.
        :param str callback_uri: The callback_uri you want to receive IPNs for
            this user on.",1,1,1,3
"def get_token(request):
    
    if (not request.META.get(header_name_to_django(auth_token_settings.HEADER_NAME)) and
            config.CHAMBER_MULTIDOMAINS_OVERTAKER_AUTH_COOKIE_NAME):
        ovetaker_auth_token = request.COOKIES.get(config.CHAMBER_MULTIDOMAINS_OVERTAKER_AUTH_COOKIE_NAME)
        token = get_object_or_none(Token, key=ovetaker_auth_token, is_active=True)
        if utils.get_user_from_token(token).is_authenticated():
            return token

    return utils.get_token(request)","Returns the token model instance associated with the given request token key.
    If no user is retrieved AnonymousToken is returned.",0,0,3,3
"def get_trace(
        self,
        project_id,
        trace_id,
        retry=google.api_core.gapic_v1.method.DEFAULT,
        timeout=google.api_core.gapic_v1.method.DEFAULT,
        metadata=None,
    ):
        
        
        if ""get_trace"" not in self._inner_api_calls:
            self._inner_api_calls[
                ""get_trace""
            ] = google.api_core.gapic_v1.method.wrap_method(
                self.transport.get_trace,
                default_retry=self._method_configs[""GetTrace""].retry,
                default_timeout=self._method_configs[""GetTrace""].timeout,
                client_info=self._client_info,
            )

        request = trace_pb2.GetTraceRequest(project_id=project_id, trace_id=trace_id)
        return self._inner_api_calls[""get_trace""](
            request, retry=retry, timeout=timeout, metadata=metadata
        )","Gets a single trace by its ID.

        Example:
            >>> from google.cloud import trace_v1
            >>>
            >>> client = trace_v1.TraceServiceClient()
            >>>
            >>> # TODO: Initialize `project_id`:
            >>> project_id = ''
            >>>
            >>> # TODO: Initialize `trace_id`:
            >>> trace_id = ''
            >>>
            >>> response = client.get_trace(project_id, trace_id)

        Args:
            project_id (str): ID of the Cloud project where the trace data is stored.
            trace_id (str): ID of the trace to return.
            retry (Optional[google.api_core.retry.Retry]):  A retry object used
                to retry requests. If ``None`` is specified, requests will not
                be retried.
            timeout (Optional[float]): The amount of time, in seconds, to wait
                for the request to complete. Note that if ``retry`` is
                specified, the timeout applies to each individual attempt.
            metadata (Optional[Sequence[Tuple[str, str]]]): Additional metadata
                that is provided to the method.

        Returns:
            A :class:`~google.cloud.trace_v1.types.Trace` instance.

        Raises:
            google.api_core.exceptions.GoogleAPICallError: If the request
                    failed for any reason.
            google.api_core.exceptions.RetryError: If the request failed due
                    to a retryable error and retry attempts failed.
            ValueError: If the parameters are invalid.",0,1,1,2
"def get_unicode(data):
    
    
    if isinstance(data, str):
        return safe_unicode(data)

    
    if isinstance(data, list):
        return [get_unicode(item) for item in data]

    
    if isinstance(data, dict):
        return {
            get_unicode(key): get_unicode(value)
            for key, value in data.iteritems()
        }
    
    return data","Convert string values to unicode even if they belong to lists or dicts.
    :param data: an object.
    :return: The object with all string values converted to unicode.",0,0,3,3
"def get_url_endpoint(self):
        
        endpoint = self.url
        if self.type not in ('Hypermap:WorldMap',):
            endpoint = 'registry/%s/layer/%s/map/wmts/1.0.0/WMTSCapabilities.xml' % (
                self.catalog.slug,
                self.id
            )
        return endpoint","Returns the Hypermap endpoint for a layer.
        This endpoint will be the WMTS MapProxy endpoint, only for WM we use the original endpoint.",0,0,1,1
"def get_usage(self):
        

        resp = requests.get(FITNESS_URL, timeout=30)
        resp.raise_for_status()

        soup = BeautifulSoup(resp.text, ""html5lib"")
        eastern = pytz.timezone('US/Eastern')
        output = []
        for item in soup.findAll(""div"", {""class"": ""barChart""}):
            data = [x.strip() for x in item.get_text(""\n"").strip().split(""\n"")]
            data = [x for x in data if x]
            name = re.sub(r""\s*(Hours)?\s*-?\s*(CLOSED|OPEN)?$"", """", data[0], re.I).strip()
            output.append({
                ""name"": name,
                ""open"": ""Open"" in data[1],
                ""count"": int(data[2].rsplit("" "", 1)[-1]),
                ""updated"": eastern.localize(datetime.datetime.strptime(data[3][8:].strip(), '%m/%d/%Y %I:%M %p')).isoformat(),
                ""percent"": int(data[4][:-1])
            })
        return output",Get fitness locations and their current usage.,0,1,1,2
"def get_user(prompt=None):
    
    
    try:
        env_user = getpass.getuser()
    except KeyError:
        env_user = ''
    if prompt is None:
        prompt = ""Please enter your user name""
    if env_user is None or env_user == '':
        user = input('%s: ' % prompt)
    else:
        user = input('%s [%s]: ' % (prompt, env_user))
        if user == '':
            user = env_user
    return user","Prompts the user for his login name, defaulting to the USER environment
    variable. Returns a string containing the username.
    May throw an exception if EOF is given by the user.

    :type  prompt: str|None
    :param prompt: The user prompt or the default one if None.
    :rtype:  string
    :return: A username.",1,0,1,2
"def get_user_info(self, recipient_id, fields=None):
        
        params = {}
        if fields is not None and isinstance(fields, (list, tuple)):
            params['fields'] = "","".join(fields)

        params.update(self.auth_args)

        request_endpoint = '{0}/{1}'.format(self.graph_url, recipient_id)
        response = requests.get(request_endpoint, params=params)
        if response.status_code == 200:
            return response.json()

        return None","Getting information about the user
        https://developers.facebook.com/docs/messenger-platform/user-profile
        Input:
          recipient_id: recipient id to send to
        Output:
          Response from API as <dict>",0,1,0,1
"def get_value(data, key):
    
    ref = data
    try:
        for subkey in key.split('.'):
            if isinstance(ref, dict):
                ref = ref[subkey]
            else:
                print('CRITICAL: Cannot use subkey %s on non-dictionary element' % subkey)
                return None
        return ref

    
    except KeyError:
        return None","Follow the dot notation to get the proper field, then perform the action

       Args:
           data: the data as a dictionary (required to be a dictionary)
           key: the key (as dot notation) into the data that gives the field (IP.src)

        Returns:
           the value of the field(subfield) if it exist, otherwise None",0,0,1,1
"def get_verify_command(self, signature_filename, data_filename,
                           keystore=None):
        
        cmd = [self.gpg, '--status-fd', '2', '--no-tty']
        if keystore is None:
            keystore = self.gpg_home
        if keystore:
            cmd.extend(['--homedir', keystore])
        cmd.extend(['--verify', signature_filename, data_filename])
        logger.debug('invoking: %s', ' '.join(cmd))
        return cmd","Return a suitable command for verifying a file.

        :param signature_filename: The pathname to the file containing the
                                   signature.
        :param data_filename: The pathname to the file containing the
                              signed data.
        :param keystore: The path to a directory which contains the keys
                         used in verification. If not specified, the
                         instance's ``gpg_home`` attribute is used instead.
        :return: The verifying command as a list suitable to be
                 passed to :class:`subprocess.Popen`.",0,1,1,2
"def get_version():
    
    try:
        version_file = open(os.path.join(ROOT_DIR, 'mollie', 'api', 'version.py'), encoding='utf=8')
    except TypeError:
        
        version_file = open(os.path.join(ROOT_DIR, 'mollie', 'api', 'version.py'))
    contents = version_file.read()
    match = re.search(r'VERSION = [\'""]([^\'""]+)', contents)
    if match:
        return match.group(1)
    else:
        raise RuntimeError(""Can't determine package version"")","Read the version from a file (mollie/api/version.py) in the repository.

    We can't import here since we might import from an installed version.",2,0,2,4
"def get_version_string(version):
    
    version_len = len(version)
    if version_len == 3:
        version_string = '%d.%d.%d' % version
    elif version_len == 4:
        version_string = '%d.%d.%d-%s' % version
    else:
        raise Exception(
            'Version tuple is non-semver-compliant {} length!'.format(version_len)
        )
    return version_string","Translate a version tuple into a string.

    Specify the __version__ as a tuple for more precise comparisons, and
    translate it to __version_string__ for when that's needed.

    This function exists primarily for easier unit testing.

    Args:
      version (Tuple[int, int, int, str]): three ints and an optional string.

    Returns:
      version_string (str): the tuple translated into a string per semver.org",1,0,2,3
"def get_vm_host_info(hostip, auth, url):
    
    hostid = get_dev_details(hostip, auth, url)['id']
    f_url = url + ""/imcrs/vrm/host?hostId="" + str(hostid)
    response = requests.get(f_url, auth=auth, headers=HEADERS)
    try:
        if response.status_code == 200:
            if len(response.text) > 0:
                return json.loads(response.text)
        elif response.status_code == 204:
            print(""Device is not a supported Hypervisor"")
            return ""Device is not a supported Hypervisor""
    except requests.exceptions.RequestException as error:
        return ""Error:\n"" + str(error) + "" get_vm_host_info: An Error has occured""","function takes hostId as input to RESTFUL call to HP IMC

    :param hostip: int or string of hostip of Hypervisor host

    :param auth: requests auth object #usually auth.creds from auth pyhpeimc.auth.class

    :param url: base url of IMC RS interface #usually auth.url from pyhpeimc.auth.authclass

    :return: Dictionary contraining the information for the target VM host

    :rtype: dict

    >>> from pyhpeimc.auth import *

    >>> from pyhpeimc.plat.vrm import *

    >>> auth = IMCAuth(""http://"", ""10.101.0.203"", ""8080"", ""admin"", ""admin"")

    >>> host_info = get_vm_host_info('10.101.0.6', auth.creds, auth.url)

    >>> assert type(host_info) is dict

    >>> assert len(host_info) == 10

    >>> assert 'cpuFeg' in host_info

    >>> assert 'cpuNum' in host_info

    >>> assert 'devId' in host_info

    >>> assert 'devIp' in host_info

    >>> assert 'diskSize' in host_info

    >>> assert 'memory' in host_info

    >>> assert 'parentDevId' in host_info

    >>> assert 'porductFlag' in host_info

    >>> assert 'serverName' in host_info

    >>> assert 'vendor' in host_info",1,1,0,2
"def get_vmss(access_token, subscription_id, resource_group, vmss_name):
    
    endpoint = ''.join([get_rm_endpoint(),
                        '/subscriptions/', subscription_id,
                        '/resourceGroups/', resource_group,
                        '/providers/Microsoft.Compute/virtualMachineScaleSets/', vmss_name,
                        '?api-version=', COMP_API])
    return do_get(endpoint, access_token)","Get virtual machine scale set details.

    Args:
        access_token (str): A valid Azure authentication token.
        subscription_id (str): Azure subscription id.
        resource_group (str): Azure resource group name.
        vmss_name (str): Name of the virtual machine scale set.

    Returns:
        HTTP response. JSON body of scale set properties.",0,1,1,2
"def get_weather_data(filename='weather.csv', **kwargs):
    r

    if 'datapath' not in kwargs:
        kwargs['datapath'] = os.path.join(os.path.split(
            os.path.dirname(__file__))[0], 'example')
    file = os.path.join(kwargs['datapath'], filename)
    
    weather_df = pd.read_csv(
        file, index_col=0, header=[0, 1],
        date_parser=lambda idx: pd.to_datetime(idx, utc=True))
    
    weather_df.index = pd.to_datetime(weather_df.index).tz_convert(
        'Europe/Berlin')
    
    weather_df.columns = [weather_df.axes[1].levels[0][
                              weather_df.axes[1].codes[0]],
                          weather_df.axes[1].levels[1][
                              weather_df.axes[1].codes[1]].astype(int)]
    return weather_df","r""""""
    Imports weather data from a file.

    The data include wind speed at two different heights in m/s, air
    temperature in two different heights in K, surface roughness length in m
    and air pressure in Pa. The file is located in the example folder of the
    windpowerlib. The height in m for which the data applies is specified in
    the second row.

    Parameters
    ----------
    filename : string
        Filename of the weather data file. Default: 'weather.csv'.

    Other Parameters
    ----------------
    datapath : string, optional
        Path where the weather data file is stored.
        Default: 'windpowerlib/example'.

    Returns
    -------
    weather_df : pandas.DataFrame
            DataFrame with time series for wind speed `wind_speed` in m/s,
            temperature `temperature` in K, roughness length `roughness_length`
            in m, and pressure `pressure` in Pa.
            The columns of the DataFrame are a MultiIndex where the first level
            contains the variable name as string (e.g. 'wind_speed') and the
            second level contains the height as integer at which it applies
            (e.g. 10, if it was measured at a height of 10 m).",1,0,1,2
"def get_width(self, c, default=0, match_only=None):
        
        return self.getattr(c=c,
                            attr='width',
                            default=default,
                            match_only=match_only)","Get the display width of a component. Wraps `getattr()`.

        Development note: Cannot define this as a `partial()` because I want
        to maintain the order of arguments in `getattr()`.

        Args:
            c (component): The component to look up.
            default (float): The width to return in the event of no match.
            match_only (list of str): The component attributes to include in the
                comparison. Default: All of them.

        Returns:
            float. The width of the matching Decor in the Legend.",0,0,1,1
"def get_witness(self, work, siglum, text_class=WitnessText):
        
        filename = os.path.join(work, siglum + '.txt')
        self._logger.debug('Creating WitnessText object from {}'.format(
            filename))
        with open(os.path.join(self._path, filename), encoding='utf-8') \
                as fh:
            content = fh.read()
        return text_class(work, siglum, content, self._tokenizer)","Returns a `WitnessText` representing the file associated with
        `work` and `siglum`.

        Combined, `work` and `siglum` form the basis of a filename for
        retrieving the text.

        :param work: name of work
        :type work: `str`
        :param siglum: siglum of witness
        :type siglum: `str`
        :rtype: `WitnessText`",1,1,1,3
"def get_words(file_path=None, content=None, extension=None):
    
    if (file_path is None and (content is None or extension is None) or
                    file_path and content and extension):
        error_msg = ('Must provide `file_path` or `content` and `extension`')
        raise Exception(error_msg)

    if file_path and content is None and extension is None:
        extension = os.path.splitext(file_path)[1]
        with open(file_path) as infile:
            content = infile.read()

    if extension in ['.css']:
        regex = re.compile(r'([^a-zA-Z-])')
    elif extension in ['.R', '.c', '.md', '.cpp', '.java', '.py']:
        regex = re.compile(r'([^a-zA-Z_])')
    else:
        regex = re.compile(r'([^a-zA-Z])')

    words = sorted(set(regex.sub(r' ', content).split()))
    return words","Extract all words from a source code file to be used in code completion.

    Extract the list of words that contains the file in the editor,
    to carry out the inline completion similar to VSCode.",2,0,2,4
"def get_work_artifact_link_types(self):
        
        response = self._send(http_method='GET',
                              location_id='1a31de40-e318-41cd-a6c6-881077df52e3',
                              version='5.0-preview.1')
        return self._deserialize('[WorkArtifactLink]', self._unwrap_collection(response))","GetWorkArtifactLinkTypes.
        [Preview API] Get the list of work item tracking outbound artifact link types.
        :rtype: [WorkArtifactLink]",0,1,1,2
"def get_work_item_next_states_on_checkin_action(self, ids, action=None):
        
        query_parameters = {}
        if ids is not None:
            ids = "","".join(map(str, ids))
            query_parameters['ids'] = self._serialize.query('ids', ids, 'str')
        if action is not None:
            query_parameters['action'] = self._serialize.query('action', action, 'str')
        response = self._send(http_method='GET',
                              location_id='afae844b-e2f6-44c2-8053-17b3bb936a40',
                              version='5.1-preview.1',
                              query_parameters=query_parameters)
        return self._deserialize('[WorkItemNextStateOnTransition]', self._unwrap_collection(response))","GetWorkItemNextStatesOnCheckinAction.
        [Preview API] Returns the next state on the given work item IDs.
        :param [int] ids: list of work item ids
        :param str action: possible actions. Currently only supports checkin
        :rtype: [WorkItemNextStateOnTransition]",0,1,0,1
"def getobjpath(obj, path):
    
    if not path:
        return obj
    if path.startswith(""[""):
        item = path[1:path.index(""]"")]
        return getobjpath(obj[item], path[len(item) + 2:])
    if path.startswith("".""):
        path = path[1:]
    if ""."" in path or ""["" in path:
        dot_idx = path.find(""."")
        bracket_idx = path.find(""["")
        if dot_idx == -1 or bracket_idx < dot_idx:
            idx = bracket_idx
            next_idx = idx
        else:
            idx = dot_idx
            next_idx = idx + 1
        attr = path[:idx]
        return getobjpath(getattr(obj, attr), path[next_idx:])
    return getattr(obj, path)","Returns an item or attribute of the object recursively.
    Item names are specified between brackets, eg: [item].
    Attribute names are prefixed with a dot (the first one is optional), eg: .attr
    Example: getobjpath(obj, ""attr1.attr2[item].attr3"")",0,0,3,3
"def gevent_run(app, port=5000, log=None, error_log=None, address='',
               monkey_patch=True, start=True, **kwargs):  
    
    if log is None:
        log = app.logger
    if error_log is None:
        error_log = app.logger
    if monkey_patch:
        from gevent import monkey

        monkey.patch_all()

    from gevent.wsgi import WSGIServer
    http_server = WSGIServer((address, port), app, log=log, error_log=error_log,
                             **kwargs)
    if start:
        http_server.serve_forever()
    return http_server","Run your app in gevent.wsgi.WSGIServer

    :param app: wsgi application, ex. Microservice instance
    :param port: int, listen port, default 5000
    :param address: str, listen address, default: """"
    :param log: logger instance, default app.logger
    :param error_log: logger instance, default app.logger
    :param monkey_patch: boolean, use gevent.monkey.patch_all() for patching standard modules, default: True
    :param start: boolean, if True, server will be start (server.serve_forever())
    :param kwargs: other params for WSGIServer(**kwargs)
    :return: server",0,1,1,2
"def gf_lehmann(eig_e, eig_states, d_dag, beta, omega, d=None):
    

    ew = np.exp(-beta*eig_e)
    zet = ew.sum()
    G = np.zeros_like(omega)
    basis_create = np.dot(eig_states.T, d_dag.dot(eig_states))
    if d is None:
        tmat = np.square(basis_create)
    else:
        tmat = np.dot(eig_states.T, d.T.dot(eig_states))*basis_create

    tmat *= np.add.outer(ew, ew)
    gap = np.add.outer(-eig_e, eig_e)

    N = eig_e.size
    for i, j in product(range(N), range(N)):
        G += tmat[i, j] / (omega + gap[i, j])
    return G / zet","Outputs the lehmann representation of the greens function
       omega has to be given, as matsubara or real frequencies",0,0,1,1
"def goodFormater(badFormat, outputPath, year, length):
    

    devcom = 'PHAS' + badFormat['Cand'][0]

    goodFormat = {devcom: []}

    
    for row in list(badFormat.values())[1:]:
        goodFormat[devcom].append(int(row[0]))  

        for i in range(length-1):
            
            goodFormat.setdefault(row[(2 * i) + 1], [])
            
            goodFormat[row[(2*i)+1]].append(int(row[2*(i + 1)]))

    goodFormat.pop('0')

    goodFormat['Averages'] = everyonesAverage(year, badFormat, length)
    if outputPath is not None:  

        results = csv.writer(outputPath.open(mode='w'), delimiter=',')
        
        results.writerow(goodFormat.keys())
        
        
        results.writerows(itertools.zip_longest(
            *goodFormat.values(), fillvalue=''))

    return goodFormat","[summary]

    reformats the input results into a dictionary with module names as keys and their respective results as values

    outputs to csv if outputPath is specified

    Arguments:
        badFormat {dict} -- candNumber : [results for candidate]
        outputPath {str} -- the path to output to
        year {int} -- the year candidateNumber is in
        length {int} -- length of each row in badFormat divided by 2


    Returns:
        dictionary -- module : [results for module]
        saves to file if output path is specified",1,0,1,2
"def grab_definition(self, url):
        
        re_description = re.compile('Description:(.+?\\n)')
        re_table_name = re.compile(""(\w+ Table.+)"")
        if url.startswith('//'):
            url = 'http:' + url
        elif url.startswith('/'):
            url = 'http://www.epa.gov' + url
        try:
            html = urlopen(url).read()
            doc = lh.fromstring(html)
            main = doc.cssselect('
            text = main.text_content()
            definition = re_description.search(text).group(1).strip()
        except (AttributeError, IndexError, TypeError, HTTPError):
            print url
        else:
            value = re_table_name.sub('', definition)
            return value
        return url","Grab the column definition of a table from the EPA using a combination
        of regular expressions and lxml.",0,1,1,2
"def graph_from_file(filename, bidirectional=False, simplify=True,
                    retain_all=False, name='unnamed'):
    
    
    response_jsons = [overpass_json_from_file(filename)]

    
    G = create_graph(response_jsons, bidirectional=bidirectional,
                     retain_all=retain_all, name=name)

    
    if simplify:
        G = simplify_graph(G)

    log('graph_from_file() returning graph with {:,} nodes and {:,} edges'.format(len(list(G.nodes())), len(list(G.edges()))))
    return G","Create a networkx graph from OSM data in an XML file.

    Parameters
    ----------
    filename : string
        the name of a file containing OSM XML data
    bidirectional : bool
        if True, create bidirectional edges for one-way streets
    simplify : bool
        if True, simplify the graph topology
    retain_all : bool
        if True, return the entire graph even if it is not connected
    name : string
        the name of the graph

    Returns
    -------
    networkx multidigraph",1,1,1,3
"def gridsearch_color_plot(model, x_param, y_param, X=None, y=None, ax=None,
                          **kwargs):
    
    
    visualizer = GridSearchColorPlot(model, x_param, y_param, ax=ax, **kwargs)

    
    if X is not None:
        visualizer.fit(X, y)
    else:
        visualizer.draw()

    
    return visualizer.ax","Quick method:
    Create a color plot showing the best grid search scores across two
    parameters.

    This helper function is a quick wrapper to utilize GridSearchColorPlot
    for one-off analysis.

    If no `X` data is passed, the model is assumed to be fit already. This
    allows quick exploration without waiting for the grid search to re-run.

    Parameters
    ----------
    model : Scikit-Learn grid search object
        Should be an instance of GridSearchCV. If not, an exception is raised.
        The model may be fit or unfit.

    x_param : string
        The name of the parameter to be visualized on the horizontal axis.

    y_param : string
        The name of the parameter to be visualized on the vertical axis.

    metric : string (default 'mean_test_score')
        The field from the grid search's `cv_results` that we want to display.

    X  : ndarray or DataFrame of shape n x m or None (default None)
        A matrix of n instances with m features. If not None, forces the
        GridSearchCV object to be fit.

    y  : ndarray or Series of length n or None (default None)
        An array or series of target or class values.

    ax : matplotlib axes
        The axes to plot the figure on.

    classes : list of strings
        The names of the classes in the target

    Returns
    -------
    ax : matplotlib axes
        Returns the axes that the classification report was drawn on.",0,0,2,2
"def handle(self, *args, **options):
        
        try:
            admin = User.objects.get(username='admin')
        except User.DoesNotExist:
            admin = User(
                username='admin',
                first_name='admin',
                last_name='admin',
                email='admin@localhost.localdomain',
                is_staff=True,
                is_active=True,
                is_superuser=True,
            )
        admin.set_password('admin')
        admin.save()",Load a default admin user,1,0,1,2
"def handle(self, connection_id, message_content):
        
        try:
            request = self._request_proto()
            request.ParseFromString(message_content)
        except DecodeError:
            LOGGER.info('Protobuf %s failed to deserialize', request)
            return self._wrap_result(self._status.INTERNAL_ERROR)

        try:
            response = self._respond(request)
        except _ResponseFailed as e:
            response = e.status

        return self._wrap_result(response)","Handles parsing incoming requests, and wrapping the final response.

        Args:
            connection_id (str): ZMQ identity sent over ZMQ socket
            message_content (bytes): Byte encoded request protobuf to be parsed

        Returns:
            HandlerResult: result to be sent in response back to client",0,1,2,3
"def handle_api_errors(self, status_code, json_data):
        
        error_list = []

        if 'error' in json_data and len(json_data) == 1 \
        and isinstance(json_data, dict) and isinstance(json_data['error'], list):
            error_list = json_data['error']

        elif status_code == 400:
            for main_error_key in json_data:
                for sub_error_key in json_data[main_error_key]:
                    error_list.extend(json_data[main_error_key][sub_error_key])

        
        if error_list:
            error_string = u' '.join(error_list)
            raise TagCubeAPIException(error_string)","This method parses all the HTTP responses sent by the REST API and
        raises exceptions if required. Basically tries to find responses with
        this format:

            {
                'error': ['The domain foo.com already exists.']
            }

        Or this other:
            {
                ""scans"": {
                    ""__all__"": [
                        ""Not a verified domain. You need to verify...""
                    ]
                }
            }

        And raise TagCubeAPIException with the correct message.

        :param status_code: The HTTP response code
        :param json_data: The HTTP response body decoded as JSON",1,0,2,3
"def handle_or_else(self, orelse, test):
        
        if isinstance(orelse[0], ast.If):
            control_flow_node = self.visit(orelse[0])
            
            control_flow_node.test.label = 'el' + control_flow_node.test.label

            test.connect(control_flow_node.test)
            return control_flow_node.last_nodes
        else:
            else_connect_statements = self.stmt_star_handler(
                orelse,
                prev_node_to_avoid=self.nodes[-1]
            )
            test.connect(else_connect_statements.first_statement)
            return else_connect_statements.last_statements","Handle the orelse part of an if or try node.

        Args:
            orelse(list[Node])
            test(Node)

        Returns:
            The last nodes of the orelse branch.",0,0,2,2
"def handle_pre_response(self, item_session: ItemSession) -> Actions:
        
        action = self.consult_pre_response_hook(item_session)

        if action == Actions.RETRY:
            item_session.set_status(Status.skipped)
        elif action == Actions.FINISH:
            item_session.set_status(Status.done)
        elif action == Actions.STOP:
            raise HookStop('Script requested immediate stop.')

        return action",Process a response that is starting.,1,0,2,3
"def handle_server_filter(self, request, table=None):
        
        if not table:
            table = self.get_table()
        filter_info = self.get_server_filter_info(request, table)
        if filter_info is None:
            return False
        request.session[filter_info['value_param']] = filter_info['value']
        if filter_info['field_param']:
            request.session[filter_info['field_param']] = filter_info['field']
        return filter_info['changed']","Update the table server filter information in the session.

        Returns True if the filter has been changed.",0,0,1,1
"def has_changed(self, field_name: str = None) -> bool:
        
        changed = self._diff_with_initial.keys()

        if self._meta.get_field(field_name).get_internal_type() == 'ForeignKey':
            if not field_name.endswith('_id'):
                field_name = field_name+'_id'

        if field_name in changed:
            return True

        return False",Check if a field has changed since the model was instantiated.,0,0,1,1
"def has_changed(self, initial, data):
        ""Detects if the data was changed. This is added in 1.6.""
        if initial is None and data is None:
            return False
        if data and not hasattr(data, '__iter__'):
            data = self.widget.decompress(data)
        initial = self.to_python(initial)
        data = self.to_python(data)
        if hasattr(self, '_coerce'):
            data = self._coerce(data)
        if isinstance(data, Model) and isinstance(initial, Model):
            return model_vars(data) != model_vars(initial)
        else:
            return data != initial",Detects if the data was changed. This is added in 1.6.,0,0,1,1
"def has_in_subscriptions(self, subscription):
        
        assert isinstance(subscription, github.Repository.Repository), subscription
        status, headers, data = self._requester.requestJson(
            ""GET"",
            ""/user/subscriptions/"" + subscription._identity
        )
        return status == 204",":calls: `GET /user/subscriptions/:owner/:repo <http://developer.github.com/v3/activity/watching>`_
        :param subscription: :class:`github.Repository.Repository`
        :rtype: bool",0,1,0,1
"def has_publish_permission(self, request, obj=None):
        
        
        
        if is_automatic_publishing_enabled(self.model):
            return False
        user_obj = request.user
        if not user_obj.is_active:
            return False
        if user_obj.is_superuser:
            return True
        
        if user_obj.has_perm('%s.can_publish' % self.opts.app_label):
            return True
        
        
        if user_obj.has_perm('%s.can_republish' % self.opts.app_label) and \
                obj and getattr(obj, 'has_been_published', False):
            return True
        
        return False","Determines if the user has permissions to publish.

        :param request: Django request object.
        :param obj: The object to determine if the user has
        permissions to publish.
        :return: Boolean.",0,0,1,1
"def has_role(user, roles):
    
    if user and user.is_superuser:
        return True

    if not isinstance(roles, list):
        roles = [roles]

    normalized_roles = []
    for role in roles:
        if not inspect.isclass(role):
            role = RolesManager.retrieve_role(role)

        normalized_roles.append(role)

    user_roles = get_user_roles(user)

    return any([role in user_roles for role in normalized_roles])",Check if a user has any of the given roles.,0,0,1,1
"def has_storage(self, address):
        
        storage = self._world_state[address]['storage']
        array = storage.array
        while not isinstance(array, ArrayVariable):
            if isinstance(array, ArrayStore):
                return True
            array = array.array
        return False","True if something has been written to the storage.
        Note that if a slot has been erased from the storage this function may
        lose any meaning.",0,0,1,1
"def hash_host(hostname, salt=None):
        
        if salt is None:
            salt = os.urandom(sha1().digest_size)
        else:
            if salt.startswith('|1|'):
                salt = salt.split('|')[2]
            salt = decodebytes(b(salt))
        assert len(salt) == sha1().digest_size
        hmac = HMAC(salt, b(hostname), sha1).digest()
        hostkey = '|1|%s|%s' % (u(encodebytes(salt)), u(encodebytes(hmac)))
        return hostkey.replace('\n', '')","Return a ""hashed"" form of the hostname, as used by OpenSSH when storing
        hashed hostnames in the known_hosts file.

        :param str hostname: the hostname to hash
        :param str salt: optional salt to use when hashing (must be 20 bytes long)
        :return: the hashed hostname as a `str`",0,0,3,3
"def header_body_from_content(content):
    
    m = _CLASSIFIED_BY_PATTERN.search(content)
    idx = m and m.end() or 0
    m = _SUMMARY_PATTERN.search(content)
    summary_idx = m and m.start() or None
    m = _FIRST_PARAGRAPH_PATTERN.search(content)
    para_idx = m and m.start() or None
    if summary_idx and para_idx:
        idx = max(idx, min(summary_idx, para_idx))
    elif summary_idx:
        idx = max(summary_idx, idx)
    elif para_idx:
        idx = max(para_idx, idx)
    if idx > 0:
        return content[:idx], content[idx:]
    return None, None","\
    Tries to extract the header and the message from the cable content.

    The header is something like

        UNCLASSIFIED ...
        SUBJECT ...
        REF ...

    while the message begins usually with a summary

        1. SUMMARY ...
        ...
        10. ...
    
    Returns (header, msg) or (None, None) if the header/message cannot be 
    detected.
    
    `content`
        The ""content"" part of a cable.",0,0,2,2
"def header_canonical(self, header_name):
        
        
        
        header_name = header_name.lower()
        if header_name == 'content-type':
            return 'CONTENT-TYPE'
        elif header_name == 'content-length':
            return 'CONTENT-LENGTH'
        return 'HTTP_%s' % header_name.replace('-', '_').upper()",Translate HTTP headers to Django header names.,0,0,1,1
"def headers_to_dict(headers):
    
    hdrs = {}
    for h, v in headers:
        h = h.lower()
        if h in hdrs:
            if isinstance(hdrs[h], list):
                hdrs[h].append(v)
            else:
                hdrs[h] = [hdrs[h], v]
        else:
            hdrs[h] = v
    return hdrs","Converts a sequence of (name, value) tuples into a dict where if
    a given name occurs more than once its value in the dict will be
    a list of values.",0,0,2,2
"def headpart_types(self, method, input=True):
        
        result = []
        if input:
            headers = method.soap.input.headers
        else:
            headers = method.soap.output.headers
        for header in headers:
            part = header.part
            if part.element is not None:
                query = ElementQuery(part.element)
            else:
                query = TypeQuery(part.type)
            pt = query.execute(self.schema())
            if pt is None:
                raise TypeNotFound(query.ref)
            if part.type is not None:
                pt = PartElement(part.name, pt)
            if input:
                if pt.name is None:
                    result.append((part.name, pt))
                else:
                    result.append((pt.name, pt))
            else:
                result.append(pt)
        return result","Get a list of I{parameter definitions} (pdef) defined for the specified
        method.
        Each I{pdef} is a tuple (I{name}, L{xsd.sxbase.SchemaObject})
        @param method: A service method.
        @type method: I{service.Method}
        @param input: Defines input/output message.
        @type input: boolean
        @return:  A list of parameter definitions
        @rtype: [I{pdef},]",1,0,2,3
"def hessian(self, q, t=0.):
        
        if (self.R is not None and
                not np.allclose(np.diag(self.R), 1., atol=1e-15, rtol=0)):
            raise NotImplementedError(""Computing Hessian matrices for rotated ""
                                      ""potentials is currently not supported."")
        q = self._remove_units_prepare_shape(q)
        orig_shape,q = self._get_c_valid_arr(q)
        t = self._validate_prepare_time(t, q)
        ret_unit = 1 / self.units['time']**2
        hess = np.moveaxis(self._hessian(q, t=t), 0, -1)
        return hess.reshape((orig_shape[0], orig_shape[0]) + orig_shape[1:]) * ret_unit","Compute the Hessian of the potential at the given position(s).

        Parameters
        ----------
        q : `~gala.dynamics.PhaseSpacePosition`, `~astropy.units.Quantity`, array_like
            The position to compute the value of the potential. If the
            input position object has no units (i.e. is an `~numpy.ndarray`),
            it is assumed to be in the same unit system as the potential.

        Returns
        -------
        hess : `~astropy.units.Quantity`
            The Hessian matrix of second derivatives of the potential. If the input
            position has shape ``q.shape``, the output energy will have shape
            ``(q.shape[0],q.shape[0]) + q.shape[1:]``. That is, an ``n_dim`` by
            ``n_dim`` array (matrix) for each position.",1,0,2,3
"def history_file(self, location=None):
        
        if location:
            
            if os.path.exists(location):
                return location
            else:
                logger.warn(""The specified history file %s doesn't exist"",
                            location)
        filenames = []
        for base in ['CHANGES', 'HISTORY', 'CHANGELOG']:
            filenames.append(base)
            for extension in ['rst', 'txt', 'markdown']:
                filenames.append('.'.join([base, extension]))
        history = self.filefind(filenames)
        if history:
            return history",Return history file location.,0,1,1,2
"def hostinterface_update(interfaceid, **kwargs):
    
    conn_args = _login(**kwargs)
    ret = {}
    try:
        if conn_args:
            method = 'hostinterface.update'
            params = {""interfaceid"": interfaceid}
            params = _params_extend(params, **kwargs)
            ret = _query(method, params, conn_args['url'], conn_args['auth'])
            return ret['result']['interfaceids']
        else:
            raise KeyError
    except KeyError:
        return ret",".. versionadded:: 2016.3.0

    Update host interface

    .. note::
        This function accepts all standard hostinterface: keyword argument
        names differ depending on your zabbix version, see here__.

        .. __: https://www.zabbix.com/documentation/2.4/manual/api/reference/hostinterface/object#host_interface

    :param interfaceid: ID of the hostinterface to update

    :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring)

    :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring)

    :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring)

    :return: ID of the updated host interface, False on failure.

    CLI Example:
    .. code-block:: bash

        salt '*' zabbix.hostinterface_update 6 ip_=0.0.0.2",1,1,2,4
"def hostname_for_event(self, clean_server_name):
        
        uri = urlsplit(clean_server_name)
        if '@' in uri.netloc:
            hostname = uri.netloc.split('@')[1].split(':')[0]
        else:
            hostname = uri.netloc.split(':')[0]
        if hostname == 'localhost':
            hostname = self.hostname
        return hostname",Return a reasonable hostname for a replset membership event to mention.,0,0,1,1
"def hpo_genes(phenotype_ids, username, password):
    
    if phenotype_ids:
        try:
            results = query_phenomizer.query(username, password, phenotype_ids)
            return [result for result in results
                    if result['p_value'] is not None]
        except SystemExit, RuntimeError:
            pass
    return None","Return list of HGNC symbols matching HPO phenotype ids.

    Args:
        phenotype_ids (list): list of phenotype ids
        username (str): username to connect to phenomizer
        password (str): password to connect to phenomizer

    Returns:
        query_result: a list of dictionaries on the form
        {
            'p_value': float,
            'gene_id': str,
            'omim_id': int,
            'orphanet_id': int,
            'decipher_id': int,
            'any_id': int,
            'mode_of_inheritance': str,
            'description': str,
            'raw_line': str
        }",1,0,1,2
"def htmlABF(ID,group,d,folder,overwrite=False):
    
    fname=folder+""/swhlab4/%s_index.html""%ID
    if overwrite is False and os.path.exists(fname):
        return
    html=TEMPLATES['abf']
    html=html.replace(""~ID~"",ID)
    html=html.replace(""~CONTENT~"",htmlABFcontent(ID,group,d))
    print("" <- writing [%s]""%os.path.basename(fname))
    with open(fname,'w') as f:
        f.write(html)
    return","given an ID and the dict of files, generate a static html for that abf.",1,0,1,2
"def html_listify(tree, root_xl_element, extensions, list_type='ol'):
    
    for node in tree:
        li_elm = etree.SubElement(root_xl_element, 'li')
        if node['id'] not in extensions:  
            span_elm = lxml.html.fragment_fromstring(
                node['title'], create_parent='span')
            li_elm.append(span_elm)
        else:
            a_elm = lxml.html.fragment_fromstring(
                node['title'], create_parent='a')
            a_elm.set('href', ''.join([node['id'], extensions[node['id']]]))
            li_elm.append(a_elm)
        if node['id'] is not None and node['id'] != 'subcol':
            li_elm.set('cnx-archive-uri', node['id'])
        if node['shortId'] is not None:
            li_elm.set('cnx-archive-shortid', node['shortId'])
        if 'contents' in node:
            elm = etree.SubElement(li_elm, list_type)
            html_listify(node['contents'], elm, extensions)","Convert a node tree into an xhtml nested list-of-lists.

       This will create 'li' elements under the root_xl_element,
       additional sublists of the type passed as list_type. The contents
       of each li depends on the extensions dictonary: the keys of this
       dictionary are the ids of tree elements that are repesented by files
       in the epub, with associated filename extensions as the value. Those
       nodes will be rendered as links to the reassembled filename: i.e.
       id='abc-2345-54e4' {'abc-2345-54e4': 'xhtml'} -> abc-2345-54e4.xhtml
       Other nodes will render as spans. If the node has id or short id values,
       the associated li will be populated with cnx-archive-uri and
       cnx-archive-shortid attributes, respectively",0,0,1,1
"def http_get_metadata(metadata_path, timeout=__HTTP_DEFAULT_TIMEOUT_SEC):
  
  metadata_path = __METADATA_PREFIX + metadata_path
  try:
    response = urllib2.urlopen(metadata_path, None, timeout)
    if response.getcode() != 200:
      raise IOError(""Non-200 response "" + str(response.getcode()) + "" reading "" + metadata_path)
    return response.read()
  except urllib2.URLError as error:
    raise IOError(""URLError in http_get_metadata: "" + repr(error))","Fetch AWS metadata from http://169.254.169.254/latest/meta-data/<metadata_path>
  ARGS:
    metadata_path - the optional path and required key to the EC2 metadata (e.g. ""instance-id"")
  RETURN:
    response content on success
  RAISE:
    URLError if there was a problem reading metadata",2,1,2,5
"def i2c_monitor_read(self):
        
        data = array.array('H', (0,) * self.BUFFER_SIZE)
        ret = api.py_aa_i2c_monitor_read(self.handle, self.BUFFER_SIZE,
                data)
        _raise_error_if_negative(ret)
        del data[ret:]
        return data.tolist()","Retrieved any data fetched by the monitor.

        This function has an integrated timeout mechanism. You should use
        :func:`poll` to determine if there is any data available.

        Returns a list of data bytes and special symbols. There are three
        special symbols: `I2C_MONITOR_NACK`, I2C_MONITOR_START and
        I2C_MONITOR_STOP.",1,0,0,1
"def i2c_reply(self, data):
        

        reply_data = []
        address = (data[0] & 0x7f) + (data[1] << 7)
        register = data[2] & 0x7f + data[3] << 7
        reply_data.append(register)
        for i in range(4, len(data), 2):
            data_item = (data[i] & 0x7f) + (data[i + 1] << 7)
            reply_data.append(data_item)
        
        if address in self.i2c_map:
            i2c_data = self.i2c_map.get(address, None)

            i2c_data[1] = reply_data
            self.i2c_map[address] = i2c_data
            
            
            if i2c_data[0] is not None:
                i2c_data[0]([self.pymata.I2C, address, reply_data])","This method receives replies to i2c_read requests. It stores the data for each i2c device
        address in a dictionary called i2c_map. The data is retrieved via a call to i2c_get_read_data()
        in pymata.py
        It a callback was specified in pymata.i2c_read, the raw data is sent through the callback

        :param data: raw data returned from i2c device",0,1,1,2
"def identify_image(image):
    
    try:
        
        attrs = tuple(getattr(image, attr) for attr in UNIQUE_IMAGE_ATTRIBUTES)
    except AttributeError:
        
        attrs = tuple(image.get(attr, None) for attr in UNIQUE_IMAGE_ATTRIBUTES)
    ui = UniqueImage(*attrs)
    
    
    return ui._replace(
        unified=ui.unified or False, additional_variants=ui.additional_variants or []
    )","Provides a tuple of image's UNIQUE_IMAGE_ATTRIBUTES. Note:
    this is not guaranteed to be unique (and will often not be)
    for pre-1.1 metadata, as subvariant did not exist. Provided as
    a function so consumers can use it on plain image dicts read from
    the metadata or PDC.",0,0,1,1
"def idx_num_to_name(L):
    
    logger_jsons.info(""enter idx_num_to_name"")

    try:
        if ""paleoData"" in L:
            L[""paleoData""] = _import_data(L[""paleoData""], ""paleo"")
        if ""chronData"" in L:
            L[""chronData""] = _import_data(L[""chronData""], ""chron"")
    except Exception as e:
        logger_jsons.error(""idx_num_to_name: {}"".format(e))
        print(""Error: idx_name_to_num: {}"".format(e))

    logger_jsons.info(""exit idx_num_to_name"")
    return L","Switch from index-by-number to index-by-name.

    :param dict L: Metadata
    :return dict L: Metadata",0,3,1,4
"def iget(self, irods_path, attempts=1, pause=15):
        
        if attempts > 1:
            cmd = 
            cmd = lstrip(cmd)
            cmd = cmd.format(attempts, irods_path, pause)
            self.add(cmd)
        else:
            self.add('iget -v ""{}""'.format(irods_path))","Add an iget command to retrieve a file from iRODS.

        Parameters
        ----------
            irods_path: str
                Filepath which should be fetched using iget
            attempts: int (default: 1)
                Number of retries, if iRODS access fails
            pause: int (default: 15)
                Pause between two access attempts in seconds",0,0,1,1
"def image_psf_shape_tag_from_image_psf_shape(image_psf_shape):
    
    if image_psf_shape is None:
        return ''
    else:
        y = str(image_psf_shape[0])
        x = str(image_psf_shape[1])
        return ('_image_psf_' + y + 'x' + x)","Generate an image psf shape tag, to customize phase names based on size of the image PSF that the original PSF \
    is trimmed to for faster run times.

    This changes the phase name 'phase_name' as follows:

    image_psf_shape = 1 -> phase_name
    image_psf_shape = 2 -> phase_name_image_psf_shape_2
    image_psf_shape = 2 -> phase_name_image_psf_shape_2",0,0,1,1
"def img2wav(path, min_x, max_x, min_y, max_y, window_size=3):
    
    image = Image.open(path).convert(""L"")
    matrix = np.array(image)[::-1]

    
    matrix[np.where(matrix >= 128)] = 255
    matrix[np.where(matrix < 128)] = 0

    tick_x = (max_x - min_x) / matrix.shape[1]
    tick_y = (max_y - min_y) / matrix.shape[0]

    x, y = list(), list()
    for i in range(matrix.shape[1]):
        window = expand_window(  
            i, window_size, matrix.shape[1])
        margin_dots_y_indices = np.where(matrix[:, window] == 0)[0]
        
        if len(margin_dots_y_indices) > 0:
            x.append(min_x + (i + 1) * tick_x)
            y.append(min_y + margin_dots_y_indices.mean() * tick_y)

    return np.array(x), np.array(y)","Generate 1-D data ``y=f(x)`` from a black/white image.

    Suppose we have an image like that:

    .. image:: images/waveform.png
        :align: center

    Put some codes::

        >>> from weatherlab.math.img2waveform import img2wav
        >>> import matplotlib.pyplot as plt
        >>> x, y = img2wav(r""testdata\img2waveform\waveform.png"", 
        ...                  min_x=0.0, max_x=288, 
        ...                  min_y=15.0, max_y=35.0, 
        ...                  window_size=15)
        >>> plt.plot(x, y)
        >>> plt.show()

    Then you got nicely sampled data:

    .. image:: images\waveform_pyplot.png
        :align: center

    :param path: the image file path
    :type path: string
    :param min_x: minimum value of x axis
    :type min_x: number
    :param max_x: maximum value of x axis
    :type max_x: number
    :param min_y: minimum value of y axis
    :type min_y: number
    :param max_y: maximum value of y axis
    :type max_y: number
    :param window_size: the slide window
    :type window_size: int

    Note:

    In python, a numpy array that represent a image is from left to the right, 
    top to the bottom, but in coordinate, it's from bottom to the top. So we 
    use ::-1 for a reverse output",1,0,3,4
"def importFile(self, srcUrl, sharedFileName=None, hardlink=False):
        
        
        
        
        
        srcUrl = urlparse.urlparse(srcUrl)
        otherCls = self._findJobStoreForUrl(srcUrl)
        return self._importFile(otherCls, srcUrl, sharedFileName=sharedFileName, hardlink=hardlink)","Imports the file at the given URL into job store. The ID of the newly imported file is
        returned. If the name of a shared file name is provided, the file will be imported as
        such and None is returned.

        Currently supported schemes are:

            - 's3' for objects in Amazon S3
                e.g. s3://bucket/key

            - 'wasb' for blobs in Azure Blob Storage
                e.g. wasb://container/blob

            - 'file' for local files
                e.g. file:///local/file/path

            - 'http'
                e.g. http://someurl.com/path

            - 'gs'
                e.g. gs://bucket/file

        :param str srcUrl: URL that points to a file or object in the storage mechanism of a
                supported URL scheme e.g. a blob in an Azure Blob Storage container.

        :param str sharedFileName: Optional name to assign to the imported file within the job store

        :return: The jobStoreFileId of the imported file or None if sharedFileName was given
        :rtype: toil.fileStore.FileID or None",1,0,0,1
"def import_from_path(path):
    
    try:
        return importlib.import_module(path)
    except ImportError:
        if '.' not in path:
            raise
        module_name, attr_name = path.rsplit('.', 1)
        if not does_module_exist(module_name):
            raise ImportError(""No object found at '{}'"".format(path))
        mod = importlib.import_module(module_name)

        if not hasattr(mod, attr_name):
            raise ImportError(""No object found at '{}'"".format(path))
        return getattr(mod, attr_name)","Imports a package, module or attribute from path
    Thanks http://stackoverflow.com/a/14050282/1267398

    >>> import_from_path('os.path')
    <module 'posixpath' ...
    >>> import_from_path('os.path.basename')
    <function basename at ...
    >>> import_from_path('os')
    <module 'os' from ...
    >>> import_from_path('getrektcunt')
    Traceback (most recent call last):
    ImportError:
    >>> import_from_path('os.dummyfunc')
    Traceback (most recent call last):
    ImportError:
    >>> import_from_path('os.dummyfunc.dummylol')
    Traceback (most recent call last):
    ImportError:",3,0,3,6
"def import_modules(self):
        
        modules = self.get_modules()
        log.info(""import service modules: "" + str(modules))
        try:
            for module in modules:
                __import__(module)
        except ImportError as error:
            raise ImportModulesError(error.msg)",Import customer's service module.,1,1,2,4
"def import_profiles(self, profile_area, wait_for_completion=True,
                        operation_timeout=None):
        
        body = {'profile-area': profile_area}
        result = self.manager.session.post(
            self.uri + '/operations/import-profiles',
            body,
            wait_for_completion=wait_for_completion,
            operation_timeout=operation_timeout)
        return result","Import activation profiles and/or system activity profiles for this CPC
        from the SE hard drive into the CPC using the HMC operation
        ""Import Profiles"".

        This operation is not permitted when the CPC is in DPM mode.

        Authorization requirements:

        * Object-access permission to this CPC.
        * Task permission for the ""CIM Actions ExportSettingsData"" task.

        Parameters:

          profile_area (int):
            The numbered hard drive area (1-4) from which the profiles are
            imported.

          wait_for_completion (bool):
            Boolean controlling whether this method should wait for completion
            of the requested asynchronous HMC operation, as follows:

            * If `True`, this method will wait for completion of the
              asynchronous job performing the operation.

            * If `False`, this method will return immediately once the HMC has
              accepted the request to perform the operation.

          operation_timeout (:term:`number`):
            Timeout in seconds, for waiting for completion of the asynchronous
            job performing the operation. The special value 0 means that no
            timeout is set. `None` means that the default async operation
            timeout of the session is used. If the timeout expires when
            `wait_for_completion=True`, a
            :exc:`~zhmcclient.OperationTimeout` is raised.

        Returns:

          `None` or :class:`~zhmcclient.Job`:

            If `wait_for_completion` is `True`, returns `None`.

            If `wait_for_completion` is `False`, returns a
            :class:`~zhmcclient.Job` object representing the asynchronously
            executing job on the HMC.

        Raises:

          :exc:`~zhmcclient.HTTPError`
          :exc:`~zhmcclient.ParseError`
          :exc:`~zhmcclient.AuthError`
          :exc:`~zhmcclient.ConnectionError`
          :exc:`~zhmcclient.OperationTimeout`: The timeout expired while
            waiting for completion of the operation.",0,1,0,1
"def increment(self, conn, seqName, transaction = False, incCount=1):
        
	try:
	    seqTable = ""%sS"" %seqName
	    tlock = ""lock tables %s write"" %seqTable
	    self.dbi.processData(tlock, [], conn, transaction)
	    sql = ""select ID from %s"" % seqTable
	    result = self.dbi.processData(sql, [], conn, transaction)
	    resultlist = self.formatDict(result)
	    newSeq = resultlist[0]['id']+incCount
	    sql = ""UPDATE %s SET ID=:seq_count"" % seqTable
	    seqparms={""seq_count"" : newSeq}
	    self.dbi.processData(sql, seqparms, conn, transaction)
	    tunlock = ""unlock tables""
	    self.dbi.processData(tunlock, [], conn, transaction)
	    return newSeq
	except:
	    
	    tunlock = ""unlock tables""
	    self.dbi.processData(tunlock, [], conn, transaction)
	    raise","increments the sequence `seqName` by default `Incremented by one`
        and returns its value",5,0,1,6
"def index():
    
    page = request.args.get('page', 1, type=int)
    per_page = request.args.get('per_page', 5, type=int)
    q = request.args.get('q', '')

    groups = Group.query_by_user(current_user, eager=True)
    if q:
        groups = Group.search(groups, q)
    groups = groups.paginate(page, per_page=per_page)

    requests = Membership.query_requests(current_user).count()
    invitations = Membership.query_invitations(current_user).count()

    return render_template(
        'invenio_groups/index.html',
        groups=groups,
        requests=requests,
        invitations=invitations,
        page=page,
        per_page=per_page,
        q=q
    )",List all user memberships.,1,1,0,2
"def index(self, fields, name=None, table=None, **kwargs):
        
        table = self.get_table(table)
        name = self._index_default_name(fields, name)
        fields = parse.parse_fields(fields)
        fields = self.columns(table, fields, reflect=True)
        session = self.session_new()
        index = Index(name, *fields)
        logger.info('Writing new index %s: %s' % (name, fields))
        result = index.create(self.engine)
        session.commit()
        return result","Build a new index on a cube.

        Examples:
            + index('field_name')

        :param fields: A single field or a list of (key, direction) pairs
        :param name: (optional) Custom name to use for this index
        :param collection: cube name
        :param owner: username of cube owner",1,1,1,3
"def infer(query, replacements=None, root_type=None,
          libs=(""stdcore"", ""stdmath"")):
    
    
    if root_type:
        type_scope = scope.ScopeStack(std_core.MODULE, root_type)
    else:
        type_scope = scope.ScopeStack(std_core.MODULE)

    stdcore_included = False
    for lib in libs:
        if lib == ""stdcore"":
            stdcore_included = True
            continue

        module = std_core.LibraryModule.ALL_MODULES.get(lib)
        if not module:
            raise TypeError(""No standard library module %r."" % lib)

        type_scope = scope.ScopeStack(module, type_scope)

    if not stdcore_included:
        raise TypeError(""'stdcore' must always be included."")

    query = q.Query(query, params=replacements)
    return infer_type.infer_type(query, type_scope)","Determine the type of the query's output without actually running it.

    Arguments:
        query: A query object or string with the query.
        replacements: Built-time parameters to the query, either as dict or as
            an array (for positional interpolation).
        root_type: The types of variables to be supplied to the query inference.
        libs: What standard libraries should be taken into account for the
            inference.

    Returns:
        The type of the query's output, if it can be determined. If undecidable,
        returns efilter.protocol.AnyType.

        NOTE: The inference returns the type of a row in the results, not of the
        actual Python object returned by 'apply'. For example, if a query
        returns multiple rows, each one of which is an integer, the type of the
        output is considered to be int, not a collection of rows.

    Examples:
        infer(""5 + 5"") # -> INumber

        infer(""SELECT * FROM people WHERE age > 10"") # -> AnyType

        # If root_type implements the IStructured reflection API:
        infer(""SELECT * FROM people WHERE age > 10"", root_type=...) # -> dict",2,0,3,5
"def infer_ast(src):
    
    if isinstance(src, _ast.AST):
        return src
    elif isinstance(src, str):
        return _ast.parse(src)
    else:
        
        
        
        
        
        src = cypy.fn_get_source(src)
        return _ast.parse(src)","Attempts to infer an abstract syntax tree from the provided value.
    
    - Python ast.AST instances are passed through.
    - Strings are parsed. A SyntaxError is raised if invalid.
    - Functions are sent through cypy.fn_get_source to get a source
      string, then parsed. If the source can't be a found, an exception is 
      raised by fn_get_source.
      
    .. WARNING:: Functions defined on the iPython command line do not have 
                 their source saved. A bug has been filed: 
                 
                 http://github.com/ipython/ipython/issues/issue/120",0,0,1,1
"def inherit_dict(base, namespace, attr_name,
                     inherit=lambda k, v: True):
        

        items = []

        
        base_dict = getattr(base, attr_name, {})
        new_dict = namespace.setdefault(attr_name, {})
        for key, value in base_dict.items():
            
            
            if key in new_dict or (inherit and not inherit(key, value)):
                continue

            
            if inherit:
                new_dict[key] = value

            
            items.append((key, value))

        return items","Perform inheritance of dictionaries.  Returns a list of key
        and value pairs for values that were inherited, for
        post-processing.

        :param base: The base class being considered; see
                     ``iter_bases()``.
        :param namespace: The dictionary of the new class being built.
        :param attr_name: The name of the attribute containing the
                          dictionary to be inherited.
        :param inherit: Filtering function to determine if a given
                        item should be inherited.  If ``False`` or
                        ``None``, item will not be added, but will be
                        included in the returned items.  If a
                        function, the function will be called with the
                        key and value, and the item will be added and
                        included in the items list only if the
                        function returns ``True``.  By default, all
                        items are added and included in the items
                        list.",0,0,2,2
"def init(port=None, do_not_exit=False, disable_tls=False, log_level='WARNING'):
    
    logger = logging.getLogger('xenon')
    logger.setLevel(logging.INFO)

    logger_handler = logging.StreamHandler()
    logger_handler.setFormatter(logging.Formatter(style='{'))
    logger_handler.setLevel(getattr(logging, log_level))
    logger.addHandler(logger_handler)

    if port is None:
        port = find_free_port()

    if __server__.process is not None:
        logger.warning(
            ""You tried to run init(), but the server is already running."")
        return __server__

    __server__.port = port
    __server__.disable_tls = disable_tls
    __server__.__enter__()

    if not do_not_exit:
        atexit.register(__server__.__exit__, None, None, None)

    return __server__","Start the Xenon GRPC server on the specified port, or, if a service
    is already running on that port, connect to that.

    If no port is given, a random port is selected. This means that, by
    default, every python instance will start its own instance of a xenon-grpc
    process.

    :param port: the port number
    :param do_not_exit: by default the GRPC server is shut down after Python
        exits (through the `atexit` module), setting this value to `True` will
        prevent that from happening.",0,1,3,4
"def init(self):
        
        url = 'https://finance.yahoo.com/quote/%s/history' % (self.ticker)
        r = requests.get(url)
        txt = r.content
        cookie = r.cookies['B']
        pattern = re.compile('.*""CrumbStore"":\{""crumb"":""(?P<crumb>[^""]+)""\}')

        for line in txt.splitlines():
            m = pattern.match(line.decode(""utf-8""))
            if m is not None:
                crumb = m.groupdict()['crumb']
                crumb = crumb.replace(u'\\u002F', '/')
        return cookie, crumb",Returns a tuple pair of cookie and crumb used in the request,0,1,2,3
"def init_app(self, app):
        
        if not hasattr(app, 'extensions'):
            app.extensions = {}
        mailer = BaseMailer(app.config.get('MARROWMAILER_CONFIG') or self.default_config)
        app.extensions['marrowmailer'] = mailer
        app.marrowmailer = self","Initialize the extension. Configuration will be 
        obtained from ``app.config['MARROWMAILER_CONFIG']``. If no 
        configuration is found the mailer will be configured to 
        send emails asynchrously via SMTP on localhost without 
        authentication. The created ``Mailer`` instance is written
        to ``app.marrowmailer``.",1,1,0,2
"def init_app(self, app=None, blueprint=None, additional_blueprints=None):
        
        if app is not None:
            self.app = app

        if blueprint is not None:
            self.blueprint = blueprint

        for resource in self.resources:
            self.route(resource['resource'],
                       resource['view'],
                       *resource['urls'],
                       url_rule_options=resource['url_rule_options'])

        if self.blueprint is not None:
            self.app.register_blueprint(self.blueprint)

        if additional_blueprints is not None:
            for blueprint in additional_blueprints:
                self.app.register_blueprint(blueprint)

        self.app.config.setdefault('PAGE_SIZE', 30)","Update flask application with our api

        :param Application app: a flask application",0,0,1,1
"def init_mysql_db(username, host, database, port='', password='', initTime=False):
    
    
    mysql_base_url = 'mysql://'
    
    if password != '':
        password = ':%s' % password
        
    if port != '':
        port = ':%s' % port
        
    sqlalchemy_url = '%s%s%s@%s%s/%s' % (
                      mysql_base_url,
                      username,
                      password,
                      host,
                      port,
                      database
                      )
    
    init_time = init_db(sqlalchemy_url)
    
    if initTime:
        print('TIME: {0} seconds'.format(init_time))
    
    return sqlalchemy_url","Initialize MySQL Database
    
    .. note:: mysql-python or similar driver required
    
    Args:
        username(str): Database username.
        host(str): Database host URL.
        database(str): Database name.
        port(Optional[int,str]): Database port.
        password(Optional[str]): Database password.
        initTime(Optional[bool]): If True, it will print the amount of time to generate database.

    Example::
    
        from gsshapy.lib.db_tools import init_mysql_db, create_session
        
        sqlalchemy_url = init_mysql_db(username='gsshapy', 
                                       host='localhost', 
                                       database='gsshapy_mysql_tutorial', 
                                       port='5432', 
                                       password='pass')
                                       
        db_work_sessionmaker = get_sessionmaker(sqlalchemy_url)

        db_work_session = db_work_sessionmaker()
        ##DO WORK
        
        db_work_session.close()",2,0,1,3
"def initialize(self, conf, ctx):
        
        config = get_config()['ResultTopicBolt']
        kafka_class = import_name(config['kafka_class'])
        self.client = kafka_class(**config['kafka_init'])
        self.topic = self.client.topics[config['topic']]
        self.producer = self.topic.get_producer()

        
        self.tweet_shelf = shelf_from_config(config, index='pre_kafka_shelf')","Initialization steps:

        1. Connect to Kafka.
        2. Prepare Kafka producer for `tweet` topic.
        3. Prepare to track tweets published to topic, to avoid redundant data.",0,1,2,3
"def inject_dependency(self, dependent, dependency):
    
    if dependent not in self._target_by_address:
      raise ValueError('Cannot inject dependency from {dependent} on {dependency} because the'
                       ' dependent is not in the BuildGraph.'
                       .format(dependent=dependent, dependency=dependency))

    
    
    
    
    
    
    

    if dependency not in self._target_by_address:
      logger.warning('Injecting dependency from {dependent} on {dependency}, but the dependency'
                     ' is not in the BuildGraph.  This probably indicates a dependency cycle, but'
                     ' it is not an error until sort_targets is called on a subgraph containing'
                     ' the cycle.'
                     .format(dependent=dependent, dependency=dependency))

    if dependency in self.dependencies_of(dependent):
      logger.debug('{dependent} already depends on {dependency}'
                   .format(dependent=dependent, dependency=dependency))
    else:
      self._target_dependencies_by_address[dependent].add(dependency)
      self._target_dependees_by_address[dependency].add(dependent)","Injects a dependency from `dependent` onto `dependency`.

    It is an error to inject a dependency if the dependent doesn't already exist, but the reverse
    is not an error.

    :API: public

    :param Address dependent: The (already injected) address of a Target to which `dependency`
      is being added.
    :param Address dependency: The dependency to be injected.",1,2,2,5
"def input(name, default=None, foreach=None):
    
    assert default is None or foreach is None
    value = foreach if foreach is not None else default
    value = StreamSpec(value) if isinstance(value, Node) else value
    foreach = foreach is not None
    spec = InputSpec(name, value, foreach)
    def deco(func):
        .format(spec)
        specs = func.__dict__.setdefault('__marv_input_specs__', OrderedDict())
        if spec.name in specs:
            raise InputNameCollision(spec.name)
        specs[spec.name] = spec
        return func
    return deco","Decorator to declare input for a node.

    Plain inputs, that is plain python objects, are directly passed to
    the node. Whereas streams generated by other nodes are requested
    and once the handles of all input streams are available the node
    is instantiated.

    Args:
        name (str): Name of the node function argument the input will
            be passed to.
        default: An optional default value for the input. This can be
            any python object or another node.
        foreach (bool): This parameter is currently not supported and
            only for internal usage.

    Returns:
        The original function decorated with this input
        specification. A function is turned into a node by the
        :func:`node` decorator.",1,0,3,4
"def insert(cls, cur, table: str, values: dict):
        
        keys = cls._COMMA.join(values.keys())
        value_place_holder = cls._PLACEHOLDER * len(values)
        query = cls._insert_string.format(table, keys, value_place_holder[:-1])
        yield from cur.execute(query, tuple(values.values()))
        return (yield from cur.fetchone())","Creates an insert statement with only chosen fields

        Args:
            table: a string indicating the name of the table
            values: a dict of fields and values to be inserted

        Returns:
            A 'Record' object with table columns as properties",2,0,1,3
"def insert(self, i, x):
        
        if i == len(self):  
            self.append(x)
        elif len(self.matches) > i:
            
            insert_index = self.matches[i].getparent().index(self.matches[i])
            _create_xml_node(self.xast, self.node, self.context, insert_index)
            
            self[i] = x
        else:
            raise IndexError(""Can't insert '%s' at index %d - list length is only %d"" \
                            % (x, i, len(self)))",Insert an item (x) at a given position (i).,1,0,1,2
"def insert(self, index, item):
        
        if issubclass(item.__class__, self._pyof_class):
            list.insert(self, index, item)
        else:
            raise exceptions.WrongListItemType(item.__class__.__name__,
                                               self._pyof_class.__name__)","Insert an item at the specified index.

        Args:
            index (int): Position to insert the item.
            item: Item to be inserted. It must have the type specified in the
                constructor.

        Raises:
            :exc:`~.exceptions.WrongListItemType`: If the item has a different
                type than the one specified in the constructor.",1,0,2,3
"def install(verbose=True,
            verbose_destination=sys.__stderr__.fileno() if hasattr(sys.__stderr__, 'fileno') else sys.__stderr__,
            strict=True,
            **kwargs):
    
    
    global _MANHOLE

    with _LOCK:
        if _MANHOLE is None:
            _MANHOLE = Manhole()
        else:
            if strict:
                raise AlreadyInstalled(""Manhole already installed!"")
            else:
                _LOG.release()
                _MANHOLE.release()  

    _LOG.configure(verbose, verbose_destination)
    _MANHOLE.configure(**kwargs)  
    return _MANHOLE","Installs the manhole.

    Args:
        verbose (bool): Set it to ``False`` to squelch the logging.
        verbose_destination (file descriptor or handle): Destination for verbose messages. Default is unbuffered stderr
            (stderr ``2`` file descriptor).
        patch_fork (bool): Set it to ``False`` if you don't want your ``os.fork`` and ``os.forkpy`` monkeypatched
        activate_on (int or signal name): set to ``""USR1""``, ``""USR2""`` or some other signal name, or a number if you
            want the Manhole thread to start when this signal is sent. This is desireable in case you don't want the
            thread active all the time.
        oneshot_on (int or signal name): Set to ``""USR1""``, ``""USR2""`` or some other signal name, or a number if you
            want the Manhole to listen for connection in the signal handler. This is desireable in case you don't want
            threads at all.
        thread (bool): Start the always-on ManholeThread. Default: ``True``. Automatically switched to ``False`` if
            ``oneshort_on`` or ``activate_on`` are used.
        sigmask (list of ints or signal names): Will set the signal mask to the given list (using
            ``signalfd.sigprocmask``). No action is done if ``signalfd`` is not importable.
            **NOTE**: This is done so that the Manhole thread doesn't *steal* any signals; Normally that is fine because
            Python will force all the signal handling to be run in the main thread but signalfd doesn't.
        socket_path (str): Use a specific path for the unix domain socket (instead of ``/tmp/manhole-<pid>``). This
            disables ``patch_fork`` as children cannot reuse the same path.
        reinstall_delay (float): Delay the unix domain socket creation *reinstall_delay* seconds. This
            alleviates cleanup failures when using fork+exec patterns.
        locals (dict): Names to add to manhole interactive shell locals.
        daemon_connection (bool): The connection thread is daemonic (dies on app exit). Default: ``False``.
        redirect_stderr (bool): Redirect output from stderr to manhole console. Default: ``True``.
        connection_handler (function): Connection handler to use. Use ``""exec""`` for simple implementation without
            output redirection or your own function. (warning: this is for advanced users). Default: ``""repl""``.",1,0,2,3
"def install_ca_cert(ca_cert, name=None):
    
    if not ca_cert:
        return
    if not isinstance(ca_cert, bytes):
        ca_cert = ca_cert.encode('utf8')
    if not name:
        name = 'juju-{}'.format(charm_name())
    cert_file = '/usr/local/share/ca-certificates/{}.crt'.format(name)
    new_hash = hashlib.md5(ca_cert).hexdigest()
    if file_hash(cert_file) == new_hash:
        return
    log(""Installing new CA cert at: {}"".format(cert_file), level=INFO)
    write_file(cert_file, ca_cert)
    subprocess.check_call(['update-ca-certificates', '--fresh'])","Install the given cert as a trusted CA.

    The ``name`` is the stem of the filename where the cert is written, and if
    not provided, it will default to ``juju-{charm_name}``.

    If the cert is empty or None, or is unchanged, nothing is done.",1,1,1,3
"def install_theme(path_to_theme):
    
    pref_init()
    
    filename = basename(path_to_theme)
    dest = join(THEMES_DIR, filename)
    copy(path_to_theme, dest)
    
    zf = zipfile.ZipFile(dest)
    
    
    zf.extractall(THEMES_DIR)  
    
    unlink(dest)",Pass a path to a theme file which will be extracted to the themes directory.,1,0,1,2
"def int2letters(x, alphabet):
    
    base = len(alphabet)
    if x < 0:
        raise ValueError('Only non-negative numbers are supported. Encounterd %s' % x)
    letters = []
    quotient = x
    while quotient >= 0:
        quotient, remainder = divmod(quotient, base)
        quotient -= 1
        letters.append(alphabet[remainder])
    letters.reverse()
    return ''.join(letters)","Return the alphabet representation of a non-negative integer x.
    For example, with alphabet=['a','b']
    0 -> 'a'
    1 -> 'b'
    2 -> 'aa'
    3 -> 'ab'
    4 -> 'ba'

    Modified from:
    http://stackoverflow.com/questions/2267362/convert-integer-to-a-string-in-a-given-numeric-base-in-python",1,0,2,3
"def interaction_method(self, kind, x):
        
        if self.info is None or self.code != ERR_INTERACTION_REQUIRED:
            raise InteractionError(
                'not an interaction-required error (code {})'.format(
                    self.code)
            )
        entry = self.info.interaction_methods.get(kind)
        if entry is None:
            raise InteractionMethodNotFound(
                'interaction method {} not found'.format(kind)
            )
        return x.from_dict(entry)","Checks whether the error is an InteractionRequired error
        that implements the method with the given name, and JSON-unmarshals the
        method-specific data into x by calling its from_dict method
        with the deserialized JSON object.
        @param kind The interaction method kind (string).
        @param x A class with a class method from_dict that returns a new
        instance of the interaction info for the given kind.
        @return The result of x.from_dict.",2,0,3,5
"def interpolate(self, factor, minInfo, maxInfo, round=True, suppressError=True):
        
        factor = normalizers.normalizeInterpolationFactor(factor)
        if not isinstance(minInfo, BaseInfo):
            raise TypeError((""Interpolation to an instance of %r can not be ""
                             ""performed from an instance of %r."") %
                            (self.__class__.__name__, minInfo.__class__.__name__))
        if not isinstance(maxInfo, BaseInfo):
            raise TypeError((""Interpolation to an instance of %r can not be ""
                             ""performed from an instance of %r."") %
                            (self.__class__.__name__, maxInfo.__class__.__name__))
        round = normalizers.normalizeBoolean(round)
        suppressError = normalizers.normalizeBoolean(suppressError)
        self._interpolate(factor, minInfo, maxInfo,
                          round=round, suppressError=suppressError)","Interpolate all pairs between minInfo and maxInfo.
        The interpolation occurs on a 0 to 1.0 range where minInfo
        is located at 0 and maxInfo is located at 1.0.

        factor is the interpolation value. It may be less than 0
        and greater than 1.0. It may be a number (integer, float)
        or a tuple of two numbers. If it is a tuple, the first
        number indicates the x factor and the second number
        indicates the y factor.

        round indicates if the result should be rounded to integers.

        suppressError indicates if incompatible data should be ignored
        or if an error should be raised when such incompatibilities are found.",2,0,3,5
"def intersection(self, coordinates, objects=False):
        

        if objects:
            return self._intersection_obj(coordinates, objects)

        p_mins, p_maxs = self.get_coordinate_pointers(coordinates)

        p_num_results = ctypes.c_uint64(0)

        it = ctypes.pointer(ctypes.c_int64())

        core.rt.Index_Intersects_id(self.handle,
                                    p_mins,
                                    p_maxs,
                                    self.properties.dimension,
                                    ctypes.byref(it),
                                    ctypes.byref(p_num_results))
        return self._get_ids(it, p_num_results.value)","Return ids or objects in the index that intersect the given
        coordinates.

        :param coordinates: sequence or array
            This may be an object that satisfies the numpy array
            protocol, providing the index's dimension * 2 coordinate
            pairs representing the `mink` and `maxk` coordinates in
            each dimension defining the bounds of the query window.

        :param objects: True or False or 'raw'
            If True, the intersection method will return index objects that
            were pickled when they were stored with each index entry, as well
            as the id and bounds of the index entries. If 'raw', the objects
            will be returned without the :class:`rtree.index.Item` wrapper.

        The following example queries the index for any objects any objects
        that were stored in the index intersect the bounds given in the
        coordinates::

            >>> from rtree import index
            >>> idx = index.Index()
            >>> idx.insert(4321,
            ...            (34.3776829412, 26.7375853734, 49.3776829412,
            ...             41.7375853734),
            ...            obj=42)

            >>> hits = list(idx.intersection((0, 0, 60, 60), objects=True))
            >>> [(item.object, item.bbox) for item in hits if item.id == 4321]
            ... # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
            [(42, [34.37768294..., 26.73758537..., 49.37768294...,
                   41.73758537...])]

        If the :class:`rtree.index.Item` wrapper is not used, it is faster to
        request the 'raw' objects::

            >>> list(idx.intersection((0, 0, 60, 60), objects=""raw""))
            [42]",0,0,2,2
"def intersection(self, other, sort=False):
        
        self._validate_sort_keyword(sort)
        self._assert_can_do_setop(other)
        other, result_names = self._convert_can_do_setop(other)

        if self.equals(other):
            return self

        self_tuples = self._ndarray_values
        other_tuples = other._ndarray_values
        uniq_tuples = set(self_tuples) & set(other_tuples)

        if sort is None:
            uniq_tuples = sorted(uniq_tuples)

        if len(uniq_tuples) == 0:
            return MultiIndex(levels=self.levels,
                              codes=[[]] * self.nlevels,
                              names=result_names, verify_integrity=False)
        else:
            return MultiIndex.from_arrays(lzip(*uniq_tuples), sortorder=0,
                                          names=result_names)","Form the intersection of two MultiIndex objects.

        Parameters
        ----------
        other : MultiIndex or array / Index of tuples
        sort : False or None, default False
            Sort the resulting MultiIndex if possible

            .. versionadded:: 0.24.0

            .. versionchanged:: 0.24.1

               Changed the default from ``True`` to ``False``, to match
               behaviour from before 0.24.0

        Returns
        -------
        Index",0,0,3,3
"def intersects_first(self,
                         ray_origins,
                         ray_directions):
        

        ray_origins = np.asanyarray(deepcopy(ray_origins))
        ray_directions = np.asanyarray(ray_directions)

        triangle_index = self._scene.run(ray_origins,
                                         ray_directions)
        return triangle_index","Find the index of the first triangle a ray hits.


        Parameters
        ----------
        ray_origins:    (n,3) float, origins of rays
        ray_directions: (n,3) float, direction (vector) of rays

        Returns
        ----------
        triangle_index: (n,) int, index of triangle ray hit, or -1 if not hit",0,0,1,1
"def inv(self, q_data, max_iterations=100, tollerance=1e-5):
        
        q_data = numpy.asfarray(q_data)
        assert numpy.all((q_data >= 0) & (q_data <= 1)), ""sanitize your inputs!""
        shape = q_data.shape
        q_data = q_data.reshape(len(self), -1)
        x_data = evaluation.evaluate_inverse(self, q_data)
        lower, upper = evaluation.evaluate_bound(self, x_data)
        x_data = numpy.clip(x_data, a_min=lower, a_max=upper)
        x_data = x_data.reshape(shape)
        return x_data","Inverse Rosenblatt transformation.

        If possible the transformation is done analytically. If not possible,
        transformation is approximated using an algorithm that alternates
        between Newton-Raphson and binary search.

        Args:
            q_data (numpy.ndarray):
                Probabilities to be inverse. If any values are outside ``[0,
                1]``, error will be raised. ``q_data.shape`` must be compatible
                with distribution shape.
            max_iterations (int):
                If approximation is used, this sets the maximum number of
                allowed iterations in the Newton-Raphson algorithm.
            tollerance (float):
                If approximation is used, this set the error tolerance level
                required to define a sample as converged.

        Returns:
            (numpy.ndarray):
                Inverted probability values where
                ``out.shape == q_data.shape``.",0,0,1,1
"def invalidate_cache(self, klass, extra=None, **kwargs):
        

        extra = extra or kwargs.pop('extra', {})
        for group in self._registry.values():
            if klass in group.models:
                e = extra.get(group.key)
                group.invalidate_cache(klass, extra=e, **kwargs)","Invalidate a cache for a specific class.

        This will loop through all registered groups that have registered
        the given model class and call their invalidate_cache method.

        All keyword arguments will be directly passed through to the
        group's invalidate_cache method, with the exception of **extra**
        as noted below.

        :param klass: The model class that need some invalidation.

        :param extra: A dictionary where the key corresponds to the name \
        of a group where this model is registered and a value that is a \
        list that will be passed as the extra keyword argument when \
        calling invalidate_cache on that group. In this way you can \
        specify specific extra values to invalidate only for specific \
        groups.",0,0,1,1
"def invert_dependencies(targets):
  
  roots = set()
  inverted_deps = defaultdict(OrderedSet)  
  visited = set()
  path = OrderedSet()

  def invert(tgt):
    if tgt in path:
      path_list = list(path)
      cycle_head = path_list.index(tgt)
      cycle = path_list[cycle_head:] + [tgt]
      raise CycleException(cycle)
    path.add(tgt)
    if tgt not in visited:
      visited.add(tgt)
      if tgt.dependencies:
        for dependency in tgt.dependencies:
          inverted_deps[dependency].add(tgt)
          invert(dependency)
      else:
        roots.add(tgt)

    path.remove(tgt)

  for target in targets:
    invert(target)

  return roots, inverted_deps",":API: public

  :return: the full graph of dependencies for `targets` and the list of roots.",0,0,2,2
"def is_address_valid(self, address):
        
        try:
            mbi = self.mquery(address)
        except WindowsError:
            e = sys.exc_info()[1]
            if e.winerror == win32.ERROR_INVALID_PARAMETER:
                return False
            raise
        return True","Determines if an address is a valid user mode address.

        @type  address: int
        @param address: Memory address to query.

        @rtype:  bool
        @return: C{True} if the address is a valid user mode address.

        @raise WindowsError: An exception is raised on error.",1,0,1,2
"def is_bad_link(node):
    
    if node.tag != ""a"":
        return False

    name = node.get(""name"")
    href = node.get(""href"")
    if name and not href:
        return True

    if href:
        href_parts = href.split(""
        if len(href_parts) == 2 and len(href_parts[1]) > 25:
            return True

    return False","Helper to determine if the node is link that is useless.

    We've hit articles with many multiple links that should be cleaned out
    because they're just there to pollute the space. See tests for examples.",0,0,1,1
"def is_blast_result_trunc(qstart, qend, sstart, send, qlen, slen):
        
        q_match_len = abs(qstart - qend) + 1
        s_max = max(sstart, send)
        s_min = min(sstart, send)
        return (q_match_len < qlen) and (s_max >= slen or s_min <= 1)","Check if a query sequence is truncated by the end of a subject sequence

        Args:
            qstart (int): Query sequence start index
            qend (int): Query sequence end index
            sstart (int): Subject sequence start index
            send (int): Subject sequence end index
            qlen (int): Query sequence length
            slen (int): Subject sequence length

        Returns:
            bool: Result truncated by subject sequence end?",0,0,1,1
"def is_compliant(self, path):
        
        same_templates = self.templates_match(path)
        same_content = self.contents_match(path)
        same_permissions = self.permissions_match(path)

        if same_content and same_permissions and same_templates:
            return True

        return False","Determines if the templated file is compliant.

        A templated file is only compliant if it has not changed (as
        determined by its sha256 hashsum) AND its file permissions are set
        appropriately.

        :param path: the path to check compliance.",0,0,1,1
"def is_default(name=None, index=None):
    

    if not is_configured():
        raise JutException('No configurations available, please run `jut config add`')

    count = 1
    for configuration in _CONFIG.sections():

        if index != None:
            if _CONFIG.has_option(configuration, 'default') and count == index:
                return True

        if name != None:
            if _CONFIG.has_option(configuration, 'default') and configuration == name:
                return True

        count += 1

    return False",returns True if the specified configuration is the default one,1,0,2,3
"def is_dict_equal(d1, d2, keys=None, ignore_none_values=True):
    
    if keys or ignore_none_values:
        d1 = {k: v for k, v in d1.items() if (keys is None or k in keys) and (v is not None or not ignore_none_values)}
        d2 = {k: v for k, v in d2.items() if (keys is None or k in keys) and (v is not None or not ignore_none_values)}

    return d1 == d2","Compares two dictionaries to see if they are equal
    :param d1: the first dictionary
    :param d2: the second dictionary
    :param keys: the keys to limit the comparison to (optional)
    :param ignore_none_values: whether to ignore none values
    :return: true if the dictionaries are equal, else false",0,0,1,1
"def is_dimensionless_standard_name(xml_tree, standard_name):
    
    
    if not isinstance(standard_name, basestring):
        return False
    found_standard_name = xml_tree.find("".//entry[@id='{}']"".format(standard_name))
    if found_standard_name is not None:
        canonical_units = found_standard_name.find('canonical_units')
        
        
        
        
        dimless_units = r'1(?:e-?(?:1|2|3|6|9|12|15|18|21|24))?$'
        return canonical_units is None or re.match(dimless_units,
                                                   canonical_units.text)
    
    else:
        return False","Returns True if the units for the associated standard name are
    dimensionless.  Dimensionless standard names include those that have no
    units and units that are defined as constant units in the CF standard name
    table i.e. '1', or '1e-3'.",0,0,2,2
"def is_duplicate_content_url(url1, url2):
    
    if url1 == url2:
        return True
    if url2 in url1:
        url1 = shorten_duplicate_content_url(url1)
        if not url2.endswith('/') and url1.endswith('/'):
            url2 += '/'
        return url1 == url2
    if url1 in url2:
        url2 = shorten_duplicate_content_url(url2)
        if not url1.endswith('/') and url2.endswith('/'):
            url1 += '/'
        return url1 == url2
    return False",Check if both URLs are allowed to point to the same content.,0,0,1,1
"def is_file(package):
    
    if hasattr(package, ""keys""):
        return any(key for key in package.keys() if key in [""file"", ""path""])

    if os.path.exists(str(package)):
        return True

    for start in SCHEME_LIST:
        if str(package).startswith(start):
            return True

    return False",Determine if a package name is for a File dependency.,0,0,1,1
"def is_invalid_operation(self, callsign, timestamp=datetime.utcnow().replace(tzinfo=UTC)):
        

        callsign = callsign.strip().upper()

        if self._lookuptype == ""clublogxml"":

            return self._check_inv_operation_for_date(callsign, timestamp, self._invalid_operations, self._invalid_operations_index)

        elif self._lookuptype == ""redis"":

            data_dict, index = self._get_dicts_from_redis(""_inv_op_"", ""_inv_op_index_"", self._redis_prefix, callsign)
            return self._check_inv_operation_for_date(callsign, timestamp, data_dict, index)

        
        raise KeyError","Returns True if an operations is known as invalid

        Args:
            callsign (string): Amateur Radio callsign
            timestamp (datetime, optional): datetime in UTC (tzinfo=pytz.UTC)

        Returns:
            bool: True if a record exists for this callsign (at the given time)

        Raises:
            KeyError: No matching callsign found
            APIKeyMissingError: API Key for Clublog missing or incorrect

        Example:
           The following code checks the Clublog XML database if the operation is valid for two dates.

           >>> from pyhamtools import LookupLib
           >>> from datetime import datetime
           >>> import pytz
           >>> my_lookuplib = LookupLib(lookuptype=""clublogxml"", apikey=""myapikey"")
           >>> print my_lookuplib.is_invalid_operation(""5W1CFN"")
           True
           >>> try:
           >>>   timestamp = datetime(year=2012, month=1, day=31).replace(tzinfo=pytz.UTC)
           >>>   my_lookuplib.is_invalid_operation(""5W1CFN"", timestamp)
           >>> except KeyError:
           >>>   print ""Seems to be invalid operation before 31.1.2012""
           Seems to be an invalid operation before 31.1.2012

        Note:
            This method is available for

            - clublogxml
            - redis",1,0,2,3
"def is_modified(row, dialect):
    
    ins = inspect(row)
    modified_cols = set(get_column_keys(ins.mapper)) - ins.unmodified
    for col_name in modified_cols:
        current_value = get_column_attribute(row, col_name, dialect=dialect)
        previous_value = get_column_attribute(row, col_name, use_dirty=False, dialect=dialect)
        if previous_value != current_value:
            return True
    return False","Has the row data been modified?

    This method inspects the row, and iterates over all columns looking for changes
    to the (processed) data, skipping over unmodified columns.

    :param row: SQLAlchemy model instance
    :param dialect: :py:class:`~sqlalchemy.engine.interfaces.Dialect`
    :return: True if any columns were modified, else False",0,0,2,2
"def is_on(self):
        
        if self._type == 'Occupancy':
            return self.status not in CONST.STATUS_ONLINE
        return self.status not in (CONST.STATUS_OFF, CONST.STATUS_OFFLINE,
                                   CONST.STATUS_CLOSED)","Get sensor state.

        Assume offline or open (worst case).",0,0,1,1
"def is_private(self, key, sources):
        
        
        if key == ENTRY.ALIAS:
            return False
        return all([
            SOURCE.PRIVATE in self.get_source_by_alias(x)
            for x in sources.split(',')
        ])",Check if attribute is private.,0,0,1,1
"def is_user_profile_valid(user_profile):
  

  if not user_profile:
    return False

  if not type(user_profile) is dict:
    return False

  if UserProfile.USER_ID_KEY not in user_profile:
    return False

  if UserProfile.EXPERIMENT_BUCKET_MAP_KEY not in user_profile:
    return False

  experiment_bucket_map = user_profile.get(UserProfile.EXPERIMENT_BUCKET_MAP_KEY)
  if not type(experiment_bucket_map) is dict:
    return False

  for decision in experiment_bucket_map.values():
    if type(decision) is not dict or UserProfile.VARIATION_ID_KEY not in decision:
      return False

  return True","Determine if provided user profile is valid or not.

  Args:
    user_profile: User's profile which needs to be validated.

  Returns:
    Boolean depending upon whether profile is valid or not.",0,1,0,1
"def is_win64():
    
    
    
    
    

    
    
    
    
    
    
    
    
    global _is_win64
    if _is_win64 is None:
        
        
        _is_win64 = False
        if os.environ.get('PROCESSOR_ARCHITECTURE', 'x86') != 'x86':
            _is_win64 = True
        if os.environ.get('PROCESSOR_ARCHITEW6432'):
            _is_win64 = True
        if os.environ.get('ProgramW6432'):
            _is_win64 = True
    return _is_win64","Return true if running on windows 64 bits.

    Works whether python itself runs in 64 bits or 32 bits.",0,0,2,2
"def item(*args, **kwargs):
    
    ret = {}
    default = kwargs.get('default', '')
    delimiter = kwargs.get('delimiter', DEFAULT_TARGET_DELIM)
    pillarenv = kwargs.get('pillarenv', None)
    saltenv = kwargs.get('saltenv', None)

    pillar_dict = __pillar__ \
        if all(x is None for x in (saltenv, pillarenv)) \
        else items(saltenv=saltenv, pillarenv=pillarenv)

    try:
        for arg in args:
            ret[arg] = salt.utils.data.traverse_dict_and_list(
                pillar_dict,
                arg,
                default,
                delimiter)
    except KeyError:
        pass

    return ret",".. versionadded:: 0.16.2

    Return one or more pillar entries from the :ref:`in-memory pillar data
    <pillar-in-memory>`.

    delimiter
        Delimiter used to traverse nested dictionaries.

        .. note::
            This is different from :py:func:`pillar.get
            <salt.modules.pillar.get>` in that no default value can be
            specified. :py:func:`pillar.get <salt.modules.pillar.get>` should
            probably still be used in most cases to retrieve nested pillar
            values, as it is a bit more flexible. One reason to use this
            function instead of :py:func:`pillar.get <salt.modules.pillar.get>`
            however is when it is desirable to retrieve the values of more than
            one key, since :py:func:`pillar.get <salt.modules.pillar.get>` can
            only retrieve one key at a time.

        .. versionadded:: 2015.8.0

    pillarenv
        If specified, this function will query the master to generate fresh
        pillar data on the fly, specifically from the requested pillar
        environment. Note that this can produce different pillar data than
        executing this function without an environment, as its normal behavior
        is just to return a value from minion's pillar data in memory (which
        can be sourced from more than one pillar environment).

        Using this argument will not affect the pillar data in memory. It will
        however be slightly slower and use more resources on the master due to
        the need for the master to generate and send the minion fresh pillar
        data. This tradeoff in performance however allows for the use case
        where pillar data is desired only from a single environment.

        .. versionadded:: 2017.7.6,2018.3.1

    saltenv
        Included only for compatibility with
        :conf_minion:`pillarenv_from_saltenv`, and is otherwise ignored.

        .. versionadded:: 2017.7.6,2018.3.1

    CLI Examples:

    .. code-block:: bash

        salt '*' pillar.item foo
        salt '*' pillar.item foo:bar
        salt '*' pillar.item foo bar baz",1,0,1,2
"def iter_blobs(self, predicate=lambda t: True):
        
        for entry in mviter(self.entries):
            blob = entry.to_blob(self.repo)
            blob.size = entry.size
            output = (entry.stage, blob)
            if predicate(output):
                yield output",":return: Iterator yielding tuples of Blob objects and stages, tuple(stage, Blob)

        :param predicate:
            Function(t) returning True if tuple(stage, Blob) should be yielded by the
            iterator. A default filter, the BlobFilter, allows you to yield blobs
            only if they match a given list of paths.",0,0,1,1
"def iter_project(projects, key_file=None):
    

    def decorator(func):
        @wraps(func)
        def decorated_function(*args, **kwargs):
            item_list = []
            exception_map = {}
            for project in projects:
                if isinstance(project, string_types):
                    kwargs['project'] = project
                    if key_file:
                        kwargs['key_file'] = key_file
                elif isinstance(project, dict):
                    kwargs['project'] = project['project']
                    kwargs['key_file'] = project['key_file']
                itm, exc = func(*args, **kwargs)
                item_list.extend(itm)
                exception_map.update(exc)
            return (item_list, exception_map)

        return decorated_function

    return decorator","Call decorated function for each item in project list.

    Note: the function 'decorated' is expected to return a value plus a dictionary of exceptions.

    If item in list is a dictionary, we look for a 'project' and 'key_file' entry, respectively.
    If item in list is of type string_types, we assume it is the project string. Default credentials
    will be used by the underlying client library.

    :param projects: list of project strings or list of dictionaries
                     Example: {'project':..., 'keyfile':...}. Required.
    :type projects: ``list`` of ``str`` or ``list`` of ``dict``

    :param key_file: path on disk to keyfile, for use with all projects
    :type key_file: ``str``

    :returns: tuple containing a list of function output and an exceptions map
    :rtype: ``tuple of ``list``, ``dict``",0,0,2,2
"def jdout(api_response):
    
    try:
        
        output = json.dumps(api_response.cgx_content, indent=4)
    except (TypeError, ValueError, AttributeError):
        
        try:
            output = json.dumps(api_response, indent=4)
        except (TypeError, ValueError, AttributeError):
            
            output = api_response
    return output","JD Output function. Does quick pretty printing of a CloudGenix Response body. This function returns a string
    instead of directly printing content.

      **Parameters:**

      - **api_response:** A CloudGenix-attribute extended `requests.Response` object

    **Returns:** Pretty-formatted text of the Response body",0,0,1,1
"def jira_connection(config):
    
    global _jira_connection
    if _jira_connection:
        return _jira_connection
    else:
        jira_options = {'server': config.get('jira').get('url')}

        cookies = configuration._get_cookies_as_dict()
        jira_connection = jira_ext.JIRA(options=jira_options)
        session = jira_connection._session

        reused_session = False

        if cookies:
            requests.utils.add_dict_to_cookiejar(session.cookies, cookies)
            try:
                jira_connection.session()
                reused_session = True
            except Exception:
                pass

        if not reused_session:
            session.auth = (config['jira']['username'], base64.b64decode(config['jira']['password']))
            jira_connection.session()
            session.auth = None

            cookie_jar_hash = requests.utils.dict_from_cookiejar(session.cookies)
            for key, value in cookie_jar_hash.iteritems():
                configuration._save_cookie(key, value)

        _jira_connection = jira_connection
        return _jira_connection","Gets a JIRA API connection.  If a connection has already been created the existing connection
    will be returned.",0,1,2,3
"def join(l, conj=CONJUNCTION, im_a_moron=MORON_MODE, separator=COMMA):
    

    collector = []
    left = len(l)
    separator = separator + SPACE
    conj = conj + SPACE

    for _l in l[:]:

        left += -1

        collector.append(_l)
        if left == 1:
            if len(l) == 2 or im_a_moron:
                collector.append(SPACE)
            else:
                collector.append(separator)

            collector.append(conj)

        elif left is not 0:
            collector.append(separator)

    return unicode(str().join(collector))",Joins lists of words. Oxford comma and all.,0,0,1,1
"def join_tags(tags):
    
    names = []
    delimiter = settings.TAGGIT_SELECTIZE['DELIMITER']
    for tag in tags:
        name = tag.name
        if delimiter in name or ' ' in name:
            names.append('""%s""' % name)
        else:
            names.append(name)
    return delimiter.join(sorted(names))","Given list of ``Tag`` instances, creates a string representation of
    the list suitable for editing by the user, such that submitting the
    given string representation back without changing it will give the
    same list of tags.

    Tag names which contain DELIMITER will be double quoted.

    Adapted from Taggit's _edit_string_for_tags()

    Ported from Jonathan Buchanan's `django-tagging
    <http://django-tagging.googlecode.com/>`_",0,0,2,2
"def json_handler(cls, func):
        
        @cls.handler
        @functools.wraps(func)
        def wrapper(self, request, suffix=''):
            
            if request.method != ""POST"":
                return JsonHandlerError(405, ""Method must be POST"").get_response(allow=[""POST""])
            try:
                request_json = json.loads(request.body)
            except ValueError:
                return JsonHandlerError(400, ""Invalid JSON"").get_response()
            try:
                response = func(self, request_json, suffix)
            except JsonHandlerError as err:
                return err.get_response()
            if isinstance(response, Response):
                return response
            else:
                return Response(json.dumps(response), content_type='application/json', charset='utf8')
        return wrapper","Wrap a handler to consume and produce JSON.

        Rather than a Request object, the method will now be passed the
        JSON-decoded body of the request. The request should be a POST request
        in order to use this method. Any data returned by the function
        will be JSON-encoded and returned as the response.

        The wrapped function can raise JsonHandlerError to return an error
        response with a non-200 status code.

        This decorator will return a 405 HTTP status code if the method is not
        POST.
        This decorator will return a 400 status code if the body contains
        invalid JSON.",1,2,4,7
"def jsonex_request(url, data, headers=None):
    
    
    url, headers = _parse_authentication(url)
    headers['Content-Type'] = 'application/json'

    
    try:
        req = Request(url, headers=headers)
        response = urlopen(req, jsonex_dumps(data))
        res_str = response.read()
        res = jsonex_loads(res_str)
    except HTTPError as e:
        if 'Content-Type' in e.headers and e.headers['Content-Type'] == 'application/json':
            res = jsonex_loads(e.read())
        else:
            raise exc.ServerError('Server at ""{}"" failed: {}'.format(url, e))
    except URLError as e:
        raise exc.ConnectionError('Connection to ""{}"" failed: {}'.format(url, e))

    
    if 'error' in res:  
        raise res['error']  

    return res","Make a request with JsonEx
    :param url: URL
    :type url: str
    :param data: Data to POST
    :type data: dict
    :return: Response
    :rtype: dict
    :raises exc.ConnectionError: Connection error
    :raises exc.ServerError: Remote server error (unknown)
    :raises exc.ProviderError: any errors reported by the remote",3,1,3,7
"def jstimestamp(dte):
    
    days = date(dte.year, dte.month, 1).toordinal() - _EPOCH_ORD + dte.day - 1
    hours = days*24
    
    if isinstance(dte,datetime):
        hours += dte.hour
        minutes = hours*60 + dte.minute
        seconds = minutes*60 + dte.second
        return 1000*seconds + int(0.001*dte.microsecond)
    else:
        return 3600000*hours",Convert a date or datetime object into a javsacript timestamp.,0,0,2,2
"def jwt_verify_token(headers):
    
    
    token = headers.get(
        current_app.config['OAUTH2SERVER_JWT_AUTH_HEADER']
    )
    if token is None:
        raise JWTInvalidHeaderError
    
    authentication_type = \
        current_app.config['OAUTH2SERVER_JWT_AUTH_HEADER_TYPE']
    
    if authentication_type is not None:
        
        prefix, token = token.split()
        
        if prefix != authentication_type:
            raise JWTInvalidHeaderError

    try:
        
        decode = jwt_decode_token(token)
        
        if current_user.get_id() != decode.get('sub'):
            raise JWTInvalidIssuer
        return decode
    except _JWTDecodeError as exc:
        raise_from(JWTDecodeError(), exc)
    except _JWTExpiredToken as exc:
        raise_from(JWTExpiredToken(), exc)","Verify the JWT token.

    :param dict headers: The request headers.
    :returns: The token data.
    :rtype: dict",5,0,1,6
"def kanji(self, levels=None):
        
        url = WANIKANI_BASE.format(self.api_key, 'kanji')
        if levels:
            url += '/{0}'.format(levels)
        data = self.get(url)

        for item in data['requested_information']:
            yield Kanji(item)",":param levels: An optional argument of declaring a single or
            comma-delimited list of levels is available, as seen in the example
            as 1. An example of a comma-delimited list of levels is 1,2,5,9.
        :type levels: str or None

        http://www.wanikani.com/api/v1.2#kanji-list",0,1,2,3
"def keystone_process(body, message):
    
    event_type = body['event_type']
    process = keystone_customer_process.get(event_type)
    if process is not None:
        process(body, message)
    else:
        matched = False
        process_wildcard = None
        for pattern in keystone_customer_process_wildcard.keys():
            if pattern.match(event_type):
                process_wildcard = keystone_customer_process_wildcard.get(pattern)
                matched = True
                break
        if matched:
            process_wildcard(body, message)
        else:
            default_process(body, message)
    message.ack()","This function deal with the keystone notification.

    First, find process from customer_process that not include wildcard.
    if not find from customer_process, then find process from customer_process_wildcard.
    if not find from customer_process_wildcard, then use ternya default process.
    :param body: dict of openstack notification.
    :param message: kombu Message class
    :return:",0,0,5,5
"def kill_given_tasks(self, task_ids, scale=False, force=None):
        
        params = {'scale': scale}
        if force is not None:
            params['force'] = force
        data = json.dumps({""ids"": task_ids})
        response = self._do_request(
            'POST', '/v2/tasks/delete', params=params, data=data)
        return response == 200","Kill a list of given tasks.

        :param list[str] task_ids: tasks to kill
        :param bool scale: if true, scale down the app by the number of tasks killed
        :param bool force: if true, ignore any current running deployments

        :return: True on success
        :rtype: bool",0,1,0,1
"def kill_process(self):
        
        if self.process.poll() is not None:
            return
        try:
            if hasattr(self.process, 'terminate'):
                self.process.terminate()
            elif os.name != 'nt':
                os.kill(self.process.pid, 9)
            else:
                import ctypes
                handle = int(self.process._handle)
                ctypes.windll.kernel32.TerminateProcess(handle, -1)
        except OSError:
            pass",Stop the process,0,1,0,1
"def knn_initialize(
        X,
        missing_mask,
        verbose=False,
        min_dist=1e-6,
        max_dist_multiplier=1e6):
    
    X_row_major = X.copy(""C"")
    if missing_mask.sum() != np.isnan(X_row_major).sum():
        
        
        X_row_major[missing_mask] = np.nan
    D = all_pairs_normalized_distances(X_row_major)
    D_finite_flat = D[np.isfinite(D)]
    if len(D_finite_flat) > 0:
        max_dist = max_dist_multiplier * max(1, D_finite_flat.max())
    else:
        max_dist = max_dist_multiplier
    
    
    np.fill_diagonal(D, max_dist)
    D[D < min_dist] = min_dist  
    D[D > max_dist] = max_dist  
    return X_row_major, D, max_dist","Fill X with NaN values if necessary, construct the n_samples x n_samples
    distance matrix and set the self-distance of each row to infinity.

    Returns contents of X laid out in row-major, the distance matrix,
    and an ""effective infinity"" which is larger than any entry of the
    distance matrix.",0,0,4,4
"def kwargs_to_variable_assignment(kwargs: dict, value_representation=repr,
                                  assignment_operator: str = ' = ',
                                  statement_separator: str = '\n',
                                  statement_per_line: bool = False) -> str:
    
    code = []
    join_str = '\n' if statement_per_line else ''
    for key, value in kwargs.items():
        code.append(key + assignment_operator +
                    value_representation(value)+statement_separator)
    return join_str.join(code)","Convert a dictionary into a string with assignments

    Each assignment is constructed based on:
    key assignment_operator value_representation(value) statement_separator,
    where key and value are the key and value of the dictionary.
    Moreover one can seprate the assignment statements by new lines.

    Parameters
    ----------
    kwargs : dict

    assignment_operator: str, optional:
        Assignment operator ("" = "" in python)
    value_representation: str, optinal
        How to represent the value in the assignments (repr function in python)
    statement_separator : str, optional:
        Statement separator (new line in python)
    statement_per_line: bool, optional
        Insert each statement on a different line

    Returns
    -------
    str
        All the assignemnts.

    >>> kwargs_to_variable_assignment({'a': 2, 'b': ""abc""})
    ""a = 2\\nb = 'abc'\\n""
    >>> kwargs_to_variable_assignment({'a':2 ,'b': ""abc""}, statement_per_line=True)
    ""a = 2\\n\\nb = 'abc'\\n""
    >>> kwargs_to_variable_assignment({'a': 2})
    'a = 2\\n'
    >>> kwargs_to_variable_assignment({'a': 2}, statement_per_line=True)
    'a = 2\\n'",0,0,1,1
"def labels(self, pores=[], throats=[], element=None, mode='union'):
        r
        
        if (sp.size(pores) == 0) and (sp.size(throats) == 0):
            labels = PrintableList(self.keys(element=element, mode='labels'))
        elif (sp.size(pores) > 0) and (sp.size(throats) > 0):
            raise Exception('Cannot perform label query on pores and ' +
                            'throats simultaneously')
        elif sp.size(pores) > 0:
            labels = self._get_labels(element='pore', locations=pores,
                                      mode=mode)
        elif sp.size(throats) > 0:
            labels = self._get_labels(element='throat', locations=throats,
                                      mode=mode)
        return labels","r""""""
        Returns a list of labels present on the object

        Additionally, this function can return labels applied to a specified
        set of pores or throats

        Parameters
        ----------
        element : string
            Controls whether pore or throat labels are returned.  If empty then
            both are returned (default).

        pores (or throats) : array_like
            The pores (or throats) whose labels are sought.  If left empty a
            list containing all pore and throat labels is returned.

        mode : string, optional
            Controls how the query should be performed.  Only applicable
            when ``pores`` or ``throats`` are specified:

            **'or', 'union', 'any'**: (default) Returns the labels that are
            assigned to *any* of the given locations.

            **'and', 'intersection', 'all'**: Labels that are present on *all*
            the given locations.

            **'xor', 'exclusive_or'** : Labels that are present on *only one*
            of the given locations.

            **'nor', 'none', 'not'**: Labels that are *not* present on any of
            the given locations.

            **'nand'**: Labels that are present on *all but one* of the given
            locations

            **'xnor'**: Labels that are present on *more than one* of the given
            locations.  'nxor' is also accepted.

        Returns
        -------
        A list containing the labels on the object.  If ``pores`` or
        ``throats`` are given, the results are filtered according to the
        specified ``mode``.

        See Also
        --------
        props
        keys

        Notes
        -----
        Technically, *'nand'* and *'xnor'* should also return pores with *none*
        of the labels but these are not included.  This makes the returned list
        more useful.

        Examples
        --------
        >>> import openpnm as op
        >>> pn = op.network.Cubic(shape=[5, 5, 5])
        >>> pn.labels(pores=[11, 12])
        ['pore.all', 'pore.front', 'pore.internal', 'pore.surface']",0,0,1,1
"def langlinks(
            self,
            page: 'WikipediaPage',
            **kwargs
    ) -> PagesDict:
        

        params = {
            'action': 'query',
            'prop': 'langlinks',
            'titles': page.title,
            'lllimit': 500,
            'llprop': 'url',
        }

        used_params = kwargs
        used_params.update(params)

        raw = self._query(
            page,
            used_params
        )
        self._common_attributes(raw['query'], page)
        pages = raw['query']['pages']
        for k, v in pages.items():
            if k == '-1':
                page._attributes['pageid'] = -1
                return {}
            else:
                return self._build_langlinks(v, page)
        return {}","Returns langlinks of the page with respect to parameters

        API Calls for parameters:

        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Blanglinks
        - https://www.mediawiki.org/wiki/API:Langlinks

        :param page: :class:`WikipediaPage`
        :param kwargs: parameters used in API call
        :return: links to pages in other languages",0,1,1,2
"def lastAnchor(self, block, column):
        
        currentPos = -1
        currentBlock = None
        currentColumn = None
        currentChar = None
        for char in '({[':
            try:
                foundBlock, foundColumn = self.findBracketBackward(block, column, char)
            except ValueError:
                continue
            else:
                pos = foundBlock.position() + foundColumn
                if pos > currentPos:
                    currentBlock = foundBlock
                    currentColumn = foundColumn
                    currentChar = char
                    currentPos = pos

        return currentBlock, currentColumn, currentChar","Find the last open bracket before the current line.
        Return (block, column, char) or (None, None, None)",0,0,1,1
"def launch_train_with_config(config, trainer):
    
    if is_tfv2():
        tfv1.disable_eager_execution()

    assert isinstance(trainer, SingleCostTrainer), trainer
    assert isinstance(config, TrainConfig), config
    assert config.model is not None
    assert config.dataflow is not None or config.data is not None

    model = config.model
    input = config.data or config.dataflow
    input = apply_default_prefetch(input, trainer)

    
    
    
    trainer.setup_graph(
        model.get_input_signature(), input,
        model.build_graph, model.get_optimizer)
    _check_unused_regularization()
    trainer.train_with_defaults(
        callbacks=config.callbacks,
        monitors=config.monitors,
        session_creator=config.session_creator,
        session_init=config.session_init,
        steps_per_epoch=config.steps_per_epoch,
        starting_epoch=config.starting_epoch,
        max_epoch=config.max_epoch,
        extra_callbacks=config.extra_callbacks)","Train with a :class:`TrainConfig` and a :class:`Trainer`, to
    present the simple and old training interface. It basically does the following
    3 things (and you can easily do them by yourself if you need more control):

    1. Setup the input with automatic prefetching heuristics,
       from `config.data` or `config.dataflow`.
    2. Call `trainer.setup_graph` with the input as well as `config.model`.
    3. Call `trainer.train` with rest of the attributes of config.

    See the `related tutorial
    <https://tensorpack.readthedocs.io/tutorial/training-interface.html#with-modeldesc-and-trainconfig>`_
    to learn more.

    Args:
        config (TrainConfig):
        trainer (Trainer): an instance of :class:`SingleCostTrainer`.

    Example:

    .. code-block:: python

        launch_train_with_config(
            config, SyncMultiGPUTrainerParameterServer(8, ps_device='gpu'))",0,0,3,3
"def layers(self):
        
        if self._layers is None:
            self.__init()
        lyrs = []
        for lyr in self._layers:
            lyr['object'] = GlobeServiceLayer(url=self._url + ""/%s"" % lyr['id'],
                                              securityHandler=self._securityHandler,
                                              proxy_port=self._proxy_port,
                                              proxy_url=self._proxy_url)
            lyrs.append(lyr)

        return lyrs",gets the globe service layers,0,0,1,1
"def layers(self):
        
        if self._layers is None:
            self.__init()
        lyrs = []
        for lyr in self._layers:
            url = self._url + ""/%s"" % lyr['id']
            lyr['object'] = MobileServiceLayer(url=url,
                                               securityHandler=self._securityHandler,
                                               proxy_url=self._proxy_url,
                                               proxy_port=self._proxy_port,
                                               initialize=False)
        return self._layers",gets the service layers,0,1,0,1
"def left(self, expand=None):
        
        if expand == None:
            x = 0
            y = self.y
            w = self.x
            h = self.h
        else:
            x = self.x-expand
            y = self.y
            w = expand
            h = self.h
        return Region(x, y, w, h).clipRegionToScreen()","Returns a new Region left of the current region with a width of ``expand`` pixels.

        Does not include the current region. If range is omitted, it reaches to the left border
        of the screen. The new region has the same height and y-position as the current region.",0,0,1,1
"def lengths_to_mask(*lengths, **kwargs):
    
    
    lengths = [l.squeeze().tolist() if torch.is_tensor(l) else l for l in lengths]

    
    lengths = [l if isinstance(l, list) else [l] for l in lengths]
    assert all(len(l) == len(lengths[0]) for l in lengths)
    batch_size = len(lengths[0])
    other_dimensions = tuple([int(max(l)) for l in lengths])
    mask = torch.zeros(batch_size, *other_dimensions, **kwargs)
    for i, length in enumerate(zip(*tuple(lengths))):
        mask[i][[slice(int(l)) for l in length]].fill_(1)
    return mask.byte()","Given a list of lengths, create a batch mask.

    Example:
        >>> lengths_to_mask([1, 2, 3])
        tensor([[1, 0, 0],
                [1, 1, 0],
                [1, 1, 1]], dtype=torch.uint8)
        >>> lengths_to_mask([1, 2, 2], [1, 2, 2])
        tensor([[[1, 0],
                 [0, 0]],
        <BLANKLINE>
                [[1, 1],
                 [1, 1]],
        <BLANKLINE>
                [[1, 1],
                 [1, 1]]], dtype=torch.uint8)

    Args:
        *lengths (list of int or torch.Tensor)
        **kwargs: Keyword arguments passed to ``torch.zeros`` upon initially creating the returned
          tensor.

    Returns:
        torch.ByteTensor",0,0,1,1
"def lifetimes(self, dates, include_start_date, country_codes):
        
        if isinstance(country_codes, string_types):
            raise TypeError(
                ""Got string {!r} instead of an iterable of strings in ""
                ""AssetFinder.lifetimes."".format(country_codes),
            )

        
        country_codes = frozenset(country_codes)

        lifetimes = self._asset_lifetimes.get(country_codes)
        if lifetimes is None:
            self._asset_lifetimes[country_codes] = lifetimes = (
                self._compute_asset_lifetimes(country_codes)
            )

        raw_dates = as_column(dates.asi8)
        if include_start_date:
            mask = lifetimes.start <= raw_dates
        else:
            mask = lifetimes.start < raw_dates
        mask &= (raw_dates <= lifetimes.end)

        return pd.DataFrame(mask, index=dates, columns=lifetimes.sid)","Compute a DataFrame representing asset lifetimes for the specified date
        range.

        Parameters
        ----------
        dates : pd.DatetimeIndex
            The dates for which to compute lifetimes.
        include_start_date : bool
            Whether or not to count the asset as alive on its start_date.

            This is useful in a backtesting context where `lifetimes` is being
            used to signify ""do I have data for this asset as of the morning of
            this date?""  For many financial metrics, (e.g. daily close), data
            isn't available for an asset until the end of the asset's first
            day.
        country_codes : iterable[str]
            The country codes to get lifetimes for.

        Returns
        -------
        lifetimes : pd.DataFrame
            A frame of dtype bool with `dates` as index and an Int64Index of
            assets as columns.  The value at `lifetimes.loc[date, asset]` will
            be True iff `asset` existed on `date`.  If `include_start_date` is
            False, then lifetimes.loc[date, asset] will be false when date ==
            asset.start_date.

        See Also
        --------
        numpy.putmask
        zipline.pipeline.engine.SimplePipelineEngine._compute_root_mask",1,0,3,4
"def linexp(value, in_min, in_max, out_min, out_max, clip=True):
        
        if out_min == 0:
            return None
        if in_min == in_max:
            if value == in_min and out_min == out_max:
                return out_min
            return None

        output = math.pow(out_max / out_min, (value - in_min) / (in_max - in_min)) * out_min
        if clip:
            output = Mapping.clip_value(output, out_min, out_max)
        return output","maps value \in linear range [in_min,in_max] corresponding value \in exponential range [out_min, out_max]
        (extrapolating if needed)

        :param value: value to be mapped 
        :param in_min: input range minimum
        :param in_max: input range maximum
        :param out_min: what input range minimum is mapped to
        :param out_max: what input range maximum is mapped to
        :param clip: if True, the output value is clipped to range [out_min, out_max]        
        :return: mapping from value in linear input range to value in exponential output range (extrapolating if needed)",0,0,3,3
"def link_rich_menu_to_user(self, user_id, rich_menu_id, timeout=None):
        
        self._post(
            '/v2/bot/user/{user_id}/richmenu/{rich_menu_id}'.format(
                user_id=user_id,
                rich_menu_id=rich_menu_id
            ),
            timeout=timeout
        )","Call link rich menu to user API.

        https://developers.line.me/en/docs/messaging-api/reference/#link-rich-menu-to-user

        :param str user_id: ID of an uploaded rich menu
        :param str rich_menu_id: ID of the user
        :type timeout: float | tuple(float, float)",0,1,0,1
"def list(cls,
             l,
             order=None,
             header=None,
             output=""table"",
             sort_keys=True,
             show_none=""""
             ):
        
        d = {}
        count = 0
        for entry in l:
            name = str(count)
            d[name] = entry
            count += 1
        return cls.dict(d,
                        order=order,
                        header=header,
                        sort_keys=sort_keys,
                        output=output,
                        show_none=show_none)",":param l: l is a list not a dict
        :param order:
        :param header:
        :param output:
        :param sort_keys:
        :param show_none:
        :return:",0,0,1,1
"def list(self, actor_sid=values.unset, event_type=values.unset,
             resource_sid=values.unset, source_ip_address=values.unset,
             start_date=values.unset, end_date=values.unset, limit=None,
             page_size=None):
        
        return list(self.stream(
            actor_sid=actor_sid,
            event_type=event_type,
            resource_sid=resource_sid,
            source_ip_address=source_ip_address,
            start_date=start_date,
            end_date=end_date,
            limit=limit,
            page_size=page_size,
        ))","Lists EventInstance records from the API as a list.
        Unlike stream(), this operation is eager and will load `limit` records into
        memory before returning.

        :param unicode actor_sid: Only include Events initiated by this Actor
        :param unicode event_type: Only include Events of this EventType
        :param unicode resource_sid: Only include Events referring to this resource
        :param unicode source_ip_address: Only include Events that originated from this IP address
        :param datetime start_date: Only show events on or after this date
        :param datetime end_date: Only show events on or before this date
        :param int limit: Upper limit for the number of records to return. list() guarantees
                          never to return more than limit.  Default is no limit
        :param int page_size: Number of records to fetch per request, when not set will use
                              the default value of 50 records.  If no page_size is defined
                              but a limit is defined, list() will attempt to read the limit
                              with the most efficient page size, i.e. min(limit, 1000)

        :returns: Generator that will yield up to limit results
        :rtype: list[twilio.rest.monitor.v1.event.EventInstance]",0,1,0,1
"def list(self, count=10):
    
    import IPython
    data = []
    
    
    for _, model in zip(range(count), self.get_iterator()):
      element = {'name': model['name']}
      if 'defaultVersion' in model:
        version_short_name = model['defaultVersion']['name'].split('/')[-1]
        element['defaultVersion'] = version_short_name
      data.append(element)

    IPython.display.display(
        datalab.utils.commands.render_dictionary(data, ['name', 'defaultVersion']))","List models under the current project in a table view.

    Args:
      count: upper limit of the number of models to list.
    Raises:
      Exception if it is called in a non-IPython environment.",1,0,1,2
"def list(self, limit=50, offset=0):
        
        logger.info(""list(limit=%s, offset=%s) [lid=%s,pid=%s]"", limit, offset, self.__lid, self.__pid)
        evt = self._client._request_point_value_list(self.__lid, self.__pid, self._type, limit=limit, offset=offset)

        self._client._wait_and_except_if_failed(evt)
        return evt.payload['values']","List `all` the values on this Point.

        Returns QAPI list function payload

        Raises [IOTException](./Exceptions.m.html#IoticAgent.IOT.Exceptions.IOTException)
        containing the error if the infrastructure detects a problem

        Raises [LinkException](../Core/AmqpLink.m.html#IoticAgent.Core.AmqpLink.LinkException)
        if there is a communications problem between you and the infrastructure

        `limit` (optional) (integer) Return this many value details

        `offset` (optional) (integer) Return value details starting at this offset",2,0,1,3
"def list(self, per_page=None, page=None, status=None, service='facebook'):
        

        params = {}

        if per_page is not None:
            params['per_page'] = per_page
        if page is not None:
            params['page'] = page
        if status:
            params['status'] = status

        return self.request.get(service + '/task', params)","Get a list of Pylon tasks

            :param per_page: How many tasks to display per page
            :type per_page: int
            :param page: Which page of tasks to display
            :type page: int
            :param status: The status of the tasks to list
            :type page: string
            :param service: The PYLON service (facebook)
            :type service: str
            :return: dict of REST API output with headers attached
            :rtype: :class:`~datasift.request.DictResponse`
            :raises: :class:`~datasift.exceptions.DataSiftApiException`,
                :class:`requests.exceptions.HTTPError`",1,1,0,2
"def list(self, policy_id, page=None):
        
        filters = [
            'policy_id={0}'.format(policy_id),
            'page={0}'.format(page) if page else None
        ]

        return self._get(
            url='{0}alerts_conditions.json'.format(self.URL),
            headers=self.headers,
            params=self.build_param_string(filters)
        )","This API endpoint returns a paginated list of alert conditions associated with the
        given policy_id.

        This API endpoint returns a paginated list of the alert conditions
        associated with your New Relic account. Alert conditions can be filtered
        by their name, list of IDs, type (application, key_transaction, or
        server) or whether or not policies are archived (defaults to filtering
        archived policies).

        :type policy_id: int
        :param policy_id: Alert policy id

        :type page: int
        :param page: Pagination index

        :rtype: dict
        :return: The JSON response of the API, with an additional 'pages' key
            if there are paginated results

        ::

            {
                ""conditions"": [
                    {
                        ""id"": ""integer"",
                        ""type"": ""string"",
                        ""condition_scope"":  ""string"",
                        ""name"": ""string"",
                        ""enabled"": ""boolean"",
                        ""entities"": [
                            ""integer""
                        ],
                        ""metric"": ""string"",
                        ""runbook_url"": ""string"",
                        ""terms"": [
                            {
                                ""duration"": ""string"",
                                ""operator"": ""string"",
                                ""priority"": ""string"",
                                ""threshold"": ""string"",
                                ""time_function"": ""string""
                            }
                        ],
                        ""user_defined"": {
                            ""metric"": ""string"",
                            ""value_function"": ""string""
                        }
                    }
                ]
            }",0,1,0,1
"def listMethods(self, pid, sdefpid=None):
        
        
        

        

        uri = 'objects/%(pid)s/methods' % {'pid': pid}
        if sdefpid:
            uri += '/' + sdefpid
        return self.get(uri, params=self.format_xml)","List available service methods.

        :param pid: object pid
        :param sDefPid: service definition pid
        :rtype: :class:`requests.models.Response`",0,1,1,2
"def list_all_categories(cls, **kwargs):
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async'):
            return cls._list_all_categories_with_http_info(**kwargs)
        else:
            (data) = cls._list_all_categories_with_http_info(**kwargs)
            return data","List Categories

        Return a list of Categories
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async=True
        >>> thread = api.list_all_categories(async=True)
        >>> result = thread.get()

        :param async bool
        :param int page: page number
        :param int size: page size
        :param str sort: page order
        :return: page[Category]
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def list_all_promotions(cls, **kwargs):
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async'):
            return cls._list_all_promotions_with_http_info(**kwargs)
        else:
            (data) = cls._list_all_promotions_with_http_info(**kwargs)
            return data","List Promotions

        Return a list of Promotions
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async=True
        >>> thread = api.list_all_promotions(async=True)
        >>> result = thread.get()

        :param async bool
        :param int page: page number
        :param int size: page size
        :param str sort: page order
        :return: page[Promotion]
                 If the method is called asynchronously,
                 returns the request thread.",0,2,1,3
"def list_all_store_credit_payments(cls, **kwargs):
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async'):
            return cls._list_all_store_credit_payments_with_http_info(**kwargs)
        else:
            (data) = cls._list_all_store_credit_payments_with_http_info(**kwargs)
            return data","List StoreCreditPayments

        Return a list of StoreCreditPayments
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async=True
        >>> thread = api.list_all_store_credit_payments(async=True)
        >>> result = thread.get()

        :param async bool
        :param int page: page number
        :param int size: page size
        :param str sort: page order
        :return: page[StoreCreditPayment]
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def list_consumers(self, publisher_id=None):
        
        query_parameters = {}
        if publisher_id is not None:
            query_parameters['publisherId'] = self._serialize.query('publisher_id', publisher_id, 'str')
        response = self._send(http_method='GET',
                              location_id='4301c514-5f34-4f5d-a145-f0ea7b5b7d19',
                              version='5.0',
                              query_parameters=query_parameters)
        return self._deserialize('[Consumer]', self._unwrap_collection(response))","ListConsumers.
        Get a list of available service hook consumer services. Optionally filter by consumers that support at least one event type from the specific publisher.
        :param str publisher_id:
        :rtype: [Consumer]",0,1,1,2
"def list_containers(self):
    
    results = []
    for image in self._bucket.list_blobs():
        if image.metadata is not None:
            if ""type"" in image.metadata:
                if image.metadata['type'] == ""container"":
                    results.append(image)

    if len(results) == 0:
        bot.info(""No containers found, based on metadata type:container"")

    return results","return a list of containers, determined by finding the metadata field
       ""type"" with value ""container."" We alert the user to no containers 
       if results is empty, and exit

       {'metadata': {'items': 
                              [
                               {'key': 'type', 'value': 'container'}, ... 
                              ]
                    }
       }",0,0,3,3
"def list_datasets(self, get_global_public):
        
        appending = """"
        if get_global_public:
            appending = ""public""
        url = self.url() + ""/resource/{}dataset/"".format(appending)
        req = self.remote_utils.get_url(url)

        if req.status_code is not 200:
            raise RemoteDataNotFoundError('Could not find {}'.format(req.text))
        else:
            return req.json()","Lists datasets in resources. Setting 'get_global_public' to 'True'
        will retrieve all public datasets in cloud. 'False' will get user's
        public datasets.

        Arguments:
            get_global_public (bool): True if user wants all public datasets in
                                      cloud. False if user wants only their
                                      public datasets.

        Returns:
            dict: Returns datasets in JSON format",1,1,1,3
"def list_insights_components(access_token, subscription_id, resource_group):
    
    endpoint = ''.join([get_rm_endpoint(),
                        '/subscriptions/', subscription_id,
                        '/resourceGroups/', resource_group,
                        '/providers/microsoft.insights/',
                        '/components?api-version=', INSIGHTS_COMPONENTS_API])
    return do_get(endpoint, access_token)","List the Microsoft Insights components in a resource group.

    Args:
        access_token (str): A valid Azure authentication token.
        subscription_id (str): Azure subscription id.
        resource_group (str): Azure resource group name.

    Returns:
        HTTP response. JSON body of components.",0,1,0,1
"def list_namespaced_event(self, namespace, **kwargs):
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.list_namespaced_event_with_http_info(namespace, **kwargs)
        else:
            (data) = self.list_namespaced_event_with_http_info(namespace, **kwargs)
            return data","list or watch objects of kind Event
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.list_namespaced_event(namespace, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \""next key\"".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.
        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.
        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.
        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.
        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.
        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.
        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.
        :return: V1EventList
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def list_of_various(self, tup_tree, acceptable):
        

        result = []

        for child in kids(tup_tree):
            if name(child) not in acceptable:
                raise CIMXMLParseError(
                    _format(""Element {0!A} has invalid child element {1!A} ""
                            ""(allowed are child elements {2!A})"",
                            name(tup_tree), name(child), acceptable),
                    conn_id=self.conn_id)
            result.append(self.parse_any(child))

        return result","Parse zero or more of a list/tuple of elements from the child nodes.

        Each element of the list can be any type from the list of acceptable
        nodes.",1,0,2,3
"def list_questions_in_quiz_or_submission(self, quiz_id, course_id, quiz_submission_attempt=None, quiz_submission_id=None):
        
        path = {}
        data = {}
        params = {}

        
        
        path[""course_id""] = course_id

        
        
        path[""quiz_id""] = quiz_id

        
        
        if quiz_submission_id is not None:
            params[""quiz_submission_id""] = quiz_submission_id

        
        
        if quiz_submission_attempt is not None:
            params[""quiz_submission_attempt""] = quiz_submission_attempt

        self.logger.debug(""GET /api/v1/courses/{course_id}/quizzes/{quiz_id}/questions with query params: {params} and form data: {data}"".format(params=params, data=data, **path))
        return self.generic_request(""GET"", ""/api/v1/courses/{course_id}/quizzes/{quiz_id}/questions"".format(**path), data=data, params=params, all_pages=True)","List questions in a quiz or a submission.

        Returns the list of QuizQuestions in this quiz.",0,2,1,3
"def list_queues(self, prefix=None, num_results=None, include_metadata=False,
                    marker=None, timeout=None):
        
        include = 'metadata' if include_metadata else None
        operation_context = _OperationContext(location_lock=True)
        kwargs = {'prefix': prefix, 'max_results': num_results, 'include': include,
                  'marker': marker, 'timeout': timeout, '_context': operation_context}
        resp = self._list_queues(**kwargs)

        return ListGenerator(resp, self._list_queues, (), kwargs)","Returns a generator to list the queues. The generator will lazily follow 
        the continuation tokens returned by the service and stop when all queues 
        have been returned or num_results is reached.

        If num_results is specified and the account has more than that number of 
        queues, the generator will have a populated next_marker field once it 
        finishes. This marker can be used to create a new generator if more 
        results are desired.

        :param str prefix:
            Filters the results to return only queues with names that begin
            with the specified prefix.
        :param int num_results:
            The maximum number of queues to return.
        :param bool include_metadata:
            Specifies that container metadata be returned in the response.
        :param str marker:
            An opaque continuation token. This value can be retrieved from the 
            next_marker field of a previous generator object if num_results was 
            specified and that generator has finished enumerating results. If 
            specified, this generator will begin returning results from the point 
            where the previous generator stopped.
        :param int timeout:
            The server timeout, expressed in seconds. This function may make multiple 
            calls to the service in which case the timeout value specified will be 
            applied to each individual call.",0,1,1,2
"def list_relations(self):
        
        _ = self._execute('select * from relations').fetchall()
        for i in _:
            
            src, name, dst = i
            src = self.deserialize(
                next(self._execute('select code from objects where id=?',(src,)))[0]
            )
            dst = self.deserialize(
                next(self._execute('select code from objects where id=?',(dst,)))[0]
            )
            yield src, name, dst","list every relation in the database as (src, relation, dst)",1,0,1,2
"def list_roles(self):
    
        

        title = '%s.list_certificates' % self.__class__.__name__

    
        self.printer('Querying AWS for iam roles.')
        try:
            response = self.connection.list_roles()
        except:
            raise AWSConnectionError(title)

    
        role_list = []
        if 'Roles' in response.keys():
            for role in response['Roles']:
                role_list.append(role['RoleName'])

    
        if role_list:
            print_out = 'Found iam roles'
            if len(role_list) > 1:
                print_out += 's'
            from labpack.parsing.grammar import join_words
            print_out += ' %s.' % join_words(role_list)
            self.printer(print_out)
        else:
            self.printer('No iam roles found.')

        self.role_list = role_list
        
        return self.role_list","a method to retrieve a list of server certificates

        :return: list with certificate name strings",1,1,2,4
"def list_runner_book(self, market_id, selection_id, handicap=None, price_projection=None, order_projection=None,
                         match_projection=None, include_overall_position=None, partition_matched_by_strategy_ref=None,
                         customer_strategy_refs=None, currency_code=None, matched_since=None, bet_ids=None, locale=None,
                         session=None, lightweight=None):
        
        params = clean_locals(locals())
        method = '%s%s' % (self.URI, 'listRunnerBook')
        (response, elapsed_time) = self.request(method, params, session)
        return self.process_response(response, resources.MarketBook, elapsed_time, lightweight)","Returns a list of dynamic data about a market and a specified runner. 
        Dynamic data includes prices, the status of the market, the status of selections, 
        the traded volume, and the status of any orders you have placed in the market

        :param unicode market_id: The unique id for the market
        :param int selection_id: The unique id for the selection in the market
        :param double handicap: The projection of price data you want to receive in the response
        :param dict price_projection: The projection of price data you want to receive in the response
        :param str order_projection: The orders you want to receive in the response
        :param str match_projection: If you ask for orders, specifies the representation of matches
        :param bool include_overall_position: If you ask for orders, returns matches for each selection
        :param bool partition_matched_by_strategy_ref: If you ask for orders, returns the breakdown of matches
        by strategy for each selection
        :param list customer_strategy_refs: If you ask for orders, restricts the results to orders matching
        any of the specified set of customer defined strategies
        :param str currency_code: A Betfair standard currency code
        :param str matched_since: If you ask for orders, restricts the results to orders that have at
        least one fragment matched since the specified date
        :param list bet_ids: If you ask for orders, restricts the results to orders with the specified bet IDs
        :param str locale: The language used for the response
        :param requests.session session: Requests session object
        :param bool lightweight: If True will return dict not a resource

        :rtype: list[resources.MarketBook]",0,1,1,2
"def list_storage_accounts_sub(access_token, subscription_id):
    
    endpoint = ''.join([get_rm_endpoint(),
                        '/subscriptions/', subscription_id,
                        '/providers/Microsoft.Storage/storageAccounts',
                        '?api-version=', STORAGE_API])
    return do_get(endpoint, access_token)","List the storage accounts in the specified subscription.

    Args:
        access_token (str): A valid Azure authentication token.
        subscription_id (str): Azure subscription id.

    Returns:
        HTTP response. JSON body list of storage accounts.",0,1,0,1
"def list_tickets(self, **kwargs):
        

        filter_name = 'new_and_my_open'
        if 'filter_name' in kwargs:
            filter_name = kwargs['filter_name']
            del kwargs['filter_name']

        url = 'tickets'
        if filter_name is not None:
            url += '?filter=%s&' % filter_name
        else:
            url += '?'
        page = 1 if not 'page' in kwargs else kwargs['page']
        per_page = 100 if not 'per_page' in kwargs else kwargs['per_page']
        tickets = []

        
        
        while True:
            this_page = self._api._get(url + 'page=%d&per_page=%d'
                                       % (page, per_page), kwargs)
            tickets += this_page
            if len(this_page) < per_page or 'page' in kwargs:
                break
            page += 1

        return [Ticket(**t) for t in tickets]","List all tickets, optionally filtered by a view. Specify filters as
        keyword arguments, such as:

        filter_name = one of ['new_and_my_open', 'watching', 'spam', 'deleted',
                              None]
            (defaults to 'new_and_my_open')
            Passing None means that no named filter will be passed to
            Freshdesk, which mimics the behavior of the 'all_tickets' filter
            in v1 of the API.

        Multiple filters are AND'd together.",0,1,2,3
"def listen_on_socket(self, socket_name=None):
        
        def connection_cb(pipe_connection):
            
            
            with context():
                connection = ServerConnection(self, pipe_connection)

            self.connections.append(connection)

        self.socket_name = bind_and_listen_on_socket(socket_name, connection_cb)

        



        logger.info('Listening on %r.' % self.socket_name)
        return self.socket_name","Listen for clients on a Unix socket.
        Returns the socket name.",0,2,0,2
"def listify(args):
    
    if args:
        if isinstance(args, list):
            return args
        elif isinstance(args, (set, tuple, GeneratorType,
                               range, past.builtins.xrange)):
            return list(args)
        return [args]
    return []","Return args as a list.

    If already a list - return as is.

    >>> listify([1, 2, 3])
    [1, 2, 3]

    If a set - return as a list.

    >>> listify(set([1, 2, 3]))
    [1, 2, 3]

    If a tuple - return as a list.

    >>> listify(tuple([1, 2, 3]))
    [1, 2, 3]

    If a generator (also range / xrange) - return as a list.

    >>> listify(x + 1 for x in range(3))
    [1, 2, 3]
    >>> from past.builtins import xrange
    >>> from builtins import range
    >>> listify(xrange(1, 4))
    [1, 2, 3]
    >>> listify(range(1, 4))
    [1, 2, 3]

    If a single instance of something that isn't any of the above - put as a
    single element of the returned list.

    >>> listify(1)
    [1]

    If ""empty"" (None or False or '' or anything else that evaluates to False),
    return an empty list ([]).

    >>> listify(None)
    []
    >>> listify(False)
    []
    >>> listify('')
    []
    >>> listify(0)
    []
    >>> listify([])
    []",0,0,1,1
"def lldp(interface='', **kwargs):  

    

    proxy_output = salt.utils.napalm.call(
        napalm_device,  
        'get_lldp_neighbors_detail',
        **{
        }
    )

    if not proxy_output.get('result'):
        return proxy_output

    lldp_neighbors = proxy_output.get('out')

    if interface:
        lldp_neighbors = {interface: lldp_neighbors.get(interface)}

    proxy_output.update({
        'out': lldp_neighbors
    })

    return proxy_output","Returns a detailed view of the LLDP neighbors.

    :param interface: interface name to filter on

    :return:          A dictionary with the LLDL neighbors. The keys are the
        interfaces with LLDP activated on.

    CLI Example:

    .. code-block:: bash

        salt '*' net.lldp
        salt '*' net.lldp interface='TenGigE0/0/0/8'

    Example output:

    .. code-block:: python

        {
            'TenGigE0/0/0/8': [
                {
                    'parent_interface': 'Bundle-Ether8',
                    'interface_description': 'TenGigE0/0/0/8',
                    'remote_chassis_id': '8c60.4f69.e96c',
                    'remote_system_name': 'switch',
                    'remote_port': 'Eth2/2/1',
                    'remote_port_description': 'Ethernet2/2/1',
                    'remote_system_description': 'Cisco Nexus Operating System (NX-OS) Software 7.1(0)N1(1a)
                          TAC support: http://www.cisco.com/tac
                          Copyright (c) 2002-2015, Cisco Systems, Inc. All rights reserved.',
                    'remote_system_capab': 'B, R',
                    'remote_system_enable_capab': 'B'
                }
            ]
        }",0,1,1,2
"def lml(self):
        
        terms = self._terms
        yKiy = terms[""yKiy""]
        mKiy = terms[""mKiy""]
        mKim = terms[""mKim""]

        lml = -self._df * log2pi + self._logdet_MM - self._logdetK
        lml -= self._logdetH
        lml += -yKiy - mKim + 2 * mKiy

        return lml / 2","Log of the marginal likelihood.

        Let 𝐲 = vec(Y), M = A⊗X, and H = MᵀK⁻¹M. The restricted log of the marginal
        likelihood is given by [R07]_::

            2⋅log(p(𝐲)) = -(n⋅p - c⋅p) log(2π) + log(｜MᵀM｜) - log(｜K｜) - log(｜H｜)
                - (𝐲-𝐦)ᵀ K⁻¹ (𝐲-𝐦),

        where 𝐦 = M𝛃 for 𝛃 = H⁻¹MᵀK⁻¹𝐲.

        For implementation purpose, let X = (L₀ ⊗ G) and R = (L₁ ⊗ I)(L₁ ⊗ I)ᵀ.
        The covariance can be written as::

            K = XXᵀ + R.

        From the Woodbury matrix identity, we have

            𝐲ᵀK⁻¹𝐲 = 𝐲ᵀR⁻¹𝐲 - 𝐲ᵀR⁻¹XZ⁻¹XᵀR⁻¹𝐲,

        where Z = I + XᵀR⁻¹X. Note that R⁻¹ = (U₁S₁⁻¹U₁ᵀ) ⊗ I and ::

            XᵀR⁻¹𝐲 = (L₀ᵀW ⊗ Gᵀ)𝐲 = vec(GᵀYWL₀),

        where W = U₁S₁⁻¹U₁ᵀ. The term GᵀY can be calculated only once and it will form a
        r×p matrix. We similarly have ::

            XᵀR⁻¹M = (L₀ᵀWA) ⊗ (GᵀX),

        for which GᵀX is pre-computed.

        The log-determinant of the covariance matrix is given by

            log(｜K｜) = log(｜Z｜) - log(｜R⁻¹｜) = log(｜Z｜) - 2·n·log(｜U₁S₁⁻½｜).

        The log of the marginal likelihood can be rewritten as::

            2⋅log(p(𝐲)) = -(n⋅p - c⋅p) log(2π) + log(｜MᵀM｜)
            - log(｜Z｜) + 2·n·log(｜U₁S₁⁻½｜)
            - log(｜MᵀR⁻¹M - MᵀR⁻¹XZ⁻¹XᵀR⁻¹M｜)
            - 𝐲ᵀR⁻¹𝐲 + (𝐲ᵀR⁻¹X)Z⁻¹(XᵀR⁻¹𝐲)
            - 𝐦ᵀR⁻¹𝐦 + (𝐦ᵀR⁻¹X)Z⁻¹(XᵀR⁻¹𝐦)
            + 2𝐲ᵀR⁻¹𝐦 - 2(𝐲ᵀR⁻¹X)Z⁻¹(XᵀR⁻¹𝐦).

        Returns
        -------
        lml : float
            Log of the marginal likelihood.

        References
        ----------
        .. [R07] LaMotte, L. R. (2007). A direct derivation of the REML likelihood
           function. Statistical Papers, 48(2), 321-327.",0,0,1,1
"def load(cls, folder):
        
        popset = PopulationSet.load_hdf(os.path.join(folder,'popset.h5'))
        sigfile = os.path.join(folder,'trsig.pkl')
        with open(sigfile, 'rb') as f:
            trsig = pickle.load(f)
        return cls(trsig, popset, folder=folder)","Loads PopulationSet from folder

        ``popset.h5`` and ``trsig.pkl`` must exist in folder.

        :param folder:
            Folder from which to load.",1,0,0,1
"def load(file_object, *transformers, **kwargs):
    
    
    ignore_remaining_data = kwargs.get(""ignore_remaining_data"", False)

    marshaller = JavaObjectUnmarshaller(
        file_object, kwargs.get(""use_numpy_arrays"", False)
    )

    
    for transformer in transformers:
        marshaller.add_transformer(transformer)
    marshaller.add_transformer(DefaultObjectTransformer())

    
    return marshaller.readObject(ignore_remaining_data=ignore_remaining_data)","Deserializes Java primitive data and objects serialized using
    ObjectOutputStream from a file-like object.

    :param file_object: A file-like object
    :param transformers: Custom transformers to use
    :param ignore_remaining_data: If True, don't log an error when unused
                                  trailing bytes are remaining
    :return: The deserialized object",0,0,1,1
"def load(obj, env=None, silent=True, key=None, filename=None):
    
    if yaml is None:  
        BaseLoader.warn_not_installed(obj, ""yaml"")
        return

    
    
    
    yaml_reader = getattr(yaml, obj.get(""YAML_LOADER_FOR_DYNACONF""), yaml.load)
    if yaml_reader.__name__ == ""unsafe_load"":  
        warn(
            ""yaml.unsafe_load is deprecated.""
            "" Please read https://msg.pyyaml.org/load for full details.""
            "" Try to use full_load or safe_load.""
        )

    loader = BaseLoader(
        obj=obj,
        env=env,
        identifier=""yaml"",
        extensions=YAML_EXTENSIONS,
        file_reader=yaml_reader,
        string_reader=yaml_reader,
    )
    loader.load(filename=filename, key=key, silent=silent)","Reads and loads in to ""obj"" a single key or all keys from source file.

    :param obj: the settings instance
    :param env: settings current env default='development'
    :param silent: if errors should raise
    :param key: if defined load a single key, else load all in env
    :param filename: Optional custom filename to load
    :return: None",2,0,0,2
"def load(self, container_name, slot, label=None, share=False):
        
        if share:
            raise NotImplementedError(""Sharing not supported"")

        try:
            name = self.LW_TRANSLATION[container_name]
        except KeyError:
            if container_name in self.LW_NO_EQUIVALENT:
                raise NotImplementedError(""Labware {} is not supported""
                                          .format(container_name))
            elif container_name in ('magdeck', 'tempdeck'):
                raise NotImplementedError(""Module load not yet implemented"")
            else:
                name = container_name

        return self._ctx.load_labware_by_name(name, slot, label)","Load a piece of labware by specifying its name and position.

        This method calls :py:meth:`.ProtocolContext.load_labware_by_name`;
        see that documentation for more information on arguments and return
        values. Calls to this function should be replaced with calls to
        :py:meth:`.Protocolcontext.load_labware_by_name`.

        In addition, this function contains translations between old
        labware names and new labware names.",3,0,4,7
"def load(self, filename=None, refresh=False):
        
        filename = filename or self.data_file()
        dirname = os.path.dirname(filename)

        if refresh is False:
            try:
                data = None
                with open(filename) as fp:
                    data = json.load(fp)
                self.clear()
                self.update(data)
                return
            except (ValueError, IOError):
                
                pass

        data = self.refresh()
        self.clear()
        self.update(data)

        if not os.path.isdir(dirname):
            os.makedirs(dirname)
        with open(filename, 'w') as fp:
            json.dump(data, fp,
                      sort_keys=True,
                      indent=2,
                      separators=(',', ': '))","Try to load the data from a pre existing data file if it exists.
        If the data file does not exist, refresh the data and save it in
        the data file for future use.
        The data file is a json file.

        :param filename: The filename to save or fetch the data from.
        :param refresh:  Whether to force refresh the data or not",1,0,3,4
"def load_act_node(self) -> ActNode:
        
        act_nodes = ActNode.build_body(self.node.body)

        if not act_nodes:
            raise ValidationError(self.first_line_no, self.node.col_offset, 'AAA01 no Act block found in test')

        
        
        for a_n in act_nodes[1:]:
            if a_n.block_type in [ActNodeType.marked_act, ActNodeType.result_assignment]:
                raise ValidationError(
                    self.first_line_no,
                    self.node.col_offset,
                    'AAA02 multiple Act blocks found in test',
                )

        return act_nodes[0]","Raises:
            ValidationError: AAA01 when no act block is found and AAA02 when
                multiple act blocks are found.",2,0,3,5
"def load_backends(config, callback, internal_attributes):
    
    backend_modules = _load_plugins(config.get(""CUSTOM_PLUGIN_MODULE_PATHS""), config[""BACKEND_MODULES""], backend_filter,
                                    config[""BASE""], internal_attributes, callback)
    logger.info(""Setup backends: %s"" % [backend.name for backend in backend_modules])
    return backend_modules","Load all backend modules specified in the config

    :type config: satosa.satosa_config.SATOSAConfig
    :type callback:
    (satosa.context.Context, satosa.internal.InternalData) -> satosa.response.Response
    :type internal_attributes: dict[string, dict[str, str | list[str]]]
    :rtype: Sequence[satosa.backends.base.BackendModule]

    :param config: The configuration of the satosa proxy
    :param callback: Function that will be called by the backend after the authentication is done.ç
    :return: A list of backend modules",0,1,1,2
"def load_candidate_config(self, filename=None, config=None):
        
        configuration = ''

        if filename is None:
            configuration = config
        else:
            with open(filename) as f:
                configuration = f.read()

        rpc_command = '<CLI><Configuration>{configuration}</Configuration></CLI>'.format(
            configuration=escape_xml(configuration)  
        )

        try:
            self._execute_rpc(rpc_command)
        except InvalidInputError as e:
            self.discard_config()
            raise InvalidInputError(e.args[0], self)","Load candidate confguration.

        Populate the attribute candidate_config with the desired
        configuration and loads it into the router. You can populate it from
        a file or from a string. If you send both a filename and a string
        containing the configuration, the file takes precedence.

        :param filename:  Path to the file containing the desired
                          configuration. By default is None.
        :param config:    String containing the desired configuration.",2,0,3,5
"def load_context(context, file_path=None):
    
    if not file_path:
        file_path = _get_context_filepath()
    if os.path.exists(file_path):
        with io.open(file_path, encoding='utf-8') as f:
            for line in f:
                execute(line, context)",Load a Context object in place from user data directory.,1,0,0,1
"def load_credentials(flnm):
    
    flnm = os.path.expanduser(os.path.expandvars(flnm))
    with open(flnm, 'r') as fl: dat = fl.read(1024 * 8)
    
    try:              return str_to_credentials(dat)
    except Exception: raise ValueError('File %s does not contain a valid credential string' % flnm)","load_credentials(filename) yields the credentials stored in the given file as a tuple
      (key, secret). The file must contain <key>:<secret> on a single line. If the file does not
      contain valid credentials, then an exception is raised. Yields (key, secret).

    Optionally, if the file contains exactly two lines, then the key is taken as the first line and
    the secret is taken as the second line. This can be used if ':' is a valid character in either
    the key or secret for a particular service.

    Note that if the key/secret are more than 8kb in size, this function's behaviour is undefined.",2,0,0,2
"def load_file_obj(self,
                      file_obj,
                      key,
                      bucket_name=None,
                      replace=False,
                      encrypt=False):
        
        if not bucket_name:
            (bucket_name, key) = self.parse_s3_url(key)

        if not replace and self.check_for_key(key, bucket_name):
            raise ValueError(""The key {key} already exists."".format(key=key))

        extra_args = {}
        if encrypt:
            extra_args['ServerSideEncryption'] = ""AES256""

        client = self.get_conn()
        client.upload_fileobj(file_obj, bucket_name, key, ExtraArgs=extra_args)","Loads a file object to S3

        :param file_obj: The file-like object to set as the content for the S3 key.
        :type file_obj: file-like object
        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which to store the file
        :type bucket_name: str
        :param replace: A flag that indicates whether to overwrite the key
            if it already exists.
        :type replace: bool
        :param encrypt: If True, S3 encrypts the file on the server,
            and the file is stored in encrypted form at rest in S3.
        :type encrypt: bool",2,0,2,4
"def load_fileobj(fileobj, gz = None, xmldoc = None, contenthandler = None):
        
        fileobj = MD5File(fileobj)
        md5obj = fileobj.md5obj
        if gz or gz is None:
                fileobj = RewindableInputFile(fileobj)
                magic = fileobj.read(2)
                fileobj.seek(0, os.SEEK_SET)
                if gz or magic == '\037\213':
                        fileobj = gzip.GzipFile(mode = ""rb"", fileobj = fileobj)
        if xmldoc is None:
                xmldoc = ligolw.Document()
        ligolw.make_parser(contenthandler(xmldoc)).parse(fileobj)
        return xmldoc, md5obj.hexdigest()","Parse the contents of the file object fileobj, and return the
        contents as a LIGO Light Weight document tree.  The file object
        does not need to be seekable.

        If the gz parameter is None (the default) then gzip compressed data
        will be automatically detected and decompressed, otherwise
        decompression can be forced on or off by setting gz to True or
        False respectively.

        If the optional xmldoc argument is provided and not None, the
        parsed XML tree will be appended to that document, otherwise a new
        document will be created.  The return value is a tuple, the first
        element of the tuple is the XML document and the second is a string
        containing the MD5 digest in hex digits of the bytestream that was
        parsed.

        Example:

        >>> from pycbc_glue.ligolw import ligolw
        >>> import StringIO
        >>> f = StringIO.StringIO('<?xml version=""1.0"" encoding=""utf-8"" ?><!DOCTYPE LIGO_LW SYSTEM ""http://ldas-sw.ligo.caltech.edu/doc/ligolwAPI/html/ligolw_dtd.txt""><LIGO_LW><Table Name=""demo:table""><Column Name=""name"" Type=""lstring""/><Column Name=""value"" Type=""real8""/><Stream Name=""demo:table"" Type=""Local"" Delimiter="","">""mass"",0.5,""velocity"",34</Stream></Table></LIGO_LW>')
        >>> xmldoc, digest = load_fileobj(f, contenthandler = ligolw.LIGOLWContentHandler)
        >>> digest
        '6bdcc4726b892aad913531684024ed8e'

        The contenthandler argument specifies the SAX content handler to
        use when parsing the document.  The contenthandler is a required
        argument.  See the pycbc_glue.ligolw package documentation for typical
        parsing scenario involving a custom content handler.  See
        pycbc_glue.ligolw.ligolw.PartialLIGOLWContentHandler and
        pycbc_glue.ligolw.ligolw.FilteringLIGOLWContentHandler for examples of
        custom content handlers used to load subsets of documents into
        memory.",1,0,4,5
"def load_from_json(question_json):
    
    data = json.loads(question_json)
    if isinstance(data, list):
        return load_from_list(data)
    if isinstance(data, dict):
        return load_from_dict(data)
    raise TypeError(
        'Json contained a %s variable when a dict or list was expected',
        type(data))","Load Questions from a JSON string.
    :return: A list of Question objects with associated data if the JSON
             contains a list or a Question if the JSON contains a dict.
    :return type: List or Dict",1,0,2,3
"def load_json_file(file, decoder=None):
    
    if decoder is None:
        decoder = DateTimeDecoder
    if not hasattr(file, ""read""):
        with io.open(file, ""r"", encoding=""utf-8"") as f:
            return json.load(f, object_hook=decoder.decode)
    return json.load(file, object_hook=decoder.decode)","Load data from json file

    :param file: Readable object or path to file
    :type file: FileIO | str
    :param decoder: Use custom json decoder
    :type decoder: T <= DateTimeDecoder
    :return: Json data
    :rtype: None | int | float | str | list | dict",1,0,0,1
"def load_labeled_events(filename, delimiter=r'\s+'):
    r
    
    events, labels = load_delimited(filename, [float, str], delimiter)
    events = np.array(events)
    
    try:
        util.validate_events(events)
    except ValueError as error:
        warnings.warn(error.args[0])

    return events, labels","r""""""Import labeled time-stamp events from an annotation file.  The file should
    consist of two columns; the first having numeric values corresponding to
    the event times and the second having string labels for each event.  This
    is primarily useful for processing labeled events which lack duration, such
    as beats with metric beat number or onsets with an instrument label.

    Parameters
    ----------
    filename : str
        Path to the annotation file
    delimiter : str
        Separator regular expression.
        By default, lines will be split by any amount of whitespace.

    Returns
    -------
    event_times : np.ndarray
        array of event times (float)
    labels : list of str
        list of labels",2,0,0,2
"def load_map(map, src_file, output_dir, scale=1, cache_dir=None, datasources_cfg=None, user_styles=[], verbose=False):
    
    scheme, n, path, p, q, f = urlparse(src_file)
    
    if scheme in ('file', ''):
        assert exists(src_file), ""We'd prefer an input file that exists to one that doesn't""
    
    if cache_dir is None:
        cache_dir = expanduser(CACHE_DIR)
        
        
        if not isdir(cache_dir):
            mkdir(cache_dir)
            chmod(cache_dir, 0755)

    dirs = Directories(output_dir, realpath(cache_dir), dirname(src_file))
    compile(src_file, dirs, verbose, datasources_cfg=datasources_cfg, user_styles=user_styles, scale=scale).to_mapnik(map, dirs)","Apply a stylesheet source file to a given mapnik Map instance, like mapnik.load_map().
    
        Parameters:
        
          map:
            Instance of mapnik.Map.
        
          src_file:
            Location of stylesheet .mml file. Can be relative path, absolute path,
            or fully-qualified URL of a remote stylesheet.
        
          output_dir:
            ...
        
        Keyword Parameters:
        
          scale:
            Optional scale value for output map, 2 doubles the size for high-res displays.
        
          cache_dir:
            ...
        
          datasources_cfg:
            ...
        
          user_styles:
            A optional list of files or URLs, that override styles defined in
            the map source. These are evaluated in order, with declarations from
            later styles overriding those from earlier styles.
        
          verbose:
            ...",2,0,0,2
"def load_message(self, message, **kwargs):
        
        kwargs.update(message)
        message = kwargs
        try:
            message = Message(**message)
        except TypeError as e:
            message = self._convert_postmark_to_native(kwargs)
            if message:
                message = Message(**message)
            else:
                raise e
        return message","Create a :class:`Message` from a message data `dict`.

        :param message: A `dict` of message data.
        :param \*\*kwargs: Additional keyword arguments to construct
            :class:`Message` with.
        :rtype: :class:`Message`",1,0,2,3
"def load_plugin_by_name(name):
    
    plugins = load(PLUGIN_NAMESPACE)
    full_name = ""%s.%s"" % (PLUGIN_NAMESPACE, name)
    try:
        plugins = (plugin for plugin in plugins if plugin.__name__ == full_name)
        plugin = next(plugins)
        return plugin
    except StopIteration:
        raise UnknownPlugin([plugin.__name__.split('.').pop() for plugin in plugins])","Load the plugin with the specified name.
    
    >>> plugin = load_plugin_by_name('default')
    >>> api = dir(plugin)
    >>> 'build_package' in api
    True
    >>> 'get_version' in api
    True
    >>> 'set_package_version' in api
    True
    >>> 'set_version' in api
    True",1,1,0,2
"def load_post(self, wp_post_id):
        
        path = ""sites/{}/posts/{}"".format(self.site_id, wp_post_id)
        response = self.get(path)

        if response.ok and response.text:

            api_post = response.json()

            self.get_ref_data_map(bulk_mode=False)
            self.load_wp_post(api_post, bulk_mode=False)

            
            try:
                post = Post.objects.get(site_id=self.site_id, wp_id=wp_post_id)
            except Exception as ex:
                logger.exception(""Unable to load post with wp_post_id={}:\n{}"".format(wp_post_id, ex.message))
            else:
                return post
        else:
            logger.warning(""Unable to load post with wp_post_id={}:\n{}"".format(wp_post_id, response.text))","Refresh local content for a single post from the the WordPress REST API.
        This can be called from a webhook on the WordPress side when a post is updated.

        :param wp_post_id: the wordpress post ID
        :return: the fully loaded local post object",0,3,1,4
"def load_re_from_file(filepath):
    
    regexp = None
    with open(filepath,'r') as mlfile:
        flagstr = """"
        for line in mlfile:
            cleanline = re.sub(""//.*$"", """", line)
            if re.search(""^\s*$"", cleanline):
                continue
            if re.search (""^
                flagstr = cleanline[1:]
                continue
            if regexp is not None:
                raise Exception(""Regular expression file format error"")
            else:
                regexp = cleanline.rstrip('\n')
    flags = 0
    if ""i"" in flagstr:
        flags |= re.I
    from pydsl.grammar.definition import RegularExpression
    return RegularExpression(regexp, flags)",Converts a re file to Regular Grammar instance,1,0,2,3
"def load_site(self, purge_first=False, full=False, modified_after=None, type=None, status=None, batch_size=None):
        
        
        self.purge_first = purge_first
        self.full = full
        self.modified_after = modified_after
        self.batch_size = batch_size or 100

        if type is None:
            type = ""all""

        if status is None:
            status = ""publish""

        if type in [""all"", ""ref_data""]:
            self.load_categories()
            self.load_tags()
            self.load_authors()
            self.load_media()

        
        if type in [""all"", ""attachment"", ""post"", ""page""]:
            self.get_ref_data_map()

        
        if type == ""all"":
            for post_type in [""attachment"", ""post"", ""page""]:
                self.load_posts(post_type=post_type, status=status)
        elif type in [""attachment"", ""post"", ""page""]:
            self.load_posts(post_type=type, status=status)","Sync content from a WordPress.com site via the REST API.

        :param purge_first: Should we remove all local content first? Careful, destructive!
        :param full: If True, crawl backwards chronologically through all content, not just recently modified
                     Default is False, only load recently modified content.
        :param modified_after: If None, pick up where we left off last time; otherwise go back to this point in time
                               Default is None, pick up where we left off last time.
        :param type: the type(s) of processing:
            - all: loads all content
            - ref_data: just loads categories, tags, authors, and media
            - post: just loads posts with post_type=post, and related ref data
            - page: just loads posts with post_type=page, and related ref data
            - attachment: just loads posts with post_type=attachment, and related ref data
        :param status: the post statuses to load:
            - publish: loads published posts (default)
            - private: loads private posts
            - draft: loads draft posts
            - pending: loads pending posts
            - future: loads future posts
            - trash: loads posts in the trash
            - any: loads posts with any status
        :param batch_size: The number of posts to request from the WP API for each page
                           Note this doesn't apply to smaller requests such as tags, categories, etc.
        :return: None",0,2,1,3
"def loads(data, validate=False, **kwargs):
    
    d = json.loads(data, **kwargs)
    content_spec = d[""content-spec""]
    Payload = CONTENT_SPECS[content_spec]
    payload = Payload.load(d)
    if validate:
        errors = payload.problems()
        if errors:
            raise ValidationError(errors)
    return payload","Load a PPMP message from the JSON-formatted string in `data`. When
    `validate` is set, raise `ValidationError`. Additional keyword
    arguments are the same that are accepted by `json.loads`,
    e.g. `indent` to get a pretty output.",1,0,2,3
"def loads_config(s,
                 parser_params=JSONParserParams(),
                 string_to_scalar_converter=DefaultStringToScalarConverter()):
    
    parser = JSONParser(parser_params)
    object_builder_params = ConfigObjectBuilderParams(string_to_scalar_converter=string_to_scalar_converter)
    listener = ObjectBuilderParserListener(object_builder_params)
    parser.parse(s, listener)
    return listener.result","Works similar to the loads() function but this one returns a json object hierarchy
    that wraps all json objects, arrays and scalars to provide a nice config query syntax.
    For example:
    my_config = loads_config(json_string)
    ip_address = my_config.servers.reverse_proxy.ip_address()
    port = my_config.servers.reverse_proxy.port(80)

    Note that the raw unwrapped values can be fetched with the __call__ operator.
    This operator has the following signature: __call__(default=None, mapper=None).
    Fetching a value without specifying a default value means that the value is required
    and it has to be in the config. If it isn't there then a JSONConfigValueNotFoundError
    is raised. The optional mapper parameter can be a function that receives the unwrapped
    value and it can return something that may be based on the input parameter. You can
    also use this mapper parameter to pass a function that performs checking on the
    value and raises an exception (eg. ValueError) on error.

    If you specify a default value and the required config value is not present then
    default is returned. In this case mapper isn't called with the default value.",0,0,1,1
"def local_independencies(self, variables):
        
        independencies = Independencies()
        for variable in [variables] if isinstance(variables, str) else variables:
            if variable != self.parent_node:
                independencies.add_assertions(
                    [variable, list(
                        set(self.children_nodes) - set(variable)), self.parent_node])
        return independencies","Returns an instance of Independencies containing the local independencies
        of each of the variables.


        Parameters
        ----------
        variables: str or array like
            variables whose local independencies are to found.

        Examples
        --------
        >>> from pgmpy.models import NaiveBayes
        >>> model = NaiveBayes()
        >>> model.add_edges_from([('a', 'b'), ('a', 'c'), ('a', 'd')])
        >>> ind = model.local_independencies('b')
        >>> ind
        (b _|_ d, c | a)",0,0,1,1
"def log(self, message, severity=INFO, tag=u""""):
        
        entry = _LogEntry(
            severity=severity,
            time=datetime.datetime.now(),
            tag=tag,
            indentation=self.indentation,
            message=self._sanitize(message)
        )
        self.entries.append(entry)
        if self.tee:
            gf.safe_print(entry.pretty_print(show_datetime=self.tee_show_datetime))
        return entry.time","Add a given message to the log, and return its time.

        :param string message: the message to be added
        :param severity: the severity of the message
        :type  severity: :class:`~aeneas.logger.Logger`
        :param string tag: the tag associated with the message;
                           usually, the name of the class generating the entry
        :rtype: datetime",0,1,1,2
"def log(self, source: str, target: str, flags: int, msg: str, mtype: str) -> None:
        
        entry = Log(source=str(source), target=target, flags=flags, msg=msg, type=mtype, time=datetime.now())
        with self.session_scope() as session:
            session.add(entry)
            session.flush()","Logs a message to the database.

        | source: The source of the message.
        | target: The target of the message.
        | flags: Is the user a operator or voiced?
        | msg: The text of the message.
        | msg: The type of message.
        | time: The current time (Unix Epoch).",1,0,0,1
"def log_game_start(self, players, terrain, numbers, ports):
        
        self.reset()
        self._set_players(players)
        self._logln('{} v{}'.format(__name__, __version__))
        self._logln('timestamp: {0}'.format(self.timestamp_str()))
        self._log_players(players)
        self._log_board_terrain(terrain)
        self._log_board_numbers(numbers)
        self._log_board_ports(ports)
        self._logln('...CATAN!')","Begin a game.

        Erase the log, set the timestamp, set the players, and write the log header.

        The robber is assumed to start on the desert (or off-board).

        :param players: iterable of catan.game.Player objects
        :param terrain: list of 19 catan.board.Terrain objects.
        :param numbers: list of 19 catan.board.HexNumber objects.
        :param ports: list of catan.board.Port objects.",0,0,4,4
"def log_to_file(filename, level=DEBUG):
    
    l = logging.getLogger(""paramiko"")
    if len(l.handlers) > 0:
        return
    l.setLevel(level)
    f = open(filename, 'w')
    lh = logging.StreamHandler(f)
    lh.setFormatter(logging.Formatter('%(levelname)-.3s [%(asctime)s.%(msecs)03d] thr=%(_threadid)-3d %(name)s: %(message)s',
                                      '%Y%m%d-%H:%M:%S'))
    l.addHandler(lh)","send paramiko logs to a logfile, if they're not already going somewhere",1,0,1,2
"def login(request):
    
    if request.method == 'GET':
        return render(request, 'user_login.html', {}, help_text=login.__doc__)
    elif request.method == 'POST':
        credentials = json_body(request.body.decode(""utf-8""))
        user = auth.authenticate(
            username=credentials.get('username', ''),
            password=credentials.get('password', ''),
        )
        if user is None:
            return render_json(request, {
                'error': _('Password or username does not match.'),
                'error_type': 'password_username_not_match'
            }, template='user_json.html', status=401)
        if not user.is_active:
            return render_json(request, {
                'error': _('The account has not been activated.'),
                'error_type': 'account_not_activated'
            }, template='user_json.html', status=401)
        auth.login(request, user)
        request.method = ""GET""
        return profile(request)
    else:
        return HttpResponseBadRequest(""method %s is not allowed"".format(request.method))","Log in

    GET parameters:
        html
            turn on the HTML version of the API

    POST parameters (JSON):
        username:
            user's name
        password:
            user's password",5,0,1,6
"def login(self, data, api_version=""v2.0""):
        

        cur_ctlr = self._parent_class.controller

        url = str(cur_ctlr) + ""/{}/api/login"".format(api_version)

        api_logger.debug(""URL = %s"", url)
        return self._parent_class.rest_call(url, ""post"", data=data, sensitive=True)","Login api

          **Parameters:**:

          - **data**: Dictionary containing data to POST as JSON
          - **api_version**: API version to use (default v2.0)

        **Returns:** requests.Response object extended with cgx_status and cgx_content properties.",0,1,0,1
"def login_handler(self, config=None, prefix=None, **args):
        
        headers = {}
        headers['Access-control-allow-origin'] = '*'
        headers['Content-type'] = 'text/html'
        auth = request.authorization
        if (auth and auth.username == auth.password):
            return self.set_cookie_close_window_response(
                ""valid-http-basic-login"")
        else:
            headers['WWW-Authenticate'] = (
                'Basic realm=""HTTP-Basic-Auth at %s (u=p to login)""' %
                (self.name))
            return make_response("""", 401, headers)","HTTP Basic login handler.

        Respond with 401 and WWW-Authenticate header if there are no
        credentials or bad credentials. If there are credentials then
        simply check for username equal to password for validity.",0,0,3,3
"def lookup(self, token):
    
    
    try:
      key, envservice = token.split("","")
    except ValueError:
      return None
    
    try:
      env, service = envservice.split(""/"")
    except ValueError as e:
      raise RuntimeError(""Request:{} can't resolve to env, service. {}"".format(envservice, e.message))

    return self._s3_get(env, service, key)","Return key version if found, None otherwise
    Lookup should look like this:
      pattern:
      <key>,<env>/<service>
      example:
      ami-id,staging/core",1,1,1,3
"def lookupSpatialReferenceID(wellKnownText):
    
    payload = {'mode': 'wkt',
               'terms': wellKnownText}

    try:
        r = requests.get('http://prj2epsg.org/search.json', params=payload)
    except requests.exceptions.ConnectionError:
        print(""SRID Lookup Error: Could not automatically determine spatial ""
              ""reference ID, because there is no internet connection. ""
              ""Please check connection and try again."")
        exit(1)

    if r.status_code == 200:
        json = r.json()

        for code in json['codes']:
            return code['code']","This function can be used to look up the EPSG spatial reference system using the web service available at:
    http://prj2epsg.org

    Args:
        wellKnownText (str): The Well Known Text definition of the spatial reference system.

    Returns:
        int: Spatial Reference ID",1,1,0,2
"def lookup_prefix(self, prefix, timestamp=timestamp_now):
        

        prefix = prefix.strip().upper()

        if self._lookuptype == ""clublogxml"" or self._lookuptype == ""countryfile"":

            return self._check_data_for_date(prefix, timestamp, self._prefixes, self._prefixes_index)

        elif self._lookuptype == ""redis"":

            data_dict, index = self._get_dicts_from_redis(""_prefix_"", ""_prefix_index_"", self._redis_prefix, prefix)
            return self._check_data_for_date(prefix, timestamp, data_dict, index)

        
        raise KeyError","Returns lookup data of a Prefix

        Args:
            prefix (string): Prefix of a Amateur Radio callsign
            timestamp (datetime, optional): datetime in UTC (tzinfo=pytz.UTC)

        Returns:
            dict: Dictionary containing the country specific data of the Prefix

        Raises:
            KeyError: No matching Prefix found
            APIKeyMissingError: API Key for Clublog missing or incorrect

        Example:
           The following code shows how to obtain the information for the prefix ""DH"" from the countryfile.com
           database (default database).

           >>> from pyhamtools import LookupLib
           >>> myLookupLib = LookupLib()
           >>> print myLookupLib.lookup_prefix(""DH"")
           {
            'adif': 230,
            'country': u'Fed. Rep. of Germany',
            'longitude': 10.0,
            'cqz': 14,
            'ituz': 28,
            'latitude': 51.0,
            'continent': u'EU'
           }

        Note:
            This method is available for

            - clublogxml
            - countryfile
            - redis",1,2,2,5
"def lookup_subjects(self, subject_lookup):
        
        content = self._serialize.body(subject_lookup, 'GraphSubjectLookup')
        response = self._send(http_method='POST',
                              location_id='4dd4d168-11f2-48c4-83e8-756fa0de027c',
                              version='5.1-preview.1',
                              content=content)
        return self._deserialize('{GraphSubject}', self._unwrap_collection(response))","LookupSubjects.
        [Preview API] Resolve descriptors to users, groups or scopes (Subjects) in a batch.
        :param :class:`<GraphSubjectLookup> <azure.devops.v5_1.graph.models.GraphSubjectLookup>` subject_lookup: A list of descriptors that specifies a subset of subjects to retrieve. Each descriptor uniquely identifies the subject across all instance scopes, but only at a single point in time.
        :rtype: {GraphSubject}",0,1,1,2
"def mac(self, data, uid=None, algorithm=None):
        
        
        if not isinstance(data, six.binary_type):
            raise TypeError(""data must be bytes"")
        if uid is not None:
            if not isinstance(uid, six.string_types):
                raise TypeError(""uid must be a string"")
        if algorithm is not None:
            if not isinstance(algorithm, enums.CryptographicAlgorithm):
                raise TypeError(
                    ""algorithm must be a CryptographicAlgorithm enumeration"")

        parameters_attribute = self._build_cryptographic_parameters(
            {'cryptographic_algorithm': algorithm}
        )

        
        result = self.proxy.mac(data, uid, parameters_attribute)

        status = result.result_status.value
        if status == enums.ResultStatus.SUCCESS:
            uid = result.uuid.value
            mac_data = result.mac_data.value
            return uid, mac_data
        else:
            reason = result.result_reason.value
            message = result.result_message.value
            raise exceptions.KmipOperationFailure(status, reason, message)","Get the message authentication code for data.

        Args:
            data (string): The data to be MACed.
            uid (string): The unique ID of the managed object that is the key
                to use for the MAC operation.
            algorithm (CryptographicAlgorithm): An enumeration defining the
                algorithm to use to generate the MAC.

        Returns:
            string: The unique ID of the managed object that is the key
                to use for the MAC operation.
            string: The data MACed

        Raises:
            ClientConnectionNotOpen: if the client connection is unusable
            KmipOperationFailure: if the operation result is a failure
            TypeError: if the input arguments are invalid",4,0,5,9
"def mag_cal_report_encode(self, compass_id, cal_mask, cal_status, autosaved, fitness, ofs_x, ofs_y, ofs_z, diag_x, diag_y, diag_z, offdiag_x, offdiag_y, offdiag_z):
                
                return MAVLink_mag_cal_report_message(compass_id, cal_mask, cal_status, autosaved, fitness, ofs_x, ofs_y, ofs_z, diag_x, diag_y, diag_z, offdiag_x, offdiag_y, offdiag_z)","Reports results of completed compass calibration. Sent until
                MAG_CAL_ACK received.

                compass_id                : Compass being calibrated (uint8_t)
                cal_mask                  : Bitmask of compasses being calibrated (uint8_t)
                cal_status                : Status (see MAG_CAL_STATUS enum) (uint8_t)
                autosaved                 : 0=requires a MAV_CMD_DO_ACCEPT_MAG_CAL, 1=saved to parameters (uint8_t)
                fitness                   : RMS milligauss residuals (float)
                ofs_x                     : X offset (float)
                ofs_y                     : Y offset (float)
                ofs_z                     : Z offset (float)
                diag_x                    : X diagonal (matrix 11) (float)
                diag_y                    : Y diagonal (matrix 22) (float)
                diag_z                    : Z diagonal (matrix 33) (float)
                offdiag_x                 : X off-diagonal (matrix 12 and 21) (float)
                offdiag_y                 : Y off-diagonal (matrix 13 and 31) (float)
                offdiag_z                 : Z off-diagonal (matrix 32 and 23) (float)",0,1,0,1
"def main(reactor):
    
    control_ep = UNIXClientEndpoint(reactor, '/var/run/tor/control')
    tor = yield txtorcon.connect(reactor, control_ep)
    state = yield tor.create_state()
    print(""Closing all circuits:"")
    for circuit in list(state.circuits.values()):
        path = '->'.join(map(lambda r: r.id_hex, circuit.path))
        print(""Circuit {} through {}"".format(circuit.id, path))
        for stream in circuit.streams:
            print(""  Stream {} to {}"".format(stream.id, stream.target_host))
            yield stream.close()
            print(""  closed"")
        yield circuit.close()
        print(""closed"")
    yield tor.quit()",Close all open streams and circuits in the Tor we connect to,0,0,2,2
"def makeService(opt):
    
    ret = get(config=opt['config'], messages=opt['messages'],
              pidDir=opt['pid'], freq=opt['frequency'])
    pm = ret.getServiceNamed(""procmon"")
    pm.threshold = opt[""threshold""]
    pm.killTime = opt[""killtime""]
    pm.minRestartDelay = opt[""minrestartdelay""]
    pm.maxRestartDelay = opt[""maxrestartdelay""]
    return ret","Return a service based on parsed command-line options

    :param opt: dict-like object. Relevant keys are config, messages,
                pid, frequency, threshold, killtime, minrestartdelay
                and maxrestartdelay
    :returns: service, {twisted.application.interfaces.IService}",0,1,1,2
"def make_all_day(self):
        
        if self.all_day:
            
            return

        begin_day = self.begin.floor('day')
        end_day = self.end.floor('day')

        self._begin = begin_day

        
        if begin_day == end_day:
            self._end_time = None
        else:
            self._end_time = end_day + timedelta(days=1)

        self._duration = None
        self._begin_precision = 'day'","Transforms self to an all-day event.

        The event will span all the days from the begin to the end day.",0,0,1,1
"def make_api_call(self, method, url, json_params=None):
        

        url = self.BRANCH_BASE_URI+url

        if self.verbose is True:
            print(""Making web request: {}"".format(url))

        if json_params is not None:
            encoded_params = json.dumps(json_params)
            headers = {'Content-Type': 'application/json'}
        else:
            encoded_params = None
            headers = {}

        if encoded_params is not None and self.verbose is True:
            print(""Params: {}"".format(encoded_params))

        request = Request(url, encoded_params.encode('utf-8'), headers)
        request.get_method = lambda: method
        response = urlopen(request).read()
        return json.loads(response.decode('utf-8'))","Accesses the branch API
        :param method: The HTTP method
        :param url: The URL
        :param json_params: JSON parameters
        :return: The parsed response",0,1,1,2
"def make_checkerboard_mask(column_distance, row_distance, column_offset=0, row_offset=0, default=0, value=1):
    
    col_shape = (336,)
    col = np.full(col_shape, fill_value=default, dtype=np.uint8)
    col[::row_distance] = value
    shape = (80, 336)
    chessboard_mask = np.full(shape, fill_value=default, dtype=np.uint8)
    chessboard_mask[column_offset::column_distance * 2] = np.roll(col, row_offset)
    chessboard_mask[column_distance + column_offset::column_distance * 2] = np.roll(col, row_distance / 2 + row_offset)
    return chessboard_mask","Generate chessboard/checkerboard mask.

    Parameters
    ----------
    column_distance : int
        Column distance of the enabled pixels.
    row_distance : int
        Row distance of the enabled pixels.
    column_offset : int
        Additional column offset which shifts the columns by the given amount.
    column_offset : int
        Additional row offset which shifts the rows by the given amount.

    Returns
    -------
    ndarray
        Chessboard mask.

    Example
    -------
    Input:
    column_distance : 6
    row_distance : 2

    Output:
    [[1 0 0 0 0 0 1 0 0 0 ... 0 0 0 0 1 0 0 0 0 0]
     [0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0]
     [0 0 0 1 0 0 0 0 0 1 ... 0 1 0 0 0 0 0 1 0 0]
     ...
     [0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0]
     [0 0 0 1 0 0 0 0 0 1 ... 0 1 0 0 0 0 0 1 0 0]
     [0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0]]",0,0,1,1
"def make_config(path):
    
    apps, user = load_legacy_config()

    apps = {a.instance: a._asdict() for a in apps}
    users = {user_id(user): user._asdict()} if user else {}
    active_user = user_id(user) if user else None

    config = {
        ""apps"": apps,
        ""users"": users,
        ""active_user"": active_user,
    }

    print_out(""Creating config file at <blue>{}</blue>"".format(path))

    
    os.makedirs(dirname(path), exist_ok=True)

    with open(path, 'w') as f:
        json.dump(config, f, indent=True)","Creates a config file.

    Attempts to load data from legacy config files if they exist.",2,0,1,3
"def make_counts(
    n_samples=1000,
    n_features=100,
    n_informative=2,
    scale=1.0,
    chunks=100,
    random_state=None,
):
    
    rng = dask_ml.utils.check_random_state(random_state)

    X = rng.normal(0, 1, size=(n_samples, n_features), chunks=(chunks, n_features))
    informative_idx = rng.choice(n_features, n_informative, chunks=n_informative)
    beta = (rng.random(n_features, chunks=n_features) - 1) * scale

    informative_idx, beta = dask.compute(informative_idx, beta)

    z0 = X[:, informative_idx].dot(beta[informative_idx])
    rate = da.exp(z0)
    y = rng.poisson(rate, size=1, chunks=(chunks,))
    return X, y","Generate a dummy dataset for modeling count data.

    Parameters
    ----------
    n_samples : int
        number of rows in the output array
    n_features : int
        number of columns (features) in the output array
    n_informative : int
        number of features that are correlated with the outcome
    scale : float
        Scale the true coefficient array by this
    chunks : int
        Number of rows per dask array block.
    random_state : int, RandomState instance or None (default)
        Determines random number generation for dataset creation. Pass an int
        for reproducible output across multiple function calls.
        See :term:`Glossary <random_state>`.

    Returns
    -------
    X : dask.array, size ``(n_samples, n_features)``
    y : dask.array, size ``(n_samples,)``
        array of non-negative integer-valued data

    Examples
    --------
    >>> X, y = make_counts()",0,0,1,1
"def make_dynamical_potential_func(kBT_Gamma, density, SpringPotnlFunc):
    
    def PotentialFunc(xdata, Radius):
        
        mass = ((4/3)*pi*((Radius*10**-9)**3))*density
        yfit=(kBT_Gamma/mass)
        Y = yfit*SpringPotnlFunc(xdata)
        return Y
    return PotentialFunc","Creates the function that calculates the potential given
    the position (in volts) and the radius of the particle. 

    Parameters
    ----------
    kBT_Gamma : float
        Value of kB*T/Gamma
    density : float
        density of the nanoparticle
    SpringPotnlFunc : function
        Function which takes the value of position (in volts)
        and returns the spring potential
    
    Returns
    -------
    PotentialFunc : function
        function that calculates the potential given
        the position (in volts) and the radius of the 
        particle.",0,0,2,2
"def make_eventrule(date_rule, time_rule, cal, half_days=True):
    
    _check_if_not_called(date_rule)
    _check_if_not_called(time_rule)

    if half_days:
        inner_rule = date_rule & time_rule
    else:
        inner_rule = date_rule & time_rule & NotHalfDay()

    opd = OncePerDay(rule=inner_rule)
    
    opd.cal = cal
    return opd",Constructs an event rule from the factory api.,0,0,1,1
"def make_linked_folder(sym_path):
    
    source_path = path_for(sym_path)
    if os.path.exists(source_path) and not os.path.exists(sym_path):
        logger.debug(""Source {} exists but target {} does not. Removing source."".format(source_path, sym_path))
        shutil.rmtree(source_path)
    try:
        logger.debug(""Making source {}"".format(source_path))
        os.mkdir(source_path)
        logger.debug(""Success"")
    except FileExistsError as e:
        logger.info(""Source already existed"")
        logger.debug(e)
    try:
        logger.debug(""Making linking from source {} to sym {}"".format(source_path, sym_path))
        os.symlink(source_path, sym_path)
        logger.debug(""Success"")
    except FileExistsError as e:
        logger.debug(""Sym already existed"")
        logger.debug(e)
    return source_path","Create a folder in the ~/.autolens directory and create a sym link to it at the provided path.

    If both folders already exist then nothing is changed. If the source folder exists but the destination folder does
    not then the source folder is removed and replaced so as to conform to the behaviour that the user would expect
    should they delete the sym linked folder.

    Parameters
    ----------
    sym_path: str
        The path where multinest output is apparently saved

    Returns
    -------
    actual_path: str
        The path where multinest output is actually saved",2,5,3,10
"def make_mask(self, clean=True, dry=False):
        

        cc = self.conconf
        
        mask = np.ones(self.rec_cnt) == True  
        for cond in cc.conditions_list('cond'):
            try:
                mask = mask & self._mask_array(cond)
            except Exception:
                print cond
                print 'produced an error:'
                raise           

        mask = mask & datautils.startstop_bool(self)

        samplerate = cc.get_condition('samplerate')
        if samplerate is not None:
            samplerate = float(samplerate)
        mask = datautils.duration_bool(mask, cc.get_condition('duration'),
                                       samplerate)

        if dry:
            return
        if not clean and self.mask is not None:
            self.mask = self.mask & mask
        else:
            self.mask = mask","Set the attribute self.mask to a mask based on
        the conditions.

        clean: bool
            If not True, let the current mask be a condition as well. If
            True, the mask is set solely on the pack's current
            conditions

        dry: bool
            If True, only try to make a mask, but don't touch self.mask

        This method is called automatically unless ``no_auto`` is set to
        True, whenever conditions are updated.

        .. seealso::
           :meth:`~channelpack.ChannelPack.pprint_conditions`",0,0,4,4
"def make_ocs_request(self, method, service, action, **kwargs):
        

        accepted_codes = kwargs.pop('accepted_codes', [100])

        res = self._make_ocs_request(method, service, action, **kwargs)
        if res.status_code == 200:
            tree = ET.fromstring(res.content)
            self._check_ocs_status(tree, accepted_codes=accepted_codes)
            return res

        raise OCSResponseError(res)","Makes a OCS API request and analyses the response

        :param method: HTTP method
        :param service: service name
        :param action: action path
        :param \*\*kwargs: optional arguments that ``requests.Request.request`` accepts
        :returns :class:`requests.Response` instance",0,1,1,2
"def make_refresh_on_demand_service(injector_component):
        
        LOGGER.debug(""InjectorCachedComponentService.make_refresh_on_demand_service"")
        args = {
            'service_q': injector_component.id,
            'treatment_callback': injector_component.refresh,
            'service_name': injector_component.id + "" - On Demand Refreshing Service""
        }
        return InjectorCachedComponentService.driver.make_service(args)","create a refresh on demand service listening to refresh order on the component admin queue
        :param injector_component: the injector_component to bind with the new refresh on demande service
        :return: the created service",0,2,0,2
"def make_request(self, image, *features):
        
        return {
            ""image"": {
                ""content"": self.image_to_base64(image)
            },
            ""features"": [{
                             ""type"": feature.type_,
                             ""maxResults"": feature.max_results
                         } for feature in features]
        }","Makes single image request
        :param image: One of file object, path, or URL
        :param features: Recognition features
        :return:",0,0,1,1
"def make_table(self,
                   tablename: str,
                   fieldspeclist: FIELDSPECLIST_TYPE,
                   dynamic: bool = False,
                   compressed: bool = False) -> Optional[int]:
        
        if self.table_exists(tablename):
            log.info(""Skipping creation of table "" + tablename +
                     "" (already exists)"")
            return None
        if not self.is_mysql():
            dynamic = False
            compressed = False
        
        sql = .format(
            tablename=tablename,
            fieldspecs=self.fielddefsql_from_fieldspeclist(fieldspeclist),
            dynamic=""ROW_FORMAT=DYNAMIC"" if dynamic else """",
            compressed=""ROW_FORMAT=COMPRESSED"" if compressed else """",
        )
        log.info(""Creating table "" + tablename)
        return self.db_exec_literal(sql)","Makes a table, if it doesn't already exist.",1,2,0,3
"def make_update(cls, table, set_query, where=None):
        

        validate_table_name(table)
        if typepy.is_null_string(set_query):
            raise ValueError(""SET query is null"")

        query_list = [""UPDATE {:s}"".format(Table(table)), ""SET {:s}"".format(set_query)]
        if where and isinstance(where, (six.text_type, Where, And, Or)):
            query_list.append(""WHERE {:s}"".format(where))

        return "" "".join(query_list)","Make UPDATE query.

        :param str table: Table name of executing the query.
        :param str set_query: SET part of the UPDATE query.
        :param str where:
            Add a WHERE clause to execute query,
            if the value is not |None|.
        :return: Query of SQLite.
        :rtype: str
        :raises ValueError: If ``set_query`` is empty string.
        :raises simplesqlite.NameValidationError:
            |raises_validate_table_name|",2,0,1,3
"def make_urls_hyperlinks(text: str) -> str:
    
    find_url = r
    replace_url = r'<a href=""\1"">\1</a>'
    find_email = re.compile(r'([.\w\-]+@(\w[\w\-]+\.)+[\w\-]+)')
    
    
    replace_email = r'<a href=""mailto:\1"">\1</a>'
    text = re.sub(find_url, replace_url, text)
    text = re.sub(find_email, replace_email, text)
    return text","Adds hyperlinks to text that appears to contain URLs.

    See

    - http://stackoverflow.com/questions/1071191

      - ... except that double-replaces everything; e.g. try with
        ``text = ""me@somewhere.com me@somewhere.com""``

    - http://stackp.online.fr/?p=19",0,0,1,1
"def make_xpath_ranges(html, phrase):
    
    if not html:
        return []
    if not isinstance(phrase, unicode):
        try:
            phrase = phrase.decode('utf8')
        except:
            logger.info('failed %r.decode(""utf8"")', exc_info=True)
            return []

    phrase_re = re.compile(
        phrase, flags=re.UNICODE | re.IGNORECASE | re.MULTILINE)
    spans = []
    for match in phrase_re.finditer(html, overlapped=False):
        spans.append(match.span())  

    
    
    try:
        xpath_ranges = list(char_offsets_to_xpaths(html, spans))
    except:
        logger.info('failed to get xpaths', exc_info=True)
        return []
    ranges = []
    for xpath_range in filter(None, xpath_ranges):
        ranges.append(dict(
            start=dict(node=xpath_range.start_xpath,
                       idx=xpath_range.start_offset),
            end=dict(node=xpath_range.end_xpath,
                     idx=xpath_range.end_offset)))

    return ranges","Given a HTML string and a `phrase`, build a regex to find offsets
    for the phrase, and then build a list of `XPathRange` objects for
    it.  If this fails, return empty list.",0,1,2,3
"def makeringlatticeCIJ(n, k, seed=None):
    
    rng = get_rng(seed)
    
    CIJ = np.zeros((n, n))
    CIJ1 = np.ones((n, n))
    kk = 0
    count = 0
    seq = range(1, n)
    seq2 = range(n - 1, 0, -1)

    
    while kk < k:
        count += 1
        dCIJ = np.triu(CIJ1, seq[count]) - np.triu(CIJ1, seq[count] + 1)
        dCIJ2 = np.triu(CIJ1, seq2[count]) - np.triu(CIJ1, seq2[count] + 1)
        dCIJ = dCIJ + dCIJ.T + dCIJ2 + dCIJ2.T
        CIJ += dCIJ
        kk = int(np.sum(CIJ))

    
    overby = kk - k
    if overby:
        i, j = np.where(dCIJ)
        rp = rng.permutation(np.size(i))
        for ii in range(overby):
            CIJ[i[rp[ii]], j[rp[ii]]] = 0

    return CIJ","This function generates a directed lattice network with toroidal
    boundary counditions (i.e. with ring-like ""wrapping around"").

    Parameters
    ----------
    N : int
        number of vertices
    K : int
        number of edges
    seed : hashable, optional
        If None (default), use the np.random's global random state to generate random numbers.
        Otherwise, use a new np.random.RandomState instance seeded with the given value.

    Returns
    -------
    CIJ : NxN np.ndarray
        connection matrix

    Notes
    -----
    The lattice is made by placing connections as close as possible
    to the main diagonal, with wrapping around. No connections are made
    on the main diagonal. In/Outdegree is kept approx. constant at K/N.",0,0,1,1
"def makesvg(self, right_text, status=None, left_text=None,
                left_color=None, config=None):
        
        right_color = config['color_scheme'].get(status, ""

        left_text = left_text or config['left_text']
        left_color = left_color or config['left_color']

        left = {
            ""color"": left_color,
            ""text"": left_text,
            ""width"": self.textwidth(left_text, config)
        }
        right = {
            ""color"": right_color,
            ""text"": right_text,
            ""width"": self.textwidth(right_text, config)
        }

        template = self.env.get_template(config['template_name'].format(**config))
        return template.render(left=left, right=right, config=config)","Renders an SVG from the template, using the specified data",1,0,0,1
"def map(self, func, *iterables, **kwargs):
        
        with self._lock:
            if self._closing:
                raise RuntimeError('pool is closing/closed')
            timeout = kwargs.pop('timeout', None)
            futures = []
            for args in zip(*iterables):
                result = Future()
                self._queue.put_nowait((func, args, result))
                futures.append(result)
            self._spawn_workers()
        try:
            with switch_back(timeout):
                for future in futures:
                    yield future.result()
        except Exception:
            
            for future in futures:
                if not future.done():
                    future.cancel()
            raise","Apply *func* to the elements of the sequences in *iterables*.

        All invocations of *func* are run in the pool. If multiple iterables
        are provided, then *func* must take this many arguments, and is applied
        with one element from each iterable. All iterables must yield the same
        number of elements.

        An optional *timeout* keyword argument may be provided to specify a
        timeout.

        This returns a generator yielding the results.",0,0,4,4
"def map(self,index_name, index_type, map_value):
        
        request = self.session
        url = 'http://%s:%s/%s/%s/_mapping' % (self.host, self.port, index_name, index_type)
        content = { index_type : { 'properties' : map_value } }
        if self.verbose:
            print content
        response = request.put(url,content)
        return response",Enable a specific map for an index and type,0,1,0,1
"def map_as_series(self, func, value_size=None, dtype=None, chunk_size='auto'):
        
        blocks = self.toblocks(chunk_size=chunk_size)

        if value_size is not None:
            dims = list(blocks.blockshape)
            dims[0] = value_size
        else:
            dims = None

        def f(block):
            return apply_along_axis(func, 0, block)

        return blocks.map(f, value_shape=dims, dtype=dtype).toimages()","Efficiently apply a function to images as series data.

        For images data that represent image sequences, this method
        applies a function to each pixel's series, and then returns to
        the images format, using an efficient intermediate block
        representation.

        Parameters
        ----------
        func : function
            Function to apply to each time series. Should take one-dimensional
            ndarray and return the transformed one-dimensional ndarray.

        value_size : int, optional, default = None
            Size of the one-dimensional ndarray resulting from application of
            func. If not supplied, will be automatically inferred for an extra
            computational cost.

        dtype : str, optional, default = None
            dtype of one-dimensional ndarray resulting from application of func.
            If not supplied it will be automatically inferred for an extra computational cost.

        chunk_size : str or tuple, size of image chunk used during conversion, default = 'auto'
            String interpreted as memory size (in kilobytes, e.g. '64').
            The exception is the string 'auto'. In spark mode, 'auto' will choose a chunk size to make the
            resulting blocks ~100 MB in size. In local mode, 'auto' will create a single block.
            Tuple of ints interpreted as 'pixels per dimension'.",0,0,4,4
"def mask_image(image, mask, level=1, binarize=False):
    
    leveluse = level
    if type(leveluse) is np.ndarray:
        leveluse = level.tolist()
    image_out = image.clone() * 0
    for mylevel in leveluse:
        temp = threshold_image( mask, mylevel, mylevel )
        if binarize:
            image_out = image_out + temp
        else:
            image_out = image_out + temp * image
    return image_out","Mask an input image by a mask image.  If the mask image has multiple labels,
    it is possible to specify which label(s) to mask at.

    ANTsR function: `maskImage`

    Arguments
    ---------
    image : ANTsImage
        Input image.

    mask : ANTsImage
        Mask or label image.

    level : scalar or tuple of scalars
        Level(s) at which to mask image. If vector or list of values, output image is non-zero at all locations where label image matches any of the levels specified.

    binarize : boolean
        whether binarize the output image

    Returns
    -------
    ANTsImage

    Example
    -------
    >>> import ants
    >>> myimage = ants.image_read(ants.get_ants_data('r16'))
    >>> mask = ants.get_mask(myimage)
    >>> myimage_mask = ants.mask_image(myimage, mask, 3)
    >>> seg = ants.kmeans_segmentation(myimage, 3)
    >>> myimage_mask = ants.mask_image(myimage, seg['segmentation'], (1,3))",0,0,2,2
"def master_main(painter, router, select, delay):
    
    next_paint = 0
    while True:
        msg = select.get()
        parse_output(msg.receiver.host, msg.unpickle())
        if next_paint < time.time():
            next_paint = time.time() + delay
            painter.paint()","Loop until CTRL+C is pressed, waiting for the next result delivered by the
    Select. Use parse_output() to turn that result ('ps' command output) into
    rich data, and finally repaint the screen if the repaint delay has passed.",1,0,1,2
"def matches(self, properties):
        
        
        
        generator = (
            criterion.matches(properties) for criterion in self.subfilters
        )

        
        if self.operator == OR:
            result = any(generator)
        else:
            result = all(generator)
            if self.operator == NOT:
                
                return not result

        return result","Tests if the given properties matches this LDAP filter and its children

        :param properties: A dictionary of properties
        :return: True if the properties matches this filter, else False",0,0,1,1
"def maverage(size):
  
  size_inv = 1. / size

  @tostream
  def maverage_filter(sig, zero=0.):
    data = deque((zero * size_inv for _ in xrange(size)), maxlen=size)
    mean_value = zero
    for el in sig:
      mean_value -= data.popleft()
      new_value = el * size_inv
      data.append(new_value)
      mean_value += new_value
      yield mean_value

  return maverage_filter","Moving average

  This is the only strategy that uses a ``collections.deque`` object
  instead of a ZFilter instance. Fast, but without extra capabilites such
  as a frequency response plotting method.

  Parameters
  ----------
  size :
    Data block window size. Should be an integer.

  Returns
  -------
  A callable that accepts two parameters: a signal ``sig`` and the starting
  memory element ``zero`` that behaves like the ``LinearFilter.__call__``
  arguments. The output from that callable is a Stream instance, and has
  no decimation applied.

  See Also
  --------
  envelope :
    Signal envelope (time domain) strategies.",0,0,1,1
"def maxflow(V,M,source,sink):
    
    
    model = Model(""maxflow"")
    
    f = {} 
    for (i,j) in M:
        f[i,j] = model.addVar(lb=-M[i,j], ub=M[i,j], name=""flow(%s,%s)""%(i,j))

    cons = {}
    for i in V:
        if i != source and i != sink:
            cons[i] = model.addCons(
                quicksum(f[i,j] for j in V if i<j and (i,j) in M) - \
                quicksum(f[j,i] for j in V if i>j and (j,i) in M) == 0,
                ""FlowCons(%s)""%i)

    model.setObjective(quicksum(f[i,j] for (i,j) in M if i==source), ""maximize"")

    
    model.data = f,cons
    return model","maxflow: maximize flow from source to sink, taking into account arc capacities M
    Parameters:
        - V: set of vertices
        - M[i,j]: dictionary or capacity for arcs (i,j)
        - source: flow origin
        - sink: flow target
    Returns a model, ready to be solved.",1,0,1,2
"def maximum_hline_bundle(self, y0, x0, x1):
        
        x_range = range(x0, x1 + 1) if x0 < x1 else range(x0, x1 - 1, -1)
        hlines = [[(x, y0, 0, k) for x in x_range] for k in range(self.L)]
        return list(filter(self._contains_line, hlines))","Compute a maximum set of horizontal lines in the unit cells ``(x,y0)``
        for :math:`x0 \leq x \leq x1`.

        INPUTS:
            y0,x0,x1: int

        OUTPUT:
            list of lists of qubits",0,0,1,1
"def mean_pairwise_similarity(
    collection, metric=sim, mean_func=hmean, symmetric=False
):
    
    if not callable(mean_func):
        raise ValueError('mean_func must be a function')
    if not callable(metric):
        raise ValueError('metric must be a function')

    if hasattr(collection, 'split'):
        collection = collection.split()
    if not hasattr(collection, '__iter__'):
        raise ValueError('collection is neither a string nor iterable type')
    elif len(collection) < 2:
        raise ValueError('collection has fewer than two members')

    collection = list(collection)

    pairwise_values = []

    for i in range(len(collection)):
        for j in range(i + 1, len(collection)):
            pairwise_values.append(metric(collection[i], collection[j]))
            if symmetric:
                pairwise_values.append(metric(collection[j], collection[i]))

    return mean_func(pairwise_values)","Calculate the mean pairwise similarity of a collection of strings.

    Takes the mean of the pairwise similarity between each member of a
    collection, optionally in both directions (for asymmetric similarity
    metrics.

    Parameters
    ----------
    collection : list
        A collection of terms or a string that can be split
    metric : function
        A similarity metric function
    mean_func : function
        A mean function that takes a list of values and returns a float
    symmetric : bool
        Set to True if all pairwise similarities should be calculated in both
        directions

    Returns
    -------
    float
        The mean pairwise similarity of a collection of strings

    Raises
    ------
    ValueError
        mean_func must be a function
    ValueError
        metric must be a function
    ValueError
        collection is neither a string nor iterable type
    ValueError
        collection has fewer than two members

    Examples
    --------
    >>> round(mean_pairwise_similarity(['Christopher', 'Kristof',
    ... 'Christobal']), 12)
    0.519801980198
    >>> round(mean_pairwise_similarity(['Niall', 'Neal', 'Neil']), 12)
    0.545454545455",4,0,6,10
"def media_upload(self, filename, *args, **kwargs):
        
        f = kwargs.pop('file', None)
        headers, post_data = API._pack_image(filename, 4883, form_field='media', f=f)
        kwargs.update({'headers': headers, 'post_data': post_data})

        return bind_api(
            api=self,
            path='/media/upload.json',
            method='POST',
            payload_type='media',
            allowed_param=[],
            require_auth=True,
            upload_api=True
        )(*args, **kwargs)",":reference: https://developer.twitter.com/en/docs/media/upload-media/api-reference/post-media-upload
            :allowed_param:",0,1,0,1
"def merge(*args, **kwargs):
    
    from .table import (is_itable, ITable)
    
    choose_fn = None
    if 'choose' in kwargs:
        choose_fn = kwargs['choose']
    if len(kwargs) > 1 or (len(kwargs) > 0 and 'choose' not in kwargs):
        raise ValueError('Unidentified options given to merge: %s' (kwargs.keys(),))
    
    maps = flatten_maps(*args)
    if len(maps) == 0: return ps.m()
    elif len(maps) == 1: return maps[0]
    coll = collect(maps)
    if choose_fn is None: choose_fn = _choose_last
    def curry_choice(k, args): return lambda:choose_fn(k, args)
    resmap = lazy_map({k:curry_choice(k, v) for (k,v) in six.iteritems(coll)})
    
    if is_itable(maps[0]):
        n = maps[0].row_count
        if all(is_itable(m) and m.row_count == n for m in maps):
            return ITable(resmap, n)
    
    return resmap","merge(...) lazily collapses all arguments, which must be python Mapping objects of some kind,
      into a single mapping from left-to-right. The mapping that is returned is a lazy persistent
      object that does not request the value of a key from any of the maps provided until they are
      requested of it; in this fashion it preserves the laziness of immutable map objects that are
      passed to it. Arguments may be mappings or lists/tuples of mappings.

    If all of the arguments passed to merge are pimms itables with the same row_count, then an
    itable object is returned instead of a lazy map.

    The following options are accepted:
    * choose (default None) specifies a function that chooses from which map, of those maps given
      to merge, the value should be drawn when keys overlap. The function is always passed two
      arguments: the key for which the conflict occurs and a list of maps containing that key; it
      should return the value to which the key should be mapped. The default uses the first map.",1,0,4,5
"def merge(self, commit_message='', sha=None):
        
        parameters = {'commit_message': commit_message}
        if sha:
            parameters['sha'] = sha
        url = self._build_url('merge', base_url=self._api)
        json = self._json(self._put(url, data=dumps(parameters)), 200)
        self.merge_commit_sha = json['sha']
        return json['merged']","Merge this pull request.

        :param str commit_message: (optional), message to be used for the
            merge commit
        :returns: bool",0,1,1,2
"def merge_scrfiles(self, remove_scrfiles=True):
        
        scr_files = list(filter(None, [task.outdir.has_abiext(""SCR"") for task in self]))

        self.history.info(""Will call mrgscr to merge %s SCR files:\n"" % len(scr_files))
        assert len(scr_files) == len(self)

        mrgscr = wrappers.Mrgscr(manager=self[0].manager, verbose=1)
        final_scr = mrgscr.merge_qpoints(self.outdir.path, scr_files, out_prefix=""out"")

        if remove_scrfiles:
            for scr_file in scr_files:
                try:
                    os.remove(scr_file)
                except IOError:
                    pass

        return final_scr","This method is called when all the q-points have been computed.
        It runs `mrgscr` in sequential on the local machine to produce
        the final SCR file in the outdir of the `Work`.
        If remove_scrfiles is True, the partial SCR files are removed after the merge.",0,0,3,3
"def metadata(self, default=True):
        
        
        
        default = 'true' if default else 'false'
        add_url = 'runs?getOnlyDefaultRuns={}&includeMetadata=true'
        url = self.base_url + add_url.format(default)
        headers = {'Authorization': 'Bearer {}'.format(self.auth())}
        r = requests.get(url, headers=headers)
        df = pd.read_json(r.content, orient='records')

        def extract(row):
            return (
                pd.concat([row[['model', 'scenario']],
                           pd.Series(row.metadata)])
                .to_frame()
                .T
                .set_index(['model', 'scenario'])
            )

        return pd.concat([extract(row) for idx, row in df.iterrows()],
                         sort=False).reset_index()","Metadata of scenarios in the connected data source

        Parameter
        ---------
        default : bool, optional, default True
            Return *only* the default version of each Scenario.
            Any (`model`, `scenario`) without a default version is omitted.
            If :obj:`False`, return all versions.",0,1,1,2
"def migrate_data(self, data, from_state):
        
        if not self.default:
            return

        self.default.prepare(data, from_state)
        for instance in data:
            value = self.default.get_default_for(instance, from_state)
            if not value and not self.schema.get('required', True):
                continue

            
            container = getattr(instance, self.schema_type, {})
            dict_dot(container, '.'.join(self.field), value)
            setattr(instance, self.schema_type, container)
            instance.save()","Migrate data objects.

        :param data: Queryset containing all data objects that need
            to be migrated
        :param from_state: Database model state",0,0,1,1
"def migration_creatr(migration_file, create, table):
    
    if not check():
        click.echo(Fore.RED + 'ERROR: Ensure you are in a bast app to run the create:migration command')
        return

    migration = CreateMigration()
    if table is None:
        table = snake_case(migration_file)
    file = migration.create_file(snake_case(migration_file), table=table, create=create)
    click.echo(Fore.GREEN + 'Migration file created at %s' % file)",Name of the migration file,1,0,1,2
"def mkdir(path,
          owner=None,
          grant_perms=None,
          deny_perms=None,
          inheritance=True,
          reset=False):
    
    
    drive = os.path.splitdrive(path)[0]
    if not os.path.isdir(drive):
        raise CommandExecutionError('Drive {0} is not mapped'.format(drive))

    path = os.path.expanduser(path)
    path = os.path.expandvars(path)

    if not os.path.isdir(path):

        try:
            
            os.mkdir(path)

            
            if owner:
                salt.utils.win_dacl.set_owner(obj_name=path, principal=owner)

            
            set_perms(
                path=path,
                grant_perms=grant_perms,
                deny_perms=deny_perms,
                inheritance=inheritance,
                reset=reset)

        except WindowsError as exc:
            raise CommandExecutionError(exc)

    return True","Ensure that the directory is available and permissions are set.

    Args:

        path (str):
            The full path to the directory.

        owner (str):
            The owner of the directory. If not passed, it will be the account
            that created the directory, likely SYSTEM

        grant_perms (dict):
            A dictionary containing the user/group and the basic permissions to
            grant, ie: ``{'user': {'perms': 'basic_permission'}}``. You can also
            set the ``applies_to`` setting here. The default is
            ``this_folder_subfolders_files``. Specify another ``applies_to``
            setting like this:

            .. code-block:: yaml

                {'user': {'perms': 'full_control', 'applies_to': 'this_folder'}}

            To set advanced permissions use a list for the ``perms`` parameter,
            ie:

            .. code-block:: yaml

                {'user': {'perms': ['read_attributes', 'read_ea'], 'applies_to': 'this_folder'}}

        deny_perms (dict):
            A dictionary containing the user/group and permissions to deny along
            with the ``applies_to`` setting. Use the same format used for the
            ``grant_perms`` parameter. Remember, deny permissions supersede
            grant permissions.

        inheritance (bool):
            If True the object will inherit permissions from the parent, if
            ``False``, inheritance will be disabled. Inheritance setting will
            not apply to parent directories if they must be created.

        reset (bool):
            If ``True`` the existing DACL will be cleared and replaced with the
            settings defined in this function. If ``False``, new entries will be
            appended to the existing DACL. Default is ``False``.

            .. versionadded:: 2018.3.0

    Returns:
        bool: True if successful

    Raises:
        CommandExecutionError: If unsuccessful

    CLI Example:

    .. code-block:: bash

        # To grant the 'Users' group 'read & execute' permissions.
        salt '*' file.mkdir C:\\Temp\\ Administrators ""{'Users': {'perms': 'read_execute'}}""

        # Locally using salt call
        salt-call file.mkdir C:\\Temp\\ Administrators ""{'Users': {'perms': 'read_execute', 'applies_to': 'this_folder_only'}}""

        # Specify advanced attributes with a list
        salt '*' file.mkdir C:\\Temp\\ Administrators ""{'jsnuffy': {'perms': ['read_attributes', 'read_ea'], 'applies_to': 'this_folder_only'}}""",3,0,4,7
"def mkdir(path, recursive=True):
    
    if os.path.exists(path):
        return True
    try:
        if recursive:
            os.makedirs(path)
        else:
            os.mkdir(path)
    except OSError as error:
        log.error('mkdir: execute failed: %s (%s)' % (path, error))
        return False
    return True","Create a directory at the specified path. By default this function
    recursively creates the path.

      >>> if mkdir('/tmp/one/two'):
      ...     print('OK')
      OK",1,1,0,2
"def mmapFile(self, addr, size, perms, filename, offset=0):
        
        
        assert addr is None or isinstance(addr, int), 'Address shall be concrete'
        assert size > 0

        self.cpu._publish('will_map_memory', addr, size, perms, filename, offset)

        
        if addr is not None:
            assert addr < self.memory_size, 'Address too big'
            addr = self._floor(addr)

        
        size = self._ceil(size)

        
        addr = self._search(size, addr)

        
        for i in range(self._page(addr), self._page(addr + size)):
            assert i not in self._page2map, 'Map already used'

        
        m = FileMap(addr, size, perms, filename, offset)

        
        self._add(m)

        logger.debug('New file-memory map @%x size:%x', addr, size)
        self.cpu._publish('did_map_memory', addr, size, perms, filename, offset, addr)
        return addr","Creates a new file mapping in the memory address space.

        :param addr: the starting address (took as hint). If C{addr} is C{0} the first big enough
                     chunk of memory will be selected as starting address.
        :param size: the contents of a file mapping are initialized using C{size} bytes starting
                     at offset C{offset} in the file C{filename}.
        :param perms: the access permissions to this memory.
        :param filename: the pathname to the file to map.
        :param offset: the contents of a file mapping are initialized using C{size} bytes starting
                      at offset C{offset} in the file C{filename}.
        :return: the starting address where the file was mapped.
        :rtype: int
        :raises error:
                   - 'Address shall be concrete' if C{addr} is not an integer number.
                   - 'Address too big' if C{addr} goes beyond the limit of the memory.
                   - 'Map already used' if the piece of memory starting in C{addr} and with length C{size} isn't free.",4,3,1,8
"def mod_git_ignore(directory, ignore_item, action):
    
    if not os.path.isdir(directory):
        return
    ignore_filepath = os.path.join(directory,"".gitignore"")
    if not os.path.exists(ignore_filepath):
        items = []
    else:
        with open(ignore_filepath) as ig_file:
            items = ig_file.readlines()
    
    clean_items  = [line.strip(""\n"").strip() for line in items]
    clean_items = make_list(clean_items)
    if action == ""add"":
        if ignore_item not in clean_items:
            with open(ignore_filepath, ""w"") as ig_file:
                clean_items.append(ignore_item)
                ig_file.write(""\n"".join(clean_items) + ""\n"")
    elif action == ""remove"":
        with open(ignore_filepath, ""w"") as ig_file:
            for i, value in enumerate(clean_items):
                if value != ignore_item.lower():
                    ig_file.write(items[i])","checks if an item is in the specified gitignore file and adds it if it
    is not in the file",2,0,2,4
"def moma(self, wt_fluxes):
        
        reactions = set(self._adjustment_reactions())
        v = self._v

        obj_expr = 0
        for f_reaction, f_value in iteritems(wt_fluxes):
            if f_reaction in reactions:
                
                obj_expr += (f_value - v[f_reaction])**2

        self._prob.set_objective(obj_expr)
        self._solve(lp.ObjectiveSense.Minimize)","Minimize the redistribution of fluxes using Euclidean distance.

        Minimizing the redistribution of fluxes using a quadratic objective
        function. The distance is minimized by minimizing the sum of
        (wild type - knockout)^2.

        Args:
            wt_fluxes: Dictionary of all the wild type fluxes that will be
                used to find a close MOMA solution. Fluxes can be expiremental
                or calculated using :meth: get_fba_flux(objective).",0,0,1,1
"def monitor_key_get(service, key):
    
    try:
        output = check_output(
            ['ceph', '--id', service,
             'config-key', 'get', str(key)]).decode('UTF-8')
        return output
    except CalledProcessError as e:
        log(""Monitor config-key get failed with message: {}"".format(
            e.output))
        return None","Gets the value of an existing key in the monitor cluster.
    :param service: six.string_types. The Ceph user name to run the command under
    :param key: six.string_types.  The key to search for.
    :return: Returns the value of that key or None if not found.",0,2,1,3
"def mouse(table, day=None):
    
    where = ((""day"", day),) if day else ()
    events = db.fetch(table, where=where, order=""day"")
    for e in events: e[""dt""] = datetime.datetime.fromtimestamp(e[""stamp""])
    stats, positions, events = stats_mouse(events, table)
    days, input = db.fetch(""counts"", order=""day"", type=table), ""mouse""
    return bottle.template(""heatmap.tpl"", locals(), conf=conf)",Handler for showing mouse statistics for specified type and day.,2,0,1,3
"def mouseReleaseEvent(self, event):
        
        if event.button() == QtCore.Qt.LeftButton:
            if event.modifiers() == QtCore.Qt.ControlModifier:
                self.goBack()
            else:
                self.goForward()
        
        super(XWalkthroughWidget, self).mouseReleaseEvent(event)","Moves the slide forward when clicked.
        
        :param      event | <QtCore.QMouseEvent>",1,0,1,2
"def move(library, session, source_space, source_offset, source_width, destination_space,
         destination_offset, destination_width, length):
    
    return library.viMove(session, source_space, source_offset, source_width,
                          destination_space, destination_offset,
                          destination_width, length)","Moves a block of data.

    Corresponds to viMove function of the VISA library.

    :param library: the visa library wrapped by ctypes.
    :param session: Unique logical identifier to a session.
    :param source_space: Specifies the address space of the source.
    :param source_offset: Offset of the starting address or register from which to read.
    :param source_width: Specifies the data width of the source.
    :param destination_space: Specifies the address space of the destination.
    :param destination_offset: Offset of the starting address or register to which to write.
    :param destination_width: Specifies the data width of the destination.
    :param length: Number of elements to transfer, where the data width of the elements to transfer
                   is identical to the source data width.
    :return: return value of the library call.
    :rtype: :class:`pyvisa.constants.StatusCode`",0,1,0,1
"def msg_body_for_event(event, context):
    
    
    http_method = event.get('context', {}).get('http-method', None)
    if http_method == 'GET':
        data = event.get('params', {}).get('querystring', {})
    else:  
        data = event.get('body-json', {})
    
    msg_dict = {
        'data': serializable_dict(data),
        'event': serializable_dict(event),
        'context': serializable_dict(vars(context))
    }
    msg = json.dumps(msg_dict, sort_keys=True)
    logger.debug('Message to enqueue: %s', msg)
    return msg","Generate the JSON-serialized message body for an event.

    :param event: Lambda event that triggered the handler
    :type event: dict
    :param context: Lambda function context - see
      http://docs.aws.amazon.com/lambda/latest/dg/python-context-object.html
    :return: JSON-serialized success response
    :rtype: str",0,1,1,2
"def multi_delete(self, path_list, **kwargs):
        

        data = {
            'param': json.dumps({
                'list': [{'path': path} for path in path_list]
            }),
        }
        return self._request('file', 'delete', data=data, **kwargs)","Batch delete files or directories.

.. warning::
* After the file/directory is deleted, it is temporarily stored in the recycle bin by default. The temporary storage of deleted files or directories
does not occupy the user's space quota;
* The storage period is 10 days. It can be restored to the original path within 10 days. After 10 days, it will be permanently deleted.

:param path_list: The path list of files/directories in the network disk. The path must start with /apps/.

.. warning::
* The path length is limited to 1000;
* The path cannot contain the following characters: ``\\\\ ? | "" > < : *``;
* The file name or path name cannot start or end with ``.``
or blank characters. Blank characters include:
``\\r, \\n, \\t, space, \\0, \\x0B``.
:type path_list: list
:return: Response object",0,1,1,2
"def multiplicity(self):
        
        if self._multiplicity is None:
            
            try:
                d_structure = create_saturated_interstitial_structure(self)
            except ValueError:
                logger.debug('WARNING! Multiplicity was not able to be calculated adequately '
                             'for interstitials...setting this to 1 and skipping for now...')
                return 1

            sga = SpacegroupAnalyzer(d_structure)
            periodic_struc = sga.get_symmetrized_structure()
            poss_deflist = sorted(
                periodic_struc.get_sites_in_sphere(self.site.coords, 2, include_index=True),
                key=lambda x: x[1])
            defindex = poss_deflist[0][2]

            equivalent_sites = periodic_struc.find_equivalent_sites(periodic_struc[defindex])
            return len(equivalent_sites)

        else:
            return self._multiplicity",Returns the multiplicity of a defect site within the structure (needed for concentration analysis),0,1,2,3
"def multipublish(self, topic, messages, block=True, timeout=None,
                     raise_error=True):
        
        result = AsyncResult()
        conn = self._get_connection(block=block, timeout=timeout)

        try:
            self._response_queues[conn].append(result)
            conn.multipublish(topic, messages)
        finally:
            self._put_connection(conn)

        if raise_error:
            return result.get()

        return result","Publish an iterable of messages to the given topic.

        :param topic: the topic to publish to

        :param messages: iterable of bytestrings to publish

        :param block: wait for a connection to become available before
            publishing the message. If block is `False` and no connections
            are available, :class:`~gnsq.errors.NSQNoConnections` is raised

        :param timeout: if timeout is a positive number, it blocks at most
            ``timeout`` seconds before raising
            :class:`~gnsq.errors.NSQNoConnections`

        :param raise_error: if ``True``, it blocks until a response is received
            from the nsqd server, and any error response is raised. Otherwise
            an :class:`~gevent.event.AsyncResult` is returned",1,1,1,3
"def my_pick_non_system_keyspace(self):
        
        d = self.my_describe_keyspaces()

        def pick_non_system(klist):
            for k in klist:
                if k.name not in SYSTEM_KEYSPACES:
                    return k.name
            err = NoKeyspacesAvailable(""Can't gather information about the ""
                                       ""Cassandra ring; no non-system ""
                                       ""keyspaces available"")
            warn(err)
            raise err
        d.addCallback(pick_non_system)
        return d","Find a keyspace in the cluster which is not 'system', for the purpose
        of getting a valid ring view. Can't use 'system' or null.",1,0,2,3
"def nack(self, channel_id=None, **kwargs):  
        
        path = ""/event-service/v1/channels/{}/nack"".format(channel_id)
        r = self._httpclient.request(
            method=""POST"",
            url=self.url,
            path=path,
            **kwargs
        )
        return r","Send a negative read-acknowledgement to the service.

        Causes the channel's read point to move to its previous position
        prior to the last poll.

        Args:
            channel_id (str): The channel ID.
            **kwargs: Supported :meth:`~pancloud.httpclient.HTTPClient.request` parameters.

        Returns:
            requests.Response: Requests Response() object.

        Examples:
            Refer to ``event_nack.py`` example.",0,1,0,1
"def nack(self, requeue=True):
        
        if not self._method:
            raise AMQPMessageError(
                'Message.nack only available on incoming messages'
            )
        self._channel.basic.nack(delivery_tag=self.delivery_tag,
                                 requeue=requeue)","Negative Acknowledgement.

        :raises AMQPInvalidArgument: Invalid Parameters
        :raises AMQPChannelError: Raises if the channel encountered an error.
        :raises AMQPConnectionError: Raises if the connection
                                     encountered an error.

        :param bool requeue: Re-queue the message",1,1,0,2
"def naive_aggregation(C):
    
    if not isspmatrix_csr(C):
        raise TypeError('expected csr_matrix')

    if C.shape[0] != C.shape[1]:
        raise ValueError('expected square matrix')

    index_type = C.indptr.dtype
    num_rows = C.shape[0]

    Tj = np.empty(num_rows, dtype=index_type)  
    Cpts = np.empty(num_rows, dtype=index_type)  

    fn = amg_core.naive_aggregation

    num_aggregates = fn(num_rows, C.indptr, C.indices, Tj, Cpts)
    Cpts = Cpts[:num_aggregates]
    Tj = Tj - 1

    if num_aggregates == 0:
        
        return csr_matrix((num_rows, 1), dtype='int8'), Cpts
    else:
        shape = (num_rows, num_aggregates)
        
        Tp = np.arange(num_rows+1, dtype=index_type)
        Tx = np.ones(len(Tj), dtype='int8')
        return csr_matrix((Tx, Tj, Tp), shape=shape), Cpts","Compute the sparsity pattern of the tentative prolongator.

    Parameters
    ----------
    C : csr_matrix
        strength of connection matrix

    Returns
    -------
    AggOp : csr_matrix
        aggregation operator which determines the sparsity pattern
        of the tentative prolongator
    Cpts : array
        array of Cpts, i.e., Cpts[i] = root node of aggregate i

    Examples
    --------
    >>> from scipy.sparse import csr_matrix
    >>> from pyamg.gallery import poisson
    >>> from pyamg.aggregation.aggregate import naive_aggregation
    >>> A = poisson((4,), format='csr')   # 1D mesh with 4 vertices
    >>> A.todense()
    matrix([[ 2., -1.,  0.,  0.],
            [-1.,  2., -1.,  0.],
            [ 0., -1.,  2., -1.],
            [ 0.,  0., -1.,  2.]])
    >>> naive_aggregation(A)[0].todense() # two aggregates
    matrix([[1, 0],
            [1, 0],
            [0, 1],
            [0, 1]], dtype=int8)
    >>> A = csr_matrix([[1,0,0],[0,1,1],[0,1,1]])
    >>> A.todense()                      # first vertex is isolated
    matrix([[1, 0, 0],
            [0, 1, 1],
            [0, 1, 1]])
    >>> naive_aggregation(A)[0].todense() # two aggregates
    matrix([[1, 0],
            [0, 1],
            [0, 1]], dtype=int8)

    See Also
    --------
    amg_core.naive_aggregation

    Notes
    -----
    Differs from standard aggregation.  Each dof is considered.  If it has been
    aggregated, skip over.  Otherwise, put dof and any unaggregated neighbors
    in an aggregate.  Results in possibly much higher complexities than
    standard aggregation.",2,0,3,5
"def nearest_indices(xs, XS):
    
    xs=_ndim_coords_from_arrays(xs);
    XS = _ndim_coords_from_arrays(XS, ndim=xs.shape[1])
    if XS.shape[-1] != xs.shape[1]:
        raise ValueError(""dimensions of the points don't the sample points"")
    
    _,i = cKDTree(xs).query(XS)
    return i;","Returns indices that perform nearest interpolation given a
    set of points. Similar to scipy.interpolate.griddata with
    method set to ""nearest"". In fact, it's construction is based
    on that function and related types.

    Parameters:
    -----------

    xs -- ndarray of floats, shape (n,D)
          Data point coordinates. Can either be an array of
          shape (n,D), or a tuple of `ndim` arrays.
    XS -- ndarray of floats, shape (M, D)
          Points at which to interpolate/sample data.",1,0,2,3
"def needs_ssh(hostname, _socket=None):
    
    if hostname.lower() in ['localhost', '127.0.0.1', '127.0.1.1']:
        return False
    _socket = _socket or socket
    fqdn = _socket.getfqdn()
    if hostname == fqdn:
        return False
    local_hostname = _socket.gethostname()
    local_short_hostname = local_hostname.split('.')[0]
    if local_hostname == hostname or local_short_hostname == hostname:
        return False
    return True","Obtains remote hostname of the socket and cuts off the domain part
    of its FQDN.",0,0,2,2
"def netmiko_config(*config_commands, **kwargs):
    
    netmiko_kwargs = netmiko_args()
    kwargs.update(netmiko_kwargs)
    return __salt__['netmiko.send_config'](config_commands=config_commands,
                                           **kwargs)",".. versionadded:: 2019.2.0

    Load a list of configuration commands on the remote device, via Netmiko.

    .. warning::

        Please remember that ``netmiko`` does not have any rollback safeguards
        and any configuration change will be directly loaded into the running
        config if the platform doesn't have the concept of ``candidate`` config.

        On Junos, or other platforms that have this capability, the changes will
        not be loaded into the running config, and the user must set the
        ``commit`` argument to ``True`` to transfer the changes from the
        candidate into the running config before exiting.

    config_commands
        A list of configuration commands to be loaded on the remote device.

    config_file
        Read the configuration commands from a file. The file can equally be a
        template that can be rendered using the engine of choice (see
        ``template_engine``).

        This can be specified using the absolute path to the file, or using one
        of the following URL schemes:

        - ``salt://``, to fetch the file from the Salt fileserver.
        - ``http://`` or ``https://``
        - ``ftp://``
        - ``s3://``
        - ``swift://``

    exit_config_mode: ``True``
        Determines whether or not to exit config mode after complete.

    delay_factor: ``1``
        Factor to adjust delays.

    max_loops: ``150``
        Controls wait time in conjunction with delay_factor (default: ``150``).

    strip_prompt: ``False``
        Determines whether or not to strip the prompt (default: ``False``).

    strip_command: ``False``
        Determines whether or not to strip the command (default: ``False``).

    config_mode_command
        The command to enter into config mode.

    commit: ``False``
        Commit the configuration changes before exiting the config mode. This
        option is by default disabled, as many platforms don't have this
        capability natively.

    CLI Example:

    .. code-block:: bash

        salt '*' napalm.netmiko_config 'set system ntp peer 1.2.3.4' commit=True
        salt '*' napalm.netmiko_config https://bit.ly/2sgljCB",0,1,0,1
"def network_lopf(network, snapshots=None, solver_name=""glpk"", solver_io=None,
                 skip_pre=False, extra_functionality=None, solver_logfile=None, solver_options={},
                 keep_files=False, formulation=""angles"", ptdf_tolerance=0.,
                 free_memory={},extra_postprocessing=None):
    

    snapshots = _as_snapshots(network, snapshots)

    network_lopf_build_model(network, snapshots, skip_pre=skip_pre,
                             formulation=formulation, ptdf_tolerance=ptdf_tolerance)

    if extra_functionality is not None:
        extra_functionality(network,snapshots)

    network_lopf_prepare_solver(network, solver_name=solver_name,
                                solver_io=solver_io)

    return network_lopf_solve(network, snapshots, formulation=formulation,
                              solver_logfile=solver_logfile, solver_options=solver_options,
                              keep_files=keep_files, free_memory=free_memory,
                              extra_postprocessing=extra_postprocessing)","Linear optimal power flow for a group of snapshots.

    Parameters
    ----------
    snapshots : list or index slice
        A list of snapshots to optimise, must be a subset of
        network.snapshots, defaults to network.snapshots
    solver_name : string
        Must be a solver name that pyomo recognises and that is
        installed, e.g. ""glpk"", ""gurobi""
    solver_io : string, default None
        Solver Input-Output option, e.g. ""python"" to use ""gurobipy"" for
        solver_name=""gurobi""
    skip_pre: bool, default False
        Skip the preliminary steps of computing topology, calculating
        dependent values and finding bus controls.
    extra_functionality : callable function
        This function must take two arguments
        `extra_functionality(network,snapshots)` and is called after
        the model building is complete, but before it is sent to the
        solver. It allows the user to
        add/change constraints and add/change the objective function.
    solver_logfile : None|string
        If not None, sets the logfile option of the solver.
    solver_options : dictionary
        A dictionary with additional options that get passed to the solver.
        (e.g. {'threads':2} tells gurobi to use only 2 cpus)
    keep_files : bool, default False
        Keep the files that pyomo constructs from OPF problem
        construction, e.g. .lp file - useful for debugging
    formulation : string
        Formulation of the linear power flow equations to use; must be
        one of [""angles"",""cycles"",""kirchhoff"",""ptdf""]
    ptdf_tolerance : float
        Value below which PTDF entries are ignored
    free_memory : set, default {'pyomo'}
        Any subset of {'pypsa', 'pyomo'}. Allows to stash `pypsa` time-series
        data away while the solver runs (as a pickle to disk) and/or free
        `pyomo` data after the solution has been extracted.
    extra_postprocessing : callable function
        This function must take three arguments
        `extra_postprocessing(network,snapshots,duals)` and is called after
        the model has solved and the results are extracted. It allows the user to
        extract further information about the solution, such as additional shadow prices.

    Returns
    -------
    None",0,0,3,3
"def newEmailReport(self, name, **kwargs):
        

        
        for key in kwargs:
            if key not in ['checkid', 'frequency', 'contactids',
                           'additionalemails']:
                sys.stderr.write(""'%s'"" % key + ' is not a valid argument ' +
                                 'of newEmailReport()\n')

        parameters = {'name': name}
        for key, value in kwargs.iteritems():
            parameters[key] = value

        return self.request('POST', 'reports.email',
                            parameters).json()['message']","Creates a new email report

        Returns status message for operation

        Optional parameters:

            * checkid -- Check identifier. If omitted, this will be an
                overview report
                    Type: Integer

            * frequency -- Report frequency
                    Type: String ['monthly', 'weekly', 'daily']

            * contactids -- Comma separated list of receiving contact
                identifiers
                    Type: String

            * additionalemails -- Comma separated list of additional receiving
                emails
                    Type: String",0,1,1,2
"def next_event_description(self):
        
        if self._fancy_heap.size == 0:
            event_type = 'Nothing'
            edge_index = None
        else:
            s = [q._key() for q in self.edge2queue]
            s.sort()
            e = s[0][1]
            q = self.edge2queue[e]

            event_type = 'Arrival' if q.next_event_description() == 1 else 'Departure'
            edge_index = q.edge[2]
        return event_type, edge_index","Returns whether the next event is an arrival or a departure
        and the queue the event is accuring at.

        Returns
        -------
        des : str
            Indicates whether the next event is an arrival, a
            departure, or nothing; returns ``'Arrival'``,
            ``'Departure'``, or ``'Nothing'``.
        edge : int or ``None``
            The edge index of the edge that this event will occur at.
            If there are no events then ``None`` is returned.",0,0,5,5
"def next_intent_handler(request):
    

    message = ""Sorry, couldn't find anything in your next queue""
    end_session = True
    if True:
        user_queue = twitter_cache.user_queue(request.access_token())
        if not user_queue.is_finished():
            message = user_queue.read_out_next(MAX_RESPONSE_TWEETS)
            if not user_queue.is_finished():
                end_session = False
                message = message + "". Please, say 'next' if you want me to read out more. ""
    return alexa.create_response(message=message,
                                 end_session=end_session)",Takes care of things whenver the user says 'next',2,2,0,4
"def nii_ugzip(imfile, outpath=''):
    
    import gzip
    with gzip.open(imfile, 'rb') as f:
        s = f.read()
    
    if outpath=='':
        fout = imfile[:-3]
    else:
        fout = os.path.join(outpath, os.path.basename(imfile)[:-3])
    
    with open(fout, 'wb') as f:
        f.write(s)
    return fout",Uncompress *.gz file,1,0,0,1
"def norm(self, db_level=-3.0):
        
        if not is_number(db_level):
            raise ValueError('db_level must be a number.')

        effect_args = [
            'norm',
            '{:f}'.format(db_level)
        ]
        self.effects.extend(effect_args)
        self.effects_log.append('norm')

        return self","Normalize an audio file to a particular db level.
        This behaves identically to the gain effect with normalize=True.

        Parameters
        ----------
        db_level : float, default=-3.0
            Output volume (db)

        See Also
        --------
        gain, loudness",1,0,2,3
"def normalize(data, width):
    
    min_dat = find_min(data)
    
    off_data = []
    if min_dat < 0:
        min_dat = abs(min_dat)
        for dat in data:
            off_data.append([_d + min_dat for _d in dat])
    else:
        off_data = data
    min_dat = find_min(off_data)
    max_dat = find_max(off_data)

    if max_dat < width:
        
        
        return off_data

    
    
    
    
    norm_factor = width / float(max_dat)
    normal_dat = []
    for dat in off_data:
        normal_dat.append([_v * norm_factor for _v in dat])

    return normal_dat",Normalize the data and return it.,0,0,1,1
"def nullity_filter(df, filter=None, p=0, n=0):
    
    if filter == 'top':
        if p:
            df = df.iloc[:, [c >= p for c in df.count(axis='rows').values / len(df)]]
        if n:
            df = df.iloc[:, np.sort(np.argsort(df.count(axis='rows').values)[-n:])]
    elif filter == 'bottom':
        if p:
            df = df.iloc[:, [c <= p for c in df.count(axis='rows').values / len(df)]]
        if n:
            df = df.iloc[:, np.sort(np.argsort(df.count(axis='rows').values)[:n])]
    return df","Filters a DataFrame according to its nullity, using some combination of 'top' and 'bottom' numerical and
    percentage values. Percentages and numerical thresholds can be specified simultaneously: for example,
    to get a DataFrame with columns of at least 75% completeness but with no more than 5 columns, use
    `nullity_filter(df, filter='top', p=.75, n=5)`.

    :param df: The DataFrame whose columns are being filtered.
    :param filter: The orientation of the filter being applied to the DataFrame. One of, ""top"", ""bottom"",
    or None (default). The filter will simply return the DataFrame if you leave the filter argument unspecified or
    as None.
    :param p: A completeness ratio cut-off. If non-zero the filter will limit the DataFrame to columns with at least p
    completeness. Input should be in the range [0, 1].
    :param n: A numerical cut-off. If non-zero no more than this number of columns will be returned.
    :return: The nullity-filtered `DataFrame`.",0,0,5,5
"def numToDottedQuad(num):
    

    
    import socket, struct

    
    if num > 4294967295 or num < 0:
        raise ValueError('Not a good numeric IP: %s' % num)
    try:
        return socket.inet_ntoa(
            struct.pack('!L', long(num)))
    except (socket.error, struct.error, OverflowError):
        raise ValueError('Not a good numeric IP: %s' % num)","Convert long int to dotted quad string

    >>> numToDottedQuad(long(-1))
    Traceback (most recent call last):
    ValueError: Not a good numeric IP: -1
    >>> numToDottedQuad(long(1))
    '0.0.0.1'
    >>> numToDottedQuad(long(16777218))
    '1.0.0.2'
    >>> numToDottedQuad(long(16908291))
    '1.2.0.3'
    >>> numToDottedQuad(long(16909060))
    '1.2.3.4'
    >>> numToDottedQuad(long(4294967295))
    '255.255.255.255'
    >>> numToDottedQuad(long(4294967296))
    Traceback (most recent call last):
    ValueError: Not a good numeric IP: 4294967296",1,0,2,3
"def numericalize(self, arrs, device=None):
        
        numericalized = []
        self.nesting_field.include_lengths = False
        if self.include_lengths:
            arrs, sentence_lengths, word_lengths = arrs

        for arr in arrs:
            numericalized_ex = self.nesting_field.numericalize(
                arr, device=device)
            numericalized.append(numericalized_ex)
        padded_batch = torch.stack(numericalized)

        self.nesting_field.include_lengths = True
        if self.include_lengths:
            sentence_lengths = \
                torch.tensor(sentence_lengths, dtype=self.dtype, device=device)
            word_lengths = torch.tensor(word_lengths, dtype=self.dtype, device=device)
            return (padded_batch, sentence_lengths, word_lengths)
        return padded_batch","Convert a padded minibatch into a variable tensor.

        Each item in the minibatch will be numericalized independently and the resulting
        tensors will be stacked at the first dimension.

        Arguments:
            arr (List[List[str]]): List of tokenized and padded examples.
            device (str or torch.device): A string or instance of `torch.device`
                specifying which device the Variables are going to be created on.
                If left as default, the tensors will be created on cpu. Default: None.",0,0,2,2
"def numericallySortFilenames(names):
    

    def numericPrefix(name):
        
        count = 0
        for ch in name:
            if ch in string.digits:
                count += 1
            else:
                break
        return 0 if count == 0 else int(name[0:count])

    return sorted(names, key=lambda name: numericPrefix(basename(name)))","Sort (ascending) a list of file names by their numerical prefixes.
    The number sorted on is the numeric prefix of the basename of
    the given filename. E.g., '../output/1.json.bz2' will sort before
    '../output/10.json.bz2'.

    @param: A C{list} of file names, each of whose basename starts with a
        string of digits.
    @return: The sorted C{list} of full file names.",0,0,1,1
"def obj_box_coords_rescale(coords=None, shape=None):
    
    if coords is None:
        coords = []
    if shape is None:
        shape = [100, 200]

    imh, imw = shape[0], shape[1]
    imh = imh * 1.0  
    imw = imw * 1.0
    coords_new = list()
    for coord in coords:

        if len(coord) != 4:
            raise AssertionError(""coordinate should be 4 values : [x, y, w, h]"")

        x = coord[0] / imw
        y = coord[1] / imh
        w = coord[2] / imw
        h = coord[3] / imh
        coords_new.append([x, y, w, h])
    return coords_new","Scale down a list of coordinates from pixel unit to the ratio of image size i.e. in the range of [0, 1].

    Parameters
    ------------
    coords : list of list of 4 ints or None
        For coordinates of more than one images .e.g.[[x, y, w, h], [x, y, w, h], ...].
    shape : list of 2 int or None
        【height, width].

    Returns
    -------
    list of list of 4 numbers
        A list of new bounding boxes.


    Examples
    ---------
    >>> coords = obj_box_coords_rescale(coords=[[30, 40, 50, 50], [10, 10, 20, 20]], shape=[100, 100])
    >>> print(coords)
      [[0.3, 0.4, 0.5, 0.5], [0.1, 0.1, 0.2, 0.2]]
    >>> coords = obj_box_coords_rescale(coords=[[30, 40, 50, 50]], shape=[50, 100])
    >>> print(coords)
      [[0.3, 0.8, 0.5, 1.0]]
    >>> coords = obj_box_coords_rescale(coords=[[30, 40, 50, 50]], shape=[100, 200])
    >>> print(coords)
      [[0.15, 0.4, 0.25, 0.5]]

    Returns
    -------
    list of 4 numbers
        New coordinates.",1,0,2,3
"def object_patch_append_data(self, multihash, new_data, **kwargs):
        
        args = (multihash,)
        body, headers = multipart.stream_files(new_data, self.chunk_size)
        return self._client.request('/object/patch/append-data', args,
                                    decoder='json',
                                    data=body, headers=headers, **kwargs)","Creates a new merkledag object based on an existing one.

        The new object will have the provided data appended to it,
        and will thus have a new Hash.

        .. code-block:: python

            >>> c.object_patch_append_data(""QmZZmY … fTqm"", io.BytesIO(b""bla""))
            {'Hash': 'QmR79zQQj2aDfnrNgczUhvf2qWapEfQ82YQRt3QjrbhSb2'}

        Parameters
        ----------
        multihash : str
            The hash of an ipfs object to modify
        new_data : io.RawIOBase
            The data to append to the object's data section

        Returns
        -------
            dict : Hash of new object",0,1,1,2
"def observed(obj=None, **kwds):
    

    if obj is not None:
        if isinstance(obj, Stochastic):
            obj._observed = True
            return obj
        else:
            p = stochastic(__func__=obj, observed=True, **kwds)
            return p

    kwds['observed'] = True

    def instantiate_observed(func):
        return stochastic(func, **kwds)

    return instantiate_observed","Decorator function to instantiate data objects.
    If given a Stochastic, sets a the observed flag to True.

    Can be used as

    @observed
    def A(value = ., parent_name = .,  ...):
        return foo(value, parent_name, ...)

    or as

    @stochastic(observed=True)
    def A(value = ., parent_name = .,  ...):
        return foo(value, parent_name, ...)


    :SeeAlso:
      stochastic, Stochastic, dtrm, Deterministic, potential, Potential, Model,
      distributions",0,0,3,3
"def observed(self, band, corrected=True):
        
        if band not in 'ugriz':
            raise ValueError(""band='{0}' not recognized"".format(band))
        i = 'ugriz'.find(band)
        t, y, dy = self.lcdata.get_lightcurve(self.lcid, return_1d=False)

        if corrected:
            ext = self.obsmeta['rExt'] * self.ext_correction[band]
        else:
            ext = 0

        return t[:, i], y[:, i] - ext, dy[:, i]","Return observed values in the given band

        Parameters
        ----------
        band : str
            desired bandpass: should be one of ['u', 'g', 'r', 'i', 'z']
        corrected : bool (optional)
            If true, correct for extinction

        Returns
        -------
        t, mag, dmag : ndarrays
            The times, magnitudes, and magnitude errors for the specified band.",1,0,2,3
"def oidc_to_user_data(payload):
    
    payload = payload.copy()

    field_map = {
        'given_name': 'first_name',
        'family_name': 'last_name',
        'email': 'email',
    }
    ret = {}
    for token_attr, user_attr in field_map.items():
        if token_attr not in payload:
            continue
        ret[user_attr] = payload.pop(token_attr)
    ret.update(payload)

    return ret",Map OIDC claims to Django user fields.,0,0,1,1
"def on(self, event, handler=None):
        
        if event not in self.event_names:
            raise ValueError('Invalid event')

        def set_handler(handler):
            self.handlers[event] = handler
            return handler

        if handler is None:
            return set_handler
        set_handler(handler)","Register an event handler.

        :param event: The event name. Can be ``'connect'``, ``'message'`` or
                      ``'disconnect'``.
        :param handler: The function that should be invoked to handle the
                        event. When this parameter is not given, the method
                        acts as a decorator for the handler function.

        Example usage::

            # as a decorator:
            @eio.on('connect')
            def connect_handler():
                print('Connection request')

            # as a method:
            def message_handler(msg):
                print('Received message: ', msg)
                eio.send('response')
            eio.on('message', message_handler)",1,0,2,3
"def onMarkedSeen(
        self, threads=None, seen_ts=None, ts=None, metadata=None, msg=None
    ):
        
        log.info(
            ""Marked messages as seen in threads {} at {}s"".format(
                [(x[0], x[1].name) for x in threads], seen_ts / 1000
            )
        )","Called when the client is listening, and the client has successfully marked threads as seen

        :param threads: The threads that were marked
        :param author_id: The ID of the person who changed the emoji
        :param seen_ts: A timestamp of when the threads were seen
        :param ts: A timestamp of the action
        :param metadata: Extra metadata about the action
        :param msg: A full set of the data recieved
        :type thread_type: models.ThreadType",0,1,0,1
"def onRefreshPluginData(self, plugin_name, data):
        
        logger.info(u""onRefreshPluginData: {}"".format(plugin_name))
        if not plugin_name:
            logger.error(""Missing plugin name"")
            return
        reactor.callFromThread(self._sendJSON, {
            'msg': ""plugin_data_get"",
            'plugin_name': plugin_name
        })","Frontend requests a data refresh

        :param plugin_name: Name of plugin that changed
        :type plugin_name: str
        :param data: Additional data
        :type data: None
        :rtype: None",0,2,0,2
"def on_edited_dataframe_sync(cell_renderer, iter, new_value, column,
                             df_py_dtypes, list_store, df_data):
    
    
    
    column_name = column.get_name()
    
    i, dtype = df_py_dtypes.ix[column_name]
    
    if dtype == float:
        value = si_parse(new_value)
    elif dtype == bool:
        value = not list_store[iter][i]

    if value == list_store[iter][i]:
        
        return False
    list_store[iter][i] = value
    
    df_data[column_name].values[int(iter)] = value
    return True","Handle the `'edited'` signal from a `gtk.CellRenderer` to:

     * Update the corresponding entry in the list store.
     * Update the corresponding entry in the provided data frame instance.

    The callback can be connected to the cell renderer as follows:

        cell_renderer.connect('edited', on_edited_dataframe_sync, column,
                              list_store, df_py_dtypes, df_data)

    where `column` is the `gtk.TreeViewColumn` the cell renderer belongs to,
    and `df_py_dtypes` and `list_store` are the return values from calling
    `get_list_store` on the `df_data` data frame.

    Args:

        cell_renderer (gtk.CellRenderer)
        iter (str) : Gtk TreeView iterator
        new_value (str) : New value resulting from edit operation.
        column (gtk.TreeViewColumn) : Column containing edited cell.
        df_py_dtypes (pandas.DataFrame) : Data frame containing type
            information for columns in tree view (and `list_store`).
        list_store (gtk.ListStore) : Model containing data bound to tree view.
        df_data (pandas.DataFrame) : Data frame containing data in `list_store`.

    Returns:

        None",0,0,2,2
"def on_mouse_move(self, event):
        
        if event.is_dragging:
            dxy = event.pos - event.last_event.pos
            button = event.press_event.button

            if button == 1:
                dxy = self.canvas_tr.map(dxy)
                o = self.canvas_tr.map([0, 0])
                t = dxy - o
                self.move(t)
            elif button == 2:
                center = self.canvas_tr.map(event.press_event.pos)
                if self._aspect is None:
                    self.zoom(np.exp(dxy * (0.01, -0.01)), center)
                else:
                    s = dxy[1] * -0.01
                    self.zoom(np.exp(np.array([s, s])), center)
                    
            self.shader_map()","Mouse move handler

        Parameters
        ----------
        event : instance of Event
            The event.",1,0,1,2
"def on_nicknameinuse(self, connection, event):
        
        digits = """"
        while self.nickname[-1].isdigit():
            digits = self.nickname[-1] + digits
            self.nickname = self.nickname[:-1]
        digits = 1 if not digits else int(digits) + 1
        self.nickname += str(digits)
        self.connect(self.host, self.port, self.nickname)","Increment a digit on the nickname if it's in use, and
        re-connect.",0,1,2,3
"def on_plot_select(self,event):
        
        if not self.xdata or not self.ydata: return
        pos=event.GetPosition()
        width, height = self.canvas.get_width_height()
        pos[1] = height - pos[1]
        xpick_data,ypick_data = pos
        xdata_org = self.xdata
        ydata_org = self.ydata
        data_corrected = self.map.transData.transform(vstack([xdata_org,ydata_org]).T)
        xdata,ydata = data_corrected.T
        xdata = list(map(float,xdata))
        ydata = list(map(float,ydata))
        e = 4e0

        index = None
        for i,(x,y) in enumerate(zip(xdata,ydata)):
            if 0 < sqrt((x-xpick_data)**2. + (y-ypick_data)**2.) < e:
                index = i
                break
        if index==None: print(""Couldn't find point %.1f,%.1f""%(xpick_data,ypick_data))

        self.change_selected(index)","Select data point if cursor is in range of a data point
        @param: event -> the wx Mouseevent for that click",0,0,2,2
"def on_resize(self, event):
        
        self._update_transforms()
        
        if self._central_widget is not None:
            self._central_widget.size = self.size
            
        if len(self._vp_stack) == 0:
            self.context.set_viewport(0, 0, *self.physical_size)","Resize handler

        Parameters
        ----------
        event : instance of Event
            The resize event.",0,0,1,1
"def on_same_fs(request):
    
    filename = request.POST['filename']
    checksum_in = request.POST['checksum']

    checksum = 0
    try:
        data = open(filename, 'rb').read(32)
        checksum = zlib.adler32(data, checksum) & 0xffffffff
        if checksum == int(checksum_in):
            return HttpResponse(content=json.dumps({'success': True}),
                                content_type=JSON, status=200)
    except (IOError, ValueError):
        pass

    return HttpResponse(content=json.dumps({'success': False}),
                        content_type=JSON, status=200)","Accept a POST request to check access to a FS available by a client.

    :param request:
        `django.http.HttpRequest` object, containing mandatory parameters
        filename and checksum.",1,1,1,3
"def one_to_many(df, unitcol, manycol):
    
    subset = df[[manycol, unitcol]].drop_duplicates()
    for many in subset[manycol].unique():
        if subset[subset[manycol] == many].shape[0] > 1:
            msg = ""{} in {} has multiple values for {}"".format(many, manycol, unitcol)
            raise AssertionError(msg)

    return df","Assert that a many-to-one relationship is preserved between two
    columns. For example, a retail store will have have distinct
    departments, each with several employees. If each employee may
    only work in a single department, then the relationship of the
    department to the employees is one to many.

    Parameters
    ==========
    df : DataFrame
    unitcol : str
        The column that encapulates the groups in ``manycol``.
    manycol : str
        The column that must remain unique in the distict pairs
        between ``manycol`` and ``unitcol``

    Returns
    =======
    df : DataFrame",1,0,2,3
"def oneshot(self, query, **params):
        
        if ""exec_mode"" in params:
            raise TypeError(""Cannot specify an exec_mode to oneshot."")
        params['segmentation'] = params.get('segmentation', 'none')
        return self.post(search=query,
                         exec_mode=""oneshot"",
                         **params).body","Run a oneshot search and returns a streaming handle to the results.

        The ``InputStream`` object streams XML fragments from the server. To
        parse this stream into usable Python objects,
        pass the handle to :class:`splunklib.results.ResultsReader`::

            import splunklib.client as client
            import splunklib.results as results
            service = client.connect(...)
            rr = results.ResultsReader(service.jobs.oneshot(""search * | head 5""))
            for result in rr:
                if isinstance(result, results.Message):
                    # Diagnostic messages may be returned in the results
                    print '%s: %s' % (result.type, result.message)
                elif isinstance(result, dict):
                    # Normal events are returned as dicts
                    print result
            assert rr.is_preview == False

        The ``oneshot`` method makes a single roundtrip to the server (as opposed
        to two for :meth:`create` followed by :meth:`results`), plus at most two more
        if the ``autologin`` field of :func:`connect` is set to ``True``.

        :raises ValueError: Raised for invalid queries.

        :param query: The search query.
        :type query: ``string``
        :param params: Additional arguments (optional):

            - ""output_mode"": Specifies the output format of the results (XML,
              JSON, or CSV).

            - ""earliest_time"": Specifies the earliest time in the time range to
              search. The time string can be a UTC time (with fractional seconds),
              a relative time specifier (to now), or a formatted time string.

            - ""latest_time"": Specifies the latest time in the time range to
              search. The time string can be a UTC time (with fractional seconds),
              a relative time specifier (to now), or a formatted time string.

            - ""rf"": Specifies one or more fields to add to the search.

        :type params: ``dict``

        :return: The ``InputStream`` IO handle to raw XML returned from the server.",1,1,1,3
"def open(cls, sock, chunk_type, isatty, chunk_eof_type=None, buf_size=None, select_timeout=None):
    
    with cls.open_multi(sock,
                        (chunk_type,),
                        (isatty,),
                        chunk_eof_type,
                        buf_size,
                        select_timeout) as ctx:
      yield ctx",Yields the write side of a pipe that will copy appropriately chunked values to a socket.,1,0,0,1
"def open(self):
        
        if self.is_open:
            raise RuntimeError(""WinDivert handle is already open."")
        self._handle = windivert_dll.WinDivertOpen(self._filter, self._layer, self._priority,
                                                   self._flags)","Opens a WinDivert handle for the given filter.
        Unless otherwise specified by flags, any packet that matches the filter will be diverted to the handle.
        Diverted packets can be read by the application with receive().

        The remapped function is WinDivertOpen::

            HANDLE WinDivertOpen(
                __in const char *filter,
                __in WINDIVERT_LAYER layer,
                __in INT16 priority,
                __in UINT64 flags
            );

        For more info on the C call visit: http://reqrypt.org/windivert-doc.html#divert_open",1,1,1,3
"def open(self, packet_received):
        
        def port_open(listeningport):
            self._listeningport = listeningport
            self.ownid = self.ownid_factory(listeningport)
            logger.debug(""Port opened. Own-ID:%s"" % self.ownid)
            return None
        
        logger.debug(""Opening connection pool"")
        
        self.packet_received = packet_received
        d = self.stream_server_endpoint.listen(PoolFactory(self, self._typenames))
        d.addCallback(port_open)
        return d","Opens the port.
        
        :param packet_received: Callback which is invoked when we received a packet.
          Is passed the peer, typename, and data.

        :returns: Deferred that callbacks when we are ready to receive.",0,2,0,2
"def open_zip(cls, dbname, zipped, encoding=None, fieldnames_lower=True, case_sensitive=True):
        
        with ZipFile(zipped, 'r') as zip_:

            if not case_sensitive:
                dbname = pick_name(dbname, zip_.namelist())

            with zip_.open(dbname) as f:
                yield cls(f, encoding=encoding, fieldnames_lower=fieldnames_lower)","Context manager. Allows opening a .dbf file from zip archive.

        .. code-block::

            with Dbf.open_zip('some.dbf', 'myarch.zip') as dbf:
                ...

        :param str|unicode dbname: .dbf file name

        :param str|unicode|file zipped: .zip file path or a file-like object.

        :param str|unicode encoding: Encoding used by DB.
            This will be used if there's no encoding information in the DB itself.

        :param bool fieldnames_lower: Lowercase field names.

        :param bool case_sensitive: Whether DB filename is case sensitive.

        :rtype: Dbf",1,0,0,1
"def optimize(exp_rets, covs):
    
    _cov_inv = np.linalg.inv(covs)        

    
    _u = np.ones((len(exp_rets)))

    
    _u_cov_inv = _u.dot(_cov_inv)
    _rets_cov_inv = exp_rets.dot(_cov_inv)

    
    _m = np.empty((2, 2))
    _m[0, 0] = _rets_cov_inv.dot(exp_rets)
    _m[0, 1] = _u_cov_inv.dot(exp_rets)
    _m[1, 0] = _rets_cov_inv.dot(_u)
    _m[1, 1] = _u_cov_inv.dot(_u)

    
    _m_inv = np.linalg.inv(_m)
    a = _m_inv[0, 0] * _rets_cov_inv + _m_inv[1, 0] * _u_cov_inv
    b = _m_inv[0, 1] * _rets_cov_inv + _m_inv[1, 1] * _u_cov_inv
    least_risk_ret = _m[0, 1] / _m[1, 1]
    return a, b, least_risk_ret","Return parameters for portfolio optimization.

    Parameters
    ----------
    exp_rets : ndarray
        Vector of expected returns for each investment..
    covs : ndarray
        Covariance matrix for the given investments.

    Returns
    ---------
    a : ndarray
        The first vector (to be combined with target return as scalar)
        in the linear equation for optimal weights.
    b : ndarray
        The second (constant) vector in the linear equation for
        optimal weights.
    least_risk_ret : int
        The return achieved on the portfolio that combines the given
        equities so as to achieve the lowest possible risk.

    Notes
    ---------
    *   The length of `exp_rets` must match the number of rows
        and columns in the `covs` matrix.
    *   The weights for an optimal portfolio with expected return
        `ret` is given by the formula `w = ret * a + b` where `a`
        and `b` are the vectors returned here. The weights `w` for
        the portfolio with lowest risk are given by `w = least_risk_ret * a + b`.
    *   An exception will be raised if the covariance matrix
        is singular or if each prospective investment has the
        same expected return.",1,0,3,4
"def options(self, parser, env=None):
        
        if env is None:
            env = os.environ
        parser.add_option(
            '--sphinx-config-tpl',
            help='Path to the Sphinx configuration file template.',
        )

        super(SphinxSearchPlugin, self).options(parser, env)","Sphinx config file that can optionally take the following python
        template string arguments:

        ``database_name``
        ``database_password``
        ``database_username``
        ``database_host``
        ``database_port``
        ``sphinx_search_data_dir``
        ``searchd_log_dir``",0,0,1,1
"def order_by_first_occurrence(self):
        
        def _key(e):
            try:
                return e.occurrence_list[0].start
            except IndexError: 
                return localize(datetime.max - timedelta(days=365))

        return sorted(list(self), key=_key)",:return: The event in order of minimum occurrence.,0,0,1,1
"def output(memory, ofile=None):
    
    for m in memory:
        m = m.rstrip('\r\n\t ')  
        if m and m[0] == '
            if ofile is None:
                print(m)
            else:
                ofile.write('%s\n' % m)
            continue

        
        if m and ':' not in m:
            if ofile is None:
                print('    '),
            else:
                ofile.write('\t')

        if ofile is None:
            print(m)
        else:
            ofile.write('%s\n' % m)","Filters the output removing useless preprocessor #directives
    and writes it to the given file or to the screen if no file is passed",2,0,2,4
"def output(ret, bar, **kwargs):  
    
    if 'return_count' in ret:
        val = ret['return_count']
        
        
        
        
        if val > bar.maxval:
            bar.maxval = val
        bar.update(val)
    return ''",Update the progress bar,0,0,1,1
"def over_under(self):
        
        doc = self.get_doc()
        table = doc('table
        giTable = sportsref.utils.parse_info_table(table)
        if 'over_under' in giTable:
            ou = giTable['over_under']
            return float(ou.split()[0])
        else:
            return None","Returns the over/under for the game as a float, or np.nan if not
        available.",0,0,2,2
"def p2wsh_input_and_witness(outpoint, stack, witness_script, sequence=None):
    
    if sequence is None:
        sequence = guess_sequence(witness_script)
    stack = list(map(
        lambda x: b'' if x == 'NONE' else bytes.fromhex(x), stack.split()))
    stack.append(script_ser.serialize(witness_script))
    return tb.make_witness_input_and_witness(outpoint, sequence, stack)","Outpoint, str, str, int -> (TxIn, InputWitness)
    Create a signed witness TxIn and InputWitness from a p2wsh prevout",0,0,2,2
"def pack(self, value=None):
        
        if value is None:
            self.update_header_length()
            return super().pack()
        elif isinstance(value, type(self)):
            return value.pack()
        else:
            msg = ""{} is not an instance of {}"".format(value,
                                                       type(self).__name__)
            raise PackException(msg)","Pack the message into a binary data.

        One of the basic operations on a Message is the pack operation. During
        the packing process, we convert all message attributes to binary
        format.

        Since that this is usually used before sending the message to a switch,
        here we also call :meth:`update_header_length`.

        .. seealso:: This method call its parent's :meth:`GenericStruct.pack`
            after :meth:`update_header_length`.

        Returns:
            bytes: A binary data thats represents the Message.

        Raises:
            Exception: If there are validation errors.",1,0,2,3
"def pad(self, pad_width, **kwargs):
        
        
        kwargs.setdefault('mode', 'constant')
        if isinstance(pad_width, int):
            pad_width = (pad_width,)
        
        new = numpy.pad(self, pad_width, **kwargs).view(type(self))
        
        new.__metadata_finalize__(self)
        new._unit = self.unit
        
        new.x0 -= self.dx * pad_width[0]
        return new","Pad this series to a new size

        Parameters
        ----------
        pad_width : `int`, pair of `ints`
            number of samples by which to pad each end of the array.
            Single int to pad both ends by the same amount, or
            (before, after) `tuple` to give uneven padding
        **kwargs
            see :meth:`numpy.pad` for kwarg documentation

        Returns
        -------
        series : `Series`
            the padded version of the input

        See also
        --------
        numpy.pad
            for details on the underlying functionality",0,0,1,1
"def pad_repeat_border_corner(data, shape):
    
    new_data = np.empty(shape)
    new_data[[slice(upper) for upper in data.shape]] = data
    for i in range(len(shape)):
        selection = [slice(None)]*i + [slice(data.shape[i], None)]
        selection2 = [slice(None)]*i + [slice(data.shape[i]-1, data.shape[i])]
        new_data[selection] = new_data[selection2]
    return new_data","Similar to `pad_repeat_border`, except the padding is always done on the
    upper end of each axis and the target size is specified.

    Parameters
    ----------
    data : ndarray
        Numpy array of any dimension and type.
    shape : tuple
        Final shape of padded array. Should be tuple of length ``data.ndim``.
        If it has to pad unevenly, it will pad one more at the end of the axis
        than at the beginning.

    Examples
    --------
    >>> import deepdish as dd
    >>> import numpy as np

    Pad an array by repeating its upper borders.

    >>> shape = (3, 4)
    >>> x = np.arange(np.prod(shape)).reshape(shape)
    >>> dd.util.pad_repeat_border_corner(x, (5, 5))
    array([[  0.,   1.,   2.,   3.,   3.],
           [  4.,   5.,   6.,   7.,   7.],
           [  8.,   9.,  10.,  11.,  11.],
           [  8.,   9.,  10.,  11.,  11.],
           [  8.,   9.,  10.,  11.,  11.]])",0,0,1,1
"def page(self, from_=values.unset, to=values.unset,
             date_created_on_or_before=values.unset,
             date_created_after=values.unset, page_token=values.unset,
             page_number=values.unset, page_size=values.unset):
        
        params = values.of({
            'From': from_,
            'To': to,
            'DateCreatedOnOrBefore': serialize.iso8601_datetime(date_created_on_or_before),
            'DateCreatedAfter': serialize.iso8601_datetime(date_created_after),
            'PageToken': page_token,
            'Page': page_number,
            'PageSize': page_size,
        })

        response = self._version.page(
            'GET',
            self._uri,
            params=params,
        )

        return FaxPage(self._version, response, self._solution)","Retrieve a single page of FaxInstance records from the API.
        Request is executed immediately

        :param unicode from_: Retrieve only those faxes sent from this phone number
        :param unicode to: Retrieve only those faxes sent to this phone number
        :param datetime date_created_on_or_before: Retrieve only faxes created on or before this date
        :param datetime date_created_after: Retrieve only faxes created after this date
        :param str page_token: PageToken provided by the API
        :param int page_number: Page Number, this value is simply for client state
        :param int page_size: Number of records to return, defaults to 50

        :returns: Page of FaxInstance
        :rtype: twilio.rest.fax.v1.fax.FaxPage",0,1,0,1
"def parse(bin_payload):
    
    
    if len(bin_payload) != LENGTHS['name_consensus_hash'] + LENGTHS['value_hash']:
        log.error(""Invalid update length %s"" % len(bin_payload))
        return None 

    name_consensus_hash_bin = bin_payload[:LENGTHS['name_consensus_hash']]
    value_hash_bin = bin_payload[LENGTHS['name_consensus_hash']:]
    
    name_consensus_hash = hexlify( name_consensus_hash_bin )
    value_hash = hexlify( value_hash_bin )
  
    try:
        rc = update_sanity_test( None, name_consensus_hash, value_hash )
        if not rc:
            raise Exception(""Invalid update data"")
    except Exception, e:
        log.error(""Invalid update data"")
        return None

    return {
        'opcode': 'NAME_UPDATE',
        'name_consensus_hash': name_consensus_hash,
        'value_hash': value_hash
    }","Parse a payload to get back the name and update hash.
    NOTE: bin_payload excludes the leading three bytes.",1,2,2,5
"def parse(cls, expression):
        
        parsed = {""name"": None, ""arguments"": [], ""options"": []}

        if not expression.strip():
            raise ValueError(""Console command signature is empty."")

        expression = expression.replace(os.linesep, """")

        matches = re.match(r""[^\s]+"", expression)

        if not matches:
            raise ValueError(""Unable to determine command name from signature."")

        name = matches.group(0)
        parsed[""name""] = name

        tokens = re.findall(r""\{\s*(.*?)\s*\}"", expression)

        if tokens:
            parsed.update(cls._parameters(tokens))

        return parsed","Parse the given console command definition into a dict.

        :param expression: The expression to parse
        :type expression: str

        :rtype: dict",2,0,3,5
"def parse(path):
    

    doc = ET.parse(path).getroot()

    channel = doc.find(""./channel"")

    blog = _parse_blog(channel)
    authors = _parse_authors(channel)
    categories = _parse_categories(channel)
    tags = _parse_tags(channel)
    posts = _parse_posts(channel)

    return {
        ""blog"": blog,
        ""authors"": authors,
        ""categories"": categories,
        ""tags"": tags,
        ""posts"": posts,
    }","Parses xml and returns a formatted dict.

    Example:

        wpparser.parse(""./blog.wordpress.2014-09-26.xml"")

    Will return:

        {
        ""blog"": {
            ""tagline"": ""Tagline"",
            ""site_url"": ""http://marteinn.se/blog"",
            ""blog_url"": ""http://marteinn.se/blog"",
            ""language"": ""en-US"",
            ""title"": ""Marteinn / Blog""
        },
        ""authors: [{
            ""login"": ""admin"",
            ""last_name"": None,
            ""display_name"": ""admin"",
            ""email"": ""martin@marteinn.se"",
            ""first_name"": None}
        ],
        ""categories"": [{
            ""parent"": None,
            ""term_id"": ""3"",
            ""name"": ""Action Script"",
            ""nicename"": ""action-script"",
            ""children"": [{
                ""parent"": ""action-script"",
                ""term_id"": ""20"",
                ""name"": ""Flash related"",
                ""nicename"": ""flash-related"",
                ""children"": []
            }]
        }],
        ""tags"": [{""term_id"": ""36"", ""slug"": ""bash"", ""name"": ""Bash""}],
        ""posts"": [{
            ""creator"": ""admin"",
            ""excerpt"": None,
            ""post_date_gmt"": ""2014-09-22 20:10:40"",
            ""post_date"": ""2014-09-22 21:10:40"",
            ""post_type"": ""post"",
            ""menu_order"": ""0"",
            ""guid"": ""http://marteinn.se/blog/?p=828"",
            ""title"": ""Post Title"",
            ""comments"": [{
                ""date_gmt"": ""2014-09-24 23:08:31"",
                ""parent"": ""0"",
                ""date"": ""2014-09-25 00:08:31"",
                ""id"": ""85929"",
                ""user_id"": ""0"",
                ""author"": u""Author"",
                ""author_email"": None,
                ""author_ip"": ""111.111.111.111"",
                ""approved"": ""1"",
                ""content"": u""Comment title"",
                ""author_url"": ""http://example.com"",
                ""type"": ""pingback""
            }],
            ""content"": ""Text"",
            ""post_parent"": ""0"",
            ""post_password"": None,
            ""status"": ""publish"",
            ""description"": None,
            ""tags"": [""tag""],
            ""ping_status"": ""open"",
            ""post_id"": ""828"",
            ""link"": ""http://www.marteinn.se/blog/slug/"",
            ""pub_date"": ""Mon, 22 Sep 2014 20:10:40 +0000"",
            ""categories"": [""category""],
            ""is_sticky"": ""0"",
            ""post_name"": ""slug""
        }]
        }",0,0,2,2
"def parse(self) -> Statement:
        
        self.opt_separator()
        start = self.offset
        res = self.statement()
        if res.keyword not in [""module"", ""submodule""]:
            self.offset = start
            raise UnexpectedInput(self, ""'module' or 'submodule'"")
        if self.name is not None and res.argument != self.name:
            raise ModuleNameMismatch(res.argument, self.name)
        if self.rev:
            revst = res.find1(""revision"")
            if revst is None or revst.argument != self.rev:
                raise ModuleRevisionMismatch(revst.argument, self.rev)
        try:
            self.opt_separator()
        except EndOfInput:
            return res
        raise UnexpectedInput(self, ""end of input"")","Parse a complete YANG module or submodule.

        Args:
            mtext: YANG module text.

        Raises:
            EndOfInput: If past the end of input.
            ModuleNameMismatch: If parsed module name doesn't match `self.name`.
            ModuleRevisionMismatch: If parsed revision date doesn't match `self.rev`.
            UnexpectedInput: If top-level statement isn't ``(sub)module``.",4,0,5,9
"def parse(self):
        
        reader = self.split_records()
        for line in reader:
            self.cur_record += 1
            if not line:
                continue
            stmt_line = self.parse_record(line)
            if stmt_line:
                stmt_line.assert_valid()
                self.statement.lines.append(stmt_line)
        return self.statement","Read and parse statement

        Return Statement object

        May raise exceptions.ParseException on malformed input.",0,0,2,2
"def parse(self, buffer, inlineparent = None):
        
        if self.base is not None:
            return self.base.parse(buffer, inlineparent)
        r = self._parse(buffer, inlineparent)
        if r is None:
            return None
        (s, size) = r
        self.subclass(s)
        return (s, (size + self.padding - 1) // self.padding * self.padding)","Try to parse the struct from bytes sequence. The bytes sequence is taken from a streaming source.
        
        :param buffer: bytes sequence to be parsed from.
        
        :param inlineparent: if specified, this struct is embedded in another struct.
        
        :returns: None if the buffer does not have enough data for this struct (e.g. incomplete read
                from socket); (struct, size) else, where struct is the parsed result (usually a NamedStruct object)
                and size is the used bytes length, so you can start another parse from buffer[size:].",0,0,3,3
"def parse(self, element):
        
        result = []
        if element.text is not None and element.tag == self.identifier:
            l, k = (0, 0)
            raw = element.text.split()
            while k < len(self.values):
                dtype = self.dtype[k]
                if isinstance(self.values[k], int):
                    for i in range(self.values[k]):
                        result.append(self._caster[dtype](raw[i + l]))
                    l += self.values[k]
                    k += 1
                else:
                    
                    
                    rest = [ self._caster[dtype](val) for val in raw[l::] ]
                    result.extend(rest)
                    break
        else:
            msg.warn(""no results for parsing {} using line {}"".format(element.tag, self.identifier))

        return result","Parses the contents of the specified XML element using template info.

        :arg element: the XML element from the input file being converted.",0,0,2,2
"def parse(self, filelike, filename):
        
        self.log = log
        self.source = filelike.readlines()
        src = """".join(self.source)
        
        compile(src, filename, ""exec"")
        self.stream = TokenStream(StringIO(src))
        self.filename = filename
        self.all = None
        self.future_imports = set()
        self._accumulated_decorators = []
        return self.parse_module()",Parse the given file-like object and return its Module object.,0,0,2,2
"def parse(self, rrstr):
        
        
        if self._initialized:
            raise pycdlibexception.PyCdlibInternalError('PD record already initialized!')

        (su_len_unused, su_entry_version_unused) = struct.unpack_from('=BB', rrstr[:4], 2)

        self.padding = rrstr[4:]

        
        

        self._initialized = True","Parse a Rock Ridge Platform Dependent record out of a string.

        Parameters:
         rrstr - The string to parse the record out of.
        Returns:
         Nothing.",1,0,2,3
"def parse(self, scope):
        
        name = ''.join(self.tokens[0])
        parsed = self.process(self.tokens[1:], scope)

        if name == '%(':
            name = 'sformat'
        elif name in ('~', 'e'):
            name = 'escape'
        color = Color.Color()
        args = [
            t for t in parsed
            if not isinstance(t, string_types) or t not in '(),'
        ]
        if hasattr(self, name):
            try:
                return getattr(self, name)(*args)
            except ValueError:
                pass

        if hasattr(color, name):
            try:
                result = getattr(color, name)(*args)
                try:
                    return result + ' '
                except TypeError:
                    return result
            except ValueError:
                pass
        return name + ''.join([p for p in parsed])","Parse Node within scope.
        the functions ~( and e( map to self.escape
        and %( maps to self.sformat
        args:
            scope (Scope): Current scope",0,0,1,1
"def parse(self, target):
        
        if isinstance(target, ContentNode):
            if target.name:
                self.parent = target
                self.name.parse(self)
                self.name += target.name
            target.ruleset.append(self)
        self.root.cache['rset'][str(self.name).split()[0]].add(self)
        super(Ruleset, self).parse(target)","Parse nested rulesets
            and save it in cache.",0,0,2,2
"def parse_actor_and_date(line):
    
    actor, epoch, offset = '', 0, 0
    m = _re_actor_epoch.search(line)
    if m:
        actor, epoch, offset = m.groups()
    else:
        m = _re_only_actor.search(line)
        actor = m.group(1) if m else line or ''
    return (Actor._from_string(actor), int(epoch), utctz_to_altz(offset))","Parse out the actor (author or committer) info from a line like::

        author Tom Preston-Werner <tom@mojombo.com> 1191999972 -0700

    :return: [Actor, int_seconds_since_epoch, int_timezone_offset]",0,0,1,1
"def parse_arg(self, name, field, req, locations=None):
        
        location = field.metadata.get(""location"")
        if location:
            locations_to_check = self._validated_locations([location])
        else:
            locations_to_check = self._validated_locations(locations or self.locations)

        for location in locations_to_check:
            value = self._get_value(name, field, req=req, location=location)
            
            if value is not missing:
                return value
        return missing","Parse a single argument from a request.

        .. note::
            This method does not perform validation on the argument.

        :param str name: The name of the value.
        :param marshmallow.fields.Field field: The marshmallow `Field` for the request
            parameter.
        :param req: The request object to parse.
        :param tuple locations: The locations ('json', 'querystring', etc.) where
            to search for the value.
        :return: The unvalidated argument value or `missing` if the value cannot
            be found on the request.",0,0,3,3
"def parse_auth(header):
    
    try:
        method, data = header.split(None, 1)
        if method.lower() == 'basic':
            name, pwd = base64.b64decode(data).split(':', 1)
            return name, pwd
    except (KeyError, ValueError, TypeError):
        return None","Parse rfc2617 HTTP authentication header string (basic) and return (user,pass) tuple or None",0,0,1,1
"def parse_auth_token_from_request(self, auth_header):
        
        if not auth_header:
            raise falcon.HTTPUnauthorized(
                description='Missing Authorization Header')

        parts = auth_header.split()

        if parts[0].lower() != self.auth_header_prefix.lower():
            raise falcon.HTTPUnauthorized(
                description='Invalid Authorization Header: '
                            'Must start with {0}'.format(self.auth_header_prefix))

        elif len(parts) == 1:
            raise falcon.HTTPUnauthorized(
                description='Invalid Authorization Header: Token Missing')
        elif len(parts) > 2:
            raise falcon.HTTPUnauthorized(
                description='Invalid Authorization Header: Contains extra content')

        return parts[1]","Parses and returns Auth token from the request header. Raises
        `falcon.HTTPUnauthoried exception` with proper error message",4,0,1,5
"def parse_authorization_code_response(uri, state=None):
    
    if not is_secure_transport(uri):
        raise InsecureTransportError()

    query = urlparse.urlparse(uri).query
    params = dict(urlparse.parse_qsl(query))

    if not 'code' in params:
        raise MissingCodeError(""Missing code parameter in response."")

    if state and params.get('state', None) != state:
        raise MismatchingStateError()

    return params","Parse authorization grant response URI into a dict.

    If the resource owner grants the access request, the authorization
    server issues an authorization code and delivers it to the client by
    adding the following parameters to the query component of the
    redirection URI using the ``application/x-www-form-urlencoded`` format:

    **code**
            REQUIRED.  The authorization code generated by the
            authorization server.  The authorization code MUST expire
            shortly after it is issued to mitigate the risk of leaks.  A
            maximum authorization code lifetime of 10 minutes is
            RECOMMENDED.  The client MUST NOT use the authorization code
            more than once.  If an authorization code is used more than
            once, the authorization server MUST deny the request and SHOULD
            revoke (when possible) all tokens previously issued based on
            that authorization code.  The authorization code is bound to
            the client identifier and redirection URI.

    **state**
            REQUIRED if the ""state"" parameter was present in the client
            authorization request.  The exact value received from the
            client.

    :param uri: The full redirect URL back to the client.
    :param state: The state parameter from the authorization request.

    For example, the authorization server redirects the user-agent by
    sending the following HTTP response:

    .. code-block:: http

        HTTP/1.1 302 Found
        Location: https://client.example.com/cb?code=SplxlOBeZQQYbYS6WxSbIA
                &state=xyz",3,0,3,6
"def parse_cmdline_args(args, classes):
    
    if args is None:
        raise ValueError(""args must not be None"")
    parsed_args = {}
    for kls in classes:
        prefix = kls.configuration_key_prefix()
        name = kls.configuration_key
        if getattr(args, ""%s_%s"" % (prefix, name), False):
            logging.debug(
                ""Gathering initargs for '%s.%s'"", prefix, name)
            initargs = {}
            for arg_name in kls.init_argnames():
                val = getattr(args, ""%s_%s_%s"" %
                              (prefix, name, arg_name))
                if val is not None:
                    initargs[arg_name] = val
            if prefix not in parsed_args:
                parsed_args[prefix] = []
            parsed_args[prefix].append((name, initargs))
    return parsed_args","Parse all updater and detector related arguments from args.

    Returns a list of (""name"", { ""k"": ""v""})

    :param args: argparse arguments",1,1,3,5
"def parse_cookie(header, charset='utf-8', errors='ignore'):
    
    cookie = _ExtendedCookie()
    if header:
        cookie.load(header)
    result = {}

    
    
    
    for key, value in cookie.iteritems():
        if value.value is not None:
            result[key] = unquote_header_value(value.value) \
                .decode(charset, errors)

    return result","Parse a cookie.

    :param header: the header to be used to parse the cookie.
    :param charset: the charset for the cookie values.
    :param errors: the error behavior for the charset decoding.",0,0,1,1
"def parse_date(record):
	""Parse a date from sqlite. Assumes the date is in US/Pacific time zone.""
	dt = record.pop('datetime')
	fmts = [
		'%Y-%m-%d %H:%M:%S.%f',
		'%Y-%m-%d %H:%M:%S',
	]
	for fmt in fmts:
		try:
			dt = datetime.datetime.strptime(dt, fmt)
			break
		except ValueError:
			pass
	else:
		raise
	tz = pytz.timezone('US/Pacific')
	loc_dt = tz.localize(dt)
	record['datetime'] = loc_dt
	return record",Parse a date from sqlite. Assumes the date is in US/Pacific time zone.,1,0,2,3
"def parse_date_range(date, alt_end_date=None):
    
    NOT_ENDED = ""9999""
    all_years = re.findall(r""\d{4}"", date)

    if alt_end_date:
        NOT_ENDED = alt_end_date

    if not all_years:
        return ""****"", NOT_ENDED

    elif len(all_years) == 1:
        return all_years[0], NOT_ENDED

    return all_years[0], all_years[1]","Parse input `date` string in free-text format for four-digit long groups.

    Args:
        date (str): Input containing years.

    Returns:
        tuple: ``(from, to)`` as four-digit strings.",0,0,1,1
"def parse_dist(self, os_version):
        
        supported = {'rhel': ['rhel', 'redhat', 'red hat'],
                    'sles': ['suse', 'sles'],
                    'ubuntu': ['ubuntu']}
        os_version = os_version.lower()
        for distro, patterns in supported.items():
            for i in patterns:
                if os_version.startswith(i):
                    
                    remain = os_version.split(i, 2)[1]
                    release = self._parse_release(os_version, distro, remain)
                    return distro, release

        msg = 'Can not handle os: %s' % os_version
        raise exception.ZVMException(msg=msg)","Separate os and version from os_version.

        Possible return value are only:
        ('rhel', x.y) and ('sles', x.y) where x.y may not be digits",1,0,2,3
"def parse_files(self, fls):
        

        for f in fls:
            
            if os.path.exists(f):
                self._parser(f)
            else:
                logger.warning(""File {} does not exist"".format(f))","Public method for parsing abricate output files.

        This method is called at at class instantiation for the provided
        output files. Additional abricate output files can be added using
        this method after the class instantiation.

        Parameters
        ----------
        fls : list
            List of paths to Abricate files",0,1,1,2
"def parse_filter_params(query_params, filterable):
    
    if query_params is not None:
        filter_params = {}
        for fq in query_params.mixed():
            if fq in filterable:
                filter_params[fq] = query_params.mixed().get(fq)
        return filter_params
    else:
        return {}","Parse query_params to a filter params dict. Merge multiple values for one key to a list.
    Filter out keys that aren't filterable.

    :param query_params: query params
    :param filterable: list of filterable keys
    :return: dict of filter values",0,0,1,1
"def parse_formula(formula):
    
    if not formula_pattern.match(formula):
        raise ValueError(""%r does not look like a formula"" % (formula,))
    composition = PyComposition()
    for elem, isotope, number in atom_pattern.findall(formula):
        composition[_make_isotope_string(elem, int(isotope) if isotope else 0)] += int(number)
    return composition","Parse a chemical formula and construct a :class:`PyComposition` object

    Parameters
    ----------
    formula : :class:`str`

    Returns
    -------
    :class:`PyComposition`

    Raises
    ------
    ValueError
        If the formula doesn't match the expected pattern",1,0,3,4
"def parse_header_links(self):
        
        for linktype, linkinfo in self.url_connection.links.items():
            url = linkinfo[""url""]
            name = u""Link: header %s"" % linktype
            self.add_url(url, name=name)
        if 'Refresh' in self.headers:
            from ..htmlutil.linkparse import refresh_re
            value = self.headers['Refresh'].strip()
            mo = refresh_re.match(value)
            if mo:
                url = unicode_safe(mo.group(""url""))
                name = u""Refresh: header""
                self.add_url(url, name=name)
        if 'Content-Location' in self.headers:
            url = self.headers['Content-Location'].strip()
            name = u""Content-Location: header""
            self.add_url(url, name=name)",Parse URLs in HTTP headers Link:.,0,1,1,2
"def parse_headers(self, use_cookies, raw):
        
        if not raw:
            packet = helper.to_str(helper.read_file(self.fpth))
        else:
            packet = raw
        dat = {}

        pks = [x for x in packet.split('\n') if x.replace(' ', '')]
        url = pks[0].split(' ')[1]

        for i, cnt in enumerate(pks[1:]):
            arr = cnt.split(':')
            if len(arr) < 2:
                continue
            arr = [x.replace(' ', '') for x in arr]
            _k, v = arr[0], ':'.join(arr[1:])
            dat[_k] = v

        if use_cookies:
            try:
                self.fmt_cookies(dat.pop('Cookie'))
            except:
                pass
        self.headers = dat
        self.url = 'https://{}{}'.format(self.headers.get('Host'), url)
        return url, dat","analyze headers from file or raw messages

        :return: (url, dat)
        :rtype:",0,0,3,3
"def parse_known_args():
    
    parser = ArgumentParser()
    parser.add_argument(""-l"", ""--loop"", type=int, help=""Loop every X seconds"")
    parser.add_argument('-V', '--version',
                        action='store_true',
                        dest='version',
                        help='Print the version number and exit')
    parser.add_argument(""-u"", ""--update"",
                        action='store_true',
                        dest=""update"",
                        help=""Update the software from online repo"")
    parser.add_argument(""-p"", ""--problem"",
                        action='store_true',
                        dest=""problem"",
                        help=""Report a problem"")
    parser.add_argument(""-m"", ""--memory"",
                        action='store_true',
                        dest=""memory"",
                        help=""Run memory tests"")
    args, otherthings = parser.parse_known_args()
    return args, otherthings, parser",Parse command line arguments,0,0,1,1
"def parse_meta(meta):
    
    resources = {}
    for name in meta:
        if name.startswith(""$""):
            continue
        resources[name] = resource = {}
        for action in meta[name]:
            if action.startswith(""$""):
                continue
            url, httpmethod = res_to_url(name, action)
            resource[action] = {
                ""url"": url,
                ""method"": httpmethod
            }
    url_prefix = meta.get(""$url_prefix"", """").rstrip(""/"")
    return url_prefix, meta[""$auth""][""header""].lower(), resources","Parse metadata of API

    Args:
        meta: metadata of API
    Returns:
        tuple(url_prefix, auth_header, resources)",0,0,1,1
"def parse_param_signature(sig):
    
    match = PARAM_SIG_RE.match(sig.strip())
    if not match:
        raise RuntimeError('Parameter signature invalid, got ' + sig)
    groups = match.groups()
    modifiers = groups[0].split()
    typ, name, _, default = groups[-4:]
    return ParamTuple(name=name, typ=typ,
                      default=default, modifiers=modifiers)",Parse a parameter signature of the form: type name (= default)?,1,0,2,3
"def parse_path(path):
  
  if not path:
    raise ValueError(""Invalid path"")

  if isinstance(path, str):
    if path == ""/"":
      raise ValueError(""Invalid path"")
    if path[0] != ""/"":
      raise ValueError(""Invalid path"")
    return path.split(_PATH_SEP)[1:]
  elif isinstance(path, (tuple, list)):
    return path
  else:
    raise ValueError(""A path must be a string, tuple or list"")",Parse a rfc 6901 path.,4,0,5,9
"def parse_raw(self, raw):
        
        if not isinstance(raw, bytes):
            raise PyVLXException(""AliasArray::invalid_type_if_raw"", type_raw=type(raw))
        if len(raw) != 21:
            raise PyVLXException(""AliasArray::invalid_size"", size=len(raw))
        nbr_of_alias = raw[0]
        if nbr_of_alias > 5:
            raise PyVLXException(""AliasArray::invalid_nbr_of_alias"", nbr_of_alias=nbr_of_alias)
        for i in range(0, nbr_of_alias):
            self.alias_array_.append((raw[i*4+1:i*4+3], raw[i*4+3:i*4+5]))",Parse alias array from raw bytes.,3,0,4,7
"def parse_record(self, node):
        

        if self.current_simulation == None:
            self.raise_error('<Record> must be only be used inside a ' +
                             'simulation specification')

        if 'quantity' in node.lattrib:
            quantity = node.lattrib['quantity']
        else:
            self.raise_error('<Record> must specify a quantity.')

        scale = node.lattrib.get('scale', None)
        color  = node.lattrib.get('color', None)
        id  = node.lattrib.get('id', None)

        self.current_simulation.add_record(Record(quantity, scale, color, id))","Parses <Record>

        @param node: Node containing the <Record> element
        @type node: xml.etree.Element",2,0,3,5
"def parse_render(render):
    
    formats = {
        'jpeg': guess_all_extensions('image/jpeg'),
        'png': guess_all_extensions('image/png'),
        'gif': guess_all_extensions('image/gif'),
        'bmp': guess_all_extensions('image/x-ms-bmp'),
        'tiff': guess_all_extensions('image/tiff'),
        'xbm': guess_all_extensions('image/x-xbitmap'),
        'pdf': guess_all_extensions('application/pdf')
    }
    if not render:
        render = 'png'
    else:
        render = render.lower()
        for k, v in formats.items():
            if '.%s' % render in v:
                render = k
                break
        else:
            render = 'png'
    return render","Parse render URL parameter.

    >>> parse_render(None)
    'png'
    >>> parse_render('html')
    'png'
    >>> parse_render('png')
    'png'
    >>> parse_render('jpg')
    'jpeg'
    >>> parse_render('gif')
    'gif'",0,0,1,1
"def parse_request(cls, request_string):
        
        try:
            batch = cls.json_loads(request_string)
        except ValueError as err:
            raise errors.RPCParseError(""No valid JSON. (%s)"" % str(err))

        if isinstance(batch, (list, tuple)) and batch:
            
            
            return [cls._parse_single_request_trap_errors(request) for request in batch], True
        elif isinstance(batch, dict):
            
            return [cls._parse_single_request_trap_errors(batch)], False

        raise errors.RPCInvalidRequest(""Neither a batch array nor a single request object found in the request."")","JSONRPC allows for **batch** requests to be communicated
        as array of dicts. This method parses out each individual
        element in the batch and returns a list of tuples, each
        tuple a result of parsing of each item in the batch.

        :Returns:   | tuple of (results, is_batch_mode_flag)
                    | where:
                    | - results is a tuple describing the request
                    | - Is_batch_mode_flag is a Bool indicating if the
                    |   request came in in batch mode (as array of requests) or not.

        :Raises:    RPCParseError, RPCInvalidRequest",2,0,3,5
"def parse_soap_enveloped_saml_thingy(text, expected_tags):
    
    envelope = defusedxml.ElementTree.fromstring(text)

    
    assert envelope.tag == '{%s}Envelope' % soapenv.NAMESPACE

    assert len(envelope) >= 1
    body = None
    for part in envelope:
        if part.tag == '{%s}Body' % soapenv.NAMESPACE:
            assert len(part) == 1
            body = part
            break

    if body is None:
        return """"

    saml_part = body[0]
    if saml_part.tag in expected_tags:
        return ElementTree.tostring(saml_part, encoding=""UTF-8"")
    else:
        raise WrongMessageType(""Was '%s' expected one of %s"" % (saml_part.tag,
                                                                expected_tags))","Parses a SOAP enveloped SAML thing and returns the thing as
    a string.

    :param text: The SOAP object as XML string
    :param expected_tags: What the tag of the SAML thingy is expected to be.
    :return: SAML thingy as a string",1,0,4,5
"def parse_string_factory(
    alg, sep, splitter, input_transform, component_transform, final_transform
):
    
    
    
    orig_after_xfrm = not (alg & NS_DUMB and alg & ns.LOCALEALPHA)
    original_func = input_transform if orig_after_xfrm else _no_op
    normalize_input = _normalize_input_factory(alg)

    def func(x):
        
        
        
        x = normalize_input(x)
        x, original = input_transform(x), original_func(x)
        x = splitter(x)  
        x = py23_filter(None, x)  
        x = py23_map(component_transform, x)  
        x = sep_inserter(x, sep)  
        return final_transform(x, original)  

    return func","Create a function that will split and format a *str* into a tuple.

    Parameters
    ----------
    alg : ns enum
        Indicate how to format and split the *str*.
    sep : str
        The string character to be inserted between adjacent numeric
        objects in the returned tuple.
    splitter : callable
        A function the will accept a string and returns an iterable
        of strings where the numbers are separated from the non-numbers.
    input_transform : callable
        A function to apply to the string input *before* applying
        the *splitter* function. Must return a string.
    component_transform : callable
        A function that is operated elementwise on the output of
        *splitter*. It must accept a single string and return either
        a string or a number.
    final_transform : callable
        A function to operate on the return value as a whole. It
        must accept a tuple and a string argument - the tuple
        should be the result of applying the above functions, and the
        string is the original input value. It must return a tuple.

    Returns
    -------
    func : callable
        A function that accepts string input and returns a tuple
        containing the string split into numeric and non-numeric
        components, where the numeric components are converted into
        numeric objects. The first element is *always* a string,
        and then alternates number then string. Intended to be
        used as the *string_func* argument to *natsort_key*.

    See Also
    --------
    natsort_key
    input_string_transform_factory
    string_component_transform_factory
    final_data_transform_factory",0,0,3,3
"def parse_task_declaration(self, declaration_subAST):
        
        var_name = self.parse_declaration_name(declaration_subAST.attr(""name""))
        var_type = self.parse_declaration_type(declaration_subAST.attr(""type""))
        var_expressn = self.parse_declaration_expressn(declaration_subAST.attr(""expression""), es='')

        return (var_name, var_type, var_expressn)","Parses the declaration section of the WDL task AST subtree.

        Examples:

        String my_name
        String your_name
        Int two_chains_i_mean_names = 0

        :param declaration_subAST: Some subAST representing a task declaration
                                   like: 'String file_name'
        :return: var_name, var_type, var_value
            Example:
                Input subAST representing:   'String file_name'
                Output:  var_name='file_name', var_type='String', var_value=None",0,0,1,1
"def parse_uri(self, raw_uri, recursive):
    
    
    if recursive:
      raw_uri = directory_fmt(raw_uri)
    
    
    file_provider = self.parse_file_provider(raw_uri)
    self._validate_paths_or_fail(raw_uri, recursive)
    uri, docker_uri = self.rewrite_uris(raw_uri, file_provider)
    uri_parts = job_model.UriParts(
        directory_fmt(os.path.dirname(uri)), os.path.basename(uri))
    return docker_uri, uri_parts, file_provider","Return a valid docker_path, uri, and file provider from a flag value.",0,0,1,1
"def parse_vector(x, dtype=None):
    

    if x[0] != '(' or x[-1] != ')':
        raise NRRDError('Vector should be enclosed by parentheses.')

    
    
    vector = np.array([float(x) for x in x[1:-1].split(',')])

    
    
    if dtype is None:
        vector_trunc = vector.astype(int)

        if np.all((vector - vector_trunc) == 0):
            vector = vector_trunc
    elif dtype == int:
        vector = vector.astype(int)
    elif dtype != float:
        raise NRRDError('dtype should be None for automatic type detection, float or int')

    return vector","Parse NRRD vector from string into (N,) :class:`numpy.ndarray`.

    See :ref:`user-guide:int vector` and :ref:`user-guide:double vector` for more information on the format.

    Parameters
    ----------
    x : :class:`str`
        String containing NRRD vector
    dtype : data-type, optional
        Datatype to use for the resulting Numpy array. Datatype can be :class:`float`, :class:`int` or :obj:`None`. If
        :obj:`dtype` is :obj:`None`, then it will be automatically determined by checking any of the vector elements
        for fractional numbers. If found, then the vector will be converted to :class:`float`, otherwise :class:`int`.
        Default is to automatically determine datatype.

    Returns
    -------
    vector : (N,) :class:`numpy.ndarray`
        Vector that is parsed from the :obj:`x` string",2,0,5,7
"def parse_workflow_messages(self):
        
        if 'client_message' in self.current.spec.data:
            m = self.current.spec.data['client_message']
            self.current.msg_box(title=m.get('title'),
                                 msg=m.get('body'),
                                 typ=m.get('type', 'info'))","Transmits client message that defined in
        a workflow task's inputOutput extension

       .. code-block:: xml

            <bpmn2:extensionElements>
            <camunda:inputOutput>
            <camunda:inputParameter name=""client_message"">
            <camunda:map>
              <camunda:entry key=""title"">Teşekkürler</camunda:entry>
              <camunda:entry key=""body"">İşlem Başarılı</camunda:entry>
              <camunda:entry key=""type"">info</camunda:entry>
            </camunda:map>
            </camunda:inputParameter>
            </camunda:inputOutput>
            </bpmn2:extensionElements>",0,1,0,1
"def part(self, *args, **kwargs):
        
        
        _parts = self.parts(*args, **kwargs)

        if len(_parts) == 0:
            raise NotFoundError(""No part fits criteria"")
        if len(_parts) != 1:
            raise MultipleFoundError(""Multiple parts fit criteria"")

        return _parts[0]","Retrieve single KE-chain part.

        Uses the same interface as the :func:`parts` method but returns only a single pykechain :class:`models.Part`
        instance.

        If additional `keyword=value` arguments are provided, these are added to the request parameters. Please
        refer to the documentation of the KE-chain API for additional query parameters.

        :return: a single :class:`models.Part`
        :raises NotFoundError: When no `Part` is found
        :raises MultipleFoundError: When more than a single `Part` is found",2,0,3,5
"def partial_fit(self, X, y=None, classes=None, **fit_params):
        
        if not self.initialized_:
            self.initialize()

        self.notify('on_train_begin', X=X, y=y)
        try:
            self.fit_loop(X, y, **fit_params)
        except KeyboardInterrupt:
            pass
        self.notify('on_train_end', X=X, y=y)
        return self","Fit the module.

        If the module is initialized, it is not re-initialized, which
        means that this method should be used if you want to continue
        training a model (warm start).

        Parameters
        ----------
        X : input data, compatible with skorch.dataset.Dataset
          By default, you should be able to pass:

            * numpy arrays
            * torch tensors
            * pandas DataFrame or Series
            * scipy sparse CSR matrices
            * a dictionary of the former three
            * a list/tuple of the former three
            * a Dataset

          If this doesn't work with your data, you have to pass a
          ``Dataset`` that can deal with the data.

        y : target data, compatible with skorch.dataset.Dataset
          The same data types as for ``X`` are supported. If your X is
          a Dataset that contains the target, ``y`` may be set to
          None.

        classes : array, sahpe (n_classes,)
          Solely for sklearn compatibility, currently unused.

        **fit_params : dict
          Additional parameters passed to the ``forward`` method of
          the module and to the ``self.train_split`` call.",0,0,3,3
"def password(self, request, uuid=None):
        
        user = self.get_object()

        serializer = serializers.PasswordSerializer(data=request.data)
        serializer.is_valid(raise_exception=True)

        new_password = serializer.validated_data['password']
        user.set_password(new_password)
        user.save()

        return Response({'detail': _('Password has been successfully updated.')},
                        status=status.HTTP_200_OK)","To change a user password, submit a **POST** request to the user's RPC URL, specifying new password
        by staff user or account owner.

        Password is expected to be at least 7 symbols long and contain at least one number
        and at least one lower or upper case.

        Example of a valid request:

        .. code-block:: http

            POST /api/users/e0c058d06864441fb4f1c40dee5dd4fd/password/ HTTP/1.1
            Content-Type: application/json
            Accept: application/json
            Authorization: Token c84d653b9ec92c6cbac41c706593e66f567a7fa4
            Host: example.com

            {
                ""password"": ""nQvqHzeP123"",
            }",0,1,1,2
"def patch(self, endpoint, data, headers=None, inception=False):
        
        if not headers:
            raise BackendException(BACKEND_ERROR, ""Header If-Match required for patching an object"")

        response = self.get_response(method='PATCH', endpoint=endpoint, json=data, headers=headers)

        if response.status_code == 200:
            return self.decode(response=response)

        if response.status_code == 412:
            
            if inception:
                
                resp = self.get(endpoint)
                headers = {'If-Match': resp['_etag']}
                return self.patch(endpoint, data=data, headers=headers, inception=False)

            raise BackendException(response.status_code, response.content)
        else:  
            raise BackendException(response.status_code, response.content)","Method to update an item

        The headers must include an If-Match containing the object _etag.
            headers = {'If-Match': contact_etag}

        The data dictionary contain the fields that must be modified.

        If the patching fails because the _etag object do not match with the provided one, a
        BackendException is raised with code = 412.

        If inception is True, this method makes e new get request on the endpoint to refresh the
        _etag and then a new patch is called.

        If an HTTP 412 error occurs, a BackendException is raised. This exception is:
        - code: 412
        - message: response content
        - response: backend response

        All other HTTP error raises a BackendException.
        If some _issues are provided by the backend, this exception is:
        - code: HTTP error code
        - message: response content
        - response: JSON encoded backend response (including '_issues' dictionary ...)

        If no _issues are provided and an _error is signaled by the backend, this exception is:
        - code: backend error code
        - message: backend error message
        - response: JSON encoded backend response

        :param endpoint: endpoint (API URL)
        :type endpoint: str
        :param data: properties of item to update
        :type data: dict
        :param headers: headers (example: Content-Type). 'If-Match' required
        :type headers: dict
        :param inception: if True tries to get the last _etag
        :type inception: bool
        :return: dictionary containing patch response from the backend
        :rtype: dict",3,2,4,9
"def patch_namespaced_custom_object_scale(self, group, version, namespace, plural, name, body, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.patch_namespaced_custom_object_scale_with_http_info(group, version, namespace, plural, name, body, **kwargs)  
        else:
            (data) = self.patch_namespaced_custom_object_scale_with_http_info(group, version, namespace, plural, name, body, **kwargs)  
            return data","patch_namespaced_custom_object_scale  # noqa: E501

        partially update scale of the specified namespace scoped custom object  # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.patch_namespaced_custom_object_scale(group, version, namespace, plural, name, body, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str group: the custom resource's group (required)
        :param str version: the custom resource's version (required)
        :param str namespace: The custom resource's namespace (required)
        :param str plural: the custom resource's plural name. For TPRs this would be lowercase plural kind. (required)
        :param str name: the custom object's name (required)
        :param UNKNOWN_BASE_TYPE body: (required)
        :return: object
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def patch_namespaced_pod_preset(self, name, namespace, body, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.patch_namespaced_pod_preset_with_http_info(name, namespace, body, **kwargs)  
        else:
            (data) = self.patch_namespaced_pod_preset_with_http_info(name, namespace, body, **kwargs)  
            return data","patch_namespaced_pod_preset  # noqa: E501

        partially update the specified PodPreset  # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.patch_namespaced_pod_preset(name, namespace, body, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the PodPreset (required)
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param UNKNOWN_BASE_TYPE body: (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed
        :return: V1alpha1PodPreset
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def patch_namespaced_resource_quota_status(self, name, namespace, body, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.patch_namespaced_resource_quota_status_with_http_info(name, namespace, body, **kwargs)  
        else:
            (data) = self.patch_namespaced_resource_quota_status_with_http_info(name, namespace, body, **kwargs)  
            return data","patch_namespaced_resource_quota_status  # noqa: E501

        partially update status of the specified ResourceQuota  # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.patch_namespaced_resource_quota_status(name, namespace, body, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the ResourceQuota (required)
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param UNKNOWN_BASE_TYPE body: (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed
        :return: V1ResourceQuota
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def paths_to_top(self, term):
        
        
        if term not in self:
            print(""Term %s not found!"" % term, file=sys.stderr)
            return

        def _paths_to_top_recursive(rec):
            if rec.level == 0:
                return [[rec]]
            paths = []
            for parent in rec.parents:
                top_paths = _paths_to_top_recursive(parent)
                for top_path in top_paths:
                    top_path.append(rec)
                    paths.append(top_path)
            return paths

        go_term = self[term]
        return _paths_to_top_recursive(go_term)","Returns all possible paths to the root node

            Each path includes the term given. The order of the path is
            top -> bottom, i.e. it starts with the root and ends with the
            given term (inclusively).

            Parameters:
            -----------
            - term:
                the id of the GO term, where the paths begin (i.e. the
                accession 'GO:0003682')

            Returns:
            --------
            - a list of lists of GO Terms",0,0,1,1
"def percent_initiated_conversations(records):
    
    interactions = defaultdict(list)
    for r in records:
        interactions[r.correspondent_id].append(r)

    def _percent_initiated(grouped):
        mapped = [(1 if conv[0].direction == 'out' else 0, 1)
                  for conv in _conversations(grouped)]
        return mapped

    all_couples = [sublist for i in interactions.values()
                   for sublist in _percent_initiated(i)]

    if len(all_couples) == 0:
        init, total = 0, 0
    else:
        init, total = list(map(sum, list(zip(*all_couples))))

    return init / total if total != 0 else 0","The percentage of conversations that have been initiated by the user.

    Each call and each text conversation is weighted as a single interaction.

    See :ref:`Using bandicoot <conversations-label>` for a definition of
    conversations.",0,0,1,1
"def perform_get(self, path, x_ms_version=None):
        
        request = HTTPRequest()
        request.method = 'GET'
        request.host = self.host
        request.path = path
        request.path, request.query = self._httpclient._update_request_uri_query(request)
        request.headers = self._update_management_header(request, x_ms_version)
        response = self._perform_request(request)

        return response","Performs a GET request and returns the response.

        path:
            Path to the resource.
            Ex: '/<subscription-id>/services/hostedservices/<service-name>'
        x_ms_version:
            If specified, this is used for the x-ms-version header.
            Otherwise, self.x_ms_version is used.",0,1,0,1
"def perform_proxy_validate(self, proxied_service_ticket, headers=None):
        
        url = self._get_proxy_validate_url(ticket=proxied_service_ticket)
        logging.debug('[CAS] ProxyValidate URL: {}'.format(url))
        return self._perform_cas_call(
            url,
            ticket=proxied_service_ticket,
            headers=headers,
            )",Fetch a response from the remote CAS `proxyValidate` endpoint.,0,2,0,2
"def pick_kwargs(kwargs: dict, required_args: list, prefix: str = None):
    
    picked = {k: v for k, v in kwargs.items() if k in required_args}

    prefixed = {}
    if prefix:
        prefix = prefix + '__'
        prefixed = {
            _remove_prefix(k, prefix): v
            for k, v in kwargs.items()
            if k.startswith(prefix)
            and _remove_prefix(k, prefix) in required_args
        }

    conflicting_args = [k for k in picked if k in prefixed]
    if conflicting_args:
        raise ValueError(
            'Both prefixed and unprefixed args were specified '
            'for the following parameters: {}'.format(conflicting_args)
        )

    return tz.merge(picked, prefixed)","Given a dict of kwargs and a list of required_args, return a dict
    containing only the args in required_args.

    If prefix is specified, also search for args that begin with '{prefix}__'.
    Removes prefix in returned dict.

    Raises ValueError if both prefixed and unprefixed arg are given in kwargs.

    >>> from pprint import pprint as p # Use pprint to sort dict keys
    >>> kwargs = {'a': 1, 'b': 2, 'c': 3, 'x__d': 4}
    >>> p(pick_kwargs(kwargs, ['a', 'd']))
    {'a': 1}
    >>> p(pick_kwargs(kwargs, ['a', 'd'], prefix='x'))
    {'a': 1, 'd': 4}

    >>> pick_kwargs({'a': 1, 'x__a': 2}, ['a'], prefix='x')
    Traceback (most recent call last):
    ValueError: Both prefixed and unprefixed args were specified for the
    following parameters: ['a']",1,0,4,5
"def pid(sig):
    

    cmd = __grains__['ps']
    output = __salt__['cmd.run_stdout'](cmd, python_shell=True)

    pids = ''
    for line in output.splitlines():
        if 'status.pid' in line:
            continue
        if re.search(sig, line):
            if pids:
                pids += '\n'
            pids += line.split()[1]

    return pids","Return the PID or an empty string if the process is running or not.
    Pass a signature to use to find the process via ps.  Note you can pass
    a Python-compatible regular expression to return all pids of
    processes matching the regexp.

    .. versionchanged:: 2016.11.4
        Added support for AIX

    CLI Example:

    .. code-block:: bash

        salt '*' status.pid <sig>",1,0,2,3
"def ping(daemon, channel, data=None):
    

    if not channel:
        
        return

    
    node_name = daemon.config['control'].get('node_name')

    
    reply = ['pong']
    if node_name or data:
        reply.append(node_name or '')
    if data:
        reply.append(data)

    
    with utils.ignore_except():
        daemon.db.publish(channel, ':'.join(reply))","Process the 'ping' control message.

    :param daemon: The control daemon; used to get at the
                   configuration and the database.
    :param channel: The publish channel to which to send the
                    response.
    :param data: Optional extra data.  Will be returned as the
                 second argument of the response.

    Responds to the named channel with a command of 'pong' and
    with the node_name (if configured) and provided data as
    arguments.",1,0,1,2
"def ping(self, peer, *peers, **kwargs):
        
        
        if ""count"" in kwargs:
            kwargs.setdefault(""opts"", {""count"": kwargs[""count""]})
            del kwargs[""count""]

        args = (peer,) + peers
        return self._client.request('/ping', args, decoder='json', **kwargs)","Provides round-trip latency information for the routing system.

        Finds nodes via the routing system, sends pings, waits for pongs,
        and prints out round-trip latency information.

        .. code-block:: python

            >>> c.ping(""QmTzQ1JRkWErjk39mryYw2WVaphAZNAREyMchXzYQ7c15n"")
            [{'Success': True,  'Time': 0,
              'Text': 'Looking up peer QmTzQ1JRkWErjk39mryYw2WVaphAZN … c15n'},
             {'Success': False, 'Time': 0,
              'Text': 'Peer lookup error: routing: not found'}]

        Parameters
        ----------
        peer : str
            ID of peer to be pinged
        count : int
            Number of ping messages to send (Default: ``10``)

        Returns
        -------
            list : Progress reports from the ping",0,1,0,1
"def playlist_song_add(
		self,
		song,
		playlist,
		*,
		after=None,
		before=None,
		index=None,
		position=None
	):
		

		prev, next_ = get_ple_prev_next(
			self.playlist_songs(playlist),
			after=after,
			before=before,
			index=index,
			position=position
		)

		if 'storeId' in song:
			song_id = song['storeId']
		elif 'trackId' in song:
			song_id = song['trackId']
		else:
			song_id = song['id']

		mutation = mc_calls.PlaylistEntriesBatch.create(
			song_id, playlist['id'],
			preceding_entry_id=prev.get('id'),
			following_entry_id=next_.get('id')
		)
		self._call(mc_calls.PlaylistEntriesBatch, mutation)

		return self.playlist(playlist['id'], include_songs=True)","Add a song to a playlist.

                Note:
                        * Provide no optional arguments to add to end.
                        * Provide playlist song dicts for ``after`` and/or ``before``.
                        * Provide a zero-based ``index``.
                        * Provide a one-based ``position``.

                        Songs are inserted *at* given index or position.
                        It's also possible to add to the end by using
                        ``len(songs)`` for index or ``len(songs) + 1`` for position.

                Parameters:
                        song (dict): A song dict.
                        playlist (dict): A playlist dict.
                        after (dict, Optional): A playlist song dict ``songs`` will follow.
                        before (dict, Optional): A playlist song dict ``songs`` will precede.
                        index (int, Optional): The zero-based index position to insert ``song``.
                        position (int, Optional): The one-based position to insert ``song``.

                Returns:
                        dict: Playlist dict including songs.",0,0,1,1
"def plot(name=None, fig=None, abscissa=1, iteridx=None,
         plot_mean=False,
         foffset=1e-19, x_opt=None, fontsize=9):
    
    global last_figure_number
    if not fig:
        last_figure_number += 1
        fig = last_figure_number
    if isinstance(fig, (int, float)):
        last_figure_number = fig
    CMADataLogger(name).plot(fig, abscissa, iteridx, plot_mean, foffset,
                             x_opt, fontsize)","plot data from files written by a `CMADataLogger`,
    the call ``cma.plot(name, **argsdict)`` is a shortcut for
    ``cma.CMADataLogger(name).plot(**argsdict)``

    Arguments
    ---------
        `name`
            name of the logger, filename prefix, None evaluates to
            the default 'outcmaes'
        `fig`
            filename or figure number, or both as a tuple (any order)
        `abscissa`
            0==plot versus iteration count,
            1==plot versus function evaluation number
        `iteridx`
            iteration indices to plot

    Return `None`

    Examples
    --------
    ::

       cma.plot();  # the optimization might be still
                    # running in a different shell
       cma.savefig('fig325.png')
       cma.closefig()

       cdl = cma.CMADataLogger().downsampling().plot()
       # in case the file sizes are large

    Details
    -------
    Data from codes in other languages (C, Java, Matlab, Scilab) have the same
    format and can be plotted just the same.

    :See: `CMADataLogger`, `CMADataLogger.plot()`",1,0,1,2
"def plot(self, freq=None, figsize=(15, 5), title=None,
             logy=False, **kwargs):
        

        if title is None:
            title = self._get_default_plot_title(
                freq, 'Equity Progression')

        ser = self._get_series(freq).rebase()
        return ser.plot(figsize=figsize, logy=logy,
                        title=title, **kwargs)","Helper function for plotting the series.

        Args:
            * freq (str): Data frequency used for display purposes.
                Refer to pandas docs for valid freq strings.
            * figsize ((x,y)): figure size
            * title (str): Title if default not appropriate
            * logy (bool): log-scale for y axis
            * kwargs: passed to pandas' plot method",0,0,2,2
"def plot_expected_repeat_purchases(
    model,
    title=""Expected Number of Repeat Purchases per Customer"",
    xlabel=""Time Since First Purchase"",
    ax=None,
    label=None,
    **kwargs
):
    
    from matplotlib import pyplot as plt

    if ax is None:
        ax = plt.subplot(111)

    if plt.matplotlib.__version__ >= ""1.5"":
        color_cycle = ax._get_lines.prop_cycler
        color = coalesce(kwargs.pop(""c"", None), kwargs.pop(""color"", None), next(color_cycle)[""color""])
    else:
        color_cycle = ax._get_lines.color_cycle
        color = coalesce(kwargs.pop(""c"", None), kwargs.pop(""color"", None), next(color_cycle))

    max_T = model.data[""T""].max()

    times = np.linspace(0, max_T, 100)
    ax.plot(times, model.expected_number_of_purchases_up_to_time(times), color=color, label=label, **kwargs)

    times = np.linspace(max_T, 1.5 * max_T, 100)
    ax.plot(times, model.expected_number_of_purchases_up_to_time(times), color=color, ls=""--"", **kwargs)

    plt.title(title)
    plt.xlabel(xlabel)
    plt.legend(loc=""lower right"")
    return ax","Plot expected repeat purchases on calibration period .

    Parameters
    ----------
    model: lifetimes model
        A fitted lifetimes model.
    max_frequency: int, optional
        The maximum frequency to plot.
    title: str, optional
        Figure title
    xlabel: str, optional
        Figure xlabel
    ax: matplotlib.AxesSubplot, optional
        Using user axes
    label: str, optional
        Label for plot.
    kwargs
        Passed into the matplotlib.pyplot.plot command.

    Returns
    -------
    axes: matplotlib.AxesSubplot",0,0,1,1
"def plot_frequencies(self, mindB=None, maxdB=None, norm=True):
        
        from pylab import plot, title, xlim, grid, ylim, xlabel, ylabel
        
        self.compute_response(norm=norm)

        plot(self.frequencies, self.response)
        title(""ENBW=%2.1f"" % (self.enbw))
        ylabel('Frequency response (dB)')
        xlabel('Fraction of sampling frequency')
        
        xlim(-0.5, 0.5)
        y0, y1 = ylim()
        if mindB:
            y0 = mindB
        if maxdB is not None:
            y1 = maxdB
        else:
            y1 = max(self.response)

        ylim(y0, y1)

        grid(True)","Plot the window in the frequency domain

        :param mindB: change the default lower y bound
        :param maxdB: change the default upper lower bound
        :param bool norm: if True, normalise the frequency response.

        .. plot::
            :width: 80%
            :include-source:

            from spectrum.window import Window
            w = Window(64, name='hamming')
            w.plot_frequencies()",1,0,3,4
"def plot_pseudosections(self, column, filename=None, return_fig=False):
        
        assert column in self.data.columns

        g = self.data.groupby('frequency')
        fig, axes = plt.subplots(
            4, 2,
            figsize=(15 / 2.54, 20 / 2.54),
            sharex=True, sharey=True
        )
        for ax, (key, item) in zip(axes.flat, g):
            fig, ax, cb = PS.plot_pseudosection_type2(
                item, ax=ax, column=column
            )
            ax.set_title('f: {} Hz'.format(key))
        fig.tight_layout()
        if filename is not None:
            fig.savefig(filename, dpi=300)

        if return_fig:
            return fig
        else:
            plt.close(fig)","Create a multi-plot with one pseudosection for each frequency.

        Parameters
        ----------
        column : string
            which column to plot
        filename : None|string
            output filename. If set to None, do not write to file. Default:
            None
        return_fig : bool
            if True, return the generated figure object. Default: False

        Returns
        -------
        fig : None|matplotlib.Figure
            if return_fig is set to True, return the generated Figure object",2,0,3,5
"def plot_vzx(self, colorbar=True, cb_orientation='vertical',
                 cb_label=None, ax=None, show=True, fname=None, **kwargs):
        
        if cb_label is None:
            cb_label = self._vzx_label

        if ax is None:
            fig, axes = self.vzx.plot(colorbar=colorbar,
                                      cb_orientation=cb_orientation,
                                      cb_label=cb_label, show=False, **kwargs)
            if show:
                fig.show()

            if fname is not None:
                fig.savefig(fname)
            return fig, axes

        else:
            self.vzx.plot(colorbar=colorbar, cb_orientation=cb_orientation,
                          cb_label=cb_label, ax=ax, **kwargs)","Plot the Vzx component of the tensor.

        Usage
        -----
        x.plot_vzx([tick_interval, xlabel, ylabel, ax, colorbar,
                    cb_orientation, cb_label, show, fname])

        Parameters
        ----------
        tick_interval : list or tuple, optional, default = [30, 30]
            Intervals to use when plotting the x and y ticks. If set to None,
            ticks will not be plotted.
        xlabel : str, optional, default = 'longitude'
            Label for the longitude axis.
        ylabel : str, optional, default = 'latitude'
            Label for the latitude axis.
        ax : matplotlib axes object, optional, default = None
            A single matplotlib axes object where the plot will appear.
        colorbar : bool, optional, default = True
            If True, plot a colorbar.
        cb_orientation : str, optional, default = 'vertical'
            Orientation of the colorbar: either 'vertical' or 'horizontal'.
        cb_label : str, optional, default = '$V_{zx}$'
            Text label for the colorbar.
        show : bool, optional, default = True
            If True, plot the image to the screen.
        fname : str, optional, default = None
            If present, and if axes is not specified, save the image to the
            specified file.
        kwargs : optional
            Keyword arguements that will be sent to the SHGrid.plot()
            and plt.imshow() methods.",2,0,2,4
"def poll(self, url, initial_delay=2, delay=1, tries=20, errors=STRICT, is_complete_callback=None, **params):
        
        time.sleep(initial_delay)
        poll_response = None

        if is_complete_callback == None:
            is_complete_callback = self._default_poll_callback

        for n in range(tries):
            poll_response = self.make_request(url, headers=self._headers(),
                                              errors=errors, **params)

            if is_complete_callback(poll_response):
                return poll_response
            else:
                time.sleep(delay)

        if STRICT == errors:
            raise ExceededRetries(
                ""Failed to poll within {0} tries."".format(tries))
        else:
            return poll_response","Poll the URL
        :param url - URL to poll, should be returned by 'create_session' call
        :param initial_delay - specifies how many seconds to wait before the first poll
        :param delay - specifies how many seconds to wait between the polls
        :param tries - number of polls to perform
        :param errors - errors handling mode, see corresponding parameter in 'make_request' method
        :param params - additional query params for each poll request",1,1,2,4
"def poll_crontab(self):
        
        polled_time = self._get_current_time()
        if polled_time.second >= 30:
            self.log.debug('Skip cronjobs in {}'.format(polled_time))
            return
        for job in self._crontab:
            if not job.is_runnable(polled_time):
                continue
            job.do_action(self, polled_time)",Check crontab and run target jobs,0,1,2,3
"def polling(self, system_code=0xffff, request_code=0, time_slots=0):
        

        log.debug(""polling for system 0x{0:04x}"".format(system_code))
        if time_slots not in (0, 1, 3, 7, 15):
            log.debug(""invalid number of time slots: {0}"".format(time_slots))
            raise ValueError(""invalid number of time slots"")
        if request_code not in (0, 1, 2):
            log.debug(""invalid request code value: {0}"".format(request_code))
            raise ValueError(""invalid request code for polling"")

        timeout = 0.003625 + time_slots * 0.001208
        data = pack("">HBB"", system_code, request_code, time_slots)
        data = self.send_cmd_recv_rsp(0x00, data, timeout, send_idm=False)
        if len(data) != (16 if request_code == 0 else 18):
            log.debug(""unexpected polling response length"")
            raise Type3TagCommandError(DATA_SIZE_ERROR)

        return (data[0:8], data[8:16]) if len(data) == 16 else \
            (data[0:8], data[8:16], data[16:18])","Aquire and identify a card.

        The Polling command is used to detect the Type 3 Tags in the
        field. It is also used for initialization and anti-collision.

        The *system_code* identifies the card system to acquire. A
        card can have multiple systems. The first system that matches
        *system_code* will be activated. A value of 0xff for any of
        the two bytes works as a wildcard, thus 0xffff activates the
        very first system in the card. The card identification data
        returned are the Manufacture ID (IDm) and Manufacture
        Parameter (PMm).

        The *request_code* tells the card whether it should return
        additional information. The default value 0 requests no
        additional information. Request code 1 means that the card
        shall also return the system code, so polling for system code
        0xffff with request code 1 can be used to identify the first
        system on the card. Request code 2 asks for communication
        performance data, more precisely a bitmap of possible
        communication speeds. Not all cards provide that information.

        The number of *time_slots* determines whether there's a chance
        to receive a response if multiple Type 3 Tags are in the
        field. For the reader the number of time slots determines the
        amount of time to wait for a response. Any Type 3 Tag in the
        field, i.e. powered by the field, will choose a random time
        slot to respond. With the default *time_slots* value 0 there
        will only be one time slot available for all responses and
        multiple responses would produce a collision. More time slots
        reduce the chance of collisions (but may result in an
        application working with a tag that was just accidentially
        close enough). Only specific values should be used for
        *time_slots*, those are 0, 1, 3, 7, and 15. Other values may
        produce unexpected results depending on the tag product.

        :meth:`polling` returns either the tuple (IDm, PMm) or the
        tuple (IDm, PMm, *additional information*) depending on the
        response lengt, all as bytearrays.

        Command execution errors raise :exc:`~nfc.tag.TagCommandError`.",2,3,2,7
"def pool_update(self, pool_id, name=None, description=None, post_ids=None,
                    is_active=None, category=None):
        
        params = {
            'pool[name]': name,
            'pool[description]': description,
            'pool[post_ids]': post_ids,
            'pool[is_active]': is_active,
            'pool[category]': category
            }
        return self._get('pools/{0}.json'.format(pool_id), params,
                         method='PUT', auth=True)","Update a pool (Requires login) (UNTESTED).

        Parameters:
            pool_id (int): Where pool_id is the pool id.
            name (str):
            description (str):
            post_ids (str): List of space delimited post ids.
            is_active (int): Can be: 1, 0.
            category (str): Can be: series, collection.",0,1,0,1
"def pop(self, key=util_const.NoParam, default=util_const.NoParam):
        
        
        if key is not util_const.NoParam:
            if default is util_const.NoParam:
                return (key, self._dict.pop(key))
            else:
                return (key, self._dict.pop(key, default))
        
        try:
            
            _heap = self._heap
            _dict = self._dict
            val, key = self._heappop(_heap)
            
            while key not in _dict or _dict[key] != val:
                val, key = self._heappop(_heap)
        except IndexError:
            if len(_heap) == 0:
                raise IndexError('queue is empty')
            else:
                raise
        del _dict[key]
        return key, val",Pop the next item off the queue,1,0,3,4
"def popitem(self):
        
        pq = self.pq
        while pq:
            priority, key, val = heapq.heappop(pq)
            if val is None:
                self.removed_count -= 1

            else:
                del self.item_finder[key]
                return key, val, priority

        raise KeyError(""pop from an empty priority queue"")","remove the next prioritized [key, val, priority] and return it",1,0,1,2
"def populate(self, obj):
        
        
        
        if type(obj) is AtlasServiceInstance.Instance:
            query = { ""instance_id"" : obj.instance_id, ""binding_id"" : { ""$exists"" : False } }
        elif type(obj) is AtlasServiceBinding.Binding:
            query = { ""binding_id"" : obj.binding_id, ""instance_id"" : obj.instance.instance_id }
        else:
            raise ErrStorageTypeUnsupported(type(obj))
        
        
        try:
            result = self.broker.find_one(query)
        except:
            raise ErrStorageMongoConnection(""Populate Instance or Binding"")
        
        if result is not None:
            obj.parameters = result[""parameters""]
            
            
            obj.provisioned = True
        else:
            
            obj.provisioned = False","Populate
        
        Query mongo to get information about the obj if it exists
        
        Args:
            obj (AtlasServiceBinding.Binding or AtlasServiceInstance.Instance): instance or binding
        
        Raises:
            ErrStorageTypeUnsupported: Type unsupported.
            ErrStorageMongoConnection: Error during MongoDB communication.",3,0,2,5
"def populate(self, source=DEFAULT_SEGMENT_SERVER, segments=None,
                 pad=True, **kwargs):
        
        tmp = DataQualityDict()
        tmp[self.name] = self
        tmp.populate(source=source, segments=segments, pad=pad, **kwargs)
        return tmp[self.name]","Query the segment database for this flag's active segments.

        This method assumes all of the metadata for each flag have been
        filled. Minimally, the following attributes must be filled

        .. autosummary::

           ~DataQualityFlag.name
           ~DataQualityFlag.known

        Segments will be fetched from the database, with any
        :attr:`~DataQualityFlag.padding` added on-the-fly.

        This `DataQualityFlag` will be modified in-place.

        Parameters
        ----------
        source : `str`
            source of segments for this flag. This must be
            either a URL for a segment database or a path to a file on disk.

        segments : `SegmentList`, optional
            a list of segments during which to query, if not given,
            existing known segments for this flag will be used.

        pad : `bool`, optional, default: `True`
            apply the `~DataQualityFlag.padding` associated with this
            flag, default: `True`.

        **kwargs
            any other keyword arguments to be passed to
            :meth:`DataQualityFlag.query` or :meth:`DataQualityFlag.read`.

        Returns
        -------
        self : `DataQualityFlag`
            a reference to this flag",1,0,1,2
"def populate_token_attributes(self, response):
        

        if 'access_token' in response:
            self.access_token = response.get('access_token')

        if 'refresh_token' in response:
            self.refresh_token = response.get('refresh_token')

        if 'token_type' in response:
            self.token_type = response.get('token_type')

        if 'expires_in' in response:
            self.expires_in = response.get('expires_in')
            self._expires_at = time.time() + int(self.expires_in)

        if 'expires_at' in response:
            self._expires_at = int(response.get('expires_at'))

        if 'mac_key' in response:
            self.mac_key = response.get('mac_key')

        if 'mac_algorithm' in response:
            self.mac_algorithm = response.get('mac_algorithm')",Add attributes from a token exchange response to self.,0,0,1,1
"def post(self, service, data):
        
        url = self._url_format(service)
        data = Base._data_to_json(data)
        
        headers = {'content-type': 'application/json'}
        return self.rest_action(self._session.post, url,
                                data=data, headers=headers)","Generic POST operation for sending data to Learning Modules API.

        Data should be a JSON string or a dict.  If it is not a string,
        it is turned into a JSON string for the POST body.

        Args:
            service (str): The endpoint service to use, i.e. gradebook
            data (json or dict): the data payload

        Raises:
            requests.RequestException: Exception connection error
            ValueError: Unable to decode response content

        Returns:
            list: the json-encoded content of the response",0,1,0,1
"def post_has_mime_parts(request, parts):
    
    missing = []

    for part_type, part_name in parts:
        if part_type == 'header':
            if 'HTTP_' + part_name.upper() not in request.META:
                missing.append('{}: {}'.format(part_type, part_name))
        elif part_type == 'file':
            if part_name not in list(request.FILES.keys()):
                missing.append('{}: {}'.format(part_type, part_name))
        elif part_type == 'field':
            if part_name not in list(request.POST.keys()):
                missing.append('{}: {}'.format(part_type, part_name))
        else:
            raise d1_common.types.exceptions.ServiceFailure(
                0, 'Invalid part_type. part_type=""{}""'.format(part_type)
            )

    if len(missing) > 0:
        raise d1_common.types.exceptions.InvalidRequest(
            0,
            'Missing part(s) in MIME Multipart document. missing=""{}""'.format(
                ', '.join(missing)
            ),
        )","Validate that a MMP POST contains all required sections.

    :param request: Django Request
    :param parts: [(part_type, part_name), ...]
    :return: None or raises exception.

    Where information is stored in the request:
    part_type header: request.META['HTTP_<UPPER CASE NAME>']
    part_type file: request.FILES['<name>']
    part_type field: request.POST['<name>']",2,0,3,5
"def post_multipart(self, url, params, files):
        
        resp = requests.post(
            url,
            data=params,
            params=params,
            files=files,
            headers=self.headers,
            allow_redirects=False,
            auth=self.oauth
        )
        return self.json_parse(resp)","Generates and issues a multipart request for data files

        :param url: a string, the url you are requesting
        :param params: a dict, a key-value of all the parameters
        :param files:  a dict, matching the form '{name: file descriptor}'

        :returns: a dict parsed from the JSON response",0,1,1,2
"def post_save_update_cache(sender, instance, created, raw, **kwargs):
    
    if raw:
        return
    name = sender.__name__
    if name in cached_model_names:
        delay_cache = getattr(instance, '_delay_cache', False)
        if not delay_cache:
            from .tasks import update_cache_for_instance
            update_cache_for_instance(name, instance.pk, instance)",Update the cache when an instance is created or modified.,1,0,0,1
"def pots(self, refresh=False):
        
        if not refresh and self._cached_pots:
            return self._cached_pots

        endpoint = '/pots/listV1'
        response = self._get_response(
            method='get', endpoint=endpoint,
        )

        pots_json = response.json()['pots']
        pots = [MonzoPot(data=pot) for pot in pots_json]
        self._cached_pots = pots

        return pots","Returns a list of pots owned by the currently authorised user.

        Official docs:
            https://monzo.com/docs/#pots

        :param refresh: decides if the pots information should be refreshed.
        :type refresh: bool
        :returns: list of Monzo pots
        :rtype: list of MonzoPot",0,1,2,3
"def precision_score(df, col_true=None, col_pred='precision_result', pos_label=1, average=None):
    r
    if not col_pred:
        col_pred = get_field_name_by_role(df, FieldRole.PREDICTED_CLASS)
    mat, label_list = _run_cm_node(df, col_true, col_pred)
    class_dict = dict((label, idx) for idx, label in enumerate(label_list))
    tps = np.diag(mat)
    pred_count = np.sum(mat, axis=0)
    if average is None:
        return tps * 1.0 / pred_count
    elif average == 'binary':
        class_idx = class_dict[pos_label]
        return tps[class_idx] * 1.0 / pred_count[class_idx]
    elif average == 'micro':
        return np.sum(tps) / np.sum(pred_count)
    elif average == 'macro':
        return np.mean(tps * 1.0 / pred_count)
    elif average == 'weighted':
        support = np.sum(mat, axis=1)
        return np.sum(tps * 1.0 / pred_count * support) / np.sum(support)","r""""""
    Compute precision of a predicted DataFrame. Precision is defined as :math:`\frac{TP}{TP + TN}`

    :Parameters:
        - **df** - predicted data frame
        - **col_true** - column name of true label
        - **col_pred** - column name of predicted label, 'prediction_result' by default.
        - **pos_label** - denote the desired class label when ``average`` == `binary`
        - **average** - denote the method to compute average.
    :Returns:
        Precision score
    :Return type:
        float or numpy.array[float]

    The parameter ``average`` controls the behavior of the function.

    - When ``average`` == None (by default), precision of every class is given as a list.

    - When ``average`` == 'binary', precision of class specified in ``pos_label`` is given.

    - When ``average`` == 'micro', STP / (STP + STN) is given, where STP and STN are summations of TP and TN for every class.

    - When ``average`` == 'macro', average precision of all the class is given.

    - When ``average`` == `weighted`, average precision of all the class weighted by support of every true classes is given.

    :Example:

    Assume we have a table named 'predicted' as follows:

    ======== ===================
    label    prediction_result
    ======== ===================
    0        0
    1        2
    2        1
    0        0
    1        0
    2        1
    ======== ===================

    Different options of ``average`` parameter outputs different values:

.. code-block:: python

    >>> precision_score(predicted, 'label', average=None)
    array([ 0.66...,  0.        ,  0.        ])
    >>> precision_score(predicted, 'label', average='macro')
    0.22
    >>> precision_score(predicted, 'label', average='micro')
    0.33
    >>> precision_score(predicted, 'label', average='weighted')
    0.22",0,0,6,6
"def predict(self, **kwargs):
        
        reformatted_predict = self.reformat_predict_data()
        results = {}
        for task_inst in self.trained_tasks:
            predict = reformatted_predict[task_inst.data_format]['predict']
            kwargs['predict']=predict
            results.update({get_task_name(task_inst) : self.execute_predict_task(task_inst, predict, **kwargs)})
        return results","Do the workflow prediction (done after training, with new data)",0,0,1,1
"def predict(self, data_view_id, candidates, method=""scalar"", use_prior=True):
        

        uid = self.submit_predict_request(data_view_id, candidates, method, use_prior)

        while self.check_predict_status(data_view_id, uid)['status'] not in [""Finished"", ""Failed"", ""Killed""]:
            time.sleep(1)

        result = self.check_predict_status(data_view_id, uid)
        if result[""status""] == ""Finished"":

            paired = zip(result[""results""][""candidates""], result[""results""][""loss""])
            prediction_result_format = [{k: (p[0][k], p[1][k]) for k in p[0].keys()} for p in paired]

            return list(map(
                lambda c: _get_prediction_result_from_candidate(c), prediction_result_format
            ))
        else:
            raise RuntimeError(
                ""Prediction failed: UID={}, result={}"".format(uid, result[""status""])
            )","Predict endpoint. This simply wraps the async methods (submit and poll for status/results).

        :param data_view_id: The ID of the data view to use for prediction
        :type data_view_id: str
        :param candidates: A list of candidates to make predictions on
        :type candidates: list of dicts
        :param method: Method for propagating predictions through model graphs. ""scalar"" uses linearized uncertainty
        propagation, whereas ""scalar_from_distribution"" still returns scalar predictions but uses sampling to
        propagate uncertainty without a linear approximation.
        :type method: str (""scalar"" or ""scalar_from_distribution"")
        :param use_prior:  Whether to apply prior values implied by the property descriptors
        :type use_prior: bool
        :return: The results of the prediction
        :rtype: list of :class:`PredictionResult`",1,1,2,4
"def predict(self, input):
        

        assert self._mode == MpsGraphMode.Inference
        assert input.shape == self._ishape

        input_array = MpsFloatArray(input)
        result_handle = _ctypes.c_void_p()
        status_code = self._LIB.TCMPSPredictGraph(
            self.handle, input_array.handle, _ctypes.byref(result_handle))

        assert status_code == 0, ""Error calling TCMPSPredictGraph""
        assert result_handle, ""TCMPSPredictGraph unexpectedly returned NULL pointer""

        result = MpsFloatArray(result_handle)
        assert result.shape() == self._oshape

        return result","Submits an input batch to the model. Returns a MpsFloatArray
        representing the model predictions. Calling asnumpy() on this value will
        wait for the batch to finish and yield the predictions as a numpy array.",0,0,4,4
"def predict_ipcw(self, y):
        
        event, time = check_y_survival(y)
        Ghat = self.predict_proba(time[event])

        if (Ghat == 0.0).any():
            raise ValueError(""censoring survival function is zero at one or more time points"")

        weights = numpy.zeros(time.shape[0])
        weights[event] = 1.0 / Ghat

        return weights","Return inverse probability of censoring weights at given time points.

        :math:`\\omega_i = \\delta_i / \\hat{G}(y_i)`

        Parameters
        ----------
        y : structured array, shape = (n_samples,)
            A structured array containing the binary event indicator
            as first field, and time of event or time of censoring as
            second field.

        Returns
        -------
        ipcw : array, shape = (n_samples,)
            Inverse probability of censoring weights.",1,0,2,3
"def prepare(self):
        
        if self.depends is not None:
            for d in self.depends:
                if d.missing():
                    msg = ""Explicit dependency `%s' not found, needed by target `%s'.""
                    raise SCons.Errors.StopError(msg % (d, self))
        if self.implicit is not None:
            for i in self.implicit:
                if i.missing():
                    msg = ""Implicit dependency `%s' not found, needed by target `%s'.""
                    raise SCons.Errors.StopError(msg % (i, self))
        self.binfo = self.get_binfo()","Prepare for this Node to be built.

        This is called after the Taskmaster has decided that the Node
        is out-of-date and must be rebuilt, but before actually calling
        the method to build the Node.

        This default implementation checks that explicit or implicit
        dependencies either exist or are derived, and initializes the
        BuildInfo structure that will hold the information about how
        this node is, uh, built.

        (The existence of source files is checked separately by the
        Executor, which aggregates checks for all of the targets built
        by a specific action.)

        Overriding this method allows for for a Node subclass to remove
        the underlying file from the file system.  Note that subclass
        methods should call this base class method to get the child
        check and the BuildInfo structure.",2,0,4,6
"def prepare_migration_target(self, uuid, nics=None, port=None, tags=None):
        

        if nics is None:
            nics = []

        args = {
            'nics': nics,
            'port': port,
            'uuid': uuid
        }
        self._migrate_network_chk.check(args)

        self._client.sync('kvm.prepare_migration_target', args, tags=tags)",":param name: Name of the kvm domain that will be migrated
        :param port: A dict of host_port: container_port pairs
                       Example:
                        `port={8080: 80, 7000:7000}`
                     Only supported if default network is used
        :param nics: Configure the attached nics to the container
                     each nic object is a dict of the format
                     {
                        'type': nic_type # default, bridge, vlan, or vxlan (note, vlan and vxlan only supported by ovs)
                        'id': id # depends on the type, bridge name (bridge type) zerotier network id (zertier type), the vlan tag or the vxlan id
                     }
        :param uuid: uuid of machine to be migrated on old node
        :return:",0,1,1,2
"def prepare_notebook_metadata(nb, input_path, output_path, report_mode=False):
    
    
    nb = copy.deepcopy(nb)

    
    if report_mode:
        for cell in nb.cells:
            if cell.cell_type == 'code':
                cell.metadata['jupyter'] = cell.get('jupyter', {})
                cell.metadata['jupyter']['source_hidden'] = True

    
    nb.metadata.papermill['input_path'] = input_path
    nb.metadata.papermill['output_path'] = output_path

    return nb","Prepare metadata associated with a notebook and its cells

    Parameters
    ----------
    nb : NotebookNode
       Executable notebook object
    input_path : str
        Path to input notebook
    output_path : str
       Path to write executed notebook
    report_mode : bool, optional
       Flag to set report mode",0,0,3,3
"def preprocess(df):
    

    def process_row(idx_row):
        idx, row = idx_row

        
        tau = 1000 * row[4::3].astype(float).values
        tau = np.r_[np.median(tau), tau]

        
        duration = 1000 * row[3::3].astype(float).values

        keyname = list('.tie5Roanl') + ['enter']

        return pd.DataFrame.from_items([
            ('user', [row['subject']] * 11),
            ('session', [row['sessionIndex'] * 100 + row['rep']] * 11),
            ('tau', tau),
            ('duration', duration),
            ('event', keyname)
        ])

    df = pd.concat(map(process_row, df.iterrows())).set_index(['user', 'session'])
    return df",Convert the CMU dataset from row vectors into time/duration row observations,0,0,1,1
"def preprocess_notebook(ntbk, cr):
    

    
    for n in range(len(ntbk['cells'])):
        
        if ntbk['cells'][n]['cell_type'] == 'markdown':
            
            txt = ntbk['cells'][n]['source']
            
            txt = cr.substitute_url_with_ref(txt)
            
            ntbk['cells'][n]['source'] = txt","Process notebook object `ntbk` in preparation for conversion to an
    rst document.  This processing replaces links to online docs with
    corresponding sphinx cross-references within the local docs.
    Parameter `cr` is a CrossReferenceLookup object.",0,0,1,1
"def printDuplicatedTPEDandTFAM(tped, tfamFileName, outPrefix):
    
    
    try:
        shutil.copy(tfamFileName, outPrefix + "".duplicated_snps.tfam"")
    except IOError:
        msg = ""%(tfamFileName)s: can't copy file to "" \
              ""%(outPrefix)s.duplicated_snps.tfam"" % locals()
        raise ProgramError(msg)

    
    tpedFile = None
    try:
        tpedFile = open(outPrefix + "".duplicated_snps.tped"", ""w"")
    except IOError:
        msg = ""%(outPrefix)s.duplicated_snps.tped: can't write "" \
              ""file"" % locals()
        raise ProgramError(msg)
    for row in tped:
        print >>tpedFile, ""\t"".join(row)
    tpedFile.close()","Print the duplicated SNPs TPED and TFAM.

    :param tped: a representation of the ``tped`` of duplicated markers.
    :param tfamFileName: the name of the original ``tfam`` file.
    :param outPrefix: the output prefix.

    :type tped: numpy.array
    :type tfamFileName: str
    :type outPrefix: str

    First, it copies the original ``tfam`` into
    ``outPrefix.duplicated_snps.tfam``. Then, it prints the ``tped`` of
    duplicated markers in ``outPrefix.duplicated_snps.tped``.",2,0,4,6
"def print_details(srv):
    
    name = srv.service_type
    box = ""="" * 79
    print(""{0}\n|{1:^77}|\n{0}\n"".format(box, name))
    for action in srv.iter_actions():
        print(action.name)
        print(""~"" * len(action.name))
        print(""\n  Input"")
        for arg in action.in_args:
            print(""    "", arg)
        print(""\n  Output"")
        for arg in action.out_args:
            print(""    "", arg)

        print(""\n\n"")",Print the details of a service,1,0,0,1
"def print_node(self, name):  
        r
        if self._validate_node_name(name):
            raise RuntimeError(""Argument `name` is not valid"")
        self._node_in_tree(name)
        node = self._db[name]
        children = (
            [self._split_node_name(child)[-1] for child in node[""children""]]
            if node[""children""]
            else node[""children""]
        )
        data = (
            node[""data""][0]
            if node[""data""] and (len(node[""data""]) == 1)
            else node[""data""]
        )
        return (
            ""Name: {node_name}\n""
            ""Parent: {parent_name}\n""
            ""Children: {children_list}\n""
            ""Data: {node_data}"".format(
                node_name=name,
                parent_name=node[""parent""] if node[""parent""] else None,
                children_list="", "".join(children) if children else None,
                node_data=data if data else None,
            )
        )","r""""""
        Print node information (parent, children and data).

        :param name: Node name
        :type  name: :ref:`NodeName`

        :raises:
         * RuntimeError (Argument \`name\` is not valid)

         * RuntimeError (Node *[name]* not in tree)

        Using the same example tree created in
        :py:meth:`ptrie.Trie.add_nodes`::

            >>> from __future__ import print_function
            >>> import docs.support.ptrie_example
            >>> tobj = docs.support.ptrie_example.create_tree()
            >>> print(tobj)
            root
            ├branch1 (*)
            │├leaf1
            ││└subleaf1 (*)
            │└leaf2 (*)
            │ └subleaf2
            └branch2
            >>> print(tobj.print_node('root.branch1'))
            Name: root.branch1
            Parent: root
            Children: leaf1, leaf2
            Data: [5, 7]",2,0,1,3
"def print_response(self, input='', keep=False, *args, **kwargs):
        
        cookie = kwargs.get('cookie')
        if cookie is None:
            cookie = self.cookie or ''
        status = kwargs.get('status')
        lines = input.splitlines()
        if status and not lines:
            lines = ['']

        if cookie:
            output_template = '{cookie} {status}{cookie_char}{line}'
        else:
            output_template = '{line}'

        for i, line in enumerate(lines):
            if i != len(lines) - 1 or keep is True:
                cookie_char = '>'
            else:
                
                cookie_char = ':'

            print(output_template.format(
                cookie_char=cookie_char,
                cookie=cookie,
                status=status or '',
                line=line.strip()), file=self.stdout)","print response, if cookie is set then print that each line
        :param args:
        :param keep: if True more output is to come
        :param cookie: set a custom cookie,
                       if set to 'None' then self.cookie will be used.
                       if set to 'False' disables cookie output entirely
        :return:",0,0,6,6
"def print_result(overview, *names):
    
    if names:
        for name in names:
            toprint = overview
            for part in name.split('/'):
                toprint = toprint[part]
            print(json.dumps(toprint, indent=4, separators=(',', ': ')))
    else:
        print(json.dumps(overview, indent=4, separators=(',', ': ')))",Print the result of a verisure request,0,0,1,1
"def print_result_for_plain_cgi_script(contenttype: str,
                                      headers: TYPE_WSGI_RESPONSE_HEADERS,
                                      content: bytes,
                                      status: str = '200 OK') -> None:
    
    headers = [
        (""Status"", status),
        (""Content-Type"", contenttype),
        (""Content-Length"", str(len(content))),
    ] + headers
    sys.stdout.write(""\n"".join([h[0] + "": "" + h[1] for h in headers]) + ""\n\n"")
    sys.stdout.write(content)",Writes HTTP request result to stdout.,1,0,0,1
"def prioritize(self, item, force=False):
        
        with self.condition:
            
            
            if item in self.working or item in self.force:
                return
            self.queue.remove(item)
            if force:
                self.force.append(item)
            else:
                self.queue.appendleft(item)
            self.condition.notify_all()",Moves the item to the very left of the queue.,0,0,1,1
"def process(self, ast):  
        
        id_classifier = IdentifierClassifier()
        attached_ast = id_classifier.attach_identifier_attributes(ast)

        
        self._scope_tree_builder.enter_new_scope(ScopeVisibility.SCRIPT_LOCAL)

        traverse(attached_ast,
                 on_enter=self._enter_handler,
                 on_leave=self._leave_handler)

        self.scope_tree = self._scope_tree_builder.get_global_scope()
        self.link_registry = self._scope_tree_builder.link_registry","Build a scope tree and links between scopes and identifiers by the
        specified ast. You can access the built scope tree and the built links
        by .scope_tree and .link_registry.",0,0,3,3
"def process(self, data, **kwargs):
        
        data = super(RequestIdProcessor, self).process(data, **kwargs)
        if g and hasattr(g, 'request_id'):
            tags = data.get('tags', {})
            tags['request_id'] = g.request_id
            data['tags'] = tags
        return data",Process event data.,0,0,1,1
"def process_args(mod_id, args, type_args):
    
    res = list(args)
    if len(args) > len(type_args):
        raise ValueError(
            'Too many arguments specified for module ""{}"" (Got {}, expected '
            '{})'.format(mod_id, len(args), len(type_args))
        )
    for i in range(len(args), len(type_args)):
        arg_info = type_args[i]
        if ""default"" in arg_info:
            args.append(arg_info[""default""])
        else:
            raise ValueError(
                'Not enough module arguments supplied for module ""{}"" (Got '
                '{}, expecting {})'.format(
                    mod_id, len(args), len(type_args)
                )
            )
    return args","Takes as input a list of arguments defined on a module and the information
    about the required arguments defined on the corresponding module type.
    Validates that the number of supplied arguments is valid and fills any
    missing arguments with their default values from the module type",2,0,4,6
"def process_header(self, data):
        
        metadata = {
            ""datacolumns"": data.read_chunk(""I""),
            ""firstyear"": data.read_chunk(""I""),
            ""lastyear"": data.read_chunk(""I""),
            ""annualsteps"": data.read_chunk(""I""),
        }
        if metadata[""annualsteps""] != 1:
            raise InvalidTemporalResError(
                ""{}: Only annual files can currently be processed"".format(self.filepath)
            )

        return metadata","Reads the first part of the file to get some essential metadata

        # Returns
        return (dict): the metadata in the header",1,0,1,2
"def process_post_media_attachments(self, bulk_mode, api_post, post_media_attachments):
        
        post_media_attachments[api_post[""ID""]] = []

        for api_attachment in six.itervalues(api_post[""attachments""]):
            attachment = self.process_post_media_attachment(bulk_mode, api_attachment)
            if attachment:
                post_media_attachments[api_post[""ID""]].append(attachment)","Create or update Media objects related to a post.

        :param bulk_mode: If True, minimize db operations by bulk creating post objects
        :param api_post: the API data for the Post
        :param post_media_attachments: a mapping of Media objects keyed by post ID
        :return: None",0,1,0,1
"def process_raw_data(cls, raw_data):
        
        properties = raw_data[""properties""]

        routes = []
        raw_routes = properties.get(""routes"", [])
        for raw_route in raw_routes:
            raw_route[""parentResourceID""] = raw_data[""resourceId""]
            routes.append(Routes.from_raw_data(raw_route))
        properties[""routes""] = routes

        subnets = []
        raw_subnets = properties.get(""subnets"", [])
        for raw_subnet in raw_subnets:
            subnets.append(Resource.from_raw_data(raw_subnet))
        properties[""subnets""] = subnets

        return super(RouteTables, cls).process_raw_data(raw_data)",Create a new model using raw API response.,0,0,1,1
"def process_request(self, request, client_address):
        
        self.collect_children()
        pid = os.fork()  
        if pid:
            
            if self.active_children is None:
                self.active_children = []
            self.active_children.append(pid)
            self.close_request(request) 
            return
        else:
            
            
            try:
                self.finish_request(request, client_address)
                self.shutdown_request(request)
                os._exit(0)
            except:
                try:
                    self.handle_error(request, client_address)
                    self.shutdown_request(request)
                finally:
                    os._exit(1)",Fork a new subprocess to process the request.,0,0,2,2
"def process_sub(ref, alt_str):
    
    if len(ref) == len(alt_str):
        if len(ref) == 1:
            return record.Substitution(record.SNV, alt_str)
        else:
            return record.Substitution(record.MNV, alt_str)
    elif len(ref) > len(alt_str):
        return process_sub_grow(ref, alt_str)
    else:  
        return process_sub_shrink(ref, alt_str)",Process substitution,0,0,1,1
"def process_subprotocol(
        headers: Headers, available_subprotocols: Optional[Sequence[Subprotocol]]
    ) -> Optional[Subprotocol]:
        
        subprotocol: Optional[Subprotocol] = None

        header_values = headers.get_all(""Sec-WebSocket-Protocol"")

        if header_values:

            if available_subprotocols is None:
                raise InvalidHandshake(""No subprotocols supported"")

            parsed_header_values: Sequence[Subprotocol] = sum(
                [parse_subprotocol(header_value) for header_value in header_values], []
            )

            if len(parsed_header_values) > 1:
                subprotocols = "", "".join(parsed_header_values)
                raise InvalidHandshake(f""Multiple subprotocols: {subprotocols}"")

            subprotocol = parsed_header_values[0]

            if subprotocol not in available_subprotocols:
                raise NegotiationError(f""Unsupported subprotocol: {subprotocol}"")

        return subprotocol","Handle the Sec-WebSocket-Protocol HTTP response header.

        Check that it contains exactly one supported subprotocol.

        Return the selected subprotocol.",3,0,4,7
"def program_pixel_reg(self, enable_receiver=True):
        

        self._clear_strobes()

        
        self['PIXEL_RX'].set_en(enable_receiver)

        px_size = len(self['PIXEL_REG'][:])  
        self['SEQ']['SHIFT_IN'][0:px_size] = self['PIXEL_REG'][:]  
        self['SEQ']['PIXEL_SHIFT_EN'][0:px_size] = bitarray(px_size * '1')  

        self._run_seq(px_size + 1)","Send the pixel register to the chip and store the output.

        Loads the values of self['PIXEL_REG'] onto the chip.
        Includes enabling the clock, and loading the Control (CTR)
        and DAC shadow registers.

        if(enable_receiver), stores the output (by byte) in
        self['DATA'], retrievable via `chip['DATA'].get_data()`.",1,0,1,2
"def progress(self):
        
        now = time()
        if now - self.last_progress_t > 1:
            self.last_progress_t = now
            if self.status:
                self.uplink.status(self.status)
                self.status = None
            self.__spill_counters()
            self.uplink.progress(self.progress_value)
            self.uplink.flush()","\
        Report progress to the Java side.

        This needs to flush the uplink stream, but too many flushes can
        disrupt performance, so we actually talk to upstream once per second.",0,1,1,2
"def provider(self, service):
        
        service_provider = default_provider.get_service(str(service.provider.name.name))

        
        module_name = 'th_' + service.provider.name.name.split('Service')[1].lower()
        kwargs = {'trigger_id': str(service.id), 'cache_stack': module_name}
        return getattr(service_provider, 'process_data')(**kwargs)","get the data from (the cache of) the service provider
            :param service:
            :return: data",0,1,0,1
"def proximal_arg_scaling(prox_factory, scaling):
    r

    
    
    
    
    
    
    
    
    
    if np.isscalar(scaling):
        if scaling == 0:
            return proximal_const_func(prox_factory(1.0).domain)
        elif scaling.imag != 0:
            raise ValueError(""Complex scaling not supported."")
        else:
            scaling = float(scaling.real)
    else:
        scaling = np.asarray(scaling)

    def arg_scaling_prox_factory(sigma):
        
        scaling_square = scaling * scaling
        prox = prox_factory(sigma * scaling_square)
        space = prox.domain
        mult_inner = MultiplyOperator(scaling, domain=space, range=space)
        mult_outer = MultiplyOperator(1 / scaling, domain=space, range=space)
        return mult_outer * prox * mult_inner

    return arg_scaling_prox_factory","r""""""Calculate the proximal of function F(x * scaling).

    Parameters
    ----------
    prox_factory : callable
        A factory function that, when called with a step size, returns the
        proximal operator of ``F``
    scaling : float or sequence of floats or space element
        Scaling parameter. The permissible types depent on the stepsizes
        accepted by prox_factory. It may not contain any nonzero imaginary
        parts. If it is a scalar, it may be zero, in which case the
        resulting proxmial operator is the identity. If not a scalar,
        it may not contain any zero components.

    Returns
    -------
    prox_factory : function
        Factory for the proximal operator to be initialized

    Notes
    -----
    Given a functional :math:`F`, and scaling factor :math:`\alpha` this is
    calculated according to the rule

    .. math::
        \mathrm{prox}_{\sigma F(\alpha \, \cdot)}(x) =
        \frac{1}{\alpha}
        \mathrm{prox}_{\sigma \alpha^2 F(\cdot) }(\alpha x)

    where :math:`\sigma` is the step size.

    For reference on the identity used, see [CP2011c].

    References
    ----------
    [CP2011c] Combettes, P L, and Pesquet, J-C. *Proximal splitting
    methods in signal processing.* In:  Bauschke, H H, Burachik, R S,
    Combettes, P L, Elser, V, Luke, D R, and Wolkowicz, H. Fixed-point
    algorithms for inverse problems in science and engineering, Springer,
    2011.",1,0,2,3
"def prune(tdocs):
    
    all_terms = set([t for toks in tdocs for t in toks])
    terms = set()
    phrases = set()
    for t in all_terms:
        if gram_size(t) > 1:
            phrases.add(t)
        else:
            terms.add(t)

    
    redundant = set()
    for t in terms:
        if any(t in ph for ph in phrases):
            redundant.add(t)

    
    
    
    cleared = set()
    for t in redundant:
        if any(check_term(d, term=t) for d in tdocs):
            cleared.add(t)

    redundant = redundant.difference(cleared)

    pruned_tdocs = []
    for doc in tdocs:
        pruned_tdocs.append([t for t in doc if t not in redundant])

    return pruned_tdocs","Prune terms which are totally subsumed by a phrase

    This could be better if it just removes the individual keywords
    that occur in a phrase for each time that phrase occurs.",0,0,1,1
"def prune_clade(self, node_id):
        
        to_del_nodes = [node_id]
        while bool(to_del_nodes):
            node_id = to_del_nodes.pop(0)
            self._flag_node_as_del_and_del_in_by_target(node_id)
            ebsd = self._edge_by_source.get(node_id)
            if ebsd is not None:
                child_edges = list(ebsd.values())
                to_del_nodes.extend([i['@target'] for i in child_edges])
                del self._edge_by_source[
                    node_id]","Prune `node_id` and the edges and nodes that are tipward of it.
        Caller must delete the edge to node_id.",0,0,2,2
"def publish(func_or_workspace_id, workspace_id_or_token = None, workspace_token_or_none = None, files=(), endpoint=None):
    
    if not callable(func_or_workspace_id):
        def do_publish(func):
            func.service = _publish_worker(func, files, func_or_workspace_id, workspace_id_or_token, endpoint)
            return func
        return do_publish

    return _publish_worker(func_or_workspace_id, files, workspace_id_or_token, workspace_token_or_none, endpoint)","publishes a callable function or decorates a function to be published.  

Returns a callable, iterable object.  Calling the object will invoke the published service.
Iterating the object will give the API URL, API key, and API help url.
    
To define a function which will be published to Azure you can simply decorate it with
the @publish decorator.  This will publish the service, and then future calls to the
function will run against the operationalized version of the service in the cloud.

>>> @publish(workspace_id, workspace_token)
>>> def func(a, b): 
>>>    return a + b

After publishing you can then invoke the function using:
func.service(1, 2)

Or continue to invoke the function locally:
func(1, 2)

You can also just call publish directly to publish a function:

>>> def func(a, b): return a + b
>>> 
>>> res = publish(func, workspace_id, workspace_token)
>>> 
>>> url, api_key, help_url = res
>>> res(2, 3)
5
>>> url, api_key, help_url = res.url, res.api_key, res.help_url

The returned result will be the published service.

You can specify a list of files which should be published along with the function.
The resulting files will be stored in a subdirectory called 'Script Bundle'.  The
list of files can be one of:
    (('file1.txt', None), )                      # file is read from disk
    (('file1.txt', b'contents'), )              # file contents are provided
    ('file1.txt', 'file2.txt')                  # files are read from disk, written with same filename
    ((('file1.txt', 'destname.txt'), None), )   # file is read from disk, written with different destination name

The various formats for each filename can be freely mixed and matched.",0,1,2,3
"def publish(self, body, routing_key, exchange='amq.default',
                virtual_host='/', properties=None, payload_encoding='string'):
        
        exchange = quote(exchange, '')
        properties = properties or {}
        body = json.dumps(
            {
                'routing_key': routing_key,
                'payload': body,
                'payload_encoding': payload_encoding,
                'properties': properties,
                'vhost': virtual_host
            }
        )
        virtual_host = quote(virtual_host, '')
        return self.http_client.post(API_BASIC_PUBLISH %
                                     (
                                         virtual_host,
                                         exchange),
                                     payload=body)","Publish a Message.

        :param bytes|str|unicode body: Message payload
        :param str routing_key: Message routing key
        :param str exchange: The exchange to publish the message to
        :param str virtual_host: Virtual host name
        :param dict properties: Message properties
        :param str payload_encoding: Payload encoding.

        :raises ApiError: Raises if the remote server encountered an error.
        :raises ApiConnectionError: Raises if there was a connectivity issue.

        :rtype: dict",0,1,0,1
"def publish_proto_metadata_update(self):
        
        metadata = load_mpe_service_metadata(self.args.metadata_file)
        ipfs_hash_base58 = utils_ipfs.publish_proto_in_ipfs(self._get_ipfs_client(), self.args.protodir)
        metadata.set_simple_field(""model_ipfs_hash"", ipfs_hash_base58)
        metadata.save_pretty(self.args.metadata_file)",Publish protobuf model in ipfs and update existing metadata file,1,1,0,2
"def purge(self):
    
    try:
      return self._api.purge()
    except AttributeError:
      while True:
        lst = self.list()
        if len(lst) == 0:
          break

        for task in lst:
          self.delete(task)
        self.wait()
      return self",Deletes all tasks in the queue.,0,0,1,1
"def purgeRelationship(self, pid, subject, predicate, object, isLiteral=False,
                        datatype=None):
        

        http_args = {'subject': subject, 'predicate': predicate,
                     'object': object, 'isLiteral': isLiteral}
        if datatype is not None:
            http_args['datatype'] = datatype

        url = 'objects/%(pid)s/relationships' % {'pid': pid}
        response = self.delete(url, params=http_args)
        
        
        return response.status_code == requests.codes.ok and response.content == b'true'","Remove a relationship from an object.

        Wrapper function for
        `Fedora REST API purgeRelationship <https://wiki.duraspace.org/display/FEDORA34/REST+API#RESTAPI-purgeRelationship>`_

        :param pid: object pid
        :param subject: relationship subject
        :param predicate: relationship predicate
        :param object: relationship object
        :param isLiteral: boolean (default: false)
        :param datatype: optional datatype
        :returns: boolean; indicates whether or not a relationship was
            removed",0,1,1,2
"def push(self):
        

        if not self.canRunRemoteCmd(): 
            return None

        try:
            fetchInfo = self.repo.remotes.origin.push(no_thin=True)[0]
        except exc.GitCommandError as e:
            print(dir(e))
            print(e)
            raise


        if fetchInfo.flags & fetchInfo.ERROR:
            try:
                raise IOError(""An error occured while trying to push the GIT repository from the server. Error flag: '"" + 
                          str(fetchInfo.flags) + ""', message: '"" + str(fetchInfo.note) + ""'."")
            except:
                IOError(""An error occured while trying to push the GIT repository from the server."")
        return fetchInfo","Adding the no_thin argument to the GIT push because we had some issues pushing previously.
         According to http://stackoverflow.com/questions/16586642/git-unpack-error-on-push-to-gerrit#comment42953435_23610917,
         ""a new optimization which causes git to send as little data as possible over the network caused this bug to manifest, 
          so my guess is --no-thin just turns these optimizations off. From git push --help: ""A thin transfer significantly 
                  reduces the amount of sent data when the sender and receiver share many of the same objects in common."" (--thin is the default).""",3,1,2,6
"def push(self, remote, branch=None):
        
        pb = ProgressBar()
        pb.setup(self.name, ProgressBar.Action.PUSH)
        if branch:
            result = remote.push(branch, progress=pb)
        else: 
            result = remote.push(progress=pb)
        print()
        return result, pb.other_lines","Push a repository
        :param remote: git-remote instance
        :param branch: name of the branch to push
        :return: PushInfo, git push output lines",2,0,1,3
"def push_record_set(self, **kwargs):
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('callback'):
            return self.push_record_set_with_http_info(**kwargs)
        else:
            (data) = self.push_record_set_with_http_info(**kwargs)
            return data","Push build config set record to Brew.
        
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please define a `callback` function
        to be invoked when receiving the response.
        >>> def callback_function(response):
        >>>     pprint(response)
        >>>
        >>> thread = api.push_record_set(callback=callback_function)

        :param callback function: The callback function
            for asynchronous request. (optional)
        :param BuildConfigSetRecordPushRequestRest body:
        :return: list[ResultRest]
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def putrequest(self, method, uri):
        

        protocol = unicode(self.protocol + '://')
        url = protocol + self.host + unicode(uri)
        self._httprequest.set_timeout(self.timeout)
        self._httprequest.open(unicode(method), url)

        
        if self.cert_file is not None:
            self._httprequest.set_client_certificate(unicode(self.cert_file))",Connects to host and sends the request.,0,2,0,2
"def qseries(fseries, Q, f0, return_complex=False):
    
    
    qprime = Q / 11**(1/2.)
    norm = numpy.sqrt(315. * qprime / (128. * f0))
    window_size = 2 * int(f0 / qprime * fseries.duration) + 1
    xfrequencies = numpy.linspace(-1., 1., window_size)

    start = int((f0 - (f0 / qprime)) * fseries.duration)
    end = int(start + window_size)
    center = (start + end) / 2

    windowed = fseries[start:end] * (1 - xfrequencies ** 2) ** 2 * norm

    tlen = (len(fseries)-1) * 2
    windowed.resize(tlen)
    windowed.roll(-center)

    
    windowed = FrequencySeries(windowed, delta_f=fseries.delta_f,
                            epoch=fseries.start_time)
    ctseries = TimeSeries(zeros(tlen, dtype=numpy.complex128),
                            delta_t=fseries.delta_t)
    ifft(windowed, ctseries)

    if return_complex:
        return ctseries
    else:
        energy = ctseries.squared_norm()
        medianenergy = numpy.median(energy.numpy())
        return  energy / float(medianenergy)","Calculate the energy 'TimeSeries' for the given fseries

    Parameters
    ----------
    fseries: 'pycbc FrequencySeries'
        frequency-series data set
    Q:
        q value
    f0:
        central frequency
    return_complex: {False, bool}
        Return the raw complex series instead of the normalized power.

    Returns
    -------
    energy: '~pycbc.types.TimeSeries'
        A 'TimeSeries' of the normalized energy from the Q-transform of
        this tile against the data.",0,0,1,1
"def qtdoc_role(name, rawtext, text, lineno, inliner, options={}, content=[]):
    
    base = 'http://qt-project.org/doc/qt-4.8/'

    match = re.search('([^<]+)(<[^<>]+>)?', text)
    if match is None:
        raise ValueError

    label = match.group(1)
    if match.lastindex == 2:
        
        clsmeth = match.group(2)[1:-1]
        
        cls, meth = clsmeth.split('.')
        ref = base + cls + '.html
    else:
        ref = base + label.lower() + '.html'

    node = nodes.reference(rawtext, label, refuri=ref, **options)
    return [node], []","Links to a Qt class's doc

    Returns 2 part tuple containing list of nodes to insert into the
    document and a list of system messages.  Both are allowed to be
    empty.

    :param name: The role name used in the document.
    :param rawtext: The entire markup snippet, with role.
    :param text: The text marked with the role.
    :param lineno: The line number where rawtext appears in the input.
    :param inliner: The inliner instance that called us.
    :param options: Directive options for customization.
    :param content: The directive content for customization.",1,0,2,3
"def query(cls, volume=None, state=None, offset=None,
              limit=None, api=None):

        
        api = api or cls._API

        if volume:
            volume = Transform.to_volume(volume)

        return super(Export, cls)._query(
            url=cls._URL['query'], volume=volume, state=state, offset=offset,
            limit=limit, fields='_all', api=api
        )","Query (List) exports.
        :param volume: Optional volume identifier.
        :param state: Optional import sate.
        :param api: Api instance.
        :return: Collection object.",0,1,1,2
"def query(self, query, *args, **kwargs):
        
        if isinstance(query, six.string_types):
            query = text(query)
        _step = kwargs.pop('_step', QUERY_STEP)
        rp = self.executable.execute(query, *args, **kwargs)
        return ResultIter(rp, row_type=self.row_type, step=_step)","Run a statement on the database directly.

        Allows for the execution of arbitrary read/write queries. A query can
        either be a plain text string, or a `SQLAlchemy expression
        <http://docs.sqlalchemy.org/en/latest/core/tutorial.html#selecting>`_.
        If a plain string is passed in, it will be converted to an expression
        automatically.

        Further positional and keyword arguments will be used for parameter
        binding. To include a positional argument in your query, use question
        marks in the query (i.e. ``SELECT * FROM tbl WHERE a = ?```). For
        keyword arguments, use a bind parameter (i.e. ``SELECT * FROM tbl
        WHERE a = :foo``).
        ::

            statement = 'SELECT user, COUNT(*) c FROM photos GROUP BY user'
            for row in db.query(statement):
                print(row['user'], row['c'])

        The returned iterator will yield each result sequentially.",1,0,0,1
"def query(self, searchAreaWkt, query, count=100, ttl='5m', index=default_index):
        
        if count < 1000:
            
            search_area_polygon = from_wkt(searchAreaWkt)
            left, lower, right, upper = search_area_polygon.bounds

            params = {
                ""q"": query,
                ""count"": min(count,1000),
                ""left"": left,
                ""right"": right,
                ""lower"": lower,
                ""upper"": upper
            }

            url = self.query_index_url % index if index else self.query_url
            r = self.gbdx_connection.get(url, params=params)
            r.raise_for_status()
            return r.json()
        else:
            return list(self.query_iteratively(searchAreaWkt, query, count, ttl, index))","Perform a vector services query using the QUERY API
        (https://gbdxdocs.digitalglobe.com/docs/vs-query-list-vector-items-returns-default-fields)

        Args:
            searchAreaWkt: WKT Polygon of area to search
            query: Elastic Search query
            count: Maximum number of results to return
            ttl: Amount of time for each temporary vector page to exist

        Returns:
            List of vector results",1,0,0,1
"def query(self, sql, args=None, many=None, as_dict=False):
        
        con = self.pool.pop()
        c = None
        try:
            c = con.cursor(as_dict)
            LOGGER.debug(""Query sql: "" + sql + "" args:"" + str(args))
            c.execute(sql, args)
            if many and many > 0:
                return self._yield(con, c, many)
            else:
                return c.fetchall()

        except Exception as e:
            LOGGER.error(""Error Qeury on %s"", str(e))
            raise DBError(e.args[0], e.args[1])
        finally:
            many or (c and c.close())
            many or (con and self.pool.push(con))","The connection raw sql query,  when select table,  show table
            to fetch records, it is compatible the dbi execute method.


        :param sql string: the sql stamtement like 'select * from %s'
        :param args  list: Wen set None, will use dbi execute(sql), else
            dbi execute(sql, args), the args keep the original rules, it shuld be tuple or list of list
        :param many  int: when set, the query method will return genarate an iterate
        :param as_dict bool: when is true, the type of row will be dict, otherwise is tuple",3,2,1,6
"def query_phenomizer(usr, pwd,  *hpo_terms):
    
    base_string = 'http://compbio.charite.de/phenomizer/phenomizer/PhenomizerServiceURI'
    questions = {'mobilequery':'true', 'terms':','.join(hpo_terms), 'username':usr, 'password':pwd}
    try:
        r = requests.get(base_string, params=questions, timeout=10)
    except requests.exceptions.Timeout:
        raise RuntimeError(""The request timed out."")
        
    if not r.status_code == requests.codes.ok:
        raise RuntimeError(""Phenomizer returned a bad status code: %s"" % r.status_code)
    
    r.encoding = 'utf-8'
    
    return r","Query the phenomizer web tool
    
    Arguments:
        usr (str): A username for phenomizer
        pwd (str): A password for phenomizer
        hpo_terms (list): A list with hpo terms
    
    Returns:
        raw_answer : The raw result from phenomizer",0,2,0,2
"def queue_delete_project(self, project_id):
        
        route_values = {}
        if project_id is not None:
            route_values['projectId'] = self._serialize.url('project_id', project_id, 'str')
        response = self._send(http_method='DELETE',
                              location_id='603fe2ac-9723-48b9-88ad-09305aa6c6e1',
                              version='5.1-preview.4',
                              route_values=route_values)
        return self._deserialize('OperationReference', response)","QueueDeleteProject.
        [Preview API] Queues a project to be deleted. Use the [GetOperation](../../operations/operations/get) to periodically check for delete project status.
        :param str project_id: The project id of the project to delete.
        :rtype: :class:`<OperationReference> <azure.devops.v5_1.core.models.OperationReference>`",0,1,0,1
"def queuedb_append(path, queue_id, name, data):
    
    sql = ""INSERT INTO queue VALUES (?,?,?);""
    args = (name, queue_id, data)

    db = queuedb_open(path)
    if db is None:
        raise Exception(""Failed to open %s"" % path)

    cur = db.cursor()
    res = queuedb_query_execute(cur, sql, args)

    db.commit()
    db.close()
    return True","Append an element to the back of the queue.
    Return True on success
    Raise on error",2,0,1,3
"def queuedb_findall(path, queue_id, name=None, offset=None, limit=None):
    
    sql = ""SELECT * FROM queue WHERE queue_id = ? ORDER BY rowid ASC""
    args = (queue_id,)
    
    if name:
        sql += ' AND name = ?'
        args += (name,)

    if limit:
        sql += ' LIMIT ?'
        args += (limit,)
    
    if offset:
        sql += ' OFFSET ?'
        args += (offset,)

    sql += ';'
    
    db = queuedb_open(path)
    if db is None:
        raise Exception(""Failed to open %s"" % path)

    cur = db.cursor()
    rows = queuedb_query_execute(cur, sql, args)

    count = 0
    ret = []
    for row in rows:
        dat = {}
        dat.update(row)
        ret.append(dat)

    db.close()
    return ret","Get all queued entries for a queue and a name.
    If name is None, then find all queue entries

    Return the rows on success (empty list if not found)
    Raise on error",2,0,1,3
"def radial_sort(points,
                origin,
                normal):
    

    
    
    axis0 = [normal[0], normal[2], -normal[1]]
    axis1 = np.cross(normal, axis0)
    ptVec = points - origin
    pr0 = np.dot(ptVec, axis0)
    pr1 = np.dot(ptVec, axis1)

    
    angles = np.arctan2(pr0, pr1)

    
    return points[[np.argsort(angles)]]","Sorts a set of points radially (by angle) around an
    origin/normal.

    Parameters
    --------------
    points: (n,3) float, points in space
    origin: (3,)  float, origin to sort around
    normal: (3,)  float, vector to sort around

    Returns
    --------------
    ordered: (n,3) flot, re- ordered points in space",0,0,1,1
"def random(adjacency_mat, directed=False, random_state=None):
    
    if random_state is None:
        random_state = np.random
    elif not isinstance(random_state, np.random.RandomState):
        random_state = np.random.RandomState(random_state)

    if issparse(adjacency_mat):
        adjacency_mat = adjacency_mat.tocoo()

    
    num_nodes = adjacency_mat.shape[0]
    node_coords = random_state.rand(num_nodes, 2)

    line_vertices, arrows = _straight_line_vertices(adjacency_mat,
                                                    node_coords, directed)

    yield node_coords, line_vertices, arrows","Place the graph nodes at random places.

    Parameters
    ----------
    adjacency_mat : matrix or sparse
        The graph adjacency matrix
    directed : bool
        Whether the graph is directed. If this is True, is will also
        generate the vertices for arrows, which can be passed to an
        ArrowVisual.
    random_state : instance of RandomState | int | None
        Random state to use. Can be None to use ``np.random``.

    Yields
    ------
    (node_vertices, line_vertices, arrow_vertices) : tuple
        Yields the node and line vertices in a tuple. This layout only yields a
        single time, and has no builtin animation",0,0,1,1
"def random_output(self, max=100):
        
        output = []
        item1 = item2 = MarkovChain.START
        for i in range(max-3):
            item3 = self[(item1, item2)].roll()
            if item3 is MarkovChain.END:
                break
            output.append(item3)
            item1 = item2
            item2 = item3
        return output","Generate a list of elements from the markov chain.
            The `max` value is in place in order to prevent excessive iteration.",0,0,1,1
"def random_product(items, num=None, rng=None):
    
    import utool as ut
    rng = ut.ensure_rng(rng, 'python')
    seen = set()
    items = [list(g) for g in items]
    max_num = ut.prod(map(len, items))
    if num is None:
        num = max_num
    if num > max_num:
        raise ValueError('num exceedes maximum number of products')

    
    if num > max_num // 2:
        for prod in ut.shuffle(list(it.product(*items)), rng=rng):
            yield prod
    else:
        while len(seen) < num:
            
            idxs = tuple(rng.randint(0, len(g) - 1) for g in items)
            if idxs not in seen:
                seen.add(idxs)
                prod = tuple(g[x] for g, x in zip(items, idxs))
                yield prod","Yields `num` items from the cartesian product of items in a random order.

    Args:
        items (list of sequences): items to get caresian product of
            packed in a list or tuple.
            (note this deviates from api of it.product)

    Example:
        import utool as ut
        items = [(1, 2, 3), (4, 5, 6, 7)]
        rng = 0
        list(ut.random_product(items, rng=0))
        list(ut.random_product(items, num=3, rng=0))",1,0,2,3
"def random_replacement(random, population, parents, offspring, args):
    
    num_elites = args.setdefault('num_elites', 0)
    population.sort(reverse=True)
    num_to_replace = min(len(offspring), len(population) - num_elites) 
    valid_indices = range(num_elites, len(population))
    rep_index = random.sample(valid_indices, num_to_replace)
    for i, repind in enumerate(rep_index):
        population[repind] = offspring[i]
    return population","Performs random replacement with optional weak elitism.
    
    This function performs random replacement, which means that
    the offspring replace random members of the population, keeping
    the population size constant. Weak elitism may also be specified 
    through the `num_elites` keyword argument in args. If this is used, 
    the best `num_elites` individuals in the current population are 
    allowed to survive if they are better than the worst `num_elites`
    offspring.
    
    .. Arguments:
       random -- the random number generator object
       population -- the population of individuals
       parents -- the list of parent individuals
       offspring -- the list of offspring individuals
       args -- a dictionary of keyword arguments

    Optional keyword arguments in args:
    
    - *num_elites* -- number of elites to consider (default 0)",0,0,1,1
"def rank_subgraph_by_node_filter(graph: BELGraph,
                                 node_predicates: Union[NodePredicate, Iterable[NodePredicate]],
                                 annotation: str = 'Subgraph',
                                 reverse: bool = True,
                                 ) -> List[Tuple[str, int]]:
    
    r1 = group_nodes_by_annotation_filtered(graph, node_predicates=node_predicates, annotation=annotation)
    r2 = count_dict_values(r1)
    
    return sorted(r2.items(), key=itemgetter(1), reverse=reverse)","Rank sub-graphs by which have the most nodes matching an given filter.

    A use case for this function would be to identify which subgraphs contain the most differentially expressed
    genes.

    >>> from pybel import from_pickle
    >>> from pybel.constants import GENE
    >>> from pybel_tools.integration import overlay_type_data
    >>> from pybel_tools.summary import rank_subgraph_by_node_filter
    >>> import pandas as pd
    >>> graph = from_pickle('~/dev/bms/aetionomy/alzheimers.gpickle')
    >>> df = pd.read_csv('~/dev/bananas/data/alzheimers_dgxp.csv', columns=['Gene', 'log2fc'])
    >>> data = {gene: log2fc for _, gene, log2fc in df.itertuples()}
    >>> overlay_type_data(graph, data, 'log2fc', GENE, 'HGNC', impute=0.0)
    >>> results = rank_subgraph_by_node_filter(graph, lambda g, n: 1.3 < abs(g[n]['log2fc']))",0,0,1,1
"def raw(self, sql):
        
        res = self.cursor.execute(sql)
        if self.cursor.description is None:
            return res

        rows = self.cursor.fetchall()
        columns = [d[0] for d in self.cursor.description]

        structured_rows = []
        for row in rows:
            data = {}
            for val, col in zip(row, columns):
                data[col] = val
            structured_rows.append(DataRow(data))

        return structured_rows","Execute raw sql
        :Parameters:

        - sql: string, sql to be executed

        :Return: the result of this execution

        If it's a select, return a list with each element be a DataRow instance

        Otherwise return raw result from the cursor (Should be insert or update or delete)",2,0,1,3
"def raw_data(tag_value):
        
        data = {}
        pieces = []
        for p in tag_value.split(' '):
            pieces.extend(p.split(';'))
        
        for piece in pieces:
            kv = piece.split('=')
            
            if not len(kv) == 2:
                continue
            key, value = kv
            data[key] = value
        return data","convert the tag to a dictionary, taking values as is

        This method name and purpose are opaque...  and not true.",0,0,1,1
"def ray_get_and_free(object_ids):
    

    global _last_free_time
    global _to_free

    result = ray.get(object_ids)
    if type(object_ids) is not list:
        object_ids = [object_ids]
    _to_free.extend(object_ids)

    
    now = time.time()
    if (len(_to_free) > MAX_FREE_QUEUE_SIZE
            or now - _last_free_time > FREE_DELAY_S):
        ray.internal.free(_to_free)
        _to_free = []
        _last_free_time = now

    return result","Call ray.get and then queue the object ids for deletion.

    This function should be used whenever possible in RLlib, to optimize
    memory usage. The only exception is when an object_id is shared among
    multiple readers.

    Args:
        object_ids (ObjectID|List[ObjectID]): Object ids to fetch and free.

    Returns:
        The result of ray.get(object_ids).",0,2,1,3
"def rdkitmol(self):
        r
        if self.__rdkitmol:
            return self.__rdkitmol
        else:
            try:
                self.__rdkitmol = Chem.MolFromSmiles(self.smiles)
                return self.__rdkitmol
            except:
                return None","r'''RDKit object of the chemical, without hydrogen. If RDKit is not
        available, holds None.

        For examples of what can be done with RDKit, see
        `their website <http://www.rdkit.org/docs/GettingStartedInPython.html>`_.",0,0,2,2
"def read(self):
        
        p = os.path.join(self.path, self.name)
        try:
            with open(p) as f:
                json_text = f.read()
        except FileNotFoundError as e:
            raise JSONFileError(e) from e
        try:
            json.loads(json_text)
        except (json.JSONDecodeError, TypeError) as e:
            raise JSONFileError(f""{e} Got {p}"") from e
        return json_text",Returns the file contents as validated JSON text.,2,0,2,4
"def read(self, length, timeout_ms=0, blocking=False):
        
        self._check_device_status()
        bufp = ffi.new(""unsigned char[]"", length)
        if not timeout_ms and blocking:
            timeout_ms = -1
        if timeout_ms:
            rv = hidapi.hid_read_timeout(self._device, bufp, length,
                                         timeout_ms)
        else:
            rv = hidapi.hid_read(self._device, bufp, length)
        if rv == -1:
            raise IOError(""Failed to read from HID device: {0}""
                          .format(self._get_last_error_string()))
        elif rv == 0:
            return None
        else:
            return ffi.buffer(bufp, rv)[:]","Read an Input report from a HID device with timeout.

        Input reports are returned to the host through the `INTERRUPT IN`
        endpoint. The first byte will contain the Report number if the device
        uses numbered reports.
        By default reads are non-blocking, i.e. the method will return
        `None` if no data was available. Blocking reads can be enabled with
        :param blocking:. Additionally, a timeout for the read can be
        specified.

        :param length:      The number of bytes to read. For devices with
                            multiple reports, make sure to read an extra byte
                            for the report number.
        :param timeout_ms:  Timeout in miliseconds
        :type timeout_ms:   int
        :param blocking:    Block until data is available",1,1,2,4
"def read(self, page):
        
        log.debug(""read pages {0} to {1}"".format(page, page+3))

        data = self.transceive(""\x30""+chr(page % 256), timeout=0.005)

        if len(data) == 1 and data[0] & 0xFA == 0x00:
            log.debug(""received nak response"")
            self.target.sel_req = self.target.sdd_res[:]
            self._target = self.clf.sense(self.target)
            raise Type2TagCommandError(
                INVALID_PAGE_ERROR if self.target else nfc.tag.RECEIVE_ERROR)

        if len(data) != 16:
            log.debug(""invalid response "" + hexlify(data))
            raise Type2TagCommandError(INVALID_RESPONSE_ERROR)

        return data","Send a READ command to retrieve data from the tag.

        The *page* argument specifies the offset in multiples of 4
        bytes (i.e. page number 1 will return bytes 4 to 19). The data
        returned is a byte array of length 16 or None if the block is
        outside the readable memory range.

        Command execution errors raise :exc:`Type2TagCommandError`.",2,3,2,7
"def read(self, payloadType, elsClient):
        
        if elsClient:
            self._client = elsClient;
        elif not self.client:
            raise ValueError()
        try:
            api_response = self.client.exec_request(self.uri)
            if isinstance(api_response[payloadType], list):
                self._data = api_response[payloadType][0]
            else:
                self._data = api_response[payloadType]
            
            logger.info(""Data loaded for "" + self.uri)
            return True
        except (requests.HTTPError, requests.RequestException) as e:
            for elm in e.args:
                logger.warning(elm)
            return False","Fetches the latest data for this entity from api.elsevier.com.
            Returns True if successful; else, False.",1,3,1,5
"def read(self, sensors):
        
        payload = {'destDev': [], 'keys': list(set([s.key for s in sensors]))}
        if self.sma_sid is None:
            yield from self.new_session()
            if self.sma_sid is None:
                return False
        body = yield from self._fetch_json(URL_VALUES, payload=payload)

        
        if body.get('err') == 401:
            _LOGGER.warning(""401 error detected, closing session to force ""
                            ""another login attempt"")
            self.close_session()
            return False

        _LOGGER.debug(json.dumps(body))
        for sen in sensors:
            if sen.extract_value(body):
                _LOGGER.debug(""%s\t= %s %s"",
                              sen.name, sen.value, sen.unit)
        return True",Read a set of keys.,0,3,1,4
"def read(self, size):
        
        data = None
        while True:
            try:
                data = self.handle.recv(size)
            except socket.timeout as socket_error:
                self._reconnect(socket_error)
            except socket.error as socket_error:
                
                if socket_error.errno == errno.EINTR:
                    continue
                self._reconnect(IOError)
            if not data:
                self._reconnect(IOError)
            break
        return data","Read wrapper.

        Parameters
        ----------
        size : int
          Number of bytes to read",0,2,0,2
"def read(self, stream):
        
        def read_it(stream):
            bytes = stream.read()
            transportIn = TMemoryBuffer(bytes)
            protocolIn = TBinaryProtocol.TBinaryProtocol(transportIn)
            topology = StormTopology()
            topology.read(protocolIn)
            return topology
            
        if isinstance(stream, six.string_types):
            with open(stream, 'rb') as f:
                return read_it(f)
        else:
            return read_it(stream)",Reads the topology from a stream or file.,1,0,2,3
"def readFromProto(cls, proto):
    
    instance = cls(proto.columnCount, proto.inputWidth, proto.cellsPerColumn)

    instance.temporalImp = proto.temporalImp
    instance.learningMode = proto.learningMode
    instance.inferenceMode = proto.inferenceMode
    instance.anomalyMode = proto.anomalyMode
    instance.topDownMode = proto.topDownMode
    instance.computePredictedActiveCellIndices = (
      proto.computePredictedActiveCellIndices)
    instance.orColumnOutputs = proto.orColumnOutputs

    if instance.temporalImp == ""py"":
      tmProto = proto.backtrackingTM
    elif instance.temporalImp == ""cpp"":
      tmProto = proto.backtrackingTMCpp
    elif instance.temporalImp == ""tm_py"":
      tmProto = proto.temporalMemory
    elif instance.temporalImp == ""tm_cpp"":
      tmProto = proto.temporalMemory
    else:
      raise TypeError(
          ""Unsupported temporalImp for capnp serialization: {}"".format(
              instance.temporalImp))

    instance._tfdr = _getTPClass(proto.temporalImp).read(tmProto)

    return instance","Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.readFromProto`.

    Read state from proto object.

    :param proto: TMRegionProto capnproto object",1,0,2,3
"def readTableFromCSV(f, dialect=""excel""):
    
    rowNames = []
    columnNames = []
    matrix = []

    first = True
    for row in csv.reader(f, dialect):
        if first:
            columnNames = row[1:]
            first = False
        else:
            rowNames.append(row[0])
            matrix.append([float(c) for c in row[1:]])

    return Table(rowNames, columnNames, matrix)",Reads a table object from given CSV file.,1,0,0,1
"def read_bed(file_path, restricted_genes=None):
    
    
    bed_dict = OrderedDict()
    for bed_row in bed_generator(file_path):
        is_restrict_flag = restricted_genes is None or bed_row.gene_name in restricted_genes
        if is_restrict_flag:
            bed_dict.setdefault(bed_row.chrom, [])
            bed_dict[bed_row.chrom].append(bed_row)
    sort_chroms = sorted(bed_dict.keys(), key=lambda x: len(bed_dict[x]), reverse=True)
    bed_dict = OrderedDict((chrom, bed_dict[chrom]) for chrom in sort_chroms)
    return bed_dict","Reads BED file and populates a dictionary separating genes
    by chromosome.

    Parameters
    ----------
    file_path : str
        path to BED file
    filtered_genes: list
        list of gene names to not use

    Returns
    -------
    bed_dict: dict
        dictionary mapping chromosome keys to a list of BED lines",1,0,1,2
"def read_config(self, path):
        
        self.pyvlx.logger.info('Reading config file: ', path)
        try:
            with open(path, 'r') as filehandle:
                doc = yaml.load(filehandle)
                if 'config' not in doc:
                    raise PyVLXException('no element config found in: {0}'.format(path))
                if 'host' not in doc['config']:
                    raise PyVLXException('no element host found in: {0}'.format(path))
                if 'password' not in doc['config']:
                    raise PyVLXException('no element password found in: {0}'.format(path))
                self.host = doc['config']['host']
                self.password = doc['config']['password']
        except FileNotFoundError as ex:
            raise PyVLXException('file does not exist: {0}'.format(ex))",Read configuration file.,5,1,4,10
"def read_creds_from_ecs_container_metadata():
    
    creds = init_creds()
    try:
        ecs_metadata_relative_uri = os.environ['AWS_CONTAINER_CREDENTIALS_RELATIVE_URI']
        credentials = requests.get('http://169.254.170.2' + ecs_metadata_relative_uri, timeout = 1).json()
        for c in ['AccessKeyId', 'SecretAccessKey']:
            creds[c] = credentials[c]
            creds['SessionToken'] = credentials['Token']
        return creds
    except Exception as e:
        return False","Read credentials from ECS instance metadata (IAM role)

    :return:",0,1,1,2
"def read_data_to_asp(file: str) -> List[str]:
    
    if file.endswith("".json""):
        with open(file) as f:
            data = json.load(f)
            return schema2asp(data2schema(data))
    elif file.endswith("".csv""):
        df = pd.read_csv(file)
        df = df.where((pd.notnull(df)), None)
        data = list(df.T.to_dict().values())
        schema = data2schema(data)
        asp = schema2asp(schema)
        return asp
    else:
        raise Exception(""invalid file type"")","Reads the given JSON file and generates the ASP definition.
        Args:
            file: the json data file
        Returns:
            the asp definition.",2,0,2,4
"def read_group(self, group_id, mount_point=DEFAULT_MOUNT_POINT):
        
        api_path = '/v1/{mount_point}/group/id/{id}'.format(
            mount_point=mount_point,
            id=group_id,
        )
        response = self._adapter.get(
            url=api_path,
        )
        return response.json()","Query the group by its identifier.

        Supported methods:
            GET: /{mount_point}/group/id/{id}. Produces: 200 application/json

        :param group_id: Identifier of the group.
        :type group_id: str | unicode
        :param mount_point: The ""path"" the method/backend was mounted on.
        :type mount_point: str | unicode
        :return: The JSON response of the request.
        :rtype: requests.Response",0,1,0,1
"def read_history_file(self):
        
        histfile = self.debugger.intf[-1].histfile
        try:
            import readline
            readline.read_history_file(histfile)
        except IOError:
                pass
        except ImportError:
            pass
        return",Read the command history file -- possibly.,1,0,0,1
"def read_message(
        self, callback: Callable[[""Future[Union[None, str, bytes]]""], None] = None
    ) -> Awaitable[Union[None, str, bytes]]:
        

        awaitable = self.read_queue.get()
        if callback is not None:
            self.io_loop.add_future(asyncio.ensure_future(awaitable), callback)
        return awaitable","Reads a message from the WebSocket server.

        If on_message_callback was specified at WebSocket
        initialization, this function will never return messages

        Returns a future whose result is the message, or None
        if the connection is closed.  If a callback argument
        is given it will be called with the future when it is
        ready.",0,1,0,1
"def read_mm_uic1(fd, byte_order, dtype, count):
    
    t = fd.read(8*count)
    t = struct.unpack('%s%iI' % (byte_order, 2*count), t)
    return dict((MM_TAG_IDS[k], v) for k, v in zip(t[::2], t[1::2])
                if k in MM_TAG_IDS)",Read MM_UIC1 tag from file and return as dictionary.,1,0,1,2
"def read_namespace(self, name, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.read_namespace_with_http_info(name, **kwargs)  
        else:
            (data) = self.read_namespace_with_http_info(name, **kwargs)  
            return data","read_namespace  # noqa: E501

        read the specified Namespace  # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.read_namespace(name, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the Namespace (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param bool exact: Should the export be exact.  Exact export maintains cluster-specific fields like 'Namespace'.
        :param bool export: Should this value be exported.  Export strips fields that a user can not specify.
        :return: V1Namespace
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def read_namespaced_cron_job(self, name, namespace, **kwargs):
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.read_namespaced_cron_job_with_http_info(name, namespace, **kwargs)
        else:
            (data) = self.read_namespaced_cron_job_with_http_info(name, namespace, **kwargs)
            return data","read the specified CronJob
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.read_namespaced_cron_job(name, namespace, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the CronJob (required)
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param bool exact: Should the export be exact.  Exact export maintains cluster-specific fields like 'Namespace'. Deprecated. Planned for removal in 1.18.
        :param bool export: Should this value be exported.  Export strips fields that a user can not specify. Deprecated. Planned for removal in 1.18.
        :return: V2alpha1CronJob
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def read_namespaced_stateful_set_status(self, name, namespace, **kwargs):
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.read_namespaced_stateful_set_status_with_http_info(name, namespace, **kwargs)
        else:
            (data) = self.read_namespaced_stateful_set_status_with_http_info(name, namespace, **kwargs)
            return data","read status of the specified StatefulSet
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.read_namespaced_stateful_set_status(name, namespace, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the StatefulSet (required)
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :return: V1StatefulSet
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def read_projected_dos( self ):
        
        pdos_list = []
        for i in range( self.number_of_atoms ):
            df = self.read_atomic_dos_as_df( i+1 )
            pdos_list.append( df )
        self.pdos = np.vstack( [ np.array( df ) for df in pdos_list ] ).reshape( 
            self.number_of_atoms, self.number_of_data_points, self.number_of_channels, self.ispin )",Read the projected density of states data into,0,0,1,1
"def read_property_to_result_element(obj, propertyIdentifier, propertyArrayIndex=None):
    
    if _debug: read_property_to_result_element._debug(""read_property_to_result_element %s %r %r"", obj, propertyIdentifier, propertyArrayIndex)

    
    read_result = ReadAccessResultElementChoice()

    try:
        if not obj:
            raise ExecutionError(errorClass='object', errorCode='unknownObject')

        read_result.propertyValue = read_property_to_any(obj, propertyIdentifier, propertyArrayIndex)
        if _debug: read_property_to_result_element._debug(""    - success"")
    except PropertyError as error:
        if _debug: read_property_to_result_element._debug(""    - error: %r"", error)
        read_result.propertyAccessError = ErrorType(errorClass='property', errorCode='unknownProperty')
    except ExecutionError as error:
        if _debug: read_property_to_result_element._debug(""    - error: %r"", error)
        read_result.propertyAccessError = ErrorType(errorClass=error.errorClass, errorCode=error.errorCode)

    
    read_access_result_element = ReadAccessResultElement(
        propertyIdentifier=propertyIdentifier,
        propertyArrayIndex=propertyArrayIndex,
        readResult=read_result,
        )
    if _debug: read_property_to_result_element._debug(""    - read_access_result_element: %r"", read_access_result_element)

    
    return read_access_result_element","Read the specified property of the object, with the optional array index,
    and cast the result into an Any object.",1,0,3,4
"def read_runtime_class(self, name, **kwargs):
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.read_runtime_class_with_http_info(name, **kwargs)
        else:
            (data) = self.read_runtime_class_with_http_info(name, **kwargs)
            return data","read the specified RuntimeClass
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.read_runtime_class(name, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the RuntimeClass (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param bool exact: Should the export be exact.  Exact export maintains cluster-specific fields like 'Namespace'. Deprecated. Planned for removal in 1.18.
        :param bool export: Should this value be exported.  Export strips fields that a user can not specify. Deprecated. Planned for removal in 1.18.
        :return: V1beta1RuntimeClass
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def read_samples(self, sr=None, offset=0, duration=None):
        

        read_duration = self.duration

        if offset > 0 and read_duration is not None:
            read_duration -= offset

        if duration is not None:
            if read_duration is None:
                read_duration = duration
            else:
                read_duration = min(duration, read_duration)

        return self.track.read_samples(
            sr=sr,
            offset=self.start + offset,
            duration=read_duration
        )","Read the samples of the utterance.

        Args:
            sr (int): If None uses the sampling rate given by the track,
                      otherwise resamples to the given sampling rate.
            offset (float): Offset in seconds to read samples from.
            duration (float): If not ``None`` read only this
                              number of seconds in maximum.

        Returns:
            np.ndarray: A numpy array containing the samples
                        as a floating point (numpy.float32) time series.",0,1,1,2
"def read_secret_version(self, path, version=None, mount_point=DEFAULT_MOUNT_POINT):
        
        params = {}
        if version is not None:
            params['version'] = version
        api_path = '/v1/{mount_point}/data/{path}'.format(mount_point=mount_point, path=path)
        response = self._adapter.get(
            url=api_path,
            params=params,
        )
        return response.json()","Retrieve the secret at the specified location.

        Supported methods:
            GET: /{mount_point}/data/{path}. Produces: 200 application/json


        :param path: Specifies the path of the secret to read. This is specified as part of the URL.
        :type path: str | unicode
        :param version: Specifies the version to return. If not set the latest version is returned.
        :type version: int
        :param mount_point: The ""path"" the secret engine was mounted on.
        :type mount_point: str | unicode
        :return: The JSON response of the request.
        :rtype: dict",0,1,1,2
"def read_trailer(self):
        
        _logger.debug('Reading chunked trailer.')

        trailer_data_list = []

        while True:
            trailer_data = yield from self._connection.readline()

            trailer_data_list.append(trailer_data)

            if not trailer_data.strip():
                break

        return b''.join(trailer_data_list)","Read the HTTP trailer fields.

        Returns:
            bytes: The trailer data.

        Coroutine.",0,1,1,2
"def real_time_scheduling(self, availability, oauth, event, target_calendars=()):
        
        args = {
            'oauth': oauth,
            'event': event,
            'target_calendars': target_calendars
        }

        if availability:
            options = {}
            options['participants'] = self.map_availability_participants(availability.get('participants', None))
            options['required_duration'] = self.map_availability_required_duration(availability.get('required_duration', None))
            options['start_interval'] = self.map_availability_required_duration(availability.get('start_interval', None))
            options['buffer'] = self.map_availability_buffer(availability.get('buffer', None))

            self.translate_available_periods(availability['available_periods'])
            options['available_periods'] = availability['available_periods']
            args['availability'] = options

        return self.request_handler.post(endpoint='real_time_scheduling', data=args, use_api_key=True).json()","Generates an real time scheduling link to start the OAuth process with
        an event to be automatically upserted

        :param dict availability:  - A dict describing the availability details for the event:
            :participants      - A dict stating who is required for the availability
                                 call
            :required_duration - A dict stating the length of time the event will
                                 last for
            :available_periods - A dict stating the available periods for the event
            :start_interval    - A Integer representing the start_interval of the event
            :buffer            - A dict representing the buffer for the event
        :param dict oauth:   - A dict describing the OAuth flow required:
            :scope             - A String representing the scopes to ask for
                                 within the OAuth flow
            :redirect_uri      - A String containing a url to redirect the
                                 user to after completing the OAuth flow.
            :scope             - A String representing additional state to
                                 be passed within the OAuth flow.
        :param dict event:     - A dict describing the event
        :param list target_calendars: - An list of dics stating into which calendars
                                        to insert the created event
        See http://www.cronofy.com/developers/api#upsert-event for reference.",0,1,2,3
"def realign_seqs(block, gap_char='.', align_indels=False):
    
    
    all_chars = [list(sq['seq']) for sq in block['sequences']]
    
    
    nrows = len(all_chars)
    i = 0
    while i < len(all_chars[0]):
        rows_need_gaps = [r for r in all_chars if not r[i].islower()]
        if len(rows_need_gaps) != nrows:
            for row in rows_need_gaps:
                row.insert(i, gap_char)
        i += 1
    return [''.join(row) for row in all_chars]","Add gaps to a block so all residues in a column are equivalent.

    Given a block, containing a list of ""sequences"" (dicts) each containing a
    ""seq"" (actual string sequence, where upper=match, lower=insert, dash=gap),
    insert gaps (- or .) into the sequences s.t.

    1. columns line up properly, and
    2. all resulting sequences have the same length


    The reason this needs to be done is that the query/consensus sequence is not
    assigned gaps to account for inserts in the other sequences. We need to add
    the gaps back to obtain a normal alignment.

    `return`: a list of realigned sequence strings.",0,0,1,1
"def rebin(a, *args):
    

    shape = a.shape
    len_shape = len(shape)
    factor = np.asarray(shape) // np.asarray(args)
    ev_list = ['a.reshape('] + \
              ['args[%d], factor[%d], ' % (i, i) for i in range(len_shape)] + \
              [')'] + ['.mean(%d)' % (i+1) for i in range(len_shape)]
    
    return eval(''.join(ev_list))","See http://scipy-cookbook.readthedocs.io/items/Rebinning.html

    Note: integer division in the computation of 'factor' has been
    included to avoid the following runtime message:
    VisibleDeprecationWarning: using a non-integer number instead of
    an integer will result in an error in the future
    from __future__ import division",0,0,1,1
"def receive(self):
        
        start = 0

        while True:
            idx = self._buffer.find(INST_TERM.encode(), start)
            if idx != -1:
                
                line = self._buffer[:idx + 1].decode()
                self._buffer = self._buffer[idx + 1:]
                self.logger.debug('Received instruction: %s' % line)
                return line
            else:
                start = len(self._buffer)
                
                buf = self.client.recv(BUF_LEN)
                if not buf:
                    
                    self.close()
                    self.logger.debug(
                        'Failed to receive instruction. Closing.')
                    return None
                self._buffer.extend(buf)",Receive instructions from Guacamole guacd server.,0,2,0,2
"def receive(self, sequence, args):
        

        
        if not self._reorder:
            self._callback(*args)
            return

        
        if self._next_expected is not None and sequence < self._next_expected:
            print(""Dropping out of order packet, seq=%d"" % sequence)
            return

        self._out_of_order.append((sequence, args))
        self._out_of_order.sort(key=lambda x: x[0])

        
        while len(self._out_of_order) > 0:
            seq, args = self._out_of_order[0]

            if self._next_expected is not None and seq != self._next_expected:
                return

            self._callback(*args)
            self._out_of_order.pop(0)
            self._next_expected = seq+1","Receive one packet

        If the sequence number is one we've already seen before, it is dropped.

        If it is not the next expected sequence number, it is put into the
        _out_of_order queue to be processed once the holes in sequence number
        are filled in.

        Args:
            sequence (int): The sequence number of the received packet
            args (list): The list of packet contents that will be passed to callback
                as callback(*args)",0,0,4,4
"def receive_device_value(self, raw_value: int):
        
        new_value = self._input_to_raw_value(raw_value)
        if self.button is not None:
            if new_value > (self.button_trigger_value + 0.05) > self.__value:
                self.buttons.button_pressed(self.button.key_code)
            elif new_value < (self.button_trigger_value - 0.05) < self.__value:
                self.buttons.button_released(self.button.key_code)
        self.__value = new_value
        if new_value > self.max:
            self.max = new_value
        elif new_value < self.min:
            self.min = new_value","Set a new value, called from within the joystick implementation class when parsing the event queue.

        :param raw_value: the raw value from the joystick hardware

        :internal:",0,0,1,1
"def receive_message(
        sock, operation, request_id, max_message_size=MAX_MESSAGE_SIZE):
    
    header = _receive_data_on_socket(sock, 16)
    length = _UNPACK_INT(header[:4])[0]

    actual_op = _UNPACK_INT(header[12:])[0]
    if operation != actual_op:
        raise ProtocolError(""Got opcode %r but expected ""
                            ""%r"" % (actual_op, operation))
    
    if request_id is not None:
        response_id = _UNPACK_INT(header[8:12])[0]
        if request_id != response_id:
            raise ProtocolError(""Got response id %r but expected ""
                                ""%r"" % (response_id, request_id))
    if length <= 16:
        raise ProtocolError(""Message length (%r) not longer than standard ""
                            ""message header size (16)"" % (length,))
    if length > max_message_size:
        raise ProtocolError(""Message length (%r) is larger than server max ""
                            ""message size (%r)"" % (length, max_message_size))

    return _receive_data_on_socket(sock, length - 16)",Receive a raw BSON message or raise socket.error.,4,1,0,5
"def recompute_home(self):
        

        if self.night_start < self.night_end:
            night_filter = lambda r: self.night_end > r.datetime.time(
            ) > self.night_start
        else:
            night_filter = lambda r: not(
                self.night_end < r.datetime.time() < self.night_start)

        
        candidates = list(
            positions_binning(filter(night_filter, self._records)))

        if len(candidates) == 0:
            self.home = None
        else:
            self.home = Counter(candidates).most_common()[0][0]

        self.reset_cache()
        return self.home","Return the antenna where the user spends most of his time at night.
        None is returned if there are no candidates for a home antenna",0,0,2,2
"def recv(self, buf=None, flags=0):
        
        if buf is None:
            rtn, out_buf = wrapper.nn_recv(self.fd, flags)
        else:
            rtn, out_buf = wrapper.nn_recv(self.fd, buf, flags)
        _nn_check_positive_rtn(rtn)
        return bytes(buffer(out_buf))[:rtn]",Recieve a message.,0,1,0,1
"def recvfrom(self, bufsize, flags=0):
        
        with self._registered('re'):
            while 1:
                if self._closed:
                    raise socket.error(errno.EBADF, ""Bad file descriptor"")
                try:
                    return self._sock.recvfrom(bufsize, flags)
                except socket.error, exc:
                    if not self._blocking or exc[0] not in _BLOCKING_OP:
                        raise
                    sys.exc_clear()
                    if self._readable.wait(self.gettimeout()):
                        raise socket.timeout(""timed out"")
                    if scheduler.state.interrupted:
                        raise IOError(errno.EINTR, ""interrupted system call"")","receive data on a socket that isn't necessarily a 1-1 connection

        .. note:: this method will block until data is available to be read

        :param bufsize:
            the maximum number of bytes to receive. fewer may be returned,
            however
        :type bufsize: int
        :param flags:
            flags for the receive call. consult the unix manpage for
            ``recv(2)`` for what flags are available
        :type flags: int

        :returns:
            a two-tuple of ``(data, address)`` -- the string data received and
            the address from which it was received",4,1,4,9
"def reduce_stack(array3D, z_function):
    
    xmax, ymax, _ = array3D.shape
    projection = np.zeros((xmax, ymax), dtype=array3D.dtype)
    for x in range(xmax):
        for y in range(ymax):
            projection[x, y] = z_function(array3D[x, y, :])
    return projection","Return 2D array projection of the input 3D array.

    The input function is applied to each line of an input x, y value.

    :param array3D: 3D numpy.array
    :param z_function: function to use for the projection (e.g. :func:`max`)",0,0,1,1
"def refmap_stats(data, sample):
    
    
    mapf = os.path.join(data.dirs.refmapping, sample.name+""-mapped-sorted.bam"")
    umapf = os.path.join(data.dirs.refmapping, sample.name+""-unmapped.bam"")

    
    cmd1 = [ipyrad.bins.samtools, ""flagstat"", umapf]
    proc1 = sps.Popen(cmd1, stderr=sps.STDOUT, stdout=sps.PIPE)
    result1 = proc1.communicate()[0]

    
    cmd2 = [ipyrad.bins.samtools, ""flagstat"", mapf]
    proc2 = sps.Popen(cmd2, stderr=sps.STDOUT, stdout=sps.PIPE)
    result2 = proc2.communicate()[0]

    
    
    
    
    if ""pair"" in data.paramsdict[""datatype""]:
        sample.stats[""refseq_unmapped_reads""] = int(result1.split()[0]) / 2
        sample.stats[""refseq_mapped_reads""] = int(result2.split()[0]) / 2
    else:
        sample.stats[""refseq_unmapped_reads""] = int(result1.split()[0])
        sample.stats[""refseq_mapped_reads""] = int(result2.split()[0])

    sample_cleanup(data, sample)","Get the number of mapped and unmapped reads for a sample
    and update sample.stats",0,2,2,4
"def refresh_existing_encodings(self, encodings_from_file):
        
        update = False

        for msg in self.table_model.protocol.messages:
            i = next((i for i, d in enumerate(encodings_from_file) if d.name == msg.decoder.name), 0)
            if msg.decoder != encodings_from_file[i]:
                update = True
                msg.decoder = encodings_from_file[i]
                msg.clear_decoded_bits()
                msg.clear_encoded_bits()

        if update:
            self.refresh_table()
            self.refresh_estimated_time()","Refresh existing encodings for messages, when encoding was changed by user in dialog

        :return:",1,0,0,1
"def refresh_indices(model, block_size=100):
    
    conn = _connect(model)
    max_id = int(conn.get('%s:%s:'%(model._namespace, model._pkey)) or '0')
    block_size = max(block_size, 10)
    for i in range(1, max_id+1, block_size):
        
        models = model.get(list(range(i, i+block_size)))
        models 
        
        session.commit(all=True)
        yield min(i+block_size, max_id), max_id","This utility function will iterate over all entities of a provided model,
    refreshing their indices. This is primarily useful after adding an index
    on a column.

    Arguments:

        * *model* - the model whose entities you want to reindex
        * *block_size* - the maximum number of entities you want to fetch from
          Redis at a time, defaulting to 100

    This function will yield its progression through re-indexing all of your
    entities.

    Example use::

        for progress, total in refresh_indices(MyModel, block_size=200):
            print ""%s of %s""%(progress, total)

    .. note:: This uses the session object to handle index refresh via calls to
      ``.commit()``. If you have any outstanding entities known in the
      session, they will be committed.",0,1,1,2
"def refund(self, **kwargs):
        
        
        try:
            selfnode = self._elem
        except AttributeError:
            raise AttributeError('refund')
        url, method = None, None
        for anchor_elem in selfnode.findall('a'):
            if anchor_elem.attrib.get('name') == 'refund':
                url = anchor_elem.attrib['href']
                method = anchor_elem.attrib['method'].upper()
        if url is None or method is None:
            raise AttributeError(""refund"")  

        actionator = self._make_actionator(url, method, extra_handler=self._handle_refund_accepted)
        return actionator(**kwargs)","Refund this transaction.

        Calling this method returns the refunded transaction (that is,
        ``self``) if the refund was successful, or raises a `ResponseError` if
        an error occurred requesting the refund. After a successful call to
        `refund()`, to retrieve the new transaction representing the refund,
        use the `get_refund_transaction()` method.",2,1,1,4
"def regex_to_error_msg(regex):
    
    return re.sub('([^\\\\])[()]', '\\1', regex) \
        .replace('[ \t]*$', '') \
        .replace('^', '') \
        .replace('$', '') \
        .replace('[ \t]*', ' ') \
        .replace('[ \t]+', ' ') \
        .replace('[0-9]+', 'X') \
        \
        .replace('\\[', '[') \
        .replace('\\]', ']') \
        .replace('\\(', '(') \
        .replace('\\)', ')') \
        .replace('\\.', '.')",Format a human-readable error message from a regex,0,0,1,1
"def register(cls, code, name, hash_name=None, hash_new=None):
        
        if not _is_app_specific_func(code):
            raise ValueError(
                ""only application-specific functions can be registered"")
        
        name_mapping_data = [  
            (cls._func_from_name, name,
             ""function name is already registered for a different function""),
            (cls._func_from_hash, hash_name,
             ""hashlib name is already registered for a different function"")]
        for (mapping, nameinmap, errmsg) in name_mapping_data:
            existing_func = mapping.get(nameinmap, code)
            if existing_func != code:
                raise ValueError(errmsg, existing_func)
        
        if code in cls._func_hash:
            cls.unregister(code)
        
        cls._do_register(code, name, hash_name, hash_new)","Add an application-specific function to the registry.

        Registers a function with the given `code` (an integer) and `name` (a
        string, which is added both with only hyphens and only underscores),
        as well as an optional `hash_name` and `hash_new` constructor for
        hashlib compatibility.  If the application-specific function is
        already registered, the related data is replaced.  Registering a
        function with a `code` not in the application-specific range
        (0x00-0xff) or with names already registered for a different function
        raises a `ValueError`.

        >>> import hashlib
        >>> FuncReg.register(0x05, 'md-5', 'md5', hashlib.md5)
        >>> FuncReg.get('md-5') == FuncReg.get('md_5') == 0x05
        True
        >>> hashobj = FuncReg.hash_from_func(0x05)
        >>> hashobj.name == 'md5'
        True
        >>> FuncReg.func_from_hash(hashobj) == 0x05
        True
        >>> FuncReg.reset()
        >>> 0x05 in FuncReg
        False",2,0,4,6
"def register(self, name, encoder, decoder, content_type,
                 content_encoding='utf-8'):
        
        if encoder:
            self._encoders[name] = (content_type, content_encoding, encoder)
        if decoder:
            self._decoders[content_type] = decoder","Register a new encoder/decoder.

        :param name: A convenience name for the serialization method.

        :param encoder: A method that will be passed a python data structure
            and should return a string representing the serialized data.
            If ``None``, then only a decoder will be registered. Encoding
            will not be possible.

        :param decoder: A method that will be passed a string representing
            serialized data and should return a python data structure.
            If ``None``, then only an encoder will be registered.
            Decoding will not be possible.

        :param content_type: The mime-type describing the serialized
            structure.

        :param content_encoding: The content encoding (character set) that
            the :param:`decoder` method will be returning. Will usually be
            ``utf-8``, ``us-ascii``, or ``binary``.",0,0,3,3
"def register(self, new_reg=None):
        
        if new_reg is None:
            new_reg = messages.NewRegistration()
        action = LOG_ACME_REGISTER(registration=new_reg)
        with action.context():
            return (
                DeferredContext(
                    self.update_registration(
                        new_reg, uri=self.directory[new_reg]))
                .addErrback(self._maybe_registered, new_reg)
                .addCallback(
                    tap(lambda r: action.add_success_fields(registration=r)))
                .addActionFinish())","Create a new registration with the ACME server.

        :param ~acme.messages.NewRegistration new_reg: The registration message
            to use, or ``None`` to construct one.

        :return: The registration resource.
        :rtype: Deferred[`~acme.messages.RegistrationResource`]",0,1,1,2
"def register_rml(self, filepath, **kwargs):
        
        name = os.path.split(filepath)[-1]
        if name in self.rml_maps and self.rml_maps[name] != filepath:
            raise Exception(""RML name already registered. Filenames must be ""
                            ""unique."",
                            (self.rml_maps[name], filepath))
        self.rml_maps[name] = filepath","Registers the filepath for an rml mapping

        Args:
        -----
            filepath: the path the rml file",1,0,2,3
"def register_to_ldbd(client, program, paramdict, version = u'0', cvs_repository = u'-', cvs_entry_time = 0, comment = u'-', is_online = False, jobid = 0, domain = None, ifos = u'-'):
	
	xmldoc = ligolw.Document()
	xmldoc.appendChild(ligolw.LIGO_LW())
	process = register_to_xmldoc(xmldoc, program, paramdict, version = version, cvs_repository = cvs_repository, cvs_entry_time = cvs_entry_time, comment = comment, is_online = is_online, jobid = jobid, domain = domain, ifos = ifos)

	fake_file = StringIO.StringIO()
	xmldoc.write(fake_file)
	client.insert(fake_file.getvalue())

	return process","Register the current process and params to a database via a
	LDBDClient.  The program and paramdict arguments and any additional
	keyword arguments are the same as those for register_to_xmldoc().
	Returns the new row from the process table.",0,1,0,1
"def registration_authority_entity_id(self):
        

        if ATTR_ENTITY_REGISTRATION_AUTHORITY in self.raw:
            try:
                return self.raw[
                    ATTR_ENTITY_REGISTRATION_AUTHORITY][
                    ATTR_ENTITY_REGISTRATION_AUTHORITY_ENTITY_ID][
                    ATTR_DOLLAR_SIGN]
            except KeyError:
                pass","Some entities return the register entity id,
        but other do not. Unsure if this is a bug or
        inconsistently registered data.",0,0,1,1
"def release(self):
        
        msg = self._vehicle.message_factory.mount_configure_encode(
            0, 1,    
            mavutil.mavlink.MAV_MOUNT_MODE_RC_TARGETING,  
            1,  
            1,  
            1,  
        )
        self._vehicle.send_mavlink(msg)","Release control of the gimbal to the user (RC Control).

        This should be called once you've finished controlling the mount with either :py:func:`rotate`
        or :py:func:`target_location`. Control will automatically be released if you change vehicle mode.",0,1,1,2
"def release(self, on_element=None):
        
        if on_element:
            self.move_to_element(on_element)
        if self._driver.w3c:
            self.w3c_actions.pointer_action.release()
            self.w3c_actions.key_action.pause()
        else:
            self._actions.append(lambda: self._driver.execute(Command.MOUSE_UP, {}))
        return self","Releasing a held mouse button on an element.

        :Args:
         - on_element: The element to mouse up.
           If None, releases on current mouse position.",0,2,1,3
"def release_pool(self):
        
        if self._current_acquired > 0:
            raise PoolException(""Can't release pool: %d connection(s) still acquired"" % self._current_acquired)
        while not self._pool.empty():
            conn = self.acquire()
            conn.close()
        if self._cleanup_thread is not None:
            self._thread_event.set()
            self._cleanup_thread.join()
        self._pool = None",Release pool and all its connection,1,2,1,4
"def rem_callback(self, event, cb):
        
        if event not in self._cbs:
            raise exceptions.NoSuchEventError(self.name, event)
        c = [(x[0], x[1]) for x in self._cbs[event]]
        if not c:
            raise exceptions.NoCBError(self.name, event, cb)
        self._cbs[event].remove(c[0])","Remove a callback from this node.

        The callback is removed from the specified event.

        @param cb The callback function to remove.",2,0,3,5
"def remaining_quota(self, remaining_quota):
        
        if remaining_quota is None:
            raise ValueError(""Invalid value for `remaining_quota`, must not be `None`"")
        if remaining_quota is not None and remaining_quota < 0:
            raise ValueError(""Invalid value for `remaining_quota`, must be a value greater than or equal to `0`"")

        self._remaining_quota = remaining_quota","Sets the remaining_quota of this ServicePackageMetadata.
        Current available service package quota.

        :param remaining_quota: The remaining_quota of this ServicePackageMetadata.
        :type: int",2,0,3,5
"def remove(self):
        
        
        
        self.table.drop(self.engine)
        
        exit_msg = '%s table has been removed from %s database.' % (self.table_name, self.database_name)
        self.printer(exit_msg)
        
        return exit_msg","a method to remove the entire table 
        
        :return string with status message",1,0,0,1
"def remove(self, *nonterminals):
        
        
        for nonterm in set(nonterminals):
            _NonterminalSet._control_nonterminal(nonterm)
            if nonterm not in self:
                raise KeyError('Nonterminal ' + nonterm.__name__ + ' is not inside')
            self._grammar.rules.remove(*self._assign_map[nonterm], _validate=False)
            del self._assign_map[nonterm]
            if self._grammar.start is nonterm:
                del self._grammar.start
            super().remove(nonterm)","Remove nonterminals from the set.
        Removes also rules using this nonterminal.
        Set start symbol to None if deleting nonterminal is start symbol at the same time.
        :param nonterminals: Nonterminals to remove.",1,0,3,4
"def remove(self, obj):
        
        if type(obj) is AtlasServiceInstance.Instance:
            self.remove_instance(obj)
        elif type(obj) is AtlasServiceBinding.Binding:
            self.remove_binding(obj)
        else:
            raise ErrStorageTypeUnsupported(type(obj))","Remove 
        
        Remove an object from the MongoDB storage for caching
        
        Args:
            obj (AtlasServiceBinding.Binding or AtlasServiceInstance.Instance): instance or binding
            
        Raises:
            ErrStorageTypeUnsupported: Type unsupported.",2,0,1,3
"def removeAnchor(self, anchor):
        
        if isinstance(anchor, int):
            index = anchor
        else:
            index = self._getAnchorIndex(anchor)
        index = normalizers.normalizeIndex(index)
        if index >= self._len__anchors():
            raise ValueError(""No anchor located at index %d."" % index)
        self._removeAnchor(index)","Remove ``anchor`` from the glyph.

            >>> glyph.removeAnchor(anchor)

        ``anchor`` may be an :ref:`BaseAnchor` or an
        :ref:`type-int` representing an anchor index.",1,0,2,3
"def remove_app(name, site):
    
    current_apps = list_apps(site)

    if name not in current_apps:
        log.debug('Application already absent: %s', name)
        return True

    ps_cmd = ['Remove-WebApplication',
              '-Name', ""'{0}'"".format(name),
              '-Site', ""'{0}'"".format(site)]

    cmd_ret = _srvmgr(ps_cmd)

    if cmd_ret['retcode'] != 0:
        msg = 'Unable to remove application: {0}\nError: {1}' \
              ''.format(name, cmd_ret['stderr'])
        raise CommandExecutionError(msg)

    new_apps = list_apps(site)

    if name not in new_apps:
        log.debug('Application removed successfully: %s', name)
        return True

    log.error('Unable to remove application: %s', name)
    return False","Remove an IIS application.

    Args:
        name (str): The application name.
        site (str): The IIS site name.

    Returns:
        bool: True if successful, otherwise False

    CLI Example:

    .. code-block:: bash

        salt '*' win_iis.remove_app name='app0' site='site0'",1,3,2,6
"def remove_backslash_r(filename, encoding):
        

        with open(filename, 'r', encoding=encoding, newline=r'\n') as filereader:
            contents = filereader.read()
        contents = re.sub(r'\r', '', contents)
        with open(filename, ""w"") as filewriter:
            filewriter.truncate()
            filewriter.write(contents)","A helpful utility to remove Carriage Return from any file.
        This will read a file into memory,
        and overwrite the contents of the original file.

        TODO: This function may be a liability

        :param filename:

        :return:",2,0,0,2
"def remove_control_from_group(self, process_id, wit_ref_name, group_id, control_id):
        
        route_values = {}
        if process_id is not None:
            route_values['processId'] = self._serialize.url('process_id', process_id, 'str')
        if wit_ref_name is not None:
            route_values['witRefName'] = self._serialize.url('wit_ref_name', wit_ref_name, 'str')
        if group_id is not None:
            route_values['groupId'] = self._serialize.url('group_id', group_id, 'str')
        if control_id is not None:
            route_values['controlId'] = self._serialize.url('control_id', control_id, 'str')
        self._send(http_method='DELETE',
                   location_id='1f59b363-a2d0-4b7e-9bc6-eb9f5f3f0e58',
                   version='5.0-preview.1',
                   route_values=route_values)","RemoveControlFromGroup.
        [Preview API] Removes a control from the work item form.
        :param str process_id: The ID of the process.
        :param str wit_ref_name: The reference name of the work item type.
        :param str group_id: The ID of the group.
        :param str control_id: The ID of the control to remove.",0,1,0,1
"def remove_dashboard_tag(self, id, tag_value, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.remove_dashboard_tag_with_http_info(id, tag_value, **kwargs)  
        else:
            (data) = self.remove_dashboard_tag_with_http_info(id, tag_value, **kwargs)  
            return data","Remove a tag from a specific dashboard  # noqa: E501

          # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.remove_dashboard_tag(id, tag_value, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str id: (required)
        :param str tag_value: (required)
        :return: ResponseContainer
                 If the method is called asynchronously,
                 returns the request thread.",0,1,1,2
"def remove_docstrings(tokens):
    
    prev_tok_type = None
    for index, tok in enumerate(tokens):
        token_type = tok[0]
        if token_type == tokenize.STRING:
            if prev_tok_type == tokenize.INDENT:
                
                tokens[index][1] = '' 
                
                tokens[index-1][1] = ''
                tokens[index-2][1] = ''
            elif prev_tok_type == tokenize.NL:
                
                if tokens[index+1][0] == tokenize.NEWLINE:
                    tokens[index][1] = ''
                    
                    tokens[index+1][1] = ''
        prev_tok_type = token_type","Removes docstrings from *tokens* which is expected to be a list equivalent
    of `tokenize.generate_tokens()` (so we can update in-place).",0,0,1,1
"def remove_file(filename, verbose=True):
    
    if verbose:
        LOG.info('Deleting file %s', os.path.relpath(filename, config.BASE_DIR))
    if not os.path.exists(filename):
        LOG.warning(""File does not exist: %s"", os.path.relpath(filename, config.BASE_DIR))
    else:
        os.remove(filename)","Attempt to delete filename.
    log is boolean. If true, removal is logged.
    Log a warning if file does not exist.
    Logging filenames are relative to config.BASE_DIR to cut down on noise in output.",1,2,2,5
"def remove_from_labels(self, label):
        
        assert isinstance(label, (github.Label.Label, str, unicode)), label
        if isinstance(label, github.Label.Label):
            label = label._identity
        else:
            label = urllib.quote(label)
        headers, data = self._requester.requestJsonAndCheck(
            ""DELETE"",
            self.issue_url + ""/labels/"" + label
        )",":calls: `DELETE /repos/:owner/:repo/issues/:number/labels/:name <http://developer.github.com/v3/issues/labels>`_
        :param label: :class:`github.Label.Label` or string
        :rtype: None",0,1,2,3
"def remove_program_temp_directory():
    
    if os.path.exists(program_temp_directory):
        max_retries = 3
        curr_retries = 0
        time_between_retries = 1
        while True:
            try:
                shutil.rmtree(program_temp_directory)
                break
            except IOError:
                curr_retries += 1
                if curr_retries > max_retries:
                    raise 
                time.sleep(time_between_retries)
            except:
                print(""Cleaning up temp dir..."", file=sys.stderr)
                raise",Remove the global temp directory and all its contents.,2,0,0,2
"def remover(self, id_tipo_acesso):
        
        if not is_valid_int_param(id_tipo_acesso):
            raise InvalidParameterError(
                u'Access type id is invalid or was not informed.')

        url = 'tipoacesso/' + str(id_tipo_acesso) + '/'

        code, xml = self.submit(None, 'DELETE', url)

        return self.response(code, xml)","Removes access type by its identifier.

        :param id_tipo_acesso: Access type identifier.

        :return: None

        :raise TipoAcessoError: Access type associated with equipment, cannot be removed.
        :raise InvalidParameterError: Protocol value is invalid or none.
        :raise TipoAcessoNaoExisteError: Access type doesn't exist.
        :raise DataBaseError: Networkapi failed to access the database.
        :raise XMLError: Networkapi failed to generate the XML response.",0,1,1,2
"def rename_eventtype(self, test_name=None, test_new_name=None):
        
        if test_name and test_new_name:
            name = test_name, True
            new_name = test_new_name, True
        else:
            name = QInputDialog.getText(self, 'Rename Event Type',
                                        'Enter name of event type to rename.')
        
        if name[1]:
            new_name = QInputDialog.getText(self, 'Rename Event Type',
                                            'Enter new name for event type.')
            
            if new_name[1]:
                self.annot.rename_event_type(name[0], new_name[0])
                self.display_eventtype()
                self.update_annotations()",action: create dialog to rename event type.,2,0,2,4
"def render(dson_input, saltenv='base', sls='', **kwargs):
    
    if not isinstance(dson_input, six.string_types):
        dson_input = dson_input.read()

    log.debug('DSON input = %s', dson_input)

    if dson_input.startswith('
        dson_input = dson_input[(dson_input.find('\n') + 1):]
    if not dson_input.strip():
        return {}
    return dson.loads(dson_input)","Accepts DSON data as a string or as a file object and runs it through the
    JSON parser.

    :rtype: A Python data structure",1,0,1,2
"def render(self):
        
        value = self.value
        if value is None:
            value = []

        fmt = [Int.fmt]
        data = [len(value)]

        for item_value in value:
            if issubclass(self.item_class, Primitive):
                item = self.item_class(item_value)
            else:
                item = item_value

            item_format, item_data = item.render()
            fmt.extend(item_format)
            data.extend(item_data)

        return """".join(fmt), data","Creates a composite ``struct`` format and the data to render with it.

        The format and data are prefixed with a 32-bit integer denoting the
        number of elements, after which each of the items in the array value
        are ``render()``-ed and added to the format and data as well.",1,0,2,3
"def render(self, template, **data):
        
        if(type(template) != str):
            raise TypeError(""String expected"")
        
        env = Environment(
            loader=FileSystemLoader(os.getcwd() + '/View'),
            autoescape=select_autoescape()
        )

        template = env.get_template(template)
        return self.finish(template.render(data))",Renders the template using Jinja2 with given data arguments.,1,0,0,1
"def render_code(self):
        
        tmp_dir = os.environ.get('TMP','')
        view_code = os.path.join(tmp_dir,'view.enaml')
        if os.path.exists(view_code):
            try:
                with open(view_code) as f:
                    return f.read()
            except:
                pass
        return DEFAULT_CODE","Try to load the previous code (if we had a crash or something)
            I should allow saving.",1,0,1,2
"def render_template(self, template_parameters, template_id):
        
        route_values = {}
        if template_id is not None:
            route_values['templateId'] = self._serialize.url('template_id', template_id, 'str')
        content = self._serialize.body(template_parameters, 'TemplateParameters')
        response = self._send(http_method='POST',
                              location_id='eb5d6d1d-98a2-4bbd-9028-f9a6b2d66515',
                              version='5.1-preview.1',
                              route_values=route_values,
                              content=content)
        return self._deserialize('Template', response)","RenderTemplate.
        [Preview API]
        :param :class:`<TemplateParameters> <azure.devops.v5_1.cix.models.TemplateParameters>` template_parameters:
        :param str template_id:
        :rtype: :class:`<Template> <azure.devops.v5_1.cix.models.Template>`",1,1,0,2
"def reorder(self, dst_order, arr, src_order=None):
        
        if dst_order is None:
            dst_order = self.viewer.rgb_order
        if src_order is None:
            src_order = self.rgb_order
        if src_order != dst_order:
            arr = trcalc.reorder_image(dst_order, arr, src_order)

        return arr",Reorder the output array to match that needed by the viewer.,0,0,2,2
"def repartition(self, num_partitions, repartition_function=None):
    
    from heronpy.streamlet.impl.repartitionbolt import RepartitionStreamlet
    if repartition_function is None:
      repartition_function = lambda x: x
    repartition_streamlet = RepartitionStreamlet(num_partitions, repartition_function, self)
    self._add_child(repartition_streamlet)
    return repartition_streamlet","Return a new Streamlet containing all elements of the this streamlet but having
    num_partitions partitions. Note that this is different from num_partitions(n) in
    that new streamlet will be created by the repartition call.
    If repartiton_function is not None, it is used to decide which parititons
    (from 0 to num_partitions -1), it should route each element to.
    It could also return a list of partitions if it wants to send it to multiple
    partitions.",0,0,1,1
"def replace_all(self, findpositions, find_string, replace_string):
        
        
        post_command_event(self.main_window, self.ContentChangedMsg)

        for findpos in findpositions:
            old_code = self.grid.code_array(findpos)
            new_code = old_code.replace(find_string, replace_string)

            self.grid.code_array[findpos] = new_code

        statustext = _(""Replaced {no_cells} cells."")
        statustext = statustext.format(no_cells=len(findpositions))

        post_command_event(self.main_window, self.StatusBarMsg,
                           text=statustext)

        self.grid.ForceRefresh()","Replaces occurrences of find_string with replace_string at findpos

        and marks content as changed

        Parameters
        ----------

        findpositions: List of 3-Tuple of Integer
        \tPositions in grid that shall be replaced
        find_string: String
        \tString to be overwritten in the cell
        replace_string: String
        \tString to be used for replacement",1,1,1,3
"def replace_cancel_operation_by_id(cls, cancel_operation_id, cancel_operation, **kwargs):
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async'):
            return cls._replace_cancel_operation_by_id_with_http_info(cancel_operation_id, cancel_operation, **kwargs)
        else:
            (data) = cls._replace_cancel_operation_by_id_with_http_info(cancel_operation_id, cancel_operation, **kwargs)
            return data","Replace CancelOperation

        Replace all attributes of CancelOperation
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async=True
        >>> thread = api.replace_cancel_operation_by_id(cancel_operation_id, cancel_operation, async=True)
        >>> result = thread.get()

        :param async bool
        :param str cancel_operation_id: ID of cancelOperation to replace (required)
        :param CancelOperation cancel_operation: Attributes of cancelOperation to replace (required)
        :return: CancelOperation
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def replace_filehandler(logname, new_file, level=None, frmt=None):
    
    
    log = logging.getLogger(logname)

    
    if level is not None:
        level = get_level(level)
        explicit_level = True
    else:
        level = logging.DEBUG
        explicit_level = False

    
    if frmt is not None:
        frmt = logging.Formatter(frmt)
        explicit_frmt = True
    else:
        frmt = logging.Formatter(STANDARD_FORMAT)
        explicit_frmt = False

    
    old_filehandler = None
    for handler in log.handlers:
        
        if type(handler) == logging.FileHandler:
            old_filehandler = handler
            if not explicit_level:
                level = handler.level
            if not explicit_frmt:
                frmt = handler.formatter
            break

    
    new_filehandler = logging.FileHandler(new_file)
    new_filehandler.setLevel(level)
    new_filehandler.setFormatter(frmt)

    
    log.addHandler(new_filehandler)

    
    if old_filehandler is not None:
        old_filehandler.close()
        log.removeHandler(old_filehandler)","This utility function will remove a previous Logger FileHandler, if one
    exists, and add a new filehandler.

    Parameters:
      logname
          The name of the log to reconfigure, 'openaccess_epub' for example
      new_file
          The file location for the new FileHandler
      level
          Optional. Level of FileHandler logging, if not used then the new
          FileHandler will have the same level as the old. Pass in name strings,
          'INFO' for example
      frmt
          Optional string format of Formatter for the FileHandler, if not used
          then the new FileHandler will inherit the Formatter of the old, pass
          in format strings, '%(message)s' for example

    It is best practice to use the optional level and frmt arguments to account
    for the case where a previous FileHandler does not exist. In the case that
    they are not used and a previous FileHandler is not found, then the level
    will be set logging.DEBUG and the frmt will be set to
    openaccess_epub.utils.logs.STANDARD_FORMAT as a matter of safety.",0,0,3,3
"def replace_free_shipping_coupon_by_id(cls, free_shipping_coupon_id, free_shipping_coupon, **kwargs):
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async'):
            return cls._replace_free_shipping_coupon_by_id_with_http_info(free_shipping_coupon_id, free_shipping_coupon, **kwargs)
        else:
            (data) = cls._replace_free_shipping_coupon_by_id_with_http_info(free_shipping_coupon_id, free_shipping_coupon, **kwargs)
            return data","Replace FreeShippingCoupon

        Replace all attributes of FreeShippingCoupon
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async=True
        >>> thread = api.replace_free_shipping_coupon_by_id(free_shipping_coupon_id, free_shipping_coupon, async=True)
        >>> result = thread.get()

        :param async bool
        :param str free_shipping_coupon_id: ID of freeShippingCoupon to replace (required)
        :param FreeShippingCoupon free_shipping_coupon: Attributes of freeShippingCoupon to replace (required)
        :return: FreeShippingCoupon
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def replace_namespaced_limit_range(self, name, namespace, body, **kwargs):
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.replace_namespaced_limit_range_with_http_info(name, namespace, body, **kwargs)
        else:
            (data) = self.replace_namespaced_limit_range_with_http_info(name, namespace, body, **kwargs)
            return data","replace the specified LimitRange
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.replace_namespaced_limit_range(name, namespace, body, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the LimitRange (required)
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param V1LimitRange body: (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed
        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.
        :return: V1LimitRange
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def replace_namespaced_resource_quota(self, name, namespace, body, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.replace_namespaced_resource_quota_with_http_info(name, namespace, body, **kwargs)  
        else:
            (data) = self.replace_namespaced_resource_quota_with_http_info(name, namespace, body, **kwargs)  
            return data","replace_namespaced_resource_quota  # noqa: E501

        replace the specified ResourceQuota  # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.replace_namespaced_resource_quota(name, namespace, body, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the ResourceQuota (required)
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param V1ResourceQuota body: (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed
        :return: V1ResourceQuota
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def replace_namespaced_resource_quota_status(self, name, namespace, body, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.replace_namespaced_resource_quota_status_with_http_info(name, namespace, body, **kwargs)  
        else:
            (data) = self.replace_namespaced_resource_quota_status_with_http_info(name, namespace, body, **kwargs)  
            return data","replace_namespaced_resource_quota_status  # noqa: E501

        replace status of the specified ResourceQuota  # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.replace_namespaced_resource_quota_status(name, namespace, body, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the ResourceQuota (required)
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param V1ResourceQuota body: (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed
        :return: V1ResourceQuota
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def replace_namespaced_stateful_set_status(self, name, namespace, body, **kwargs):
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.replace_namespaced_stateful_set_status_with_http_info(name, namespace, body, **kwargs)
        else:
            (data) = self.replace_namespaced_stateful_set_status_with_http_info(name, namespace, body, **kwargs)
            return data","replace status of the specified StatefulSet
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.replace_namespaced_stateful_set_status(name, namespace, body, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the StatefulSet (required)
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param V1StatefulSet body: (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed
        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.
        :return: V1StatefulSet
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def replace_node_status(self, name, body, **kwargs):
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.replace_node_status_with_http_info(name, body, **kwargs)
        else:
            (data) = self.replace_node_status_with_http_info(name, body, **kwargs)
            return data","replace status of the specified Node
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.replace_node_status(name, body, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the Node (required)
        :param V1Node body: (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed
        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.
        :return: V1Node
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def replace_priority_class(self, name, body, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.replace_priority_class_with_http_info(name, body, **kwargs)  
        else:
            (data) = self.replace_priority_class_with_http_info(name, body, **kwargs)  
            return data","replace_priority_class  # noqa: E501

        replace the specified PriorityClass  # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.replace_priority_class(name, body, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the PriorityClass (required)
        :param V1beta1PriorityClass body: (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed
        :return: V1beta1PriorityClass
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def reply(self, incoming, user, message, prefix=None):
        
        if not isinstance(user, User):
            user = User(user)

        if isinstance(incoming, User):
            if prefix:
                self.msg(user, ""%s: %s"" % (user.nick, message))
            else:
                self.msg(user, message)
        else:
            if prefix is not False:
                self.msg(incoming, ""%s: %s"" % (user.nick, message))
            else:
                self.msg(incoming, message)","Replies to a user in a given channel or PM.

        If the specified incoming is a user, simply sends a PM to user.
        If the specified incoming is a channel, prefixes the message with the
        user's nick and sends it to the channel.

        This is specifically useful in creating responses to commands that can
        be used in either a channel or in a PM, and responding to the person
        who invoked the command.",0,1,2,3
"def report(self, message, http_context=None, user=None):
        
        stack = traceback.extract_stack()
        last_call = stack[-2]
        file_path = last_call[0]
        line_number = last_call[1]
        function_name = last_call[2]
        report_location = {
            ""filePath"": file_path,
            ""lineNumber"": line_number,
            ""functionName"": function_name,
        }

        self._send_error_report(
            message,
            http_context=http_context,
            user=user,
            report_location=report_location,
        )","Reports a message to Stackdriver Error Reporting

        https://cloud.google.com/error-reporting/docs/formatting-error-messages

        :type message: str
        :param message: A user-supplied message to report

        :type http_context: :class`google.cloud.error_reporting.HTTPContext`
        :param http_context: The HTTP request which was processed when the
                             error was triggered.

        :type user: str
        :param user: The user who caused or was affected by the crash. This
                     can be a user ID, an email address, or an arbitrary
                     token that uniquely identifies the user. When sending
                     an error report, leave this field empty if the user
                     was not logged in. In this case the Error Reporting
                     system will use other data, such as remote IP address,
                     to distinguish affected users.

        Example:

        .. code-block:: python

          >>>  client.report(""Something went wrong!"")",0,1,1,2
"def report_status(self):
        
        current_status = {
            'current_track': self.core.playback.current_track.get(),
            'state': self.core.playback.state.get(),
            'time_position': self.core.playback.time_position.get(),
        }
        send_webhook(self.config, {'status_report': current_status})
        self.report_again(current_status)",Get status of player from mopidy core and send webhook.,0,2,0,2
"def req(self, url, method='get', params=None, data=None, auth=False):
        
        self.logger.debug('fetch api<%s:%s>' % (method, url))
        if auth and self.user_alias is None:
            raise Exception('cannot fetch api<%s> without session' % url)
        s = requests.Session()
        r = s.request(method, url, params=params, data=data, cookies=self.cookies, headers=self.headers,
                      timeout=self.timeout)
        s.close()
        if r.url is not url and RE_SESSION_EXPIRE.search(r.url) is not None:
            self.expire()
            if auth:
                raise Exception('auth expired, could not fetch with<%s>' % url)
        return r","请求API

        :type url: str
        :param url: API
        
        :type method: str
        :param method: HTTP METHOD
        
        :type params: dict
        :param params: query
        
        :type data: dict
        :param data: body
        
        :type auth: bool
        :param auth: if True and session expired will raise exception
        
        :rtype: requests.Response
        :return: Response",2,1,2,5
"def req_withdraw(self, address, amount, currency, fee=0, addr_tag="""", _async=False):
        
        params = {
            'address': address,
            'amount': amount,
            'currency': currency,
            'fee': fee,
            'addr-tag': addr_tag
        }
        path = '/v1/dw/withdraw/api/create'

        return api_key_post(params, path, _async=_async)","request for virtual currency withdrawal
:param address:
:param amount:
:param currency: btc, ltc, bcc, eth, etc ... (currencies supported by Huobi Pro)
:param fee:
:param addr_tag:
:return: {
""status"": ""ok"",
""data"": 700
}",0,1,1,2
"def request(http, uri, method='GET', body=None, headers=None,
            redirections=httplib2.DEFAULT_MAX_REDIRECTS,
            connection_type=None):
    
    
    http_callable = getattr(http, 'request', http)
    return http_callable(uri, method=method, body=body, headers=headers,
                         redirections=redirections,
                         connection_type=connection_type)","Make an HTTP request with an HTTP object and arguments.

    Args:
        http: httplib2.Http, an http object to be used to make requests.
        uri: string, The URI to be requested.
        method: string, The HTTP method to use for the request. Defaults
                to 'GET'.
        body: string, The payload / body in HTTP request. By default
              there is no payload.
        headers: dict, Key-value pairs of request headers. By default
                 there are no headers.
        redirections: int, The number of allowed 203 redirects for
                      the request. Defaults to 5.
        connection_type: httplib.HTTPConnection, a subclass to be used for
                         establishing connection. If not set, the type
                         will be determined from the ``uri``.

    Returns:
        tuple, a pair of a httplib2.Response with the status code and other
        headers and the bytes of the content returned.",0,1,0,1
"def request(self, confirmed=False, timeout=None, persist=None):
        
        node = new_ele(""commit"")
        if confirmed:
            self._assert("":confirmed-commit"")
            sub_ele(node, ""confirmed"")
            if timeout is not None:
                sub_ele(node, ""confirm-timeout"").text = timeout
            if persist is not None:
                sub_ele(node, ""persist"").text = persist

        return self._request(node)","Commit the candidate configuration as the device's new current configuration. Depends on the `:candidate` capability.

        A confirmed commit (i.e. if *confirmed* is `True`) is reverted if there is no followup commit within the *timeout* interval. If no timeout is specified the confirm timeout defaults to 600 seconds (10 minutes). A confirming commit may have the *confirmed* parameter but this is not required. Depends on the `:confirmed-commit` capability.

        *confirmed* whether this is a confirmed commit

        *timeout* specifies the confirm timeout in seconds

        *persist* make the confirmed commit survive a session termination, and set a token on the ongoing confirmed commit",0,0,3,3
"def requestAvatarId(self, credentials):
        
        username, domain = credentials.username.split(""@"")
        key = self.users.key(domain, username)
        if key is None:
            return defer.fail(UnauthorizedLogin())

        def _cbPasswordChecked(passwordIsCorrect):
            if passwordIsCorrect:
                return username + '@' + domain
            else:
                raise UnauthorizedLogin()

        return defer.maybeDeferred(credentials.checkPassword,
                                   key).addCallback(_cbPasswordChecked)","Return the ID associated with these credentials.

        @param credentials: something which implements one of the interfaces in
        self.credentialInterfaces.

        @return: a Deferred which will fire a string which identifies an
        avatar, an empty tuple to specify an authenticated anonymous user
        (provided as checkers.ANONYMOUS) or fire a Failure(UnauthorizedLogin).

        @see: L{twisted.cred.credentials}",1,0,2,3
"def request_camera_sensors(blink, network, camera_id):
    
    url = ""{}/network/{}/camera/{}/signals"".format(blink.urls.base_url,
                                                   network,
                                                   camera_id)
    return http_get(blink, url)","Request camera sensor info for one camera.

    :param blink: Blink instance.
    :param network: Sync module network id.
    :param camera_id: Camera ID of camera to request sesnor info from.",0,1,0,1
"def request_entity_create(self, lid, epId=None):
        
        lid = Validation.lid_check_convert(lid)
        if epId is None:
            epId = self.__epId
        elif epId is False:
            epId = None
        else:
            epId = Validation.guid_check_convert(epId)
        logger.debug(""request_entity_create lid='%s'"", lid)
        return self._request(R_ENTITY, C_CREATE, None, {'epId': epId, 'lid': lid}, is_crud=True)","request entity create: lid = local name to user
        If epId=None (default), the current agent/EP is chosen
        If epId=False, no agent is assigned
        If epId=guid, said agent is chosen",0,2,1,3
"def request_single(self, name, content={}):
        
        resp = self.request(name, content)

        
        

        for i in resp.values():
            if type(i) == list:
                return i[0]
            elif type(i) == dict:
                return i

        return None","Simple wrapper arround request to extract a single response

        :returns: the first tag in the response body",0,1,1,2
"def required(field):
    

    def wrap(f):
        def wrappedf(*args):
            result = f(*args)
            if result is None or result == """":
                raise Exception(
                    ""Config option '%s' is required."" % field)
            else:
                return result
        return wrappedf
    return wrap","Decorator that checks if return value is set, if not, raises exception.",1,0,2,3
"def rescore(self, only_if_higher):
        

        _ = self.runtime.service(self, 'i18n').ugettext

        if not self.allows_rescore():
            raise TypeError(_('Problem does not support rescoring: {}').format(self.location))

        if not self.has_submitted_answer():
            raise ValueError(_('Cannot rescore unanswered problem: {}').format(self.location))

        new_score = self.calculate_score()
        self._publish_grade(new_score, only_if_higher)","Calculate a new raw score and save it to the block.  If only_if_higher
        is True and the score didn't improve, keep the existing score.

        Raises a TypeError if the block cannot be scored.
        Raises a ValueError if the user has not yet completed the problem.

        May also raise other errors in self.calculate_score().  Currently
        unconstrained.",2,1,3,6
"def residueCounts(self, convertCaseTo='upper'):
        
        if convertCaseTo == 'none':
            def convert(x):
                return x
        elif convertCaseTo == 'lower':
            convert = str.lower
        elif convertCaseTo == 'upper':
            convert = str.upper
        else:
            raise ValueError(
                ""convertCaseTo must be one of 'none', 'lower', or 'upper'"")

        counts = defaultdict(Counter)

        for titleAlignment in self:
            read = titleAlignment.read
            for hsp in titleAlignment.hsps:
                for (subjectOffset, residue, inMatch) in read.walkHSP(hsp):
                    counts[subjectOffset][convert(residue)] += 1

        return counts","Count residue frequencies at all sequence locations matched by reads.

        @param convertCaseTo: A C{str}, 'upper', 'lower', or 'none'.
            If 'none', case will not be converted (both the upper and lower
            case string of a residue will be present in the result if they are
            present in the read - usually due to low complexity masking).
        @return: A C{dict} whose keys are C{int} offsets into the title
            sequence and whose values are C{Counters} with the residue as keys
            and the count of that residue at that location as values.",1,0,3,4
"def resize_short(src, size, interp=2):
    
    h, w, _ = src.shape
    if h > w:
        new_h, new_w = size * h // w, size
    else:
        new_h, new_w = size, size * w // h
    return imresize(src, new_w, new_h, interp=_get_interp_method(interp, (h, w, new_h, new_w)))","Resizes shorter edge to size.

    .. note:: `resize_short` uses OpenCV (not the CV2 Python library).
       MXNet must have been built with OpenCV for `resize_short` to work.

    Resizes the original image by setting the shorter edge to size
    and setting the longer edge accordingly.
    Resizing function is called from OpenCV.

    Parameters
    ----------
    src : NDArray
        The original image.
    size : int
        The length to be set for the shorter edge.
    interp : int, optional, default=2
        Interpolation method used for resizing the image.
        Possible values:
        0: Nearest Neighbors Interpolation.
        1: Bilinear interpolation.
        2: Area-based (resampling using pixel area relation). It may be a
        preferred method for image decimation, as it gives moire-free
        results. But when the image is zoomed, it is similar to the Nearest
        Neighbors method. (used by default).
        3: Bicubic interpolation over 4x4 pixel neighborhood.
        4: Lanczos interpolation over 8x8 pixel neighborhood.
        9: Cubic for enlarge, area for shrink, bilinear for others
        10: Random select from interpolation method metioned above.
        Note:
        When shrinking an image, it will generally look best with AREA-based
        interpolation, whereas, when enlarging an image, it will generally look best
        with Bicubic (slow) or Bilinear (faster but still looks OK).
        More details can be found in the documentation of OpenCV, please refer to
        http://docs.opencv.org/master/da/d54/group__imgproc__transform.html.

    Returns
    -------
    NDArray
        An 'NDArray' containing the resized image.

    Example
    -------
    >>> with open(""flower.jpeg"", 'rb') as fp:
    ...     str_image = fp.read()
    ...
    >>> image = mx.img.imdecode(str_image)
    >>> image
    <NDArray 2321x3482x3 @cpu(0)>
    >>> size = 640
    >>> new_image = mx.img.resize_short(image, size)
    >>> new_image
    <NDArray 2321x3482x3 @cpu(0)>",0,0,1,1
"def resolve_command(self, com, data, macromodulations, timeperiods):
        
        logger.debug(""Resolving: macros in: %s, arguments: %s"",
                     com.command.command_line, com.args)
        return self.resolve_simple_macros_in_string(com.command.command_line, data,
                                                    macromodulations, timeperiods,
                                                    args=com.args)","Resolve command macros with data

        :param com: check / event handler or command call object
        :type com: object
        :param data: objects list, used to search for a specific macro (custom or object related)
        :type data:
        :return: command line with '$MACRO$' replaced with values
        :param macromodulations: the available macro modulations
        :type macromodulations: dict
        :param timeperiods: the available timeperiods
        :type timeperiods: dict
        :rtype: str",0,1,1,2
"def resolve_parameters(value, parameters, date_time=datetime.datetime.now(), macros=False):
    
    merged_parameters = Query.merge_parameters(parameters, date_time=date_time, macros=macros,
                                               types_and_values=False)
    return Query._resolve_parameters(value, merged_parameters)","Resolve a format modifier with the corresponding value.

    Args:
      value: The string (path, table, or any other artifact in a cell_body) which may have format
          modifiers. E.g. a table name could be <project-id>.<dataset-id>.logs_%(_ds)s
      parameters: The user-specified list of parameters in the cell-body.
      date_time: The timestamp at which the parameters need to be evaluated. E.g. when the table
          is <project-id>.<dataset-id>.logs_%(_ds)s, the '_ds' evaluates to the current date-time.
      macros: When true, the format modifers in the value are replaced with the corresponding
          airflow macro equivalents (like '{{ ds }}'. When false, the actual values are used (like
          '2015-12-12'.

    Returns:
      The resolved value, i.e. the value with the format modifiers replaced with the corresponding
          parameter-values. E.g. if value is <project-id>.<dataset-id>.logs_%(_ds)s, the returned
          value is something like <project-id>.<dataset-id>.logs_2017-12-21",0,0,1,1
"def resolve_setting(default, arg_value=None, env_var=None, config_value=None):
    
    if arg_value is not None:
        return arg_value
    else:
        env_value = getenv(env_var)
        if env_value is not None:
            return env_value
        else:
            if config_value is not None:
                return config_value
            else:
                return default","Resolves a setting for a configuration option. The winning value is chosen
    from multiple methods of configuration, in the following order of priority
    (top first):

    - Explicitly passed argument
    - Environment variable
    - Configuration file entry
    - Default

    :param arg_value: Explicitly passed value
    :param env_var: Environment variable name
    :type env_var: string or None
    :param config_value: Configuration entry
    :param default: Default value to if there are no overriding options
    :return: Configuration value",0,0,2,2
"def resource(self, api_path=None, base_path='/api/now', chunk_size=None, **kwargs):
        

        for path in [api_path, base_path]:
            URLBuilder.validate_path(path)

        return Resource(api_path=api_path,
                        base_path=base_path,
                        parameters=self.parameters,
                        chunk_size=chunk_size or 8192,
                        session=self.session,
                        base_url=self.base_url,
                        **kwargs)","Creates a new :class:`Resource` object after validating paths

        :param api_path: Path to the API to operate on
        :param base_path: (optional) Base path override
        :param chunk_size: Response stream parser chunk size (in bytes)
        :param **kwargs: Pass request.request parameters to the Resource object
        :return:
            - :class:`Resource` object
        :raises:
            - InvalidUsage: If a path fails validation",0,0,2,2
"def resource_filename(self, resource_name, module_name=None):
        
        if module_name is None:
            mod = self._get_calling_module()
            logger.debug(f'calling module: {mod}')
            if mod is not None:
                mod_name = mod.__name__
        if module_name is None:
            module_name = __name__
        if pkg_resources.resource_exists(mod_name, resource_name):
            res = pkg_resources.resource_filename(mod_name, resource_name)
        else:
            res = resource_name
        return Path(res)","Return a resource based on a file name.  This uses the ``pkg_resources``
        package first to find the resources.  If it doesn't find it, it returns
        a path on the file system.

        :param: resource_name the file name of the resource to obtain (or name
            if obtained from an installed module)
        :param module_name: the name of the module to obtain the data, which
            defaults to ``__name__``
        :return: a path on the file system or resource of the installed module",2,0,1,3
"def response_minify(self, response):
        

        if response.content_type == u'text/html; charset=utf-8':
            endpoint = request.endpoint or ''
            view_func = current_app.view_functions.get(endpoint, None)
            name = (
                '%s.%s' % (view_func.__module__, view_func.__name__)
                if view_func else ''
            )
            if name in self._exempt_routes:
                return response

            response.direct_passthrough = False
            response.set_data(
                self._html_minify.minify(response.get_data(as_text=True))
            )

            return response
        return response",minify response html to decrease traffic,0,0,2,2
"def restart(self):
        
        version = self.get_version()

        command = [
            ""haproxy"",
            ""-f"", self.config_file_path, ""-p"", self.pid_file_path
        ]
        if version and version >= (1, 5, 0):
            command.extend([""-L"", self.peer.name])
        if os.path.exists(self.pid_file_path):
            with open(self.pid_file_path) as fd:
                command.extend([""-sf"", fd.read().replace(""\n"", """")])

        try:
            output = subprocess.check_output(command)
        except subprocess.CalledProcessError as e:
            logger.error(""Failed to restart HAProxy: %s"", str(e))
            return

        if output:
            logging.error(""haproxy says: %s"", output)

        logger.info(""Gracefully restarted HAProxy."")",Performs a soft reload of the HAProxy process.,1,4,0,5
"def restore_webhooks(self, trigger_types, project, provider_name, service_endpoint_id=None, repository=None):
        
        route_values = {}
        if project is not None:
            route_values['project'] = self._serialize.url('project', project, 'str')
        if provider_name is not None:
            route_values['providerName'] = self._serialize.url('provider_name', provider_name, 'str')
        query_parameters = {}
        if service_endpoint_id is not None:
            query_parameters['serviceEndpointId'] = self._serialize.query('service_endpoint_id', service_endpoint_id, 'str')
        if repository is not None:
            query_parameters['repository'] = self._serialize.query('repository', repository, 'str')
        content = self._serialize.body(trigger_types, '[DefinitionTriggerType]')
        self._send(http_method='POST',
                   location_id='793bceb8-9736-4030-bd2f-fb3ce6d6b478',
                   version='5.0-preview.1',
                   route_values=route_values,
                   query_parameters=query_parameters,
                   content=content)","RestoreWebhooks.
        [Preview API] Recreates the webhooks for the specified triggers in the given source code repository.
        :param [DefinitionTriggerType] trigger_types: The types of triggers to restore webhooks for.
        :param str project: Project ID or project name
        :param str provider_name: The name of the source provider.
        :param str service_endpoint_id: If specified, the ID of the service endpoint to query. Can only be omitted for providers that do not use service endpoints, e.g. TFVC or TFGit.
        :param str repository: If specified, the vendor-specific identifier or the name of the repository to get webhooks. Can only be omitted for providers that do not support multiple repositories.",2,1,2,5
"def result(self, timeout=None):
        

        self._do_wait(timeout)

        if self._exception:
            self._last_exception.exception = None
            self._last_exception.tb_info = None
            raise self._result
        else:
            return self._result","Return the last result of the callback chain or raise the last
        exception thrown and not caught by an errback.

        This will block until the result is available.
        
        If a timeout is given and the call times out raise a TimeoutError

        If SIGINT is caught while waiting raises CancelledError.

        If cancelled while waiting raises CancelledError

        This acts much like a pythonfutures.Future.result() call
        except the entire callback processing chain is performed first.",1,0,2,3
"def resume(self):
        
        
        
        if not self.is_running():
            name = self._platform_impl._process_name
            raise NoSuchProcess(self.pid, name)
        
        if hasattr(self._platform_impl, ""resume_process""):
            self._platform_impl.resume_process()
        else:
            
            self.send_signal(signal.SIGCONT)",Resume process execution.,1,1,1,3
"def retain_all(self, items):
        
        check_not_none(items, ""Value can't be None"")
        data_items = []
        for item in items:
            check_not_none(item, ""Value can't be None"")
            data_items.append(self._to_data(item))
        return self._encode_invoke(list_compare_and_retain_all_codec, values=data_items)","Retains only the items that are contained in the specified collection. It means, items which are not present in
        the specified collection are removed from this list.

        :param items: (Collection), collections which includes the elements to be retained in this list.
        :return: (bool), ``true`` if this list changed as a result of the call.",0,0,2,2
"def retract(self, jid, node, id_, *, notify=False):
        
        retract = pubsub_xso.Retract()
        retract.node = node
        item = pubsub_xso.Item()
        item.id_ = id_
        retract.item = item
        retract.notify = notify

        iq = aioxmpp.stanza.IQ(to=jid, type_=aioxmpp.structs.IQType.SET)
        iq.payload = pubsub_xso.Request(
            retract
        )

        yield from self.client.send(iq)","Retract a previously published item from a node.

        :param jid: Address of the PubSub service.
        :type jid: :class:`aioxmpp.JID`
        :param node: Name of the PubSub node to send a notify from.
        :type node: :class:`str`
        :param id_: The ID of the item to retract.
        :type id_: :class:`str`
        :param notify: Flag indicating whether subscribers shall be notified
            about the retraction.
        :type notify: :class:`bool`
        :raises aioxmpp.errors.XMPPError: as returned by the service

        Retract an item previously published to `node` at `jid`. `id_` must be
        the ItemID of the item to retract.

        If `notify` is set to true, notifications will be generated (by setting
        the `notify` attribute on the retraction request).",0,1,1,2
"def retrieve_api_token(self):
        

        payload = self.oauth2_manager.get_access_token_params(
            refresh_token=self.refresh_token
        )
        response = requests.post(
            self.oauth2_manager.access_token_url, json=payload
        )
        response.raise_for_status()
        response_json = json.loads(response.text)
        return response_json['access_token']","Retrieve the access token from AVS.

        This function is memoized, so the
        value returned by the function will be remembered and returned by
        subsequent calls until the memo expires. This is because the access
        token lasts for one hour, then a new token needs to be requested.

        Decorators:
            helpers.expiring_memo

        Returns:
            str -- The access token for communicating with AVS",0,1,0,1
"def returner(ret):
    

    _options = _get_options(ret)
    log.debug('_options %s', _options)
    _options['hostname'] = ret.get('id')

    if 'url' not in _options or _options['url'] == '':
        log.error('nagios_nrdp.url not defined in salt config')
        return

    if 'token' not in _options or _options['token'] == '':
        log.error('nagios_nrdp.token not defined in salt config')
        return

    xml = _prepare_xml(options=_options, state=ret['return'])
    res = _post_data(options=_options, xml=xml)

    return res",Send a message to Nagios with the data,0,4,1,5
"def returner(ret):
    
    _options = _get_options(ret)

    sid = _options.get('sid', None)
    token = _options.get('token', None)
    sender = _options.get('from', None)
    receiver = _options.get('to', None)

    if sid is None or token is None:
        log.error('Twilio sid/authentication token missing')
        return None

    if sender is None or receiver is None:
        log.error('Twilio to/from fields are missing')
        return None

    client = TwilioRestClient(sid, token)

    try:
        message = client.messages.create(
            body='Minion: {0}\nCmd: {1}\nSuccess: {2}\n\nJid: {3}'.format(
                ret['id'], ret['fun'], ret['success'], ret['jid']
            ), to=receiver, from_=sender)
    except TwilioRestException as e:
        log.error(
            'Twilio [https://www.twilio.com/docs/errors/%s]',
            e.code
        )
        return False

    return True",Return a response in an SMS message,0,4,3,7
"def revoke_api_key():
    
    build = g.build
    form = forms.RevokeApiKeyForm()
    if form.validate_on_submit():
        api_key = models.ApiKey.query.get(form.id.data)
        if api_key.build_id != build.id:
            logging.debug('User does not have access to API key=%r',
                          api_key.id)
            abort(403)

        api_key.active = False
        save_admin_log(build, revoked_api_key=True, message=api_key.id)

        db.session.add(api_key)
        db.session.commit()

        ops = operations.ApiKeyOps(api_key.id, api_key.secret)
        ops.evict()

    return redirect(url_for('manage_api_keys', build_id=build.id))",Form submission handler for revoking API keys.,2,2,1,5
"def revoke_auth(self, load):
        
        load = self.__verify_load(load, ('id', 'tok'))

        if not self.opts.get('allow_minion_key_revoke', False):
            log.warning(
                'Minion %s requested key revoke, but allow_minion_key_revoke '
                'is set to False', load['id']
            )
            return load

        if load is False:
            return load
        else:
            return self.masterapi.revoke_auth(load)","Allow a minion to request revocation of its own key

        :param dict load: The minion payload

        :rtype: dict
        :return: If the load is invalid, it may be returned. No key operation is performed.

        :rtype: bool
        :return: True if key was revoked, False if not",0,2,0,2
"def rgb_to_yiq(rgb):
    
    r, g, b = rgb[0] / 255, rgb[1] / 255, rgb[2] / 255
    y = (0.299 * r) + (0.587 * g) + (0.114 * b)
    i = (0.596 * r) - (0.275 * g) - (0.321 * b)
    q = (0.212 * r) - (0.528 * g) + (0.311 * b)
    return round(y, 3), round(i, 3), round(q, 3)","Convert an RGB color representation to a YIQ color representation.

    (r, g, b) :: r -> [0, 255]
                 g -> [0, 255]
                 b -> [0, 255]

    :param rgb: A tuple of three numeric values corresponding to the red, green, and blue value.
    :return: YIQ representation of the input RGB value.
    :rtype: tuple",0,0,1,1
"def rgb_view(qimage, byteorder = 'big'):
    
    if byteorder is None:
        byteorder = _sys.byteorder
    bytes = byte_view(qimage, byteorder)
    if bytes.shape[2] != 4:
        raise ValueError(""For rgb_view, the image must have 32 bit pixel size (use RGB32, ARGB32, or ARGB32_Premultiplied)"")

    if byteorder == 'little':
        return bytes[...,:3] 
    else:
        return bytes[...,1:]","Returns RGB view of a given 32-bit color QImage_'s memory.
    Similarly to byte_view(), the result is a 3D numpy.uint8 array,
    but reduced to the rgb dimensions (without alpha), and reordered
    (using negative strides in the last dimension) to have the usual
    [R,G,B] order.  The image must have 32 bit pixel size, i.e. be
    RGB32, ARGB32, or ARGB32_Premultiplied.  (Note that in the latter
    case, the values are of course premultiplied with alpha.)

    The order of channels in the last axis depends on the `byteorder`,
    which defaults to 'big', i.e. RGB order.  You may set the argument
    `byteorder` to 'little' to get BGR, or use None which means
    sys.byteorder here, i.e. return native order for the machine the
    code is running on.

    For your convenience, `qimage` may also be a filename, see
    `Loading and Saving Images`_ in the documentation.

    :param qimage: image whose memory shall be accessed via NumPy
    :type qimage: QImage_ with 32-bit pixel type
    :param byteorder: specify order of channels in last axis
    :rtype: numpy.ndarray_ with shape (height, width, 3) and dtype uint8",1,0,2,3
"def rho_material(ID):
    r
    if ID not in materials_dict:
        ID = nearest_material(ID)
    if ID in refractories:
        rho = float(refractories[ID][0]) 
    elif ID in building_materials:
        rho = float(building_materials[ID][0]) 
    else:
        rho = ASHRAE[ID][0]
        if rho is None:
            raise Exception('Density is not available for this material')
        else:
            rho = float(rho)
    return rho","r'''Returns the density of a building, insulating, or refractory
    material from tables  in [1]_, [2]_, and [3]_. No temperature dependence is
    available. Function must be provided with either a key to one of the
    dictionaries `refractories`, `ASHRAE`, or `building_materials` - or a
    search term which will pick the closest match based on a fuzzy search. To
    determine which source the fuzzy search will pick, use the function
    `nearest_material`. Fuzzy searches are slow; it is preferable to call this
    function with a material key directly.

    Parameters
    ----------
    ID : str
        String as described above

    Returns
    -------
    rho : float
        Density of the material, [kg/m^3]

    Examples
    --------
    >>> rho_material('Board, Asbestos/cement')
    1900.0

    References
    ----------
    .. [1] ASHRAE Handbook: Fundamentals. American Society of Heating,
       Refrigerating and Air-Conditioning Engineers, Incorporated, 2013.
    .. [2] DIN EN 12524 (2000-07) Building Materials and Products
       Hygrothermal Properties - Tabulated Design Values; English Version of
       DIN EN 12524.
    .. [3] Gesellschaft, V. D. I., ed. VDI Heat Atlas. 2nd edition.
       Berlin; New York:: Springer, 2010.",1,0,2,3
"def ripple_withdrawal(self, amount, address, currency):
        
        data = {'amount': amount, 'address': address, 'currency': currency}
        response = self._post(""ripple_withdrawal/"", data=data,
                              return_json=True)
        return self._expect_true(response)",Returns true if successful.,0,1,1,2
"def rmmkdir(dir_path):
        
        
        if not os.path.isdir(dir_path) or not os.path.exists(dir_path):
            os.makedirs(dir_path)
        else:
            rmtree(dir_path, True)
            os.makedirs(dir_path)","If directory existed, then remove and make; else make it.",2,0,1,3
"def rnni(self, use_weighted_choice=False, invert_weights=False):
        
        if use_weighted_choice:
            leaves = list(self.tree._tree.leaf_edge_iter())
            e, _ = self.tree.map_event_onto_tree(excluded_edges=leaves, invert_weights=invert_weights)
        else:
            e = random.choice(self.tree.get_inner_edges())
        children = self.get_children(e)
        h = random.choice(children['head'])
        t = random.choice(children['tail'])
        self.nni(e, h, t)","Apply a random NNI operation at a randomly selected edge
        The edge can be chosen uniformly, or weighted by length --
        invert_weights favours short edges.",0,0,3,3
"def rolling_returns_cumulative(returns, window, min_periods=1, geometric=True):
    
    if geometric:
        rc = lambda x: (1. + x[np.isfinite(x)]).prod() - 1.
    else:
        rc = lambda x: (x[np.isfinite(x)]).sum()

    return pd.rolling_apply(returns, window, rc, min_periods=min_periods)","return the rolling cumulative returns

    Parameters
    ----------
    returns : DataFrame or Series
    window : number of observations
    min_periods : minimum number of observations in a window
    geometric : link the returns geometrically",0,0,2,2
"def rotate(self, matrix, tol=1e-3):
        
        matrix = SquareTensor(matrix)
        if not matrix.is_rotation(tol):
            raise ValueError(""Rotation matrix is not valid."")
        sop = SymmOp.from_rotation_and_translation(matrix,
                                                   [0., 0., 0.])
        return self.transform(sop)","Applies a rotation directly, and tests input matrix to ensure a valid
        rotation.

        Args:
            matrix (3x3 array-like): rotation matrix to be applied to tensor
            tol (float): tolerance for testing rotation matrix validity",1,0,2,3
"def route(self, arg, destination=None, waypoints=None, raw=False, **kwargs):
        
        points = _parse_points(arg, destination, waypoints)
        if len(points) < 2:
            raise ValueError('You must specify at least 2 points')

        self.rate_limit_wait()
        data = self.raw_query(points, **kwargs)
        self._last_query = time.time()

        if raw:
            return data
        return self.format_output(data)","Query a route.

        route(locations): points can be
            - a sequence of locations
            - a Shapely LineString
        route(origin, destination, waypoints=None)
             - origin and destination are a single destination
             - waypoints are the points to be inserted between the
               origin and destination
             
             If waypoints is specified, destination must also be specified

        Each location can be:
            - string (will be geocoded by the routing provider. Not all
              providers accept this as input)
            - (longitude, latitude) sequence (tuple, list, numpy array, etc.)
            - Shapely Point with x as longitude, y as latitude

        Additional parameters
        ---------------------
        raw : bool, default False
            Return the raw json dict response from the service

        Returns
        -------
        list of Route objects
        If raw is True, returns the json dict instead of converting to Route
        objects

        Examples
        --------
        mq = directions.Mapquest(key)
        routes = mq.route('1 magazine st. cambridge, ma', 
                          'south station boston, ma')

        routes = mq.route('1 magazine st. cambridge, ma', 
                          'south station boston, ma', 
                          waypoints=['700 commonwealth ave. boston, ma'])

        # Uses each point in the line as a waypoint. There is a limit to the
        # number of waypoints for each service. Consult the docs.
        line = LineString(...)
        routes = mq.route(line)  

        # Feel free to mix different location types
        routes = mq.route(line.coords[0], 'south station boston, ma',
                          waypoints=[(-71.103972, 42.349324)])",0,1,0,1
"def router_id(self, **kwargs):
        
        router_id = kwargs.pop('router_id')
        rbridge_id = kwargs.pop('rbridge_id', '1')
        callback = kwargs.pop('callback', self._callback)
        rid_args = dict(rbridge_id=rbridge_id, router_id=router_id)
        config = self._rbridge.rbridge_id_ip_rtm_config_router_id(**rid_args)
        return callback(config)","Configures device's Router ID.

        Args:
            router_id (str): Router ID for the device.
            rbridge_id (str): The rbridge ID of the device on which BGP will be
                configured in a VCS fabric.
            callback (function): A function executed upon completion of the
                method.  The only parameter passed to `callback` will be the
                ``ElementTree`` `config`.

        Returns:
            Return value of `callback`.

        Raises:
            KeyError: if `router_id` is not specified.

        Examples:
            >>> import pynos.device
            >>> conn = ('10.24.39.211', '22')
            >>> auth = ('admin', 'password')
            >>> with pynos.device.Device(conn=conn, auth=auth) as dev:
            ...     output = dev.system.router_id(router_id='10.24.39.211',
            ...     rbridge_id='225')
            ...     dev.system.router_id() # doctest: +IGNORE_EXCEPTION_DETAIL
            Traceback (most recent call last):
            KeyError",1,0,2,3
"def routers_updated(self, context, routers):
        
        LOG.debug('Got routers updated notification :%s', routers)
        if routers:
            
            if isinstance(routers[0], dict):
                routers = [router['id'] for router in routers]
            self._update_updated_routers_cache(routers)",Deal with routers modification and creation RPC message.,0,1,1,2
"def run(func):
    

    @defaults.command(help='Run the service')
    @click.pass_context
    def runserver(ctx, *args, **kwargs):
        if (ctx.parent.invoked_subcommand and
                ctx.command.name != ctx.parent.invoked_subcommand):
            return

        
        
        sys.argv = [sys.argv[0]] + [a for a in sys.argv if a[0] == '-']

        sys.exit(func())

    return runserver",Execute the provided function if there are no subcommands,0,0,2,2
"def run(self):
        
        document = {}
        document['_id'] = hashlib.sha1('%s:%s' % (
                                       self.date, self.url)).hexdigest()
        with self.input().open() as handle:
            document['content'] = handle.read().decode('utf-8', 'ignore')
        document['url'] = self.url
        document['date'] = unicode(self.date)
        with self.output().open('w') as output:
            output.write(json.dumps(document))",Construct the document id from the date and the url.,1,0,1,2
"def run(self):
        
        try:
            while True:
                message = self.connection.recv()
                result = self.on_message(message)
                if result:
                    self.connection.send(result)
        except SelenolWebSocketClosedException as ex:
            self.on_closed(0, '')
            raise SelenolWebSocketClosedException() from ex",Run the service in infinitive loop processing requests.,1,1,1,3
"def run(self, queue):
        
        
        time.sleep(random.random())

        
        obj = self.get_object()
        obj.fullname.hset('%s %s' % tuple(obj.hmget('firstname', 'lastname')))

        
        result = 'Created fullname for Person %s: %s' % (obj.pk.get(), obj.fullname.hget())

        
        self.result.set(result)

        
        return result","Create the fullname, and store a a message serving as result in the job",0,0,2,2
"def runInteraction(self, interaction, *args, **kwargs):
        
        try:
            return self._connectionPool.runInteraction(
                interaction,
                *args,
                **kwargs
            )
        except:
            d = defer.Deferred()
            d.errback()
            return d","Interact with the database and return the result.

        :param interaction: <function> method with first argument is a <adbapi.Transaction> instance
        :param args: additional positional arguments to be passed to interaction
        :param kwargs: keyword arguments to be passed to interaction
        :return: <defer>",1,0,0,1
"def run_command(cmd,
                stdout=None,
                stderr=None,
                shell=False,
                timeout=None,
                cwd=None,
                env=None):
    
    
    
    
    import psutil
    if stdout is None:
        stdout = subprocess.PIPE
    if stderr is None:
        stderr = subprocess.PIPE
    process = psutil.Popen(
        cmd, stdout=stdout, stderr=stderr, shell=shell, cwd=cwd, env=env)
    timer = None
    timer_triggered = threading.Event()
    if timeout and timeout > 0:
        
        

        def timeout_expired():
            timer_triggered.set()
            process.terminate()

        timer = threading.Timer(timeout, timeout_expired)
        timer.start()
    
    
    (out, err) = process.communicate()
    if timer is not None:
        timer.cancel()
    if timer_triggered.is_set():
        raise psutil.TimeoutExpired(timeout, pid=process.pid)
    return (process.returncode, out, err)","Runs a command in a subprocess.

    This function is very similar to subprocess.check_output. The main
    difference is that it returns the return code and std error output as well
    as supporting a timeout parameter.

    Args:
        cmd: string or list of strings, the command to run.
            See subprocess.Popen() documentation.
        stdout: file handle, the file handle to write std out to. If None is
            given, then subprocess.PIPE is used. See subprocess.Popen()
            documentation.
        stdee: file handle, the file handle to write std err to. If None is
            given, then subprocess.PIPE is used. See subprocess.Popen()
            documentation.
        shell: bool, True to run this command through the system shell,
            False to invoke it directly. See subprocess.Popen() docs.
        timeout: float, the number of seconds to wait before timing out.
            If not specified, no timeout takes effect.
        cwd: string, the path to change the child's current directory to before
            it is executed. Note that this directory is not considered when
            searching the executable, so you can't specify the program's path
            relative to cwd.
        env: dict, a mapping that defines the environment variables for the
            new process. Default behavior is inheriting the current process'
            environment.

    Returns:
        A 3-tuple of the consisting of the return code, the std output, and the
            std error.

    Raises:
        psutil.TimeoutExpired: The command timed out.",3,2,3,8
"def run_job(self, job_spec, wait_until_done=True):
        

        
        
        json_string = json.dumps({'lsid': job_spec.lsid, 'params': job_spec.params, 'tags': ['GenePattern Python Client']}, cls=GPJSONEncoder)
        if sys.version_info.major == 3:  
            json_string = bytes(json_string, 'utf-8')
        request = urllib.request.Request(self.url + '/rest/v1/jobs')
        if self.authorization_header() is not None:
            request.add_header('Authorization', self.authorization_header())
        request.add_header('Content-Type', 'application/json')
        request.add_header('User-Agent', 'GenePatternRest')
        response = urllib.request.urlopen(request, json_string)
        if response.getcode() != 201:
            print("" job POST failed, status code = %i"" % response.getcode())
            return None
        data = json.loads(response.read().decode('utf-8'))
        job = GPJob(self, data['jobId'])
        job.get_info()
        self.last_job = job  
        if wait_until_done:
            job.wait_until_done()
        return job","Runs a job defined by jobspec, optionally non-blocking.

        Takes a GPJobSpec object that defines a request to run a job, and makes the
        request to the server.  By default blocks until the job is finished by
        polling the server, but can also run asynchronously.

        Args:
            :param job_spec: A GPJobSpec object that contains the data defining the job to be run.
            :param wait_until_done: Whether to wait until the job is finished before returning.
            :return:

        Returns:
            a GPJob object that refers to the running job on the server.  If called
            synchronously, this object will contain the info associated with the
            completed job.  Otherwise, it will just wrap the URI of the running job.",0,1,3,4
"def run_ops(state, serial=False, no_wait=False):
    

    
    state.deploying = True

    
    if serial:
        _run_serial_ops(state)

    
    elif no_wait:
        _run_no_wait_ops(state)

    
    for op_hash in state.get_op_order():
        _run_single_op(state, op_hash)","Runs all operations across all servers in a configurable manner.

    Args:
        state (``pyinfra.api.State`` obj): the deploy state to execute
        serial (boolean): whether to run operations host by host
        no_wait (boolean): whether to wait for all hosts between operations",0,1,1,2
"def run_step(context):
    
    logger.debug(""started"")

    format_expression = context.get('nowUtcIn', None)

    if format_expression:
        formatted_expression = context.get_formatted_string(format_expression)
        context['nowUtc'] = datetime.now(
            timezone.utc).strftime(formatted_expression)
    else:
        context['nowUtc'] = datetime.now(timezone.utc).isoformat()

    logger.info(f""timestamp {context['nowUtc']} saved to context nowUtc"")
    logger.debug(""done"")","pypyr step saves current utc datetime to context.

    Args:
        context: pypyr.context.Context. Mandatory.
                 The following context key is optional:
                - nowUtcIn. str. Datetime formatting expression. For full list
                  of possible expressions, check here:
                  https://docs.python.org/3.7/library/datetime.html#strftime-and-strptime-behavior

    All inputs support pypyr formatting expressions.

    This step creates now in context, containing a string representation of the
    timestamp. If input formatting not specified, defaults to ISO8601.

    Default is:
    YYYY-MM-DDTHH:MM:SS.ffffff+00:00, or, if microsecond is 0,
    YYYY-MM-DDTHH:MM:SS

    Returns:
        None. updates context arg.",0,2,1,3
"def run_vcs_tool(path, action):
    
    info = get_vcs_info(get_vcs_root(path))
    tools = info['actions'][action]
    for tool, args in tools:
        if programs.find_program(tool):
            if not running_under_pytest():
                programs.run_program(tool, args, cwd=path)
            else:
                return True
            return
    else:
        cmdnames = [name for name, args in tools]
        raise ActionToolNotFound(info['name'], action, cmdnames)","If path is a valid VCS repository, run the corresponding VCS tool
    Supported VCS actions: 'commit', 'browse'
    Return False if the VCS tool is not installed",1,1,2,4
"def runs(self):
        
        api_version = self._get_api_version('runs')
        if api_version == '2018-09-01':
            from .v2018_09_01.operations import RunsOperations as OperationClass
        else:
            raise NotImplementedError(""APIVersion {} is not available"".format(api_version))
        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))","Instance depends on the API version:

           * 2018-09-01: :class:`RunsOperations<azure.mgmt.containerregistry.v2018_09_01.operations.RunsOperations>`",1,0,2,3
"def safe_start_ingest(event):
    
    try:
        ingest(event)
    except Exception:
        logger.error('Something went wrong during the upload')
        logger.error(traceback.format_exc())
        
        recording_state(event.uid, 'upload_error')
        update_event_status(event, Status.FAILED_UPLOADING)
        set_service_status_immediate(Service.INGEST, ServiceStatus.IDLE)","Start a capture process but make sure to catch any errors during this
    process, log them but otherwise ignore them.",0,3,0,3
"def sample_from_data(self, model, data):
        
        values = []
        for key, type_name in self.mapping:
            value_type = self.types[type_name]
            values.append(value_type(data[key]))
        if self.unwrap_sample:
            assert len(values) == 1
            return np.array(values[0])
        else:
            return np.array(values, dtype=object)","Convert incoming sample *data* into a numpy array.

        :param model:
          The :class:`~Model` instance to use for making predictions.
        :param data:
          A dict-like with the sample's data, typically retrieved from
          ``request.args`` or similar.",0,0,1,1
"def samplewise_norm(
        x, rescale=None, samplewise_center=False, samplewise_std_normalization=False, channel_index=2, epsilon=1e-7
):
    
    if rescale:
        x *= rescale

    if x.shape[channel_index] == 1:
        
        if samplewise_center:
            x = x - np.mean(x)
        if samplewise_std_normalization:
            x = x / np.std(x)
        return x
    elif x.shape[channel_index] == 3:
        
        if samplewise_center:
            x = x - np.mean(x, axis=channel_index, keepdims=True)
        if samplewise_std_normalization:
            x = x / (np.std(x, axis=channel_index, keepdims=True) + epsilon)
        return x
    else:
        raise Exception(""Unsupported channels %d"" % x.shape[channel_index])","Normalize an image by rescale, samplewise centering and samplewise centering in order.

    Parameters
    -----------
    x : numpy.array
        An image with dimension of [row, col, channel] (default).
    rescale : float
        Rescaling factor. If None or 0, no rescaling is applied, otherwise we multiply the data by the value provided (before applying any other transformation)
    samplewise_center : boolean
        If True, set each sample mean to 0.
    samplewise_std_normalization : boolean
        If True, divide each input by its std.
    epsilon : float
        A small position value for dividing standard deviation.

    Returns
    -------
    numpy.array
        A processed image.

    Examples
    --------
    >>> x = samplewise_norm(x, samplewise_center=True, samplewise_std_normalization=True)
    >>> print(x.shape, np.mean(x), np.std(x))
    (160, 176, 1), 0.0, 1.0

    Notes
    ------
    When samplewise_center and samplewise_std_normalization are True.
    - For greyscale image, every pixels are subtracted and divided by the mean and std of whole image.
    - For RGB image, every pixels are subtracted and divided by the mean and std of this pixel i.e. the mean and std of a pixel is 0 and 1.",1,0,4,5
"def sanitize_ep(endpoint, plural=False):
        
        
        if plural:
            if endpoint.endswith('y'):
                endpoint = endpoint[:-1] + 'ies'
            elif not endpoint.endswith('s'):
                endpoint += 's'
        else:
            
            if endpoint.endswith('ies'):
                endpoint = endpoint[:-3] + 'y'
            elif endpoint.endswith('s'):
                endpoint = endpoint[:-1]

        return endpoint","Sanitize an endpoint to a singular or plural form.

        Used mostly for convenience in the `_parse` method to grab the raw
        data from queried datasets.

        XXX: this is el cheapo (no bastante bien)",0,0,1,1
"def sanitize_http_request_body(client, event):
    
    try:
        body = force_text(event[""context""][""request""][""body""], errors=""replace"")
    except (KeyError, TypeError):
        return event
    if ""="" in body:
        sanitized_query_string = _sanitize_string(body, ""&"", ""="")
        event[""context""][""request""][""body""] = sanitized_query_string
    return event","Sanitizes http request body. This only works if the request body
    is a query-encoded string. Other types (e.g. JSON) are not handled by
    this sanitizer.

    :param client: an ElasticAPM client
    :param event: a transaction or error event
    :return: The modified event",0,0,2,2
"def save(
        self,
        path,
        name,
    ):
        
        t = 'track type=bed name=""{}_anchor1""'.format(name)
        self.bt1.saveas(path + '_anchor1.bed', trackline=t)
        self._bt1_path = path + '_anchor1.bed'
        t = 'track type=bed name=""{}_anchor2""'.format(name)
        self.bt2.saveas(path + '_anchor2.bed', trackline=t)
        self._bt2_path = path + '_anchor2.bed'
        t = 'track type=bed name=""{}_loop""'.format(name)
        self.bt_loop.saveas(path + '_loop.bed', trackline=t)
        self._bt_loop_path = path + '_loop.bed'
        t = 'track type=bed name=""{}_loop_inner""'.format(name)
        self.bt_loop_inner.saveas(path + '_loop_inner.bed', trackline=t)
        self._bt_loop_inner_path = path + '_loop_inner.bed'
        import cPickle
        cPickle.dump(self, open(path + '.pickle', 'w'))","Save AnnotatedInteractions object and bed files. The object is stored in
        a pickle and the bed files are saved as separate bed files. The object
        can be reloaded by reading the pickle using cPickle and the BedTool
        objects can be recreated using .load_saved_bts().
            
        Parameters
        ----------
        path : str
            Path to save files to. Path should include a basename for the files.
            For instance, path='~/abc' will create files like ~/abc.pickle,
            ~/abc_anchor1.bed, etc.
            
        name : str
            Descriptive name used for bed file trackline.",2,0,0,2
"def save(self):
        

        
        if hasattr(self, 'id'):
            
            params = self.deflate(self.__properties__, self)
            query = ""MATCH (n) WHERE id(n)={self} \n""
            query += ""\n"".join([""SET n.{0} = {{{1}}}"".format(key, key) + ""\n""
                                for key in params.keys()])
            for label in self.inherited_labels():
                query += ""SET n:`{0}`\n"".format(label)
            self.cypher(query, params)
        elif hasattr(self, 'deleted') and self.deleted:
            raise ValueError(""{0}.save() attempted on deleted node"".format(
                self.__class__.__name__))
        else:  
            self.id = self.create(self.__properties__)[0].id
        return self","Save the node to neo4j or raise an exception

        :return: the node instance",3,0,1,4
"def save(self):
        
        client = self._new_api_client()
        params = {'id': self.id} if hasattr(self, 'id') else {}
        action =  'patch' if hasattr(self, 'id') else 'post'
        saved_model = client.make_request(self, action, url_params=params, post_data=self._to_json)
        self.__init__(**saved_model._to_dict)",Save an instance of a Union object,0,1,1,2
"def save(self):
        
        ret = False

        
        pk = None
        if self.schema.pk.name not in self.modified_fields:
            pk = self.pk

        if pk:
            ret = self.update()
        else:
            ret = self.insert()

        return ret","persist the fields in this object into the db, this will update if _id is set, otherwise
        it will insert

        see also -- .insert(), .update()",3,0,1,4
"def save(self, config_loc=None):
        
        if not os.path.exists(_USER_CONFIG_DIR):
            
            os.makedirs(_USER_CONFIG_DIR)
        with open(_DEFAULT_PATH, 'w') as f:
            json.dump({'key': self._key, 'base_url': self._base_url,
                       'username': self._username}, f)","Saves current user credentials to user directory.

        Args:
            config_loc (str, optional): Location where credentials are to be
                stored. If no argument is provided, it will be send to the
                default location.

        Example:

            .. code::

                from cartoframes import Credentials
                creds = Credentials(username='eschbacher', key='abcdefg')
                creds.save()  # save to default location",1,0,0,1
"def save(self, filelike):
        
        
        self._shade_remaining_missing()

        
        canvas = Canvas(filelike, pagesize=self._pagesize)

        
        for page in self._pages:
            renderPDF.draw(page, canvas, 0, 0)
            canvas.showPage()

        
        canvas.save()","Save the file as a PDF.

        Parameters
        ----------
        filelike: path or file-like object
            The filename or file-like object to save the labels under. Any
            existing contents will be overwritten.",2,0,0,2
"def save(self, filepath):
        
        self._config.add_section(""RetryPolicy"")
        self._config.set(""RetryPolicy"", ""retries"", str(self.retry_policy.retries))
        self._config.set(""RetryPolicy"", ""backoff_factor"",
                         str(self.retry_policy.backoff_factor))
        self._config.set(""RetryPolicy"", ""max_backoff"",
                         str(self.retry_policy.max_backoff))
        super(RequestHTTPSenderConfiguration, self).save(filepath)","Save current configuration to file.

        :param str filepath: Path to file where settings will be saved.
        :raises: ValueError if supplied filepath cannot be written to.",4,0,0,4
"def save(self, key, kw):
        
        if key not in self:
            obj = hdf5.LiteralAttrs()
        else:
            obj = self[key]
        vars(obj).update(kw)
        self[key] = obj
        self.flush()","Update the object associated to `key` with the `kw` dictionary;
        works for LiteralAttrs objects and automatically flushes.",1,0,1,2
"def save(self, name):
        
        with open(name, 'wb+') as f:
            while True:
                buf = self._fileobj.read()
                if not buf:
                    break
                f.write(buf)","Saves the entire Docker context tarball to a separate file.

        :param name: File path to save the tarball into.
        :type name: unicode | str",1,0,0,1
"def save(self, out, kind=None, **kw):
        
        m = len(self)

        def prepare_fn_noop(o, n):
            
            return o

        def prepare_filename(o, n):
            
            return o.format(m, n)

        prepare_fn = prepare_fn_noop
        if m > 1 and isinstance(out, str_type):
            dot_idx = out.rfind('.')
            if dot_idx > -1:
                out = out[:dot_idx] + '-{0:02d}-{1:02d}' + out[dot_idx:]
                prepare_fn = prepare_filename
        for n, qrcode in enumerate(self, start=1):
            qrcode.save(prepare_fn(out, n), kind=kind, **kw)","
        Saves the sequence of QR Code to `out`.

        If `out` is a filename, this method modifies the filename and adds
        ``<Number of QR Codes>-<Current QR Code>`` to it.
        ``structured-append.svg`` becomes (if the sequence contains two QR Codes):
        ``structured-append-02-01.svg`` and ``structured-append-02-02.svg``

        Please note that using a file or file-like object may result into an
        invalid serialization format since all QR Codes are written to the same
        output.

        See :py:meth:`QRCode.save()` for a detailed enumeration of options.",1,0,3,4
"def save(self, path):
        

        path_dir = os.path.split(path)[0]
        if not os.path.exists(path_dir):
            try:
                os.makedirs(path_dir)
            except OSError:
                pass

        with open(path, 'wb') as out:
            out.write(self._bytes_body if self._bytes_body is not None
                      else b'')",Save response body to file.,1,0,0,1
"def save(self, path):
        
        filename = self.get_filename()
        path = os.path.expanduser(path)
        if os.path.isdir(path):
            if filename:
                basename = os.path.basename(filename)
                file_ = open(os.path.join(path, basename), ""wb"")
            else:
                file_ = tempfile.NamedTemporaryFile(delete=False, dir=path)
        else:
            file_ = open(path, ""wb"")  
        self.write(file_)
        file_.close()
        return file_.name","save the attachment to disk. Uses :meth:`~get_filename` in case path
        is a directory",1,0,0,1
"def save(self, path, format, binary=False, use_load_condition=False):
        
        if use_load_condition:
            if self._load_cond is None:
                raise ValueError(
                    b""use_load_condition was specified but the object is not ""
                    b""loaded from a file"")
            
            mod = self._load_cond
        else:
            mod = _select_module(format, binary)


        if self.freqs is None:
            itr = list(
                sorted(six.iteritems(self.vocab), key=lambda k_v: k_v[1]))
        else:
            itr = list(
                sorted(six.iteritems(self.vocab),
                       key=lambda k_v: self.freqs[k_v[0]], reverse=True)
            )

        with open(path, mode='wb') as f:
            mod.saver.save(f, self.vectors, itr)","Save object as word embedding file. For most arguments, you should refer
        to :func:`~word_embedding_loader.word_embedding.WordEmbedding.load`.

        Args:
            use_load_condition (bool): If `True`, options from
                :func:`~word_embedding_loader.word_embedding.WordEmbedding.load`
                is used.

        Raises:
            ValueError: ``use_load_condition == True`` but the object is not
                initialized via
                :func:`~word_embedding_loader.word_embedding.WordEmbedding.load`.",2,0,2,4
"def save_and_close_attributes(self):
        
        if not self.saveable():
            raise AttributeError(""Cannot save attribute file without a valid file"")
        if not self._db_closed:
            self._db_closed = True
            if not self._read_only:
                self.save_attributes()
            self._fd.close()","Performs the same function as save_attributes but also closes
        the attribute file.",2,0,2,4
"def save_as_pdf(self, dest_path):
        
        dest_path = self._add_extension('pdf', dest_path)
        build_dir = tempfile.mkdtemp()
        build_path = os.path.join(build_dir, 'document.tex')
        self.save_assets(build_path)
        with open(build_path, 'w') as f:
            f.write(self.render())
        pdf_path = self._build_document(build_path)
        shutil.copyfile(pdf_path, dest_path)
        shutil.rmtree(build_dir)","Save the plot as a PDF file.

        Save and render the plot using LaTeX to create a PDF file.

        :param dest_path: path of the file.",2,0,0,2
"def save_data(self, trigger_id, **data):
        
        title = self.set_title(data)
        title = HtmlEntities(title).html_entity_decode
        content = self.set_content(data)
        content = HtmlEntities(content).html_entity_decode
        if data.get('output_format'):
            
            import pypandoc
            content = pypandoc.convert(content, str(data.get('output_format')), format='html')
        return title, content","used to save data to the service
            but first of all
            make some work about the data to find
            and the data to convert
            :param trigger_id: trigger ID from which to save data
            :param data: the data to check to be used and save
            :type trigger_id: int
            :type data:  dict
            :return: the status of the save statement
            :rtype: boolean",0,1,2,3
"def save_diskspace(fname, reason, config):
    
    if config[""algorithm""].get(""save_diskspace"", False):
        for ext in ["""", "".bai""]:
            if os.path.exists(fname + ext):
                with open(fname + ext, ""w"") as out_handle:
                    out_handle.write(""File removed to save disk space: %s"" % reason)","Overwrite a file in place with a short message to save disk.

    This keeps files as a sanity check on processes working, but saves
    disk by replacing them with a short message.",1,0,0,1
"def save_function_effect(module):
    
    for intr in module.values():
        if isinstance(intr, dict):  
            save_function_effect(intr)
        else:
            fe = FunctionEffects(intr)
            IntrinsicArgumentEffects[intr] = fe
            if isinstance(intr, intrinsic.Class):
                save_function_effect(intr.fields)",Recursively save function effect for pythonic functions.,0,0,1,1
"def save_image(self, file_obj):
        
        manager = pyglet.image.get_buffer_manager()
        colorbuffer = manager.get_color_buffer()

        
        if hasattr(file_obj, 'write'):
            colorbuffer.save(file=file_obj)
        else:
            colorbuffer.save(filename=file_obj)","Save the current color buffer to a file object
        in PNG format.

        Parameters
        -------------
        file_obj: file name, or file- like object",1,0,0,1
"def save_journal(self):
        
        if self.journal_file is None:
            return

        try:
            as_text = self._commands_to_text()
            with open(self.journal_file, ""w"") as file_obj:
                file_obj.write(as_text)
        except Exception as exc:
            msg = BAD_JOURNAL.format(exc)
            print(msg, file=sys.stderr)","Save journaled commands to file.

        If there is no active journal, does nothing.

        If saving the commands to a file fails, a message will be printed to
        STDERR but the failure will be swallowed so that the extension can
        be built successfully.",1,0,2,3
"def save_rst(self, file_name='pysb_model.rst', module_name='pysb_module'):
        
        if self.model is not None:
            with open(file_name, 'wt') as fh:
                fh.write('.. _%s:\n\n' % module_name)
                fh.write('Module\n======\n\n')
                fh.write('INDRA-assembled model\n---------------------\n\n')
                fh.write('::\n\n')
                model_str = pysb.export.export(self.model, 'pysb_flat')
                model_str = '\t' + model_str.replace('\n', '\n\t')
                fh.write(model_str)","Save the assembled model as an RST file for literate modeling.

        Parameters
        ----------
        file_name : Optional[str]
            The name of the file to save the RST in.
            Default: pysb_model.rst
        module_name : Optional[str]
            The name of the python function defining the module.
            Default: pysb_module",1,0,0,1
"def scalars_to_op(cls, ops, kwargs):
    r
    from qnet.algebra.core.scalar_algebra import is_scalar
    op_ops = []
    for op in ops:
        if is_scalar(op):
            op_ops.append(op * cls._one)
        else:
            op_ops.append(op)
    return op_ops, kwargs","r'''Convert any scalar $\alpha$ in `ops` into an operator $\alpha
    \identity$",0,0,1,1
"def scale_subplots(subplots=None, xlim='auto', ylim='auto'):
    
    auto_axis = ''
    if xlim == 'auto':
        auto_axis += 'x'
    if ylim == 'auto':
        auto_axis += 'y'

    autoscale_subplots(subplots, auto_axis)

    for loc, ax in numpy.ndenumerate(subplots):
        if 'x' not in auto_axis:
            ax.set_xlim(xlim)
        if 'y' not in auto_axis:
            ax.set_ylim(ylim)","Set the x and y axis limits for a collection of subplots.

    Parameters
    -----------
    subplots : ndarray or list of matplotlib.axes.Axes

    xlim : None | 'auto' | (xmin, xmax)
        'auto' : sets the limits according to the most
        extreme values of data encountered.
    ylim : None | 'auto' | (ymin, ymax)",0,0,1,1
"def scan_band(self, band, **kwargs):
        

        kal_run_line = fn.build_kal_scan_band_string(self.kal_bin,
                                                     band, kwargs)
        raw_output = subprocess.check_output(kal_run_line.split(' '),
                                             stderr=subprocess.STDOUT)
        kal_normalized = fn.parse_kal_scan(raw_output)
        return kal_normalized","Run Kalibrate for a band.

        Supported keyword arguments:
        gain    -- Gain in dB
        device  -- Index of device to be used
        error   -- Initial frequency error in ppm",0,1,0,1
"def scene_add(frames):
        
        reader = MessageReader(frames)
        results = reader.string(""command"").uint32(""animation_id"").string(""name"").uint8_3(""color"").uint32(""velocity"").string(""config"").get()
        if results.command != ""scene.add"":
            raise MessageParserError(""Command is not 'scene.add'"")
        return (results.animation_id, results.name, np.array([results.color[0]/255, results.color[1]/255, results.color[2]/255]),
                results.velocity/1000, results.config)",parse a scene.add message,1,0,2,3
"def scene_velocity(frames):
        
        
        reader = MessageReader(frames)
        results = reader.string(""command"").uint32(""scene_id"").uint32(""velocity"").assert_end().get()
        if results.command != ""scene.velocity"":
            raise MessageParserError(""Command is not 'scene.velocity'"")
        return (results.scene_id, results.velocity/1000)",parse a scene.velocity message,1,0,2,3
"def schedule(self, campaign_id, data):
        
        if not data['schedule_time']:
            raise ValueError('You must supply a schedule_time')
        else:
            if data['schedule_time'].tzinfo is None:
                raise ValueError('The schedule_time must be in UTC')
            else:
                if data['schedule_time'].tzinfo.utcoffset(None) != timedelta(0):
                    raise ValueError('The schedule_time must be in UTC')
        if data['schedule_time'].minute not in [0, 15, 30, 45]:
            raise ValueError('The schedule_time must end on the quarter hour (00, 15, 30, 45)')
        data['schedule_time'] = data['schedule_time'].strftime('%Y-%m-%dT%H:%M:00+00:00')
        self.campaign_id = campaign_id
        return self._mc_client._post(url=self._build_path(campaign_id, 'actions/schedule'), data=data)","Schedule a campaign for delivery. If you’re using Multivariate
        Campaigns to test send times or sending RSS Campaigns, use the send
        action instead.

        :param campaign_id: The unique id for the campaign.
        :type campaign_id: :py:class:`str`
        :param data: The request body parameters
        :type data: :py:class:`dict`
        data = {
            ""schedule_time"": datetime* (A UTC timezone datetime that ends on the quarter hour [:00, :15, :30, or :45])
        }",4,0,5,9
"def schedule(self, function, args=(), kwargs={}):
        
        self._check_pool_state()

        future = Future()
        payload = TaskPayload(function, args, kwargs)
        task = Task(next(self._task_counter), future, None, payload)

        self._context.task_queue.put(task)

        return future","Schedules *function* to be run the Pool.

        *args* and *kwargs* will be forwareded to the scheduled function
        respectively as arguments and keyword arguments.

        A *concurrent.futures.Future* object is returned.",0,1,1,2
"def schedule_enable(enable):
    
    status = salt.utils.mac_utils.validate_enabled(enable)

    cmd = ['softwareupdate',
           '--schedule',
           salt.utils.mac_utils.validate_enabled(status)]
    salt.utils.mac_utils.execute_return_success(cmd)

    return salt.utils.mac_utils.validate_enabled(schedule_enabled()) == status","Enable/disable automatic update scheduling.

    :param enable: True/On/Yes/1 to turn on automatic updates. False/No/Off/0
        to turn off automatic updates. If this value is empty, the current
        status will be returned.

    :type: bool str

    :return: True if scheduling is enabled, False if disabled
    :rtype: bool

    CLI Example:

    .. code-block:: bash

       salt '*' softwareupdate.schedule_enable on|off",0,1,1,2
"def scopeMatch(assumedScopes, requiredScopeSets):
    
    for scopeSet in requiredScopeSets:
        for requiredScope in scopeSet:
            for scope in assumedScopes:
                if scope == requiredScope:
                    
                    break
                if scope.endswith(""*"") and requiredScope.startswith(scope[:-1]):
                    
                    break
            else:
                
                break
        else:
            
            return True
    
    return False","Take a list of a assumed scopes, and a list of required scope sets on
        disjunctive normal form, and check if any of the required scope sets are
        satisfied.

        Example:

            requiredScopeSets = [
                [""scopeA"", ""scopeB""],
                [""scopeC""]
            ]

        In this case assumed_scopes must contain, either:
        ""scopeA"" AND ""scopeB"", OR just ""scopeC"".",0,0,2,2
"def scope_id(self):
        
        if self.scope:
            scope_id = self.scope and self.scope.get('id')
        else:
            pseudo_self = self._client.activity(pk=self.id, fields=""id,scope"")
            if pseudo_self.scope and pseudo_self.scope.get('id'):
                self.scope = pseudo_self.scope
                scope_id = self.scope.get('id')
            else:
                raise NotFoundError(""This activity '{}'({}) does not belong to a scope, something is weird!"".
                                    format(self.name, self.id))
        return scope_id","ID of the scope this Activity belongs to.

        This property will always produce a scope_id, even when the scope object was not included in an earlier
        response.

        When the :class:`Scope` is not included in this task, it will make an additional call to the KE-chain API.

        :return: the scope id (uuid string)
        :raises NotFoundError: if the scope could not be found",1,1,2,4
"def scroll_down(self):
        
        self._index -= 1
        if self._index < 0:
            self._index = -1
            return ''
        try:
            return self._history[self._index]
        except IndexError:
            return ''",Returns the next command if any.,0,0,1,1
"def search(geo_coords, mode=2, verbose=True):
    
    if not isinstance(geo_coords, tuple) and not isinstance(geo_coords, list):
        raise TypeError('Expecting a tuple or a tuple/list of tuples')
    elif not isinstance(geo_coords[0], tuple):
        geo_coords = [geo_coords]

    _rg = RGeocoder(mode=mode, verbose=verbose)
    return _rg.query(geo_coords)",Function to query for a list of coordinates,2,0,1,3
"def search(self, query, index='default', **kwargs):
        
        
        if index not in self.conf.indexes:
            self.raise_improperly_configured(index=index)

        
        esurl = self.conf.connections[index]['URL']
        esinst = pyelasticsearch.ElasticSearch(esurl)

        query = isinstance(query, Query) and str(query) or query
        self.raw_results = esinst.search(query, index=index, **kwargs)

        return self","kwargs supported are the parameters listed at:
            http://www.elasticsearch.org/guide/reference/api/search/request-body/
        Namely: timeout, from, size and search_type.
        IMPORTANT: prepend ALL keys with ""es_"" as pyelasticsearch requires this",2,1,0,3
"def search(table: LdapObjectClass, query: Optional[Q] = None,
           database: Optional[Database] = None, base_dn: Optional[str] = None) -> Iterator[LdapObject]:
    
    fields = table.get_fields()
    db_fields = {
        name: field
        for name, field in fields.items()
        if field.db_field
    }

    database = get_database(database)
    connection = database.connection

    search_options = table.get_search_options(database)

    iterator = tldap.query.search(
        connection=connection,
        query=query,
        fields=db_fields,
        base_dn=base_dn or search_options.base_dn,
        object_classes=search_options.object_class,
        pk=search_options.pk_field,
    )

    for dn, data in iterator:
        python_data = _db_to_python(data, table, dn)
        python_data = table.on_load(python_data, database)
        yield python_data",Search for a object of given type in the database.,1,0,0,1
"def search(target_types=None, timeout=12, tries=3):
    
    if isinstance(target_types, six.string_types):
        target_types = [target_types]

    seen = set()
    timeout = float(timeout) / tries
    type_filter = set(target_types) if target_types else None
    with contextlib.closing(make_socket()) as sock:
        for i in range(tries):
            if target_types:
                for target_type in target_types:
                    request_via_socket(sock, target_type)
            for response in responses_from_socket(sock, timeout):
                discovery = Discovery(response)
                if discovery in seen:
                    continue
                seen.add(discovery)
                if type_filter and discovery.type not in target_types:
                    continue
                yield discovery","Performs a search via SSDP to discover resources.

    Args:
        target_types (sequence of strings): A sequence of :term:`resource types`
            to search for. For convenience, this can also be a single string. If
            provided, then this function will make individual requests searching
            for specific targets - this is sometimes required as some devices
            will not respond unless they are specifically requested by the search. 
            
            If not given, then a search won't be broadcast, but we will still
            listen for SSDP notification responses.

        timeout (int / float): Overall time in seconds for how long to wait for
            before no longer listening for responses.

        tries (int): How many times to send out a search request - this will be
            spread out over the timeout period.

    Yields:
        :py:class:`Discovery` instances describing each result found - any
        duplicate results will be filtered out by this function.",0,1,2,3
"def search_cloud_integration_deleted_for_facet(self, facet, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.search_cloud_integration_deleted_for_facet_with_http_info(facet, **kwargs)  
        else:
            (data) = self.search_cloud_integration_deleted_for_facet_with_http_info(facet, **kwargs)  
            return data","Lists the values of a specific facet over the customer's deleted cloud integrations  # noqa: E501

          # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.search_cloud_integration_deleted_for_facet(facet, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str facet: (required)
        :param FacetSearchRequestContainer body:
        :return: ResponseContainerFacetResponse
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def search_dashboard_deleted_for_facets(self, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.search_dashboard_deleted_for_facets_with_http_info(**kwargs)  
        else:
            (data) = self.search_dashboard_deleted_for_facets_with_http_info(**kwargs)  
            return data","Lists the values of one or more facets over the customer's deleted dashboards  # noqa: E501

          # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.search_dashboard_deleted_for_facets(async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param FacetsSearchRequestContainer body:
        :return: ResponseContainerFacetsResponseContainer
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def search_in_hdx(query, configuration=None, **kwargs):
        
        

        resources = []
        resource = Resource(configuration=configuration)
        success, result = resource._read_from_hdx('resource', query, 'query', Resource.actions()['search'])
        if result:
            count = result.get('count', None)
            if count:
                for resourcedict in result['results']:
                    resource = Resource(resourcedict, configuration=configuration)
                    resources.append(resource)
        else:
            logger.debug(result)
        return resources","Searches for resources in HDX. NOTE: Does not search dataset metadata!

        Args:
            query (str): Query
            configuration (Optional[Configuration]): HDX configuration. Defaults to global configuration.
            **kwargs: See below
            order_by (str): A field on the Resource model that orders the results
            offset (int): Apply an offset to the query
            limit (int): Apply a limit to the query
        Returns:
            List[Resource]: List of resources resulting from query",0,1,0,1
"def search_records(self, rq={}, limit=100, offset=0, sort=None,
                       fields=None, fields_exclude=FIELDS_EXCLUDE_DEFAULT):
        
        if fields is not None and fields_exclude is FIELDS_EXCLUDE_DEFAULT:
            fields_exclude = None

        return self._api_post(""/v2/search/records"",
                              rq=rq, limit=limit, offset=offset, sort=sort,
                              fields=fields, fields_exclude=fields_exclude)","rq  Search Query in iDigBio Query Format, using Record Query Fields
            sort    field to sort on, pick from Record Query Fields
            fields  a list of fields to return, specified using the fieldName parameter from Fields with type records
            fields_exclude  a list of fields to exclude, specified using the fieldName parameter from Fields with type records
            limit   max results
            offset  skip results

            Returns idigbio record format (legacy api), plus additional top level keys with parsed index terms. Returns None on error.",0,1,0,1
"def search_tagged_source_for_facet(self, facet, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.search_tagged_source_for_facet_with_http_info(facet, **kwargs)  
        else:
            (data) = self.search_tagged_source_for_facet_with_http_info(facet, **kwargs)  
            return data","Lists the values of a specific facet over the customer's sources  # noqa: E501

          # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.search_tagged_source_for_facet(facet, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str facet: (required)
        :param FacetSearchRequestContainer body:
        :return: ResponseContainerFacetResponse
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def search_unique(table, sample, unique_fields=None):
    
    if unique_fields is None:
        unique_fields = list(sample.keys())

    query = _query_data(sample, field_names=unique_fields, operators='__eq__')
    items = table.search(query)

    if len(items) == 1:
        return items[0]

    if len(items) == 0:
        return None

    raise MoreThanOneItemError('Expected to find zero or one items, but found '
                                '{} items.'.format(len(items)))","Search for items in `table` that have the same field sub-set values as in `sample`.
    Expecting it to be unique, otherwise will raise an exception.

    Parameters
    ----------
    table: tinydb.table
    sample: dict
        Sample data

    Returns
    -------
    search_result: tinydb.database.Element
        Unique item result of the search.

    Raises
    ------
    KeyError:
        If the search returns for more than one entry.",1,0,2,3
"def search_user_group_entities(self, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.search_user_group_entities_with_http_info(**kwargs)  
        else:
            (data) = self.search_user_group_entities_with_http_info(**kwargs)  
            return data","Search over a customer's user groups  # noqa: E501

          # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.search_user_group_entities(async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param SortableSearchRequest body:
        :return: ResponseContainerPagedUserGroup
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def search_web_hook_for_facet(self, facet, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.search_web_hook_for_facet_with_http_info(facet, **kwargs)  
        else:
            (data) = self.search_web_hook_for_facet_with_http_info(facet, **kwargs)  
            return data","Lists the values of a specific facet over the customer's webhooks  # noqa: E501

          # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.search_web_hook_for_facet(facet, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str facet: (required)
        :param FacetSearchRequestContainer body:
        :return: ResponseContainerFacetResponse
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def select_postponed_date(self):
        

        _form = forms.JsonForm(title=""Postponed Workflow"")
        _form.start_date = fields.DateTime(""Start Date"")
        _form.finish_date = fields.DateTime(""Finish Date"")
        _form.save_button = fields.Button(""Save"")
        self.form_out(_form)","The time intervals at which the workflow is to be extended are determined.
            .. code-block:: python

                #  request:
                   {
                   'task_inv_key': string,
                   }",0,0,1,1
"def select_tasks(self, nids=None, wslice=None, task_class=None):
        
        if nids is not None:
            assert wslice is None
            tasks = self.tasks_from_nids(nids)

        elif wslice is not None:
            tasks = []
            for work in self[wslice]:
                tasks.extend([t for t in work])
        else:
            
            tasks = list(self.iflat_tasks())

        
        if task_class is not None:
            tasks = [t for t in tasks if t.isinstance(task_class)]

        return tasks","Return a list with a subset of tasks.

        Args:
            nids: List of node identifiers.
            wslice: Slice object used to select works.
            task_class: String or class used to select tasks. Ignored if None.

        .. note::

            nids and wslice are mutually exclusive.
            If no argument is provided, the full list of tasks is returned.",0,0,2,2
"def selected_value(self, layer_purpose_key):
        
        if layer_purpose_key == layer_purpose_exposure['key']:
            role = RoleExposure
        elif layer_purpose_key == layer_purpose_hazard['key']:
            role = RoleHazard
        else:
            return None

        selected = self.tblFunctions1.selectedItems()
        if len(selected) != 1:
            return None
        try:
            return selected[0].data(role)
        except (AttributeError, NameError):
            return None","Obtain selected hazard or exposure.

        :param layer_purpose_key: A layer purpose key, can be hazard or
            exposure.
        :type layer_purpose_key: str

        :returns: A selected hazard or exposure definition.
        :rtype: dict",0,0,2,2
"def send(
            self,
            *,
            text: str,
    ) -> List[OutputRecord]:
        
        try:
            status = self.api.status_post(status=text)

            return [TootRecord(record_data={
                ""toot_id"": status[""id""],
                ""text"": text
            })]

        except mastodon.MastodonError as e:
            return [self.handle_error((f""Bot {self.bot_name} encountered an error when ""
                                      f""sending post {text} without media:\n{e}\n""),
                                     e)]","Send mastodon message.

        :param text: text to send in post.
        :returns: list of output records,
            each corresponding to either a single post,
            or an error.",1,1,1,3
"def send(self,
             sender,
             recipients,
             cc=None,
             bcc=None,
             subject='',
             body='',
             attachments=None,
             content='text'):
        
        self._server.connect()
        self._server.send(sender,
                          recipients,
                          cc,
                          bcc,
                          subject,
                          body,
                          attachments,
                          content)
        self._server.disconnect()
        return True","Sends the email by connecting and disconnecting after the send

        :param sender: The sender of the message
        :param recipients: The recipients (To:) of the message
        :param cc: The CC recipients of the message
        :param bcc: The BCC recipients of the message
        :param subject: The subject of the message
        :param body: The body of the message
        :param attachments: The attachments of the message
        :param content: The type of content the message [text/html]

        :return: True on success, False otherwise",0,2,0,2
"def send(self, data):
        
        while not self.stopped():
            try:
                self.ws.send(data)
                return
            except websocket.WebSocketConnectionClosedException:
                
                time.sleep(0.1)","This method keeps trying to send a message relying on the run method
        to reopen the websocket in case it was closed.",0,1,1,2
"def send(self, email, attachments=()):
        

        msg = email.as_mime(attachments)

        if 'From' not in msg:
            msg['From'] = self.sender_address()

        if self._conn:
            self._conn.sendmail(self.username, email.recipients,
                                msg.as_string())
        else:
            with self:
                self._conn.sendmail(self.username, email.recipients,
                                    msg.as_string())","Send an email. Connect/Disconnect if not already connected

        Arguments:
            email: Email instance to send.
            attachments: iterable containing Attachment instances",0,1,0,1
"def send(self, message, sid=None, tid=None, credit_request=None):
        
        header = self._generate_packet_header(message, sid, tid,
                                              credit_request)

        
        session = self.session_table.get(sid, None) if sid else None

        tree = None
        if tid and session:
            if tid not in session.tree_connect_table.keys():
                error_msg = ""Cannot find Tree with the ID %d in the session "" \
                            ""tree table"" % tid
                raise smbprotocol.exceptions.SMBException(error_msg)
            tree = session.tree_connect_table[tid]

        if session and session.signing_required and session.signing_key:
            self._sign(header, session)
        request = Request(header)
        self.outstanding_requests[header['message_id'].get_value()] = request

        send_data = header.pack()
        if (session and session.encrypt_data) or (tree and tree.encrypt_data):
            send_data = self._encrypt(send_data, session)

        self.transport.send(send_data)

        return request","Will send a message to the server that is passed in. The final
        unencrypted header is returned to the function that called this.

        :param message: An SMB message structure to send
        :param sid: A session_id that the message is sent for
        :param tid: A tree_id object that the message is sent for
        :param credit_request: Specifies extra credits to be requested with the
            SMB header
        :return: Request of the message that was sent",1,1,2,4
"def send(self, msg, timeout=None):
        
        try:
            timestamp = struct.pack('<I', int(msg.timestamp * 1000))
        except struct.error:
            raise ValueError('Timestamp is out of range')
        try:
            a_id = struct.pack('<I', msg.arbitration_id)
        except struct.error:
            raise ValueError('Arbitration Id is out of range')
        byte_msg = bytearray()
        byte_msg.append(0xAA)
        for i in range(0, 4):
            byte_msg.append(timestamp[i])
        byte_msg.append(msg.dlc)
        for i in range(0, 4):
            byte_msg.append(a_id[i])
        for i in range(0, msg.dlc):
            byte_msg.append(msg.data[i])
        byte_msg.append(0xBB)
        self.ser.write(byte_msg)","Send a message over the serial device.

        :param can.Message msg:
            Message to send.

            .. note:: Flags like ``extended_id``, ``is_remote_frame`` and
                      ``is_error_frame`` will be ignored.

            .. note:: If the timestamp is a float value it will be converted
                      to an integer.

        :param timeout:
            This parameter will be ignored. The timeout value of the channel is
            used instead.",2,1,2,5
"def send(self, request):
        
        if self._UNARY_REQUESTS:
            try:
                self._send_unary_request(request)
            except exceptions.GoogleAPICallError:
                _LOGGER.debug(
                    ""Exception while sending unary RPC. This is typically ""
                    ""non-fatal as stream requests are best-effort."",
                    exc_info=True,
                )
        else:
            self._rpc.send(request)",Queue a request to be sent to the RPC.,0,2,0,2
"def send(self, sender, **named):
        
        responses = []
        if not self.receivers or self.sender_receivers_cache.get(sender) is NO_RECEIVERS:
            return responses

        for receiver in self._live_receivers(sender):
            response = receiver(signal=self, sender=sender, **named)
            responses.append((receiver, response))
        return responses","Send signal from sender to all connected receivers.

        If any receiver raises an error, the error propagates back through send,
        terminating the dispatch loop. So it's possible that all receivers
        won't be called if an error is raised.

        Arguments:

            sender
                The sender of the signal. Either a specific object or None.

            named
                Named arguments which will be passed to receivers.

        Returns a list of tuple pairs [(receiver, response), ... ].",0,1,0,1
"def send(self, sender: PytgbotApiBot):
        
        return sender.send_message(
            
            text=self.text, chat_id=self.receiver, reply_to_message_id=self.reply_id, parse_mode=self.parse_mode, disable_web_page_preview=self.disable_web_page_preview, disable_notification=self.disable_notification, reply_markup=self.reply_markup
        )","Send the message via pytgbot.

        :param sender: The bot instance to send with.
        :type  sender: pytgbot.bot.Bot

        :rtype: PytgbotApiMessage",0,1,0,1
"def send(self, target, action, *args, **kwargs):
        
        target = self.monitor if target == 'monitor' else target
        mailbox = self.mailbox
        if isinstance(target, ActorProxyMonitor):
            mailbox = target.mailbox
        else:
            actor = self.get_actor(target)
            if isinstance(actor, Actor):
                
                
                return command_in_context(action, self, actor, args, kwargs)
            elif isinstance(actor, ActorProxyMonitor):
                mailbox = actor.mailbox
        if hasattr(mailbox, 'send'):
            
            return mailbox.send(action, self, target, args, kwargs)
        else:
            raise CommandError('Cannot execute ""%s"" in %s. Unknown actor %s.'
                               % (action, self, target))","Send a message to ``target`` to perform ``action`` with given
        positional ``args`` and key-valued ``kwargs``.
        Returns a coroutine or a Future.",1,1,3,5
"def send(self, to, attachment_paths=None, *args, **kwargs):
        
        self.get_object()
        self.render_message()
        self.send_email(to, attachment_paths, *args, **kwargs)
        if self.sent:
            logger.info(u""Mail has been sent to: %s "", to)
        return self.sent","This function does all the operations on eft object, that are necessary to send email.
           Usually one would use eft object like this:
                eft = EmailFromTemplate(name='sth/sth.html')
                eft.get_object()
                eft.render_message()
                eft.send_email(['email@example.com'])
                return eft.sent",1,2,0,3
"def send(self, use_open_peers=True, queue=True, **kw):
        

        if not use_open_peers:
            ip = kw.get('ip')
            port = kw.get('port')
            peer = 'http://{}:{}'.format(ip, port)
            res = arky.rest.POST.peer.transactions(peer=peer, transactions=[self.tx.tx])

        else:
            res = arky.core.sendPayload(self.tx.tx)

        if self.tx.success != '0.0%':
            self.tx.error = None
            self.tx.success = True
        else:
            self.tx.error = res['messages']
            self.tx.success = False

        self.tx.tries += 1
        self.tx.res = res

        if queue:
            self.tx.send = True

        self.__save()
        return res","send a transaction immediately. Failed transactions are picked up by the TxBroadcaster

        :param ip: specific peer IP to send tx to
        :param port: port of specific peer
        :param use_open_peers: use Arky's broadcast method",0,2,1,3
"def send(signal=Any, sender=Anonymous, *arguments, **named):
    
    
    
    responses = []
    for receiver in liveReceivers(getAllReceivers(sender, signal)):
        response = robustapply.robustApply(
            receiver,
            signal=signal,
            sender=sender,
            *arguments,
            **named
        )
        responses.append((receiver, response))
    return responses","Send signal from sender to all connected receivers.
    
    signal -- (hashable) signal value, see connect for details

    sender -- the sender of the signal
    
        if Any, only receivers registered for Any will receive
        the message.

        if Anonymous, only receivers registered to receive
        messages from Anonymous or Any will receive the message

        Otherwise can be any python object (normally one
        registered with a connect if you actually want
        something to occur).

    arguments -- positional arguments which will be passed to
        *all* receivers. Note that this may raise TypeErrors
        if the receivers do not allow the particular arguments.
        Note also that arguments are applied before named
        arguments, so they should be used with care.

    named -- named arguments which will be filtered according
        to the parameters of the receivers to only provide those
        acceptable to the receiver.

    Return a list of tuple pairs [(receiver, response), ... ]

    if any receiver raises an error, the error propagates back
    through send, terminating the dispatch loop, so it is quite
    possible to not have all receivers called if a raises an
    error.",0,1,0,1
"def send(token_hex, message, **kwargs):
    

    priority = kwargs.pop('priority', 10)
    topic = kwargs.pop('topic', None)

    alert = {
        ""title"": kwargs.pop(""event""),
        ""body"": message,
        ""action"": kwargs.pop(
            'apns_action', defaults.APNS_PROVIDER_DEFAULT_ACTION)
    }

    data = {
        ""aps"": {
            'alert': alert,
            'content-available': kwargs.pop('content_available', 0) and 1
        }
    }
    data['aps'].update(kwargs)
    payload = dumps(data, separators=(',', ':'))

    headers = {
        'apns-priority': priority
    }
    if topic is not None:
        headers['apns-topic'] = topic

    ssl_context = init_context()
    ssl_context.load_cert_chain(settings.APNS_CERT_FILE)
    connection = HTTP20Connection(
        settings.APNS_GW_HOST, settings.APNS_GW_PORT, ssl_context=ssl_context)

    stream_id = connection.request(
        'POST', '/3/device/{}'.format(token_hex), payload, headers)
    response = connection.get_response(stream_id)
    if response.status != 200:
        raise APNsError(response.read())
    return True","Site: https://apple.com
    API: https://developer.apple.com
    Desc: iOS notifications

    Installation and usage:
    pip install hyper",1,1,2,4
"def sendInvoice(self, chat_id, title, description, payload,
                    provider_token, start_parameter, currency, prices,
                    provider_data=None,
                    photo_url=None,
                    photo_size=None,
                    photo_width=None,
                    photo_height=None,
                    need_name=None,
                    need_phone_number=None,
                    need_email=None,
                    need_shipping_address=None,
                    is_flexible=None,
                    disable_notification=None,
                    reply_to_message_id=None,
                    reply_markup=None):
        
        p = _strip(locals())
        return self._api_request('sendInvoice', _rectify(p))",See: https://core.telegram.org/bots/api#sendinvoice,0,1,0,1
"def sendRequest(self, extraHeaders=""""):
        

        self.addParam('src', 'mc-python')

        params = urlencode(self._params)

        url = self._url

        if 'doc' in self._file.keys():
            headers = {}

            if (extraHeaders is not None) and (extraHeaders is dict):
                headers = headers.update(extraHeaders)

            result = requests.post(url=url, data=self._params, files=self._file, headers=headers)
            result.encoding = 'utf-8'
            return result
        else:
            headers = {'Content-Type': 'application/x-www-form-urlencoded'}
            if (extraHeaders is not None) and (extraHeaders is dict):
                headers = headers.update(extraHeaders)
            result = requests.request(""POST"", url=url, data=params, headers=headers)
            result.encoding = 'utf-8'
            return result","Sends a request to the URL specified and returns a response only if the HTTP code returned is OK

        :param extraHeaders:
             Allows to configure additional headers in the request
        :return:
            Response object set to None if there is an error",0,2,2,4
"def send_UDP_message(self, message):
        
        x = 0
        if self.tracking_enabled:
            try:
                proc = udp_messenger(self.domain_name, self.UDP_IP, self.UDP_PORT, self.sock_timeout, message)
                self.procs.append(proc)
            except Exception as e:
                logger.debug(""Usage tracking failed: {}"".format(e))
        else:
            x = -1

        return x",Send UDP message.,0,2,1,3
"def send_accept_notification(request, message=None, expires_at=None):
    
    pid, record = get_record(request.recid)
    _send_notification(
        request.sender_email,
        _(""Access request accepted""),
        ""zenodo_accessrequests/emails/accepted.tpl"",
        request=request,
        record=record,
        pid=pid,
        record_link=request.link.get_absolute_url('invenio_records_ui.recid'),
        message=message,
        expires_at=expires_at,
    )",Receiver for request-accepted signal to send email notification.,0,1,0,1
"def send_ack(self):
        

        if self.last_ack == self.proto.max_id:
            return
        LOGGER.debug(""ack (%d)"", self.proto.max_id)
        self.last_ack = self.proto.max_id
        self.send_message(f""4{to_json([self.proto.max_id])}"")",Send an ack message,0,2,0,2
"def send_bcm(bcm_socket, data):
    
    try:
        return bcm_socket.send(data)
    except OSError as e:
        base = ""Couldn't send CAN BCM frame. OS Error {}: {}\n"".format(e.errno, e.strerror)

        if e.errno == errno.EINVAL:
            raise can.CanError(base + ""You are probably referring to a non-existing frame."")

        elif e.errno == errno.ENETDOWN:
            raise can.CanError(base + ""The CAN interface appears to be down."")

        elif e.errno == errno.EBADF:
            raise can.CanError(base + ""The CAN socket appears to be closed."")

        else:
            raise e",Send raw frame to a BCM socket and handle errors.,4,1,4,9
"def send_command(self, *args, **kwargs):
        
        if len(args) >= 2:
            expect_string = args[1]
        else:
            expect_string = kwargs.get(""expect_string"")
            if expect_string is None:
                expect_string = r""(OK|ERROR|Command not recognized\.)""
                expect_string = self.RETURN + expect_string + self.RETURN
                kwargs.setdefault(""expect_string"", expect_string)

        output = super(CiscoSSHConnection, self).send_command(*args, **kwargs)
        return output","Send command to network device retrieve output until router_prompt or expect_string

        By default this method will keep waiting to receive data until the network device prompt is
        detected. The current network device prompt will be determined automatically.

        command_string = command to execute
        expect_string = pattern to search for uses re.search (use raw strings)
        delay_factor = decrease the initial delay before we start looking for data
        max_loops = number of iterations before we give up and raise an exception
        strip_prompt = strip the trailing prompt from the output
        strip_command = strip the leading command from the output",0,1,1,2
"def send_exception(self, code, exc_info=None, headers=None):
        ""send an error response including a backtrace to the client""
        if headers is None:
            headers = {}

        if not exc_info:
            exc_info = sys.exc_info()

        self.send_error_msg(code,
                            traceback.format_exception(*exc_info),
                            headers)",send an error response including a backtrace to the client,0,1,0,1
"def send_handle_get_request(self, handle, indices=None):
        


        
        url = self.make_handle_URL(handle, indices)
        LOGGER.debug('GET Request to '+url)
        head = self.__get_headers('GET')
        veri = self.__HTTPS_verify

        
        if self.__cert_needed_for_get_request():
            
            
            resp = self.__session.get(url, headers=head, verify=veri, cert=self.__cert_object)
        else:
            
            resp = self.__session.get(url, headers=head, verify=veri)
    
        
        self.__log_request_response_to_file(
            logger=REQUESTLOGGER,
            op='GET',
            handle=handle,
            url=url,
            headers=head,
            verify=veri,
            resp=resp
            )
        self.__first_request = False
        return resp","Send a HTTP GET request to the handle server to read either an entire
            handle or to some specified values from a handle record, using the
            requests module.

        :param handle: The handle.
        :param indices: Optional. A list of indices to delete. Defaults to
            None (i.e. the entire handle is deleted.). The list can contain
            integers or strings.
        :return: The server's response.",0,3,1,4
"def send_html_mail(subject, message, message_html, from_email, recipient_list,
                   priority=None, fail_silently=False, auth_user=None,
                   auth_password=None, headers={}):
    
    from django.utils.encoding import force_text
    from django.core.mail import EmailMultiAlternatives
    from mailer.models import make_message

    priority = get_priority(priority)

    
    subject = force_text(subject)
    message = force_text(message)

    msg = make_message(subject=subject,
                       body=message,
                       from_email=from_email,
                       to=recipient_list,
                       priority=priority)
    email = msg.email
    email = EmailMultiAlternatives(
        email.subject,
        email.body,
        email.from_email,
        email.to,
        headers=headers
    )
    email.attach_alternative(message_html, ""text/html"")
    msg.email = email
    msg.save()
    return 1",Function to queue HTML e-mails,0,1,0,1
"def send_media_group(chat_id, media,
                     reply_to_message_id=None, disable_notification=False,
                     **kwargs):
    

    files = []
    if len(media) < 2 or len(media) > 10:
        raise ValueError('media must contain between 2 and 10 InputMedia items')

    for i, entry in media:
        if isinstance(entry.media, InputFile):
            files.append(entry.media)  
            media[i].media = ""attach://{}"".format(entry[1][0])  

    
    params = dict(
        chat_id=chat_id,
        media=json.dumps(media)
    )

    
    params.update(
        _clean_params(
            reply_to_message_id=reply_to_message_id,
            disable_notification=disable_notification,
        )
    )

    return TelegramBotRPCRequest('sendMediaGroup', params=params, files=files, on_result=lambda result: [Message.from_result(message) for message in result], **kwargs)","Use this method to send a group of photos or videos as an album. On success, an array of the sent Messages is returned.

    :param chat_id: Unique identifier for the target chat or username of the target channel (in the format @channelusername)
    :param media: A list of InputMedia objects to be sent, must include 2–10 items
    :param reply_to_message_id: If the message is a reply, ID of the original message
    :param disable_notification: Sends the messages silently. Users will receive a notification with no sound.
    :param kwargs: Args that get passed down to :class:`TelegramBotRPCRequest`

    :type chat_id: int or str
    :type media: `list` of :class:`InputMedia`
    :type reply_to_message_id: int

    :returns: On success, an array of the sent Messages is returned.
    :rtype: TelegramBotRPCRequest",1,1,1,3
"def send_message(self, message):
        
        self._steam.send(MsgProto(EMsg.ClientFriendMsg), {
            'steamid': self.steam_id,
            'chat_entry_type': EChatEntryType.ChatMsg,
            'message': message.encode('utf8'),
            })","Send chat message to this steam user

        :param message: message to send
        :type message: str",0,1,0,1
"def send_plain(self):
        

        values = {""registration_id"": self._registration_id}

        for key, val in self._data.items():
            values[""data.%s"" % (key)] = val.encode(self.encoding)

        for key, val in self._kwargs.items():
            if val and isinstance(val, bool):
                val = 1
                values[key] = val

        data = urlencode(sorted(values.items())).encode(self.encoding)
        result = self._send(data, ""application/x-www-form-urlencoded;charset=UTF-8"")

        if result.startswith(""Error=""):
            if result in (""Error=NotRegistered"", ""Error=InvalidRegistration""):
                
                return result

            raise GCMPushError(result)

        return result",Sends a text/plain GCM message,1,1,2,4
"def send_request(self, request, async=False, local_callback=None, remote_callback=None, user_info=None):
        

        callbacks = dict()

        if local_callback:
            callbacks['local'] = local_callback

        if remote_callback:
            callbacks['remote'] = remote_callback

        connection = NURESTConnection(request=request, async=async, callback=self._did_receive_response, callbacks=callbacks)
        connection.user_info = user_info

        return connection.start()","Sends a request, calls the local callback, then the remote callback in case of async call

            Args:
                request: The request to send
                local_callback: local method that will be triggered in case of async call
                remote_callback: remote moethd that will be triggered in case of async call
                user_info: contains additionnal information to carry during the request

            Returns:
                Returns the object and connection (object, connection)",0,1,0,1
"def send_reset_password_instructions(self, user):
        
        token = self.security_utils_service.generate_reset_password_token(user)
        reset_link = url_for('security_controller.reset_password',
                             token=token, _external=True)
        self.send_mail(
            _('flask_unchained.bundles.security:email_subject.reset_password_instructions'),
            to=user.email,
            template='security/email/reset_password_instructions.html',
            user=user,
            reset_link=reset_link)
        reset_password_instructions_sent.send(app._get_current_object(),
                                              user=user, token=token)","Sends the reset password instructions email for the specified user.

        Sends signal `reset_password_instructions_sent`.

        :param user: The user to send the instructions to.",0,1,1,2
"def send_script_sync(self, conn_id, data, progress_callback):
        

        done = threading.Event()
        result = {}

        def send_script_done(conn_id, adapter_id, status, reason):
            result['success'] = status
            result['failure_reason'] = reason

            done.set()

        self.send_script_async(conn_id, data, progress_callback, send_script_done)
        done.wait()

        return result","Asynchronously send a a script to this IOTile device

        Args:
            conn_id (int): A unique identifier that will refer to this connection
            data (string): the script to send to the device
            progress_callback (callable): A function to be called with status on our progress, called as:
                progress_callback(done_count, total_count)

        Returns:
            dict: a dict with the following two entries set
                'success': a bool indicating whether we received a response to our attempted RPC
                'failure_reason': a string with the reason for the failure if success == False",0,1,0,1
"def send_shared_pin(self, topics, pin, skip_validation=False):
        
        if not self.api_key:
            raise ValueError(""You need to specify an api_key."")
        if not skip_validation:
            validate_pin(pin)

        response = _request('PUT',
            url=self.url_v1('/shared/pins/' + pin['id']),
            user_agent=self.user_agent,
            api_key=self.api_key,
            topics_list=topics,
            json=pin,
        )
        _raise_for_status(response)","Send a shared pin for the given topics.

        :param list topics: The list of topics.
        :param dict pin: The pin.
        :param bool skip_validation: Whether to skip the validation.
        :raises pypebbleapi.schemas.DocumentError: If the validation process failed.
        :raises `requests.exceptions.HTTPError`: If an HTTP error occurred.",1,1,1,3
"def send_short_lpp_packet(self, dest_id, data):
        

        pk = CRTPPacket()
        pk.port = CRTPPort.LOCALIZATION
        pk.channel = self.GENERIC_CH
        pk.data = struct.pack('<BB', self.LPS_SHORT_LPP_PACKET, dest_id) + data
        self._cf.send_packet(pk)",Send ultra-wide-band LPP packet to dest_id,0,1,0,1
"def send_venue(self, chat_id, latitude, longitude, title, address, foursquare_id=None, disable_notification=None,
                   reply_to_message_id=None, reply_markup=None):
        
        return types.Message.de_json(
            apihelper.send_venue(self.token, chat_id, latitude, longitude, title, address, foursquare_id,
                                 disable_notification, reply_to_message_id, reply_markup)
        )","Use this method to send information about a venue.
        :param chat_id: Integer or String : Unique identifier for the target chat or username of the target channel
        :param latitude: Float : Latitude of the venue
        :param longitude: Float : Longitude of the venue
        :param title: String : Name of the venue
        :param address: String : Address of the venue
        :param foursquare_id: String : Foursquare identifier of the venue
        :param disable_notification:
        :param reply_to_message_id:
        :param reply_markup:
        :return:",0,1,0,1
"def send_video_note(chat_id, video_note,
                    duration=None, length=None, reply_to_message_id=None, reply_markup=None, disable_notification=False,
                    **kwargs):
    
    files = None
    if isinstance(video_note, InputFile):
        files = [video_note]
        video = None
    elif not isinstance(video_note, str):
        raise Exception('video must be instance of InputFile or str')

    
    params = dict(
        chat_id=chat_id,
        video_note=video_note
    )

    
    params.update(
        _clean_params(
            duration=duration,
            length=length,
            reply_to_message_id=reply_to_message_id,
            reply_markup=reply_markup,
            disable_notification=disable_notification,
        )
    )

    return TelegramBotRPCRequest('sendVideoNote', params=params, files=files, on_result=Message.from_result, **kwargs)","Use this method to send video files, Telegram clients support mp4 videos (other formats may be sent as Document).

    :param chat_id: Unique identifier for the target chat or username of the target channel (in the format @channelusername)
    :param video_note: Video to send. Pass a file_id as String to send a video that exists on the Telegram servers (recommended),
                  pass an HTTP URL as a String for Telegram to get a video from the Internet, or upload a new video using multipart/form-data.
    :param duration: Duration of sent video in seconds
    :param length: Video width and height
    :param reply_to_message_id: If the message is a reply, ID of the original message
    :param reply_markup: Additional interface options. A JSON-serialized object for a
                         custom reply keyboard, instructions to hide keyboard or to
                         force a reply from the user.
    :param disable_notification: Sends the message silently. iOS users will not receive a notification, Android users
                                 will receive a notification with no sound. Other apps coming soon.
    :param kwargs: Args that get passed down to :class:`TelegramBotRPCRequest`

    :type chat_id: int or str
    :type video: InputFile or str
    :type duration: int
    :type caption: str
    :type reply_to_message_id: int
    :type reply_markup: ReplyKeyboardMarkup or ReplyKeyboardHide or ForceReply

    :returns: On success, the sent Message is returned.
    :rtype:  TelegramBotRPCRequest",1,1,2,4
"def send_with_data_passthrough(self, event):
        
        if self.our_state is ERROR:
            raise LocalProtocolError(
                ""Can't send data when our state is ERROR"")
        try:
            if type(event) is Response:
                self._clean_up_response_headers_for_sending(event)
            
            
            
            
            
            writer = self._writer
            self._process_event(self.our_role, event)
            if type(event) is ConnectionClosed:
                return None
            else:
                
                
                assert writer is not None
                data_list = []
                writer(event, data_list.append)
                return data_list
        except:
            self._process_error(self.our_role)
            raise","Identical to :meth:`send`, except that in situations where
        :meth:`send` returns a single :term:`bytes-like object`, this instead
        returns a list of them -- and when sending a :class:`Data` event, this
        list is guaranteed to contain the exact object you passed in as
        :attr:`Data.data`. See :ref:`sendfile` for discussion.",2,0,1,3
"def sensor_names(self):
        
        res = self['/attr/instrument_name']
        if isinstance(res, np.ndarray):
            res = str(res.astype(str))
        res = [x.strip() for x in res.split(',')]
        if len(res) == 1:
            return res[0]
        return res",Return standard sensor or instrument name for the file's data.,0,0,2,2
"def sentence(self, nb_words=6, variable_nb_words=True, ext_word_list=None):
        
        if nb_words <= 0:
            return ''

        if variable_nb_words:
            nb_words = self.randomize_nb_elements(nb_words, min=1)

        words = self.words(nb=nb_words, ext_word_list=ext_word_list)
        words[0] = words[0].title()

        return self.word_connector.join(words) + self.sentence_punctuation","Generate a random sentence
        :example 'Lorem ipsum dolor sit amet.'

        :param nb_words: around how many words the sentence should contain
        :param variable_nb_words: set to false if you want exactly ``nb``
            words returned, otherwise the result may include a number of words
            of ``nb`` +/-40% (with a minimum of 1)
        :param ext_word_list: a list of words you would like to have instead of
            'Lorem ipsum'.

        :rtype: str",0,0,1,1
"def sequence_weirdness(text):
    
    text2 = unicodedata.normalize('NFC', text)
    weirdness = len(WEIRDNESS_RE.findall(chars_to_classes(text2)))
    adjustment = (
        len(MOJIBAKE_SYMBOL_RE.findall(text2)) * 2 -
        len(COMMON_SYMBOL_RE.findall(text2))
    )
    return weirdness * 2 + adjustment","Determine how often a text has unexpected characters or sequences of
    characters. This metric is used to disambiguate when text should be
    re-decoded or left as is.

    We start by normalizing text in NFC form, so that penalties for
    diacritical marks don't apply to characters that know what to do with
    them.

    The following things are deemed weird:

    - Lowercase letters followed by non-ASCII uppercase letters
    - Non-Latin characters next to Latin characters
    - Un-combined diacritical marks, unless they're stacking on non-alphabetic
      characters (in languages that do that kind of thing a lot) or other
      marks
    - C1 control characters
    - Adjacent symbols from any different pair of these categories:

        - Modifier marks
        - Letter modifiers
        - Non-digit numbers
        - Symbols (including math and currency)

    The return value is the number of instances of weirdness.",0,0,1,1
"def serialize(point: Mapping, measurement=None, **extra_tags) -> bytes:
    
    tags = _serialize_tags(point, extra_tags)
    return (
        f'{_serialize_measurement(point, measurement)}'
        f'{"","" if tags else """"}{tags} '
        f'{_serialize_fields(point)} '
        f'{_serialize_timestamp(point)}'
    ).encode()",Converts dictionary-like data into a single line protocol line (point),0,0,1,1
"def serve_file(self, load):
        
        ret = {'data': '',
               'dest': ''}

        if 'env' in load:
            
            load.pop('env')

        if 'path' not in load or 'loc' not in load or 'saltenv' not in load:
            return ret
        if not isinstance(load['saltenv'], six.string_types):
            load['saltenv'] = six.text_type(load['saltenv'])

        fnd = self.find_file(load['path'], load['saltenv'])
        if not fnd.get('back'):
            return ret
        fstr = '{0}.serve_file'.format(fnd['back'])
        if fstr in self.servers:
            return self.servers[fstr](load, fnd)
        return ret",Serve up a chunk of a file,1,1,1,3
"def servers(**kwargs):  

    

    ntp_servers = salt.utils.napalm.call(
        napalm_device,  
        'get_ntp_servers',
        **{
        }
    )

    if not ntp_servers.get('result'):
        return ntp_servers

    ntp_servers_list = list(ntp_servers.get('out', {}).keys())

    ntp_servers['out'] = ntp_servers_list

    return ntp_servers","Returns a list of the configured NTP servers on the device.

    CLI Example:

    .. code-block:: bash

        salt '*' ntp.servers

    Example output:

    .. code-block:: python

        [
            '192.168.0.1',
            '172.17.17.1',
            '172.17.17.2',
            '2400:cb00:6:1024::c71b:840a'
        ]",0,1,1,2
"def service(self):
        
        if self._service is not None:
            return self._service

        info = self.search_results_info

        if info is None:
            return None

        splunkd = urlsplit(info.splunkd_uri, info.splunkd_protocol, allow_fragments=False)

        self._service = Service(
            scheme=splunkd.scheme, host=splunkd.hostname, port=splunkd.port, token=info.auth_token, app=info.ppc_app)

        return self._service","Returns a Splunk service object for this command invocation or None.

        The service object is created from the Splunkd URI and authentication
        token passed to the command invocation in the search results info file.
        This data is not passed to a command invocation by default. You must
        request it by specifying this pair of configuration settings in
        commands.conf:

           .. code-block:: python
               enableheader=true
               requires_srinfo=true

        The :code:`enableheader` setting is :code:`true` by default. Hence, you
        need not set it. The :code:`requires_srinfo` setting is false by
        default. Hence, you must set it.

        :return: :class:`splunklib.client.Service`, if :code:`enableheader` and
            :code:`requires_srinfo` are both :code:`true`. Otherwise, if either
            :code:`enableheader` or :code:`requires_srinfo` are :code:`false`,
            a value of :code:`None` is returned.",0,0,1,1
"def service_per_endpoint(self, context=None):
        
        endps = self.getattr(""endpoints"", context)
        res = {}
        for service, specs in endps.items():
            for endp, binding in specs:
                res[endp] = (service, binding)
        return res","List all endpoint this entity publishes and which service and binding
        that are behind the endpoint

        :param context: Type of entity
        :return: Dictionary with endpoint url as key and a tuple of
            service and binding as value",0,0,1,1
"def service_timeouts(cls):
        
        timer_manager = cls._timers
        while True:
            next_end = timer_manager.service_timeouts()
            sleep_time = max(next_end - time.time(), 0) if next_end else 10000
            cls._new_timer.wait(sleep_time)
            cls._new_timer.clear()","cls._timeout_watcher runs in this loop forever.
        It is usually waiting for the next timeout on the cls._new_timer Event.
        When new timers are added, that event is set so that the watcher can
        wake up and possibly set an earlier timeout.",0,0,1,1
"def session(self):
        
        return self.session_class(
            client_key=self.client_key,
            client_secret=self.client_secret,
            signature_method=self.signature_method,
            signature_type=self.signature_type,
            rsa_key=self.rsa_key,
            client_class=self.client_class,
            force_include_body=self.force_include_body,
            blueprint=self,
            base_url=self.base_url,
            **self.kwargs
        )","This is a session between the consumer (your website) and the provider
        (e.g. Twitter). It is *not* a session between a user of your website
        and your website.
        :return:",0,1,0,1
"def session(self, request: Request) -> WebSession:
        
        return WebSession(
            request,
            http_client=self._http_client,
            redirect_tracker=self._redirect_tracker_factory(),
            request_factory=self._request_factory,
            cookie_jar=self._cookie_jar,
        )","Return a fetch session.

        Args:
            request: The request to be fetched.

        Example usage::

            client = WebClient()
            session = client.session(Request('http://www.example.com'))

            with session:
                while not session.done():
                    request = session.next_request()
                    print(request)

                    response = yield from session.start()
                    print(response)

                    if session.done():
                        with open('myfile.html') as file:
                            yield from session.download(file)
                    else:
                        yield from session.download()

        Returns:
            WebSession",0,0,1,1
"def set(msg_or_dict, key, value):
    
    
    if not isinstance(msg_or_dict, (collections_abc.MutableMapping, message.Message)):
        raise TypeError(
            ""set() expected a dict or protobuf message, got {!r}."".format(
                type(msg_or_dict)
            )
        )

    
    basekey, subkey = _resolve_subkeys(key)

    
    
    if subkey is not None:
        if isinstance(msg_or_dict, collections_abc.MutableMapping):
            msg_or_dict.setdefault(basekey, {})
        set(get(msg_or_dict, basekey), subkey, value)
        return

    if isinstance(msg_or_dict, collections_abc.MutableMapping):
        msg_or_dict[key] = value
    else:
        _set_field_on_message(msg_or_dict, key, value)","Set a key's value on a protobuf Message or dictionary.

    Args:
        msg_or_dict (Union[~google.protobuf.message.Message, Mapping]): the
            object.
        key (str): The key to set.
        value (Any): The value to set.

    Raises:
        TypeError: If ``msg_or_dict`` is not a Message or dictionary.",1,0,2,3
"def set(self, alpha):
        
        if alpha > 255:
            alpha = 255
        elif alpha < 0:
            alpha = 0
        x = alpha / 255. * self.winfo_width()
        self.coords('cursor', x, 0, x, self.winfo_height())
        self._variable.set(alpha)","Set cursor position on the color corresponding to the alpha value.

        :param alpha: new alpha value (between 0 and 255)
        :type alpha: int",0,0,1,1
"def setActiveCamera(self,name):
        
        if name == self.activeCamera:
            return 
        if name not in self.world.cameras:
            raise ValueError(""Unknown camera name"")
        old = self.activeCamera
        self.activeCamera = name
        self.cam.on_activate(old)","Sets the active camera.
        
        This method also calls the :py:meth:`Camera.on_activate() <peng3d.camera.Camera.on_activate>` event handler if the camera is not already active.",1,0,2,3
"def setImage(self, image):
        
        if image.mode != '1':
            raise ValueError('The image color must be in mode \""1\"".')

        imgWidth, imgHeight = image.size
        if imgWidth != self.width or imgHeight != self.height:
            raise ValueError('The image must be same dimensions as display ( {0} x {1} ).' \
                .format(self.width, self.height))

        
        pixByte = [0x01, 0x02, 0x04, 0x08, 0x10, 0x20, 0x40, 0x80]
        bi = 0
        pixs = image.load()
        for x in range( 0, self.width ):
            for y in range( 0, self.height, 8 ):
                pixBits = 0x00
                
                for py in [0,1,2,3,4,5,6,7]:
                    pixBits |= (0x00 if pixs[x, y+py] == 0 else pixByte[py])
                self._buffer[bi] = pixBits
                bi += 1","!
        \~english
        Convert image to the buffer, The image mode must be 1 and image size 
        equal to the display size image type is Python Imaging Library image.
        @param image: a PIL image object

        \~chinese
        将图像转换为缓冲区，这个图像的色彩模式必须为 1 同时图像大小必须等于显存大小，
        图像类型： PIL Image (Python Imaging Library)
        @param image: PIL图像对象

        \n \~
        @note
        <pre>
        ssd1306.setImage( aPILImage )
        ssd1306.display()
        </pre>",2,0,3,5
"def set_attitude(roll_angle = 0.0, pitch_angle = 0.0,
                 yaw_angle = None, yaw_rate = 0.0, use_yaw_rate = False,
                 thrust = 0.5, duration = 0):
    
    send_attitude_target(roll_angle, pitch_angle,
                         yaw_angle, yaw_rate, False,
                         thrust)
    start = time.time()
    while time.time() - start < duration:
        send_attitude_target(roll_angle, pitch_angle,
                             yaw_angle, yaw_rate, False,
                             thrust)
        time.sleep(0.1)
    
    send_attitude_target(0, 0,
                         0, 0, True,
                         thrust)","Note that from AC3.3 the message should be re-sent more often than every
    second, as an ATTITUDE_TARGET order has a timeout of 1s.
    In AC3.2.1 and earlier the specified attitude persists until it is canceled.
    The code below should work on either version.
    Sending the message multiple times is the recommended way.",0,3,0,3
"def set_canned_acl(self, acl_str, validate=True, headers=None,
                       version_id=None):
        
        if not self.object_name:
            raise InvalidUriError('set_canned_acl on object-less URI (%s)' %
                                  self.uri)
        key = self.get_key(validate, headers)
        self.check_response(key, 'key', self.uri)
        key.set_canned_acl(acl_str, headers, version_id)",sets or updates a bucket's acl to a predefined (canned) value,1,1,2,4
"def set_client_format(self, desc):
        
        assert desc.mFormatID == AUDIO_ID_PCM
        check(_coreaudio.ExtAudioFileSetProperty(
            self._obj, PROP_CLIENT_DATA_FORMAT, ctypes.sizeof(desc),
            ctypes.byref(desc)
        ))
        self._client_fmt = desc","Get the client format description. This describes the
        encoding of the data that the program will read from this
        object.",1,0,1,2
"def set_cte(self, cte_id, sql):
        
        for cte in self.extra_ctes:
            if cte['id'] == cte_id:
                cte['sql'] = sql
                break
        else:
            self.extra_ctes.append(
                {'id': cte_id, 'sql': sql}
            )","This is the equivalent of what self.extra_ctes[cte_id] = sql would
        do if extra_ctes were an OrderedDict",0,0,1,1
"def set_ecdh_curve(self, curve_name=None):
        u
        if curve_name:
            retVal = SSL_CTX_set_ecdh_auto(self._ctx, 0)
            avail_curves = get_elliptic_curves()
            key = [curve for curve in avail_curves if curve.name == curve_name][0].to_EC_KEY()
            retVal &= SSL_CTX_set_tmp_ecdh(self._ctx, key)
        else:
            retVal = SSL_CTX_set_ecdh_auto(self._ctx, 1)
        return retVal","u''' Select a curve to use for ECDH(E) key exchange or set it to auto mode

        Used for server only!

        s.a. openssl.exe ecparam -list_curves

        :param None | str curve_name: None = Auto-mode, ""secp256k1"", ""secp384r1"", ...
        :return: 1 for success and 0 for failure",0,0,3,3
"def set_email(self, email, _vars=None, lists=None, templates=None, verified=0, optout=None, send=None, send_vars=None):
        
        _vars = _vars or {}
        lists = lists or []
        templates = templates or []
        send_vars = send_vars or []
        data = {'email': email,
                'vars':  _vars.copy(),
                'lists': lists,
                'templates': templates,
                'verified': int(verified)}
        if optout is not None:
            data['optout'] = optout
        if send is not None:
            data['send'] = send
        if send_vars:
            data['send_vars'] = send_vars
        return self.api_post('email', data)","DEPRECATED!
        Update information about one of your users, including adding and removing the user from lists.
        http://docs.sailthru.com/api/email",0,1,1,2
"def set_event_patterns(self, event_pat):
        
        if event_pat.shape[1] != self.n_events:
            raise ValueError((""Number of columns of event_pat must match ""
                              ""number of events""))
        self.event_pat_ = event_pat.copy()","Set HMM event patterns manually

        Rather than fitting the event patterns automatically using fit(), this
        function allows them to be set explicitly. They can then be used to
        find corresponding events in a new dataset, using find_events().

        Parameters
        ----------
        event_pat: voxel by event ndarray",1,0,2,3
"def set_kw_typeahead_input(cls):
        
        
        parent_id = cls.intput_el.parent.id
        if ""typeahead"" not in parent_id.lower():
            parent_id = cls.intput_el.parent.parent.id

        window.make_keyword_typeahead_tag(
            ""
            join(settings.API_PATH, ""kw_list.json""),
            cls.on_select_callback,
        )",Map the typeahead input to remote dataset.,0,1,0,1
"def set_long_features(self, features, columns_to_set=[], partition=2):
        
        
        features_long = self.set_features(partition=2 * partition)

        
        unwanted_features = [f for f in features.columns if f not in columns_to_set]
        features_long = features_long.drop(unwanted_features, axis=1)

        
        features_long.columns = ['long_{0}'.format(f) for f in features_long.columns]

        
        skip = partition
        return pd.concat([features[skip:].reset_index(drop=True),
                         features_long],
                         axis=1)","Sets features of double the duration

        Example: Setting 14 day RSIs to longer will create add a
            feature column of a 28 day RSIs.

        Args:
            features: Pandas DataFrame instance with columns as numpy.float32 features.
            columns_to_set: List of strings of feature names to make longer
            partition: Int of how many dates to take into consideration
                when evaluating technical analysis indicators.

        Returns:
            Pandas DataFrame instance with columns as numpy.float32 features.",0,0,1,1
"def set_node_config(self, jid, config, node=None):
        

        iq = aioxmpp.stanza.IQ(to=jid, type_=aioxmpp.structs.IQType.SET)
        iq.payload = pubsub_xso.OwnerRequest(
            pubsub_xso.OwnerConfigure(node=node)
        )
        iq.payload.payload.data = config

        yield from self.client.send(iq)","Update the configuration of a node.

        :param jid: Address of the PubSub service.
        :type jid: :class:`aioxmpp.JID`
        :param config: Configuration form
        :type config: :class:`aioxmpp.forms.Data`
        :param node: Name of the PubSub node to query.
        :type node: :class:`str`
        :raises aioxmpp.errors.XMPPError: as returned by the service
        :return: The configuration of the node.
        :rtype: :class:`~.forms.Data`

        .. seealso::

            :class:`aioxmpp.pubsub.NodeConfigForm`",0,1,0,1
"def set_power_state(self, is_on, bulb=ALL_BULBS, timeout=None):
        
        with _blocking(self.lock, self.power_state, self.light_state_event,
                       timeout):
            self.send(REQ_SET_POWER_STATE,
                      bulb, '2s', '\x00\x01' if is_on else '\x00\x00')
            self.send(REQ_GET_LIGHT_STATE, ALL_BULBS, '')
        return self.power_state",Sets the power state of one or more bulbs.,2,0,0,2
"def set_q_foreign_loads(network, cos_phi=1):
    
    foreign_buses = network.buses[network.buses.country_code != 'DE']

    network.loads_t['q_set'][network.loads.index[
        network.loads.bus.astype(str).isin(foreign_buses.index)]] = \
        network.loads_t['p_set'][network.loads.index[
            network.loads.bus.astype(str).isin(
                foreign_buses.index)]] * math.tan(math.acos(cos_phi))

    network.generators.control[network.generators.control == 'PQ'] = 'PV'

    return network","Set reative power timeseries of loads in neighbouring countries
    
    Parameters
    ----------
    network : :class:`pypsa.Network
        Overall container of PyPSA
    cos_phi: float
        Choose ration of active and reactive power of foreign loads

    Returns
    -------
    network : :class:`pypsa.Network
        Overall container of PyPSA",0,0,1,1
"def set_repo_permission(self, repo, permission):
        
        assert isinstance(repo, github.Repository.Repository), repo
        put_parameters = {
            ""permission"": permission,
        }
        headers, data = self._requester.requestJsonAndCheck(
            ""PUT"",
            self.url + ""/repos/"" + repo._identity,
            input=put_parameters
        )",":calls: `PUT /teams/:id/repos/:org/:repo <http://developer.github.com/v3/orgs/teams>`_
        :param repo: :class:`github.Repository.Repository`
        :param permission: string
        :rtype: None",0,1,0,1
"def set_schema_location(self, ns_uri, schema_location, replace=False):
        
        ni = self.__lookup_uri(ns_uri)

        if ni.schema_location == schema_location:
            return
        elif replace or ni.schema_location is None:
            ni.schema_location = schema_location
        elif schema_location is None:
            ni.schema_location = None  
        else:
            raise ConflictingSchemaLocationError(ns_uri, ni.schema_location, schema_location)","Sets the schema location of the given namespace.

        If ``replace`` is ``True``, then any existing schema location is
        replaced.  Otherwise, if the schema location is already set to a
        different value, an exception is raised.  If the schema location is set
        to None, it is effectively erased from this set (this is not considered
        ""replacement"".)

        Args:
            ns_uri (str): The namespace whose schema location is to be set
            schema_location (str): The schema location URI to set, or None
            replace (bool): Whether to replace any existing schema location

        Raises:
            NamespaceNotFoundError: If the given namespace isn't in this set.
            ConflictingSchemaLocationError: If replace is False,
                schema_location is not None, and the namespace already has a
                different schema location in this set.",1,0,2,3
"def set_secure(self, section, option, value):
        
        if self.keyring_available:
            s_option = ""%s%s"" % (section, option)
            self._unsaved[s_option] = ('set', value)
            value = self._secure_placeholder
        ConfigParser.set(self, section, option, value)","Set an option and mark it as secure.

        Any subsequent uses of 'set' or 'get' will also
        now know that this option is secure as well.",0,0,2,2
"def set_stats_params(
            self, address=None, enable_http=None,
            minify=None, no_cores=None, no_metrics=None, push_interval=None):
        
        self._set('stats-server', address)
        self._set('stats-http', enable_http, cast=bool)
        self._set('stats-minified', minify, cast=bool)
        self._set('stats-no-cores', no_cores, cast=bool)
        self._set('stats-no-metrics', no_metrics, cast=bool)
        self._set('stats-pusher-default-freq', push_interval)

        return self._section","Enables stats server on the specified address.

        * http://uwsgi.readthedocs.io/en/latest/StatsServer.html

        :param str|unicode address: Address/socket to make stats available on.

            Examples:
                * 127.0.0.1:1717
                * /tmp/statsock
                * :5050

        :param bool enable_http: Server stats over HTTP.
            Prefixes stats server json output with http headers.

        :param bool minify: Minify statistics json output.

        :param bool no_cores: Disable generation of cores-related stats.

        :param bool no_metrics: Do not include metrics in stats output.

        :param int push_interval: Set the default frequency of stats pushers in seconds/",0,0,1,1
"def set_status(self, status: Status, increment_try_count: bool=True,
                   filename: str=None):
        
        url = self.url_record.url
        assert not self._try_count_incremented, (url, status)

        if increment_try_count:
            self._try_count_incremented = True

        _logger.debug(__('Marking URL {0} status {1}.', url, status))

        url_result = URLResult()
        url_result.filename = filename

        self.app_session.factory['URLTable'].check_in(
            url,
            status,
            increment_try_count=increment_try_count,
            url_result=url_result,
        )

        self._processed = True","Mark the item with the given status.

        Args:
            status: a value from :class:`Status`.
            increment_try_count: if True, increment the ``try_count``
                value",0,2,1,3
"def set_stdout(self, stream=None):
        
        if stream is None:
            self.__stdout = sys.stdout
        else:
            assert hasattr(stream, 'read') and hasattr(stream, 'write'), ""stdout stream is not valid""
            self.__stdout = stream
        
        self.__stdoutFontFormat = self.__get_stream_fonts_attributes(stream)","Set the logger standard output stream.

        :Parameters:
           #. stdout (None, stream): The standard output stream. If None, system standard
              output will be set automatically. Otherwise any stream with read and write
              methods can be passed",1,0,2,3
"def set_type(self, form_type, css_class=None):
        
        self.form_type = form_type
        
        if css_class is None:
            self.css_class = self.get_default_css_class(form_type)
        else:
            self.css_class = css_class
        
        return ''","Maybe you have a site where you're not allowed to change the python code,
        and for some reason you need to change the form_type in a template, not
        because you want to (because it seems like a bit of a hack) but maybe you
        don't really have a choice.  Then this function was made for you.

        Sorry

        :param form_type: The new form_type
        :param css_class: If None (default) derrive this from the form_type.
                          If a value is passed in this will be the new css_class
                          for the form",0,0,3,3
"def set_user_project_permission(self, project_id, user_id, auth_role):
        
        put_data = {
            ""auth_role[id]"": auth_role
        }
        return self._put(""/projects/"" + project_id + ""/permissions/"" + user_id, put_data,
                         content_type=ContentType.form)","Send PUT request to /projects/{project_id}/permissions/{user_id/ with auth_role value.
        :param project_id: str uuid of the project
        :param user_id: str uuid of the user
        :param auth_role: str project role eg 'project_admin'
        :return: requests.Response containing the successful result",0,1,0,1
"def setop(args):
    
    from jcvi.utils.natsort import natsorted

    p = OptionParser(setop.__doc__)
    p.add_option(""--column"", default=0, type=""int"",
                 help=""The column to extract, 0-based, -1 to disable [default: %default]"")
    opts, args = p.parse_args(args)

    if len(args) != 1:
        sys.exit(not p.print_help())

    statement, = args
    fa, op, fb = statement.split()
    assert op in ('|', '&', '-', '^')

    column = opts.column
    fa = SetFile(fa, column=column)
    fb = SetFile(fb, column=column)

    if op == '|':
        t = fa | fb
    elif op == '&':
        t = fa & fb
    elif op == '-':
        t = fa - fb
    elif op == '^':
        t = fa ^ fb

    for x in natsorted(t):
        print(x)","%prog setop ""fileA & fileB"" > newfile

    Perform set operations, except on files. The files (fileA and fileB) contain
    list of ids. The operator is one of the four:

    |: union (elements found in either file)
    &: intersection (elements found in both)
    -: difference (elements in fileA but not in fileB)
    ^: symmetric difference (elementes found in either set but not both)

    Please quote the argument to avoid shell interpreting | and &.",0,0,3,3
"def setup(self, pin, value):
        
        self._validate_pin(pin)
        
        if value == GPIO.IN:
            self.iodir[int(pin/8)] |= 1 << (int(pin%8))
        elif value == GPIO.OUT:
            self.iodir[int(pin/8)] &= ~(1 << (int(pin%8)))
        else:
            raise ValueError('Unexpected value.  Must be GPIO.IN or GPIO.OUT.')
        self.write_iodir()","Set the input or output mode for a specified pin.  Mode should be
        either GPIO.OUT or GPIO.IN.",1,0,2,3
"def setup_seeds(master_seed, epochs, device):
    
    if master_seed is None:
        
        master_seed = random.SystemRandom().randint(0, 2**32 - 1)
        if get_rank() == 0:
            
            
            
            logging.info(f'Using random master seed: {master_seed}')
    else:
        
        logging.info(f'Using master seed from command line: {master_seed}')

    gnmt_print(key=mlperf_log.RUN_SET_RANDOM_SEED, value=master_seed,
               sync=False)

    
    seeding_rng = random.Random(master_seed)

    
    worker_seeds = generate_seeds(seeding_rng, get_world_size())

    
    shuffling_seeds = generate_seeds(seeding_rng, epochs)

    
    worker_seeds = broadcast_seeds(worker_seeds, device)
    shuffling_seeds = broadcast_seeds(shuffling_seeds, device)
    return worker_seeds, shuffling_seeds","Generates seeds from one master_seed.
    Function returns (worker_seeds, shuffling_seeds), worker_seeds are later
    used to initialize per-worker random number generators (mostly for
    dropouts), shuffling_seeds are for RNGs resposible for reshuffling the
    dataset before each epoch.
    Seeds are generated on worker with rank 0 and broadcasted to all other
    workers.

    :param master_seed: master RNG seed used to initialize other generators
    :param epochs: number of epochs
    :param device: torch.device (used for distributed.broadcast)",0,2,3,5
"def sg_input(shape=None, dtype=sg_floatx, name=None):
    r
    if shape is None:
        return tf.placeholder(dtype, shape=None, name=name)
    else:
        if not isinstance(shape, (list, tuple)):
            shape = [shape]
        return tf.placeholder(dtype, shape=[None] + list(shape), name=name)","r""""""Creates a placeholder.

    Args:
      shape: A tuple/list of integers. If an integers is given, it will turn to a list.
      dtype: A data type. Default is float32.
      name: A name for the placeholder.

    Returns:
      A wrapped placeholder `Tensor`.",0,0,2,2
"def share_extender(self, share: dict, results_filtered: dict):
        
        
        creator_id = share.get(""_creator"").get(""_tag"")[6:]
        share[""admin_url""] = ""{}/groups/{}/admin/shares/{}"".format(
            self.app_url, creator_id, share.get(""_id"")
        )
        
        opencat_url = ""{}/s/{}/{}"".format(
            self.oc_url, share.get(""_id""), share.get(""urlToken"")
        )
        if requests.head(opencat_url):
            share[""oc_url""] = opencat_url
        else:
            pass
        
        share[""mds_ids""] = (i.get(""_id"") for i in results_filtered)

        return share","Extend share model with additional informations.

        :param dict share: share returned by API
        :param dict results_filtered: filtered search result",0,1,1,2
"def shellcode(executables, use_defaults=True, shell='bash', complete_arguments=None):
    

    if complete_arguments is None:
        complete_options = '-o nospace -o default' if use_defaults else '-o nospace'
    else:
        complete_options = "" "".join(complete_arguments)

    if shell == 'bash':
        quoted_executables = [quote(i) for i in executables]
        executables_list = "" "".join(quoted_executables)
        code = bashcode % dict(complete_opts=complete_options, executables=executables_list)
    else:
        code = """"
        for executable in executables:
            code += tcshcode % dict(executable=executable)

    return code","Provide the shell code required to register a python executable for use with the argcomplete module.

    :param str executables: Executables to be completed (when invoked exactly with this name
    :param bool use_defaults: Whether to fallback to readline's default completion when no matches are generated.
    :param str shell: Name of the shell to output code for (bash or tcsh)
    :param complete_arguments: Arguments to call complete with
    :type complete_arguments: list(str) or None",0,0,3,3
"def short_path(path, cwd=None):
    
    if not isinstance(path, str):
        return path
    if cwd is None:
        cwd = os.getcwd()
    abspath = os.path.abspath(path)
    relpath = os.path.relpath(path, cwd)
    if len(abspath) <= len(relpath):
        return abspath
    return relpath","Return relative or absolute path name, whichever is shortest.",0,0,1,1
"def should_log(self, logger_name: str, level: str) -> bool:
        
        if (logger_name, level) not in self._should_log:
            log_level_per_rule = self._get_log_level(logger_name)
            log_level_per_rule_numeric = getattr(logging, log_level_per_rule.upper(), 10)
            log_level_event_numeric = getattr(logging, level.upper(), 10)

            should_log = log_level_event_numeric >= log_level_per_rule_numeric
            self._should_log[(logger_name, level)] = should_log
        return self._should_log[(logger_name, level)]",Returns if a message for the logger should be logged.,0,0,1,1
"def show(self, id):
        
        return self._get(
            url='{root}key_transactions/{id}.json'.format(
                root=self.URL,
                id=id
            ),
            headers=self.headers,
        )","This API endpoint returns a single Key transaction, identified its ID.

        :type id: int
        :param id: Key transaction ID

        :rtype: dict
        :return: The JSON response of the API

        ::

            {
                ""key_transaction"": {
                    ""id"": ""integer"",
                    ""name"": ""string"",
                    ""transaction_name"": ""string"",
                    ""application_summary"": {
                        ""response_time"": ""float"",
                        ""throughput"": ""float"",
                        ""error_rate"": ""float"",
                        ""apdex_target"": ""float"",
                        ""apdex_score"": ""float""
                    },
                    ""end_user_summary"": {
                        ""response_time"": ""float"",
                        ""throughput"": ""float"",
                        ""apdex_target"": ""float"",
                        ""apdex_score"": ""float""
                    },
                    ""links"": {
                        ""application"": ""integer""
                    }
                }
            }",0,1,0,1
"def show(self, ticket):
        
        if isinstance(ticket, dict):
            ticket = ticket['ticket']
        return requests.get(
            url='https://mp.weixin.qq.com/cgi-bin/showqrcode',
            params={
                'ticket': ticket
            }
        )","Exchange ticket for QR code
For details, please refer to
https://mp.weixin.qq.com/wiki?t=resource/res_main&id=mp1443433542

:param ticket: QR code ticket. You can get it through :func:`create`
:return: The returned Request object

Example::

from wechatpy import WeChatClient

client = WeChatClient('appid', 'secret')
res = client.qrcode.show('ticket data')",0,1,1,2
"def show_fuzzy_date(self):
        
        date = self.time.replace(tzinfo=None)
        if date <= datetime.now():
            diff = datetime.now() - date
            if diff.days >= 14:
                return False
        else:
            diff = date - datetime.now()
            if diff.days >= 14:
                return False

        return True","Return whether the event is in the next or previous 2 weeks.

        Determines whether to display the fuzzy date.",0,0,1,1
"def show_help(message=None):
    

    help_path = mktemp('.html')
    with open(help_path, 'wb+') as f:
        help_html = get_help_html(message)
        f.write(help_html.encode('utf8'))
        path_with_protocol = 'file://' + help_path
        QDesktopServices.openUrl(QUrl(path_with_protocol))","Open an help message in the user's browser

    :param message: An optional message object to display in the dialog.
    :type message: Message.Message",1,0,0,1
"def show_list_translations(context, item):
    
    if not item:
        return

    manager = Manager()
    manager.set_master(item)

    ct_item = ContentType.objects.get_for_model(item)

    item_language_codes = manager.get_languages_from_item(ct_item, item)
    model_language_codes = manager.get_languages_from_model(ct_item.app_label, ct_item.model)

    item_languages = [{'lang': lang, 'from_model': lang.code in model_language_codes}
                      for lang in TransLanguage.objects.filter(code__in=item_language_codes).order_by('name')]

    more_languages = [{'lang': lang, 'from_model': lang.code in model_language_codes}
                      for lang in TransLanguage.objects.exclude(main_language=True).order_by('name')]

    return render_to_string('languages/translation_language_selector.html', {
        'item_languages': item_languages,
        'more_languages': more_languages,
        'api_url': TM_API_URL,
        'app_label': manager.app_label,
        'model': manager.model_label,
        'object_pk': item.pk
    })","Return the widget to select the translations we want
    to order or delete from the item it's being edited

    :param context:
    :param item:
    :return:",0,0,1,1
"def show_observation_matrix(cnn_output_dynamic):
        
        
        observation_matrix = numpy.array(cnn_output_dynamic.allocate_observation_matrix())
        plt.imshow(observation_matrix.T, cmap = plt.get_cmap('gray'), interpolation='None', vmin = 0.0, vmax = 1.0)
        plt.show()","!
        @brief Shows observation matrix as black/white blocks.
        @details This type of visualization is convenient for observing allocated clusters.
        
        @param[in] cnn_output_dynamic (cnn_dynamic): Output dynamic of the chaotic neural network.
        
        @see show_output_dynamic
        @see show_dynamic_matrix",0,0,1,1
"def show_progress(job):
    
    start = time.time()
    last_print = 0
    last_line = 0
    for prog, total in chain(job, [(1, 1)]):
        
        if (time.time() - last_print) > .1 or prog >= total:
            delta = (time.time() - start) or .0001
            line = ""%.1f%% complete, %.1f seconds elapsed, %.1f seconds remaining""%(
                100. * prog / (total or 1), delta, total * delta / (prog or 1) - delta)
            length = len(line)
            
            line += max(last_line - length, 0) * ' '
            print(line, end=""\r"")
            last_line = length
            last_print = time.time()
    print()","This utility function will print the progress of a passed iterator job as
    started by ``refresh_indices()`` and ``clean_old_index()``.

    Usage example::

        class RomTest(Model):
            pass

        for i in xrange(1000):
            RomTest().save()

        util.show_progress(util.clean_old_index(RomTest))",1,0,1,2
"def show_top_losses(self, k:int, max_len:int=70)->None:
        
        from IPython.display import display, HTML
        items = []
        tl_val,tl_idx = self.top_losses()
        for i,idx in enumerate(tl_idx):
            if k <= 0: break
            k -= 1
            tx,cl = self.data.dl(self.ds_type).dataset[idx]
            cl = cl.data
            classes = self.data.classes
            txt = ' '.join(tx.text.split(' ')[:max_len]) if max_len is not None else tx.text
            tmp = [txt, f'{classes[self.pred_class[idx]]}', f'{classes[cl]}', f'{self.losses[idx]:.2f}',
                   f'{self.probs[idx][cl]:.2f}']
            items.append(tmp)
        items = np.array(items)
        names = ['Text', 'Prediction', 'Actual', 'Loss', 'Probability']
        df = pd.DataFrame({n:items[:,i] for i,n in enumerate(names)}, columns=names)
        with pd.option_context('display.max_colwidth', -1):
            display(HTML(df.to_html(index=False)))","Create a tabulation showing the first `k` texts in top_losses along with their prediction, actual,loss, and probability of
        actual class. `max_len` is the maximum number of tokens displayed.",1,0,2,3
"def sign_file(self, message_file, key_id=None, passphrase=None,
                  clearsign=True, detach=False, binary=False):
        
        args = ['-s' if binary else '-sa']
        if detach:
            args.append('--detach-sign')
        if clearsign:
            args.append('--clearsign')
        if key_id:
            
            args += ('--local-user', key_id)
        return self.execute(SignResult(), args, passphrase, message_file, True)","Make a signature.

        :param message_file: File-like object for sign
        :param key_id: Key for signing, default will be used if null
        :param passphrase: Key password
        :param clearsign: Make a clear text signature
        :param detach: Make a detached signature
        :param binary: If false, create ASCII armored output
        :rtype: SignResult",0,0,1,1
"def sign_python():
    
    from subprocess import check_output
    from os import system

    echo(green('\nSign python:'))
    echo(green('-' * 40))

    
    python = check_output(['which', 'python']).decode().replace('\n', '')
    echo('Interpreter: ' + yellow(python))

    
    username = check_output(['id', '-un']).decode().replace('\n', '')
    echo('Using certificate: ' + yellow(username) + '\n')

    
    cert = '""{}""'.format(username)
    cmd = ""codesign -s {cert} -f {python}"".format(cert=cert, python=python)
    system(cmd)

    echo(green('\nDONE\n'))","Sign python (MacOS)
    Signing your python interpreter using self-signed certificate is used to
    get rid of annoying firewall questions about whether to allow incoming
    connections to the interpreter that happen on each app restart. This only
    makes sense on Mac. In order to use this command you must first create
    a certificate to sign your code with. To do it:

      1. Open Keychain Access
      2. Choose: Keychain Access > Certificate Assistant > Create Certificate
      3. Important: Use your current username for certificate name (id -un)
      4. Select Certificate Type: Code Signing
      5. Select Type: Self Signed Root
      6. Check 'Let me override defaults' box
      7. Click Continue, and give it a serial number (maximum randomness)
      8. Accept defaults for the rest

    You will only need to do this once. After this is done you can use
    generated certificate to sign your Python in any project.",0,0,1,1
"def simulate_linear_model(A, x0, v, ts_length):
    r
    A = np.asarray(A)
    n = A.shape[0]
    x = np.empty((n, ts_length))
    x[:, 0] = x0
    for t in range(ts_length-1):
        
        for i in range(n):
            x[i, t+1] = v[i, t]                   
            for j in range(n):
                x[i, t+1] += A[i, j] * x[j, t]   
    return x","r""""""
    This is a separate function for simulating a vector linear system of
    the form

    .. math::

        x_{t+1} = A x_t + v_t

    given :math:`x_0` = x0

    Here :math:`x_t` and :math:`v_t` are both n x 1 and :math:`A` is n x n.

    The purpose of separating this functionality out is to target it for
    optimization by Numba.  For the same reason, matrix multiplication is
    broken down into for loops.

    Parameters
    ----------
    A : array_like or scalar(float)
        Should be n x n
    x0 : array_like
        Should be n x 1.  Initial condition
    v : np.ndarray
        Should be n x ts_length-1.  Its t-th column is used as the time t
        shock :math:`v_t`
    ts_length : int
        The length of the time series

    Returns
    --------
    x : np.ndarray
        Time series with ts_length columns, the t-th column being :math:`x_t`",0,0,1,1
"def simulate_run(res, rstate=None, return_idx=False, approx=False):
    

    if rstate is None:
        rstate = np.random

    
    new_res, samp_idx = resample_run(res, rstate=rstate, return_idx=True)

    
    new_res = jitter_run(new_res, rstate=rstate, approx=approx)

    if return_idx:
        return new_res, samp_idx
    else:
        return new_res","Probes **combined uncertainties** (statistical and sampling) on a nested
    sampling run by wrapping :meth:`jitter_run` and :meth:`resample_run`.

    Parameters
    ----------
    res : :class:`~dynesty.results.Results` instance
        The :class:`~dynesty.results.Results` instance taken from a previous
        nested sampling run.

    rstate : `~numpy.random.RandomState`, optional
        `~numpy.random.RandomState` instance.

    return_idx : bool, optional
        Whether to return the list of resampled indices used to construct
        the new run. Default is `False`.

    approx : bool, optional
        Whether to approximate all sets of uniform order statistics by their
        associated marginals (from the Beta distribution). Default is `False`.

    Returns
    -------
    new_res : :class:`~dynesty.results.Results` instance
        A new :class:`~dynesty.results.Results` instance with corresponding
        samples and weights based on our ""simulated"" samples and
        prior volumes.",0,0,2,2
"def single(self):
        
        records = list(self)
        size = len(records)
        if size == 0:
            return None
        if size != 1:
            warn(""Expected a result with a single record, but this result contains %d"" % size)
        return records[0]","Obtain the next and only remaining record from this result.

        A warning is generated if more than one record is available but
        the first of these is still returned.

        :returns: the next :class:`.Record` or :const:`None` if none remain
        :warns: if more than one record is available",0,0,2,2
"def single_gate_params(gate, params=None):
    
    if gate in ('U', 'u3'):
        return params[0], params[1], params[2]
    elif gate == 'u2':
        return np.pi / 2, params[0], params[1]
    elif gate == 'u1':
        return 0, 0, params[0]
    elif gate == 'id':
        return 0, 0, 0
    raise QiskitError('Gate is not among the valid types: %s' % gate)","Apply a single qubit gate to the qubit.

    Args:
        gate(str): the single qubit gate name
        params(list): the operation parameters op['params']
    Returns:
        tuple: a tuple of U gate parameters (theta, phi, lam)
    Raises:
        QiskitError: if the gate name is not valid",1,0,2,3
"def sink_create(
        self, project, sink_name, filter_, destination, unique_writer_identity=False
    ):
        
        parent = ""projects/%s"" % (project,)
        sink_pb = LogSink(name=sink_name, filter=filter_, destination=destination)
        created_pb = self._gapic_api.create_sink(
            parent, sink_pb, unique_writer_identity=unique_writer_identity
        )
        return MessageToDict(created_pb)","API call:  create a sink resource.

        See
        https://cloud.google.com/logging/docs/reference/v2/rest/v2/projects.sinks/create

        :type project: str
        :param project: ID of the project in which to create the sink.

        :type sink_name: str
        :param sink_name: the name of the sink

        :type filter_: str
        :param filter_: the advanced logs filter expression defining the
                        entries exported by the sink.

        :type destination: str
        :param destination: destination URI for the entries exported by
                            the sink.

        :type unique_writer_identity: bool
        :param unique_writer_identity: (Optional) determines the kind of
                                       IAM identity returned as
                                       writer_identity in the new sink.

        :rtype: dict
        :returns: The sink resource returned from the API (converted from a
                  protobuf to a dictionary).",0,1,1,2
"def size(self):
        

        if self.id.endswith(""/""):
            subqueues = self.get_known_subqueues()
            if len(subqueues) == 0:
                return 0
            else:
                with context.connections.redis.pipeline(transaction=False) as pipe:
                    for subqueue in subqueues:
                        pipe.get(""queuesize:%s"" % subqueue)
                    return [int(size or 0) for size in pipe.execute()]
        else:
            return int(context.connections.redis.get(""queuesize:%s"" % self.id) or 0)",Returns the total number of queued jobs on the queue,0,1,1,2
"def slice_to_SLICE(sliceVals, width):
    
    if sliceVals.step is not None:
        raise NotImplementedError()

    start = sliceVals.start
    stop = sliceVals.stop

    if sliceVals.start is None:
        start = INT.fromPy(width)
    else:
        start = toHVal(sliceVals.start)

    if sliceVals.stop is None:
        stop = INT.fromPy(0)
    else:
        stop = toHVal(sliceVals.stop)

    startIsVal = isinstance(start, Value)
    stopIsVal = isinstance(stop, Value)

    indexesAreValues = startIsVal and stopIsVal
    if indexesAreValues:
        updateTime = max(start.updateTime, stop.updateTime)
    else:
        updateTime = -1

    return Slice.getValueCls()((start, stop), SLICE, 1, updateTime)",convert python slice to value of SLICE hdl type,1,0,2,3
"def sma(eqdata, **kwargs):
    
    if len(eqdata.shape) > 1 and eqdata.shape[1] != 1:
        _selection = kwargs.get('selection', 'Adj Close')
        _eqdata = eqdata.loc[:, _selection]
    else:
        _eqdata = eqdata
    _window = kwargs.get('window', 20)
    _outputcol = kwargs.get('outputcol', 'SMA')
    ret = pd.DataFrame(index=_eqdata.index, columns=[_outputcol], dtype=np.float64)
    ret.loc[:, _outputcol] = _eqdata.rolling(window=_window, center=False).mean().values.flatten()
    return ret","simple moving average 

    Parameters
    ----------
    eqdata : DataFrame
    window : int, optional
        Lookback period for sma. Defaults to 20.
    outputcol : str, optional
        Column to use for output. Defaults to 'SMA'.
    selection : str, optional
        Column of eqdata on which to calculate sma. If
        `eqdata` has only 1 column, `selection` is ignored,
        and sma is calculated on that column. Defaults
        to 'Adj Close'.",0,0,3,3
"def smart_scrubb(df,col_name,error_rate = 0):
    
    scrubbed = """"
    while True:
        valcounts = df[col_name].str[-len(scrubbed)-1:].value_counts()
        if not len(valcounts):
            break
        if not valcounts[0] >= (1-error_rate) * _utils.rows(df):
            break
        scrubbed=valcounts.index[0]
    if scrubbed == '':
        return None
    which = df[col_name].str.endswith(scrubbed)
    _basics.col_scrubb(df,col_name,which,len(scrubbed),True)
    if not which.all():
        new_col_name = _basics.colname_gen(df,""{}_sb-{}"".format(col_name,scrubbed))
        df[new_col_name] = which
    return scrubbed","Scrubs from the back of an 'object' column in a DataFrame
    until the scrub would semantically alter the contents of the column. If only a 
    subset of the elements in the column are scrubbed, then a boolean array indicating which
    elements have been scrubbed is appended to the dataframe. Returns the string that was scrubbed.
    df - DataFrame
        DataFrame to scrub
    col_name - string
        Name of column to scrub
    error_rate - number, default 0
        The maximum amount of values this function can ignore while scrubbing, expressed as a
        fraction of the total amount of rows in the dataframe.",0,0,2,2
"def smooth(x, y, degree=3):
    

    if degree == 1:
        return x, y
    elif not isodd(degree):
        raise ValueError(""Degree must be an odd number"")
    else:
        window_width = int(degree / 2)
        data_size = len(y) - degree + 1

        smoothed_y = np.zeros(data_size)
        for idx in range(degree):
            smoothed_y += y[idx:idx + data_size]
        smoothed_y /= degree

        smoothed_x = x[window_width:-window_width]

        return smoothed_x, smoothed_y","Smooth y-values and return new x, y pair.",1,0,2,3
"def snapshot(self, filename=""tmp.png""):
        
        if not filename:
            filename = ""tmp.png""
        if self.handle:
            try:
                screenshot(filename, self.handle)
            except win32gui.error:
                self.handle = None
                screenshot(filename)
        else:
            screenshot(filename)

        img = aircv.imread(filename)
        os.remove(filename)

        return img","Take a screenshot and save it to `tmp.png` filename by default

        Args:
            filename: name of file where to store the screenshot

        Returns:
            display the screenshot",1,0,1,2
"def snlinverter(v_dc, p_dc, inverter):
    r

    Paco = inverter['Paco']
    Pdco = inverter['Pdco']
    Vdco = inverter['Vdco']
    Pso = inverter['Pso']
    C0 = inverter['C0']
    C1 = inverter['C1']
    C2 = inverter['C2']
    C3 = inverter['C3']
    Pnt = inverter['Pnt']

    A = Pdco * (1 + C1*(v_dc - Vdco))
    B = Pso * (1 + C2*(v_dc - Vdco))
    C = C0 * (1 + C3*(v_dc - Vdco))

    ac_power = (Paco/(A-B) - C*(A-B)) * (p_dc-B) + C*((p_dc-B)**2)
    ac_power = np.minimum(Paco, ac_power)
    ac_power = np.where(p_dc < Pso, -1.0 * abs(Pnt), ac_power)

    if isinstance(p_dc, pd.Series):
        ac_power = pd.Series(ac_power, index=p_dc.index)

    return ac_power","r'''
    Converts DC power and voltage to AC power using Sandia's
    Grid-Connected PV Inverter model.

    Determines the AC power output of an inverter given the DC voltage,
    DC power, and appropriate Sandia Grid-Connected Photovoltaic
    Inverter Model parameters. The output, ac_power, is clipped at the
    maximum power output, and gives a negative power during low-input
    power conditions, but does NOT account for maximum power point
    tracking voltage windows nor maximum current or voltage limits on
    the inverter.

    Parameters
    ----------
    v_dc : numeric
        DC voltages, in volts, which are provided as input to the
        inverter. Vdc must be >= 0.

    p_dc : numeric
        A scalar or DataFrame of DC powers, in watts, which are provided
        as input to the inverter. Pdc must be >= 0.

    inverter : dict-like
        A dict-like object defining the inverter to be used, giving the
        inverter performance parameters according to the Sandia
        Grid-Connected Photovoltaic Inverter Model (SAND 2007-5036) [1].
        A set of inverter performance parameters are provided with
        pvlib, or may be generated from a System Advisor Model (SAM) [2]
        library using retrievesam. See Notes for required keys.

    Returns
    -------
    ac_power : numeric
        Modeled AC power output given the input DC voltage, Vdc, and
        input DC power, Pdc. When ac_power would be greater than Pac0,
        it is set to Pac0 to represent inverter ""clipping"". When
        ac_power would be less than Ps0 (startup power required), then
        ac_power is set to -1*abs(Pnt) to represent nightly power
        losses. ac_power is not adjusted for maximum power point
        tracking (MPPT) voltage windows or maximum current limits of the
        inverter.

    Notes
    -----

    Required inverter keys are:

    ======   ============================================================
    Column   Description
    ======   ============================================================
    Pac0     AC-power output from inverter based on input power
             and voltage (W)
    Pdc0     DC-power input to inverter, typically assumed to be equal
             to the PV array maximum power (W)
    Vdc0     DC-voltage level at which the AC-power rating is achieved
             at the reference operating condition (V)
    Ps0      DC-power required to start the inversion process, or
             self-consumption by inverter, strongly influences inverter
             efficiency at low power levels (W)
    C0       Parameter defining the curvature (parabolic) of the
             relationship between ac-power and dc-power at the reference
             operating condition, default value of zero gives a
             linear relationship (1/W)
    C1       Empirical coefficient allowing Pdco to vary linearly
             with dc-voltage input, default value is zero (1/V)
    C2       Empirical coefficient allowing Pso to vary linearly with
             dc-voltage input, default value is zero (1/V)
    C3       Empirical coefficient allowing Co to vary linearly with
             dc-voltage input, default value is zero (1/V)
    Pnt      AC-power consumed by inverter at night (night tare) to
             maintain circuitry required to sense PV array voltage (W)
    ======   ============================================================

    References
    ----------
    [1] SAND2007-5036, ""Performance Model for Grid-Connected
    Photovoltaic Inverters by D. King, S. Gonzalez, G. Galbraith, W.
    Boyson

    [2] System Advisor Model web page. https://sam.nrel.gov.

    See also
    --------
    sapm
    singlediode",0,0,1,1
"def snooze_alert(self, id, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.snooze_alert_with_http_info(id, **kwargs)  
        else:
            (data) = self.snooze_alert_with_http_info(id, **kwargs)  
            return data","Snooze a specific alert for some number of seconds  # noqa: E501

          # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.snooze_alert(id, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str id: (required)
        :param int seconds:
        :return: ResponseContainerAlert
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def som_train(som_pointer, data, epochs, autostop):
     
    
    pointer_data = package_builder(data, c_double).create()
    
    ccore = ccore_library.get()
    ccore.som_train.restype = c_size_t
    return ccore.som_train(som_pointer, pointer_data, c_uint(epochs), autostop)","!
    @brief Trains self-organized feature map (SOM) using CCORE pyclustering library.

    @param[in] data (list): Input data - list of points where each point is represented by list of features, for example coordinates.
    @param[in] epochs (uint): Number of epochs for training.        
    @param[in] autostop (bool): Automatic termination of learining process when adaptation is not occurred.
    
    @return (uint) Number of learining iterations.",0,1,1,2
"def sort(self, *keys):
        
        s = self._clone()
        s._sort = []
        for k in keys:
            if isinstance(k, string_types) and k.startswith('-'):
                if k[1:] == '_score':
                    raise IllegalOperation('Sorting by `-_score` is not allowed.')
                k = {k[1:]: {""order"": ""desc""}}
            s._sort.append(k)
        return s","Add sorting information to the search request. If called without
        arguments it will remove all sort requirements. Otherwise it will
        replace them. Acceptable arguments are::

            'some.field'
            '-some.other.field'
            {'different.field': {'any': 'dict'}}

        so for example::

            s = Search().sort(
                'category',
                '-title',
                {""price"" : {""order"" : ""asc"", ""mode"" : ""avg""}}
            )

        will sort by ``category``, ``title`` (in descending order) and
        ``price`` in ascending order using the ``avg`` mode.

        The API returns a copy of the Search object and can thus be chained.",0,0,2,2
"def sort_by(self, attr_name, reverse=False):
        
        try:
            d = dict()
            for abspath, winfile in self.files.items():
                d[abspath] = getattr(winfile, attr_name)
            self.order = [item[0] for item in sorted(
                list(d.items()), key=lambda t: t[1], reverse = reverse)]
        except AttributeError:
            raise ValueError(""valid sortable attributes are: ""
                             ""abspath, dirname, basename, fname, ext, ""
                             ""size_on_disk, atime, ctime, mtime;"")","Sort files by one of it's attributes.
        
        **中文文档**
        
        对容器内的WinFile根据其某一个属性升序或者降序排序。",1,0,2,3
"def speak(self, text):
        
        if not self.is_valid_string(text):
            raise Exception(""%s is not ISO-8859-1 compatible."" % (text))

        
        if len(text) > 1023:
            lines = self.word_wrap(text, width=1023)
            for line in lines:
                self.queue.put(""S%s"" % (line))
        else:
            self.queue.put(""S%s"" % (text))",The main function to convert text into speech.,1,0,2,3
"def spell_check(request):
    
    data = json.loads(request.body.decode('utf-8'))
    output = {'id': data['id']}
    error = None
    status = 200
    try:
        if data['params']['lang'] not in list_languages():
            error = 'Missing {0} dictionary!'.format(data['params']['lang'])
            raise LookupError(error)
        spell_checker = checker.SpellChecker(data['params']['lang'])
        spell_checker.set_text(strip_tags(data['params']['text']))
        output['result'] = {spell_checker.word: spell_checker.suggest()
                            for err in spell_checker}
    except NameError:
        error = 'The pyenchant package is not installed!'
        logger.exception(error)
    except LookupError:
        logger.exception(error)
    except Exception:
        error = 'Unknown error!'
        logger.exception(error)
    if error is not None:
        output['error'] = error
        status = 500
    return JsonResponse(output, status=status)","Implements the TinyMCE 4 spellchecker protocol

    :param request: Django http request with JSON-RPC payload from TinyMCE 4
        containing a language code and a text to check for errors.
    :type request: django.http.request.HttpRequest
    :return: Django http response containing JSON-RPC payload
        with spellcheck results for TinyMCE 4
    :rtype: django.http.JsonResponse",1,3,2,6
"def spin_to_binary(linear, quadratic, offset):
        

        
        new_linear = {v: 2. * bias for v, bias in iteritems(linear)}

        
        new_quadratic = {}
        for (u, v), bias in iteritems(quadratic):
            new_quadratic[(u, v)] = 4. * bias
            new_linear[u] -= 2. * bias
            new_linear[v] -= 2. * bias

        
        offset += sum(itervalues(quadratic)) - sum(itervalues(linear))

        return new_linear, new_quadratic, offset","convert linear, quadratic, and offset from spin to binary.
        Does no checking of vartype. Copies all of the values into new objects.",0,0,2,2
"def split_block_by_row_length(block, split_row_length):
    
    split_blocks = []
    current_block = []
    for row in block:
        if row_content_length(row) <= split_row_length:
            if current_block:
                split_blocks.append(current_block)
            split_blocks.append([row])
            current_block = []
        else:
            current_block.append(row)
    if current_block:
        split_blocks.append(current_block)

    return split_blocks","Splits the block by finding all rows with less consequetive, non-empty rows than the
    min_row_length input.",0,0,2,2
"def split_data(data, num_slice, batch_axis=0, even_split=True):
    
    size = data.shape[batch_axis]
    if even_split and size % num_slice != 0:
        raise ValueError(
            ""data with shape %s cannot be evenly split into %d slices along axis %d. "" \
            ""Use a batch size that's multiple of %d or set even_split=False to allow "" \
            ""uneven partitioning of data.""%(
                str(data.shape), num_slice, batch_axis, num_slice))

    step = size // num_slice

    
    if not even_split and size < num_slice:
        step = 1
        num_slice = size

    if batch_axis == 0:
        slices = [data[i*step:(i+1)*step] if i < num_slice - 1 else data[i*step:size]
                  for i in range(num_slice)]
    elif even_split:
        slices = ndarray.split(data, num_outputs=num_slice, axis=batch_axis)
    else:
        slices = [ndarray.slice_axis(data, batch_axis, i*step, (i+1)*step)
                  if i < num_slice - 1 else
                  ndarray.slice_axis(data, batch_axis, i*step, size)
                  for i in range(num_slice)]
    return slices","Splits an NDArray into `num_slice` slices along `batch_axis`.
    Usually used for data parallelism where each slices is sent
    to one device (i.e. GPU).

    Parameters
    ----------
    data : NDArray
        A batch of data.
    num_slice : int
        Number of desired slices.
    batch_axis : int, default 0
        The axis along which to slice.
    even_split : bool, default True
        Whether to force all slices to have the same number of elements.
        If `True`, an error will be raised when `num_slice` does not evenly
        divide `data.shape[batch_axis]`.

    Returns
    -------
    list of NDArray
        Return value is a list even if `num_slice` is 1.",1,0,3,4
"def split_genome(genome, chunk_size=10000):
    

    chunks = []
    from Bio import SeqIO
    with open(genome) as handle:
        for record in SeqIO.parse(handle, ""fasta""):
            sequence = record.seq
            n = len(sequence)
            chunks += [str(sequence[i:min(i + chunk_size, n)])
                       for i in range(0, n, chunk_size)]
    return np.array(chunks)",Split genome into chunks of fixed size (save the last one).,1,0,1,2
"def sql_get(self, owner, id, query, **kwargs):
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('callback'):
            return self.sql_get_with_http_info(owner, id, query, **kwargs)
        else:
            (data) = self.sql_get_with_http_info(owner, id, query, **kwargs)
            return data","SQL query (via GET)
        This endpoint executes SQL queries against a dataset.  SQL results are available in a variety of formats. By default, `application/json` will be returned. Set the `Accept` header to one of the following values in accordance with your preference:  * `text/csv` * `application/json` * `application/json-l` * `application/x-ndjson`  New to SQL? Check out data.world's [SQL manual](https://docs.data.world/tutorials/dwsql/) .
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please define a `callback` function
        to be invoked when receiving the response.
        >>> def callback_function(response):
        >>>     pprint(response)
        >>>
        >>> thread = api.sql_get(owner, id, query, callback=callback_function)

        :param callback function: The callback function
            for asynchronous request. (optional)
        :param str owner: User name and unique identifier of the creator of a dataset or project. For example, in the URL: [https://data.world/jonloyens/an-intro-to-dataworld-dataset](https://data.world/jonloyens/an-intro-to-dataworld-dataset), jonloyens is the unique identifier of the owner. (required)
        :param str id: Dataset unique identifier. For example, in the URL:[https://data.world/jonloyens/an-intro-to-dataworld-dataset](https://data.world/jonloyens/an-intro-to-dataworld-dataset), an-intro-to-dataworld-dataset is the unique identifier of the dataset. (required)
        :param str query: (required)
        :param bool include_table_schema: Flags indicating to include table schema in the response.
        :return: None
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def sql_get_oids(self, where=None):
        
        table = self.lconfig.get('table')
        db = self.lconfig.get('db_schema_name') or self.lconfig.get('db')
        _oid = self.lconfig.get('_oid')
        if is_array(_oid):
            _oid = _oid[0]  
        sql = 'SELECT DISTINCT %s.%s FROM %s.%s' % (table, _oid, db, table)
        if where:
            where = [where] if isinstance(where, basestring) else list(where)
            sql += ' WHERE %s' % ' OR '.join(where)
        result = sorted([r[_oid] for r in self._load_sql(sql)])
        return result",Query source database for a distinct list of oids.,1,0,0,1
"def sqrt(
    data: AnnData,
    copy: bool = False,
    chunked: bool = False,
    chunk_size: Optional[int] = None,
) -> Optional[AnnData]:
    
    if isinstance(data, AnnData):
        adata = data.copy() if copy else data
        if chunked:
            for chunk, start, end in adata.chunked_X(chunk_size):
                adata.X[start:end] = sqrt(chunk)
        else:
            adata.X = sqrt(data.X)
        return adata if copy else None
    X = data  
    if not issparse(X):
        return np.sqrt(X)
    else:
        return X.sqrt()","Square root the data matrix.

    Computes :math:`X = \\sqrt(X)`.

    Parameters
    ----------
    data
        The (annotated) data matrix of shape ``n_obs`` × ``n_vars``.
        Rows correspond to cells and columns to genes.
    copy
        If an :class:`~scanpy.api.AnnData` is passed,
        determines whether a copy is returned.
    chunked
        Process the data matrix in chunks, which will save memory.
        Applies only to :class:`~anndata.AnnData`.
    chunk_size
        ``n_obs`` of the chunks to process the data in.

    Returns
    -------
    Returns or updates `data`, depending on `copy`.",0,0,1,1
"def sqs_create_queue(queue_name, options=None, client=None):
    

    if not client:
        client = boto3.client('sqs')

    try:

        if isinstance(options, dict):
            resp = client.create_queue(QueueName=queue_name, Attributes=options)
        else:
            resp = client.create_queue(QueueName=queue_name)

        if resp is not None:
            return {'url':resp['QueueUrl'],
                    'name':queue_name}
        else:
            LOGERROR('could not create the specified queue: %s with options: %s'
                     % (queue_name, options))
            return None

    except Exception as e:
        LOGEXCEPTION('could not create the specified queue: %s with options: %s'
                     % (queue_name, options))
        return None","This creates an SQS queue.

    Parameters
    ----------

    queue_name : str
        The name of the queue to create.

    options : dict or None
        A dict of options indicate extra attributes the queue should have.
        See the SQS docs for details. If None, no custom attributes will be
        attached to the queue.

    client : boto3.Client or None
        If None, this function will instantiate a new `boto3.Client` object to
        use in its operations. Alternatively, pass in an existing `boto3.Client`
        instance to re-use it here.

    Returns
    -------

    dict
        This returns a dict of the form::

            {'url': SQS URL of the queue,
             'name': name of the queue}",0,1,2,3
"def squeeze(name, x, factor=2, reverse=True):
  
  with tf.variable_scope(name, reuse=tf.AUTO_REUSE):
    shape = common_layers.shape_list(x)
    if factor == 1:
      return x
    height = int(shape[1])
    width = int(shape[2])
    n_channels = int(shape[3])

    if not reverse:
      assert height % factor == 0 and width % factor == 0
      x = tf.reshape(x, [-1, height//factor, factor,
                         width//factor, factor, n_channels])
      x = tf.transpose(x, [0, 1, 3, 5, 2, 4])
      x = tf.reshape(x, [-1, height//factor, width //
                         factor, n_channels*factor*factor])
    else:
      x = tf.reshape(
          x, (-1, height, width, int(n_channels/factor**2), factor, factor))
      x = tf.transpose(x, [0, 1, 4, 2, 5, 3])
      x = tf.reshape(x, (-1, int(height*factor),
                         int(width*factor), int(n_channels/factor**2)))
    return x","Block-wise spatial squeezing of x to increase the number of channels.

  Args:
    name: Used for variable scoping.
    x: 4-D Tensor of shape (batch_size X H X W X C)
    factor: Factor by which the spatial dimensions should be squeezed.
    reverse: Squueze or unsqueeze operation.

  Returns:
    x: 4-D Tensor of shape (batch_size X (H//factor) X (W//factor) X
       (cXfactor^2). If reverse is True, then it is factor = (1 / factor)",0,0,2,2
"def stack(recs, fields=None):
    
    if fields is None:
        fields = list(set.intersection(
            *[set(rec.dtype.names) for rec in recs]))
        
        if set(fields) == set(recs[0].dtype.names):
            fields = list(recs[0].dtype.names)
    return np.hstack([rec[fields] for rec in recs])","Stack common fields in multiple record arrays (concatenate them).

    Parameters
    ----------
    recs : list
        List of NumPy record arrays
    fields : list of strings, optional (default=None)
        The list of fields to include in the stacked array. If None, then
        include the fields in common to all the record arrays.

    Returns
    -------
    rec : NumPy record array
        The stacked array.",0,0,1,1
"def stage_wbem_connection(self, wbem_connection):
        
        self._conn_id = wbem_connection.conn_id

        if self.enabled:
            if self.api_detail_level is not None:
                logger = self.apilogger
                detail_level = self.api_detail_level
                max_len = self.api_maxlen
            elif self.http_detail_level is not None:
                logger = self.httplogger
                detail_level = self.http_detail_level
                max_len = self.http_maxlen
            else:
                return

            if logger.isEnabledFor(logging.DEBUG):
                conn_data = str(wbem_connection) if detail_level == 'summary' \
                    else repr(wbem_connection)

                if max_len and (len(conn_data) > max_len):
                    conn_data = conn_data[:max_len] + '...'
                logger.debug('Connection:%s %s', self._conn_id, conn_data)","Log connection information. This includes the connection id (conn_id)
        that is output with the log entry. This entry is logged if either
        http or api loggers are enable. It honors both the logger and
        detail level of either api logger if defined or http logger if defined.
        If the api logger does not exist, the output shows this as an http
        loggger output since we do not want to create an api logger for this
        specific output",0,1,0,1
"def start(self):
    
    assert not self.watching

    def selector(evt):
      if evt.is_directory:
        return False
      path = evt.path
      if path in self._last_fnames: 
        return False
      for pattern in self.skip_pattern.split("";""):
        if fnmatch(path, pattern.strip()):
          return False
      return True

    def watchdog_handler(evt):
      wx.CallAfter(self._watchdog_handler, evt)

    
    self._watching = True
    self._last_fnames = []
    self._evts = [None]
    self._run_subprocess()

    
    from .watcher import watcher
    self._watcher = watcher(path=self.directory,
                            selector=selector, handler=watchdog_handler)
    self._watcher.__enter__()",Starts watching the path and running the test jobs.,0,1,1,2
"def start_log(level=logging.DEBUG, filename=None):
    
    if filename is None:
        tstr = time.ctime()
        tstr = tstr.replace(' ', '.')
        tstr = tstr.replace(':', '.')
        filename = 'deblur.log.%s' % tstr
    logging.basicConfig(filename=filename, level=level,
                        format='%(levelname)s(%(thread)d)'
                        '%(asctime)s:%(message)s')
    logger = logging.getLogger(__name__)
    logger.info('*************************')
    logger.info('deblurring started')","start the logger for the run

    Parameters
    ----------
    level : int, optional
        logging.DEBUG, logging.INFO etc. for the log level (between 0-50).
    filename : str, optional
      name of the filename to save the log to or
      None (default) to use deblur.log.TIMESTAMP",0,0,3,3
"def stash(self, path):
        
        if os.path.isdir(path):
            new_path = self._get_directory_stash(path)
        else:
            new_path = self._get_file_stash(path)

        self._moves.append((path, new_path))
        if os.path.isdir(path) and os.path.isdir(new_path):
            
            
            
            
            
            os.rmdir(new_path)
        renames(path, new_path)
        return new_path",Stashes the directory or file and returns its new location.,1,0,2,3
"def statuses_user_timeline(self, user_id=None, screen_name=None,
                               since_id=None, count=None, max_id=None,
                               trim_user=None, exclude_replies=None,
                               contributor_details=None,
                               include_rts=None):
        
        params = {}
        set_str_param(params, 'user_id', user_id)
        set_str_param(params, 'screen_name', screen_name)
        set_str_param(params, 'since_id', since_id)
        set_int_param(params, 'count', count)
        set_str_param(params, 'max_id', max_id)
        set_bool_param(params, 'trim_user', trim_user)
        set_bool_param(params, 'exclude_replies', exclude_replies)
        set_bool_param(params, 'contributor_details', contributor_details)
        set_bool_param(params, 'include_rts', include_rts)
        return self._get_api('statuses/user_timeline.json', params)","Returns a list of the most recent tweets posted by the specified user.

        https://dev.twitter.com/docs/api/1.1/get/statuses/user_timeline

        Either ``user_id`` or ``screen_name`` must be provided.

        :param str user_id:
            The ID of the user to return tweets for.

        :param str screen_name:
            The screen name of the user to return tweets for.

        :param str since_id:
            Returns results with an ID greater than (that is, more recent than)
            the specified ID. Tweets newer than this may not be returned due to
            certain API limits.

        :param int count:
            Specifies the number of tweets to try and retrieve, up to a maximum
            of 200.

        :param str max_id:
            Returns results with an ID less than (that is, older than) or equal
            to the specified ID.

        :param bool trim_user:
            When set to ``True``, the tweet's user object includes only the
            status author's numerical ID.

        :param bool exclude_replies:
            When set to ``True``, replies will not appear in the timeline.

        :param bool contributor_details:
            This parameter enhances the contributors element of the status
            response to include the screen_name of the contributor. By default
            only the user_id of the contributor is included.

        :param bool include_rts:
            When set to ``False``, retweets will not appear in the timeline.

        :returns: A list of tweet dicts.",0,1,0,1
"def stdin(self, line, prompt=True, timeout=3):
        
        if line == EOF:
            log(""sending EOF..."")
        else:
            log(_(""sending input {}..."").format(line))

        if prompt:
            try:
                self.process.expect("".+"", timeout=timeout)
            except (TIMEOUT, EOF):
                raise Failure(_(""expected prompt for input, found none""))
            except UnicodeDecodeError:
                raise Failure(_(""output not valid ASCII text""))
        try:
            if line == EOF:
                self.process.sendeof()
            else:
                self.process.sendline(line)
        except OSError:
            pass
        return self","Send line to stdin, optionally expect a prompt.

        :param line: line to be send to stdin
        :type line: str
        :param prompt: boolean indicating whether a prompt is expected, if True absorbs \
                       all of stdout before inserting line into stdin and raises \
                       :class:`check50.Failure` if stdout is empty
        :type prompt: bool
        :param timeout: maximum number of seconds to wait for prompt
        :type timeout: int / float
        :raises check50.Failure: if ``prompt`` is set to True and no prompt is given",2,5,2,9
"def stem(self, p, metadata=None):
        
        
        
        i = 0
        j = len(p) - 1
        
        self.b = p
        self.k = j
        self.k0 = i
        if self.k <= self.k0 + 1:
            return self.b  

        
        
        
        

        self.step1ab()
        self.step1c()
        self.step2()
        self.step3()
        self.step4()
        self.step5()
        return self.b[self.k0 : self.k + 1]","In stem(p,i,j), p is a char pointer, and the string to be stemmed
        is from p[i] to p[j] inclusive. Typically i is zero and j is the
        offset to the last character of a string, (p[j+1] == '\0'). The
        stemmer adjusts the characters p[i] ... p[j] and returns the new
        end-point of the string, k. Stemming never increases word length, so
        i <= k <= j. To turn the stemmer into a module, declare 'stem' as
        extern, and delete the remainder of this file.",0,0,1,1
"def stop(self):
        
        stopped = []
        for host, _ in self.host_cores:
            if self.status(host)[0][1] == 'not-running':
                print('%s not running' % host)
                continue
            ctrl_url = 'tcp://%s:%s' % (host, self.ctrl_port)
            with z.Socket(ctrl_url, z.zmq.REQ, 'connect') as sock:
                sock.send('stop')
                stopped.append(host)
        if hasattr(self, 'streamer'):
            self.streamer.terminate()
        return 'stopped %s' % stopped","Send a ""stop"" command to all worker pools",0,1,0,1
"def stop_service(conn, service='ceph'):
    
    if is_systemd(conn):
        
        
        if is_systemd_service_active(conn, service):
            remoto.process.run(
                conn,
                [
                    'systemctl',
                    'stop',
                    '{service}'.format(service=service),
                ]
            )","Stop a service on a remote host depending on the type of init system.
    Obviously, this should be done for RHEL/Fedora/CentOS systems.

    This function does not do any kind of detection.",0,1,0,1
"def storage_record2pairwise_info(storec: StorageRecord) -> PairwiseInfo:
    

    return PairwiseInfo(
        storec.id,  
        storec.value,  
        storec.tags['~my_did'],
        storec.tags['~my_verkey'],
        {
            tag[tag.startswith('~'):]: storec.tags[tag] for tag in (storec.tags or {})  
        })","Given indy-sdk non_secrets implementation of pairwise storage record dict, return corresponding PairwiseInfo.

    :param storec: (non-secret) storage record to convert to PairwiseInfo
    :return: PairwiseInfo on record DIDs, verkeys, metadata",0,0,1,1
"def store(bank, key, data, cachedir):
    
    base = os.path.join(cachedir, os.path.normpath(bank))
    try:
        os.makedirs(base)
    except OSError as exc:
        if exc.errno != errno.EEXIST:
            raise SaltCacheError(
                'The cache directory, {0}, could not be created: {1}'.format(
                    base, exc
                )
            )

    outfile = os.path.join(base, '{0}.p'.format(key))
    tmpfh, tmpfname = tempfile.mkstemp(dir=base)
    os.close(tmpfh)
    try:
        with salt.utils.files.fopen(tmpfname, 'w+b') as fh_:
            fh_.write(__context__['serial'].dumps(data))
        
        salt.utils.atomicfile.atomic_rename(tmpfname, outfile)
    except IOError as exc:
        raise SaltCacheError(
            'There was an error writing the cache file, {0}: {1}'.format(
                base, exc
            )
        )",Store information in a file.,4,1,0,5
"def store_last_browsed_path(data):
    

    if type(data) in (tuple, list, QStringList):
        data = [foundations.strings.to_string(path) for path in data]
        last_browsed_path = foundations.common.get_first_item(data)
    elif type(data) in (unicode, QString):
        data = last_browsed_path = foundations.strings.to_string(data)
    else:
        raise TypeError(""{0} | '{1}' type is not supported!"".format(__name__, type(data)))

    if foundations.common.path_exists(last_browsed_path):
        last_browsed_path = os.path.normpath(last_browsed_path)
        if os.path.isfile(last_browsed_path):
            last_browsed_path = os.path.dirname(last_browsed_path)

        LOGGER.debug(""> Storing last browsed path: '%s'."", last_browsed_path)
        RuntimeGlobals.last_browsed_path = last_browsed_path
    return data","Defines a wrapper method used to store the last browsed path.

    :param data: Path data.
    :type data: QString or QList
    :return: Last browsed path.
    :rtype: unicode",1,1,3,5
"def str2lst(astr_input, astr_separator="" ""):
  
  alistI = astr_input.split(astr_separator)
  alistJ = []
  for i in range(0, len(alistI)):
    alistI[i] = alistI[i].strip()
    alistI[i] = alistI[i].encode('ascii')
    if len(alistI[i]):
      alistJ.append(alistI[i])
  return alistJ","Breaks a string at <astr_separator> and joins into a
  list. Steps along all list elements and strips white
  space.

  The list elements are explicitly ascii encoded.",0,0,2,2
"def str_to_bytes(value: str, expected_length: int) -> bytes:
    
    length = len(value)
    if length != expected_length:
        raise ValueError('Expects {} characters for decoding; got {}'.format(expected_length, length))

    try:
        encoded = value.encode('ascii')
    except UnicodeEncodeError as ex:
        raise ValueError('Expects value that can be encoded in ASCII charset: {}'.format(ex))

    decoding = DECODING

    
    
    for byte in encoded:
        if decoding[byte] > 31:
            raise ValueError('Non-base32 character found: ""{}""'.format(chr(byte)))

    return encoded","Convert the given string to bytes and validate it is within the Base32 character set.

    :param value: String to convert to bytes
    :type value: :class:`~str`
    :param expected_length: Expected length of the input string
    :type expected_length: :class:`~int`
    :return: Value converted to bytes.
    :rtype: :class:`~bytes`",3,0,4,7
"def stream(self, to=values.unset, from_=values.unset,
               date_sent_before=values.unset, date_sent=values.unset,
               date_sent_after=values.unset, limit=None, page_size=None):
        
        limits = self._version.read_limits(limit, page_size)

        page = self.page(
            to=to,
            from_=from_,
            date_sent_before=date_sent_before,
            date_sent=date_sent,
            date_sent_after=date_sent_after,
            page_size=limits['page_size'],
        )

        return self._version.stream(page, limits['limit'], limits['page_limit'])","Streams MessageInstance records from the API as a generator stream.
        This operation lazily loads records as efficiently as possible until the limit
        is reached.
        The results are returned as a generator, so this operation is memory efficient.

        :param unicode to: Filter by messages sent to this number
        :param unicode from_: Filter by from number
        :param datetime date_sent_before: Filter by date sent
        :param datetime date_sent: Filter by date sent
        :param datetime date_sent_after: Filter by date sent
        :param int limit: Upper limit for the number of records to return. stream()
                          guarantees to never return more than limit.  Default is no limit
        :param int page_size: Number of records to fetch per request, when not set will use
                              the default value of 50 records.  If no page_size is defined
                              but a limit is defined, stream() will attempt to read the
                              limit with the most efficient page size, i.e. min(limit, 1000)

        :returns: Generator that will yield up to limit results
        :rtype: list[twilio.rest.api.v2010.account.message.MessageInstance]",0,1,0,1
"def string(
        element_name,  
        attribute=None,  
        required=True,  
        alias=None,  
        default='',  
        omit_empty=False,  
        strip_whitespace=True,  
        hooks=None  
):
    
    
    value_parser = _string_parser(strip_whitespace)
    return _PrimitiveValue(
        element_name,
        value_parser,
        attribute,
        required,
        alias,
        default,
        omit_empty,
        hooks
    )","Create a processor for string values.

    :param strip_whitespace: Indicates whether leading and trailing whitespace should be stripped
        from parsed string values.

    See also :func:`declxml.boolean`",0,0,1,1
"def string(self, name):
        
        self._assert_is_string(name)
        frame = self._next_frame()
        try:
            val = frame.decode('utf-8')
            self.results.__dict__[name] = val
        except UnicodeError as err:
            raise MessageParserError(""Message contained invalid Unicode characters"") \
                from err
        return self",parse a string frame,1,0,2,3
"def stringReceived(self, response):
        
        correlation_id = response[0:4]
        try:
            d = self._pending.pop(correlation_id)
        except KeyError:
            self._log.warn((
                ""Response has unknown correlation ID {correlation_id!r}.""
                "" Dropping connection to {peer}.""
            ), correlation_id=correlation_id, peer=self.transport.getPeer())
            self.transport.loseConnection()
        else:
            d.callback(response)",Handle a response from the broker.,0,1,1,2
"def strip_prefix(string, prefix, regex=False):
    
    if not isinstance(string, six.string_types) or not isinstance(prefix, six.string_types):
        msg = 'Arguments to strip_prefix must be string types. Are: {s}, {p}'\
              .format(s=type(string), p=type(prefix))
        raise TypeError(msg)

    if not regex:
        prefix = re.escape(prefix)
    if not prefix.startswith('^'):
        prefix = '^({s})'.format(s=prefix)
    return _strip(string, prefix)","Strip the prefix from the string

    If 'regex' is specified, prefix is understood as a regular expression.",1,0,3,4
"def submit_and_connect(self, spec):
        
        spec = ApplicationSpec._from_any(spec)
        app_id = self.submit(spec)
        try:
            return self.connect(app_id, security=spec.master.security)
        except BaseException:
            self.kill_application(app_id)
            raise","Submit a new skein application, and wait to connect to it.

        If an error occurs before the application connects, the application is
        killed.

        Parameters
        ----------
        spec : ApplicationSpec, str, or dict
            A description of the application to run. Can be an
            ``ApplicationSpec`` object, a path to a yaml/json file, or a
            dictionary description of an application specification.

        Returns
        -------
        app_client : ApplicationClient",1,0,3,4
"def subsample_snps_map(seqchunk, nmask, maparr):
    
    
    rmask = np.zeros(seqchunk.shape[1], dtype=np.bool_)

    
    last_loc = -1
    for idx in xrange(maparr.shape[0]):
        if maparr[idx] != last_loc:
            if not nmask[idx]:
                rmask[idx] = True
            last_loc = maparr[idx]
    
    
    
    
    
    return rmask","removes ncolumns from snparray prior to matrix calculation, and 
    subsamples 'linked' snps (those from the same RAD locus) such that
    for these four samples only 1 SNP per locus is kept. This information
    comes from the 'map' array (map file).",0,0,2,2
"def subscribe_topics(self):
        
        base = self.topic
        subscribe = self.mqtt.subscribe

        
        subscribe(b""/"".join((base, b""$stats/interval/set"")))
        subscribe(b""/"".join((self.settings.MQTT_BASE_TOPIC, b""$broadcast/

        
        nodes = self.nodes
        for node in nodes:
            for topic in node.subscribe:
                topic = b""/"".join((base, topic))
                
                subscribe(topic)
                self.topic_callbacks[topic] = node.callback",subscribe to all registered device and node topics,0,2,0,2
"def substitute(sequence, offset, ref, alt):
    
    n_ref = len(ref)
    sequence_ref = sequence[offset:offset + n_ref]
    assert str(sequence_ref) == str(ref), \
        ""Reference %s at offset %d != expected reference %s"" % \
        (sequence_ref, offset, ref)
    prefix = sequence[:offset]
    suffix = sequence[offset + n_ref:]
    return prefix + alt + suffix","Mutate a sequence by substituting given `alt` at instead of `ref` at the
    given `position`.

    Parameters
    ----------
    sequence : sequence
        String of amino acids or DNA bases

    offset : int
        Base 0 offset from start of `sequence`

    ref : sequence or str
        What do we expect to find at the position?

    alt : sequence or str
        Alternate sequence to insert",0,0,2,2
"def sum_distances(self, indices, distance_matrix):
        
        combs_tup = np.array(tuple(combinations(indices, 2)))

        
        combs = np.array([[i[0] for i in combs_tup],
                          [i[1] for i in combs_tup]])

        
        dist = np.sqrt(
            np.sum(np.square(distance_matrix[combs[0], combs[1]]), axis=0))

        return dist","Calculate combinatorial distance between a select group of
        trajectories, indicated by indices

        Arguments
        ---------
        indices : tuple
        distance_matrix : numpy.ndarray (M,M)

        Returns
        -------
        numpy.ndarray

        Notes
        -----
        This function can perhaps be quickened by calculating the sum of the
        distances. The calculated distances, as they are right now,
        are only used in a relative way. Purely summing distances would lead
        to the same result, at a perhaps quicker rate.",0,0,1,1
"def sumcols(msname, col1=None, col2=None, outcol=None, cols=None, suntract=False):
    
    from pyrap.tables import table

    tab = table(msname, readonly=False)
    if cols:
        data = 0
        for col in cols:
            data += tab.getcol(col)
    else:
        if subtract:
            data = tab.getcol(col1) - tab.getcol(col2)
        else:
            data = tab.getcol(col1) + tab.getcol(col2)


    rowchunk = nrows//10 if nrows > 1000 else nrows
    for row0 in range(0, nrows, rowchunk):
        nr = min(rowchunk, nrows-row0)
        tab.putcol(outcol, data[row0:row0+nr], row0, nr)

    tab.close()","add col1 to col2, or sum columns in 'cols' list.
        If subtract, subtract col2 from col1",0,0,4,4
"def superseeded_by(self, other_service):
        
        if not other_service or \
                other_service.__class__ != self.__class__ or \
                other_service.protocol != self.protocol or \
                other_service.port != self.port:
            return False

        
        
        return not self.device_credentials and other_service.device_credentials",Return True if input service has login id and this has not.,0,0,2,2
"def synthesize_software_module_info(modules, module_types):
    
    res = {}
    for mod_id, mod_info in modules.items():
        mod_info = dict(mod_info)
        mod_type = module_types[mod_info[""type""]]

        
        mod_info[""package""] = mod_type[""package""]
        mod_info[""executable""] = mod_type[""executable""]
        if not ""categories"" in mod_info:
            mod_info[""categories""] = mod_type.get(
                ""categories"", all_categories
            )
        mod_info[""inputs""] = mod_type[""inputs""]
        mod_info[""outputs""] = mod_type[""outputs""]

        
        mod_info[""arguments""] = process_args(
            mod_id, mod_info.get(""arguments"", []), mod_type[""arguments""]
        )

        
        mod_info[""parameters""] = process_params(
            mod_id, mod_info.get(""parameters"", {}), mod_type[""parameters""]
        )
        res[mod_id] = mod_info
    return res","This function takes as input a dictionary of `modules` (mapping module IDs
    to :class:`~openag.models.SoftwareModule` objects) and a dictionary of
    `module_types` (mapping module type IDs to
    :class:`~openag.models.FirmwareModuleType` objects). For each module, it
    synthesizes the information in that module and the corresponding module
    type and returns all the results in a dictionary keyed on the ID of the
    module.",0,0,3,3
"def tab_colstack(ListOfTabArrays, mode='abort'):
    
    (data, naming) = spreadsheet.colstack(ListOfTabArrays, mode=mode, 
                                          returnnaming=True)
        
    coloring = {}
    for (i, a) in enumerate(ListOfTabArrays):
        namedict = dict([(x,y) for (j,x,y) in naming if i == j])
        for k in a.coloring:
            s = [namedict[kk] for kk in a.coloring[k]]
            if k in coloring.keys():
                coloring[k] = utils.uniqify(coloring[k] + s)
            else:
                coloring[k] = s

    for k in coloring.keys():
        s = [x for x in coloring[k] if x in data.dtype.names]
        if len(s) > 0:
            coloring[k] = s
        else:
            coloring.pop(k)

    data = data.view(tabarray)
    data.coloring = coloring
    return data","""Horizontal stacking"" of tabarrays, e.g. adding columns.

    Wrapper for :func:`tabular.spreadsheet.colstack` that deals with the 
    coloring and returns the result as a tabarray.

    Method calls::

        data = tabular.spreadsheet.colstack(ListOfTabArrays, mode=mode)",0,0,1,1
"def tag_atoms_unique_ids(self, force=False):
        
        tagged = ['unique_id' in x.tags.keys() for x in self.get_atoms()]
        if (not all(tagged)) or force:
            for m in self.get_monomers():
                for atom_type, atom in m.atoms.items():
                    atom.tags['unique_id'] = (m.unique_id, atom_type)
        return","Tags each Atom in the Assembly with its unique_id.

        Notes
        -----
        The unique_id for each atom is a tuple (a double). `unique_id[0]`
        is the unique_id for its parent `Monomer` (see `Monomer.unique_id`
        for more information). `unique_id[1]` is the atom_type in the
        `Assembly` as a string, e.g. 'CA', 'CD2'.

        Parameters
        ----------
        force : bool, optional
                If True the tag will be run even if Atoms are already tagged.
                If False, only runs if at least one Atom is not tagged.",0,0,2,2
"def tags_check_convert(cls, tags):
        
        
        if isinstance(tags, string_types):
            return [cls.__tag_check_convert(tags)]
        elif isinstance(tags, Sequence):
            if not tags:
                raise ValueError(""Tag list is empty"")
            return [cls.__tag_check_convert(tag) for tag in tags]
        else:
            raise ValueError(""tags must be a single string or list of sequence of strings"")","Accept one tag as string or multiple tags in list of strings.
        Returns list (with tags in unicode form) or raises ValueError",1,0,2,3
"def task_collection_thread_handler(self, results_queue):
        
        
        while self.tasks_to_add and not self.errors:
            max_tasks = self._max_tasks_per_request  
            chunk_tasks_to_add = []
            with self._pending_queue_lock:
                while len(chunk_tasks_to_add) < max_tasks and self.tasks_to_add:
                    chunk_tasks_to_add.append(self.tasks_to_add.pop())

            if chunk_tasks_to_add:
                self._bulk_add_tasks(results_queue, chunk_tasks_to_add)","Main method for worker to run

        Pops a chunk of tasks off the collection of pending tasks to be added and submits them to be added.

        :param collections.deque results_queue: Queue for worker to output results to",0,0,2,2
"def taxon_table(self):
        
        if self.tests:
            keys = sorted(self.tests[0].keys())
            if isinstance(self.tests, list):
                ld = [[(key, i[key]) for key in keys] for i in self.tests]
                dd = [dict(i) for i in ld]
                df = pd.DataFrame(dd)
                return df
            else:
                return pd.DataFrame(pd.Series(self.tests)).T
        else:
            return None","Returns the .tests list of taxa as a pandas dataframe. 
        By auto-generating this table from tests it means that 
        the table itself cannot be modified unless it is returned 
        and saved.",0,0,1,1
"def taxtable_to_tree(handle):
    
    c = csv.reader(handle, quoting=csv.QUOTE_NONNUMERIC)
    header = next(c)
    rootdict = dict(list(zip(header, next(c))))
    t = Tree(rootdict['tax_id'], rank=rootdict[
             'rank'], tax_name=rootdict['tax_name'])
    for l in c:
        d = dict(list(zip(header, l)))
        target = t.descendents[d['parent_id']]
        target(Tree(d['tax_id'], rank=d['rank'], tax_name=d['tax_name']))
    return t",Read a CSV taxonomy from *handle* into a Tree.,1,0,1,2
"def terminal_reserve(progress_obj, terminal_obj=None, identifier=None):
    
    if terminal_obj is None:
        terminal_obj = sys.stdout

    if identifier is None:
        identifier = ''

    if terminal_obj in TERMINAL_RESERVATION:  
        log.debug(""this terminal %s has already been added to reservation list"", terminal_obj)

        if TERMINAL_RESERVATION[terminal_obj] is progress_obj:
            log.debug(""we %s have already reserved this terminal %s"", progress_obj, terminal_obj)
            return True
        else:
            log.debug(""someone else %s has already reserved this terminal %s"", TERMINAL_RESERVATION[terminal_obj],
                      terminal_obj)
            return False
    else:  
        log.debug(""terminal %s was reserved for us %s"", terminal_obj, progress_obj)
        TERMINAL_RESERVATION[terminal_obj] = progress_obj
        return True","Registers the terminal (stdout) for printing.

    Useful to prevent multiple processes from writing progress bars
    to stdout.

    One process (server) prints to stdout and a couple of subprocesses
    do not print to the same stdout, because the server has reserved it.
    Of course, the clients have to be nice and check with
    terminal_reserve first if they should (not) print.
    Nothing is locked.

    Returns
    -------
    True if reservation was successful (or if we have already reserved this tty),
    False if there already is a reservation from another instance.",0,4,1,5
"def text_to_speech(self, text, file, voice_name=None, language=None):
        
        endpoint = 'CreateSpeech'

        data = {
            'Input': {
                'Data': text,
            },
            'OutputFormat': {
                'Codec': self.codec.upper(),
            },
            'Parameters': {
                'Rate': self.rate,
                'Volume': self.volume,
                'SentenceBreak': self.sentence_break,
                'ParagraphBreak': self.paragraph_break,
            },
            'Voice': {
                'Name': voice_name or self.voice_name,
                'Language': language or self.language,
            },
        }

        response = self._get_response('post', endpoint, data)

        file.write(response.content)","Saves given text synthesized audio file, via 'CreateSpeech' endpoint

        Docs:
            http://developer.ivona.com/en/speechcloud/actions.html#CreateSpeech

        :param text: text to synthesize
        :type text: str
        :param file: file that will be used to save the audio
        :type file: file
        :param voice_name: voice name
        :type voice_name: str
        :param language: voice language
        :type language: str",1,1,0,2
"def threadsafe_event_trigger(self, event_type):
        
        readfd, writefd = os.pipe()
        self.readers.append(readfd)

        def callback(**kwargs):
            self.queued_interrupting_events.append(event_type(**kwargs))  
            logger.warning('added event to events list %r', self.queued_interrupting_events)
            os.write(writefd, b'interrupting event!')
        return callback","Returns a callback to creates events, interrupting current event requests.

        Returned callback function will create an event of type event_type
        which will interrupt an event request if one
        is concurrently occuring, otherwise adding the event to a queue
        that will be checked on the next event request.",0,2,0,2
"def time(name=None):
    

    if name is None:
        name = 'Time Field'

    
    
    field = pp.Regex('(0[0-9]|1[0-9]|2[0-3])[0-5][0-9][0-5][0-9]')

    
    field.setParseAction(lambda t: datetime.datetime.strptime(t[0], '%H%M%S')
                         .time())

    
    field.leaveWhitespace()

    
    field.setName(name)

    return field","Creates the grammar for a Time or Duration (T) field, accepting only
    numbers in a certain pattern.

    :param name: name for the field
    :return: grammar for the date field",0,0,2,2
"def time_stats(self, start, end, type, **kwargs):
        r

        self._check_geo_param(kwargs)
        kwargs['type'] = type
        kwargs['start'] = start
        kwargs['end'] = end
        kwargs['token'] = self.token

        return self._get_response('stations/statistics', kwargs)","r"""""" Returns a dictionary of discrete time statistics (count, standard deviation, average, median, maximum,
        minimum, min time, and max time depending on user specified type) of a time series for a specified range of time
        at user specified location. Users must specify at least one geographic search parameter ('stid', 'state',
        'country', 'county', 'radius', 'bbox', 'cwa', 'nwsfirezone', 'gacc', or 'subgacc') to obtain observation data.
        Other parameters may also be included. See below mandatory and optional parameters. Also see the metadata()
        function for station IDs.

        Arguments:
        ----------
        type: string, mandatory
            Describes what statistical values will be returned. Can be one of the following values:
            ""avg""/""average""/""mean"", ""max""/""maximum"", ""min""/""minimum"", ""stdev""/""standarddeviation""/""std"", ""median""/""med"",
            ""count"", or ""all"". ""All"" will return all of the statistics.
        start: string, optional
            Start date in form of YYYYMMDDhhmm. MUST BE USED WITH THE END PARAMETER. Default time is UTC
            e.g. start=201506011800.
        end: string, optional
            End date in form of YYYYMMDDhhmm. MUST BE USED WITH THE START PARAMETER. Default time is UTC
            e.g. end=201506011800.
        obtimezone: string, optional
            Set to either UTC or local. Sets timezone of obs. Default is UTC. e.g. obtimezone='local'
        showemptystations: string, optional
            Set to '1' to show stations even if no obs exist that match the time period. Stations without obs are
            omitted by default.
        stid: string, optional
            Single or comma separated list of MesoWest station IDs. e.g. stid='kden,kslc,wbb'
        county: string, optional
            County/parish/borough (US/Canada only), full name e.g. county='Larimer'
        state: string, optional
            US state, 2-letter ID e.g. state='CO'
        country: string, optional
            Single or comma separated list of abbreviated 2 or 3 character countries e.g. country='us,ca,mx'
        radius: list, optional
            Distance from a lat/lon pt or stid as [lat,lon,radius (mi)] or [stid, radius (mi)]. e.g. radius=""-120,40,20""
        bbox: string, optional
            Stations within a [lon/lat] box in the order [lonmin,latmin,lonmax,latmax] e.g. bbox=""-120,40,-119,41""
        cwa: string, optional
            NWS county warning area. See http://www.nws.noaa.gov/organization.php for CWA list. e.g. cwa='LOX'
        nwsfirezone: string, optional
            NWS fire zones. See http://www.nws.noaa.gov/geodata/catalog/wsom/html/firezone.htm for a shapefile
            containing the full list of zones. e.g. nwsfirezone='LOX241'
        gacc: string, optional
            Name of Geographic Area Coordination Center e.g. gacc='EBCC' See http://gacc.nifc.gov/ for a list of GACCs.
        subgacc: string, optional
            Name of Sub GACC e.g. subgacc='EB07'
        vars: string, optional
            Single or comma separated list of sensor variables. Will return all stations that match one of provided
            variables. Useful for filtering all stations that sense only certain vars. Do not request vars twice in
            the query. e.g. vars='wind_speed,pressure' Use the variables function to see a list of sensor vars.
        units: string, optional
            String or set of strings and pipes separated by commas. Default is metric units. Set units='ENGLISH' for
            FREEDOM UNITS ;) Valid  other combinations are as follows: temp|C, temp|F, temp|K; speed|mps, speed|mph,
            speed|kph, speed|kts; pres|pa, pres|mb; height|m, height|ft; precip|mm, precip|cm, precip|in; alti|pa,
            alti|inhg. e.g. units='temp|F,speed|kph,metric'
        groupby: string, optional
            Results can be grouped by key words: state, county, country, cwa, nwszone, mwsfirezone, gacc, subgacc
            e.g. groupby='state'
        timeformat: string, optional
            A python format string for returning customized date-time groups for observation times. Can include
            characters. e.g. timeformat='%m/%d/%Y at %H:%M'

        Returns:
        --------
            Dictionary of discrete time statistics.

        Raises:
        -------
            None.",0,1,0,1
"def timesince(self, when):
        
        units = (
            (""year"",   60 * 60 * 24 * 365),
            (""week"",   60 * 60 * 24 * 7),
            (""day"",    60 * 60 * 24),
            (""hour"",   60 * 60),
            (""minute"", 60),
            (""second"", 1),
        )
        delta = datetime.now() - when
        total_seconds = delta.days * 60 * 60 * 24 + delta.seconds
        parts = []
        for name, seconds in units:
            value = total_seconds / seconds
            if value > 0:
                total_seconds %= seconds
                s = ""s"" if value != 1 else """"
                parts.append(""%s %s%s"" % (value, name, s))
        return "" and "".join("", "".join(parts).rsplit("", "", 1))","Returns human friendly version of the timespan between now
        and the given datetime.",0,0,1,1
"def to_annot(self, annot, category, name, s_freq=512):
        
        if 'tp_cons' == category:
            cons = consensus((self.detection, self.standard), 1, s_freq)
            events = cons.events
        
        elif 'tp_det' == category:
            events = asarray(self.detection)[self.tp.any(axis=1)]
            
        elif 'tp_std' == category:
            events = asarray(self.standard)[self.tp.any(axis=0)] 
            
        elif 'fp' == category:
            events = asarray(self.detection)[self.fp]
        
        elif 'fn' == category:
            events = asarray(self.standard)[self.fn]
        
        else:
            raise ValueError(""Invalid category."")
        
        for one_ev in events:
            annot.add_event(name,
                            (one_ev['start'], one_ev['end']),
                            chan=one_ev['chan'])","Write matched events to Wonambi XML file for visualization.
        
        Parameters
        ----------
        annot : instance of Annotations
            Annotations file
        category : str
            'tp_cons', 'tp_det', 'tp_std', 'fp' or 'fn'
        name : str
            name for the event type
        s_freq : int
            sampling frequency, in Hz, only required for 'tp_cons' category",2,0,2,4
"def to_api_repr(self):
        
        resource = copy.deepcopy(self._properties)

        
        
        query_parameters = resource[""query""].get(""queryParameters"")
        if query_parameters:
            if query_parameters[0].get(""name"") is None:
                resource[""query""][""parameterMode""] = ""POSITIONAL""
            else:
                resource[""query""][""parameterMode""] = ""NAMED""

        return resource","Build an API representation of the query job config.

        Returns:
            dict: A dictionary in the format used by the BigQuery API.",0,0,1,1
"def to_chunks(stream_or_generator):
    

    if isinstance(stream_or_generator, types.GeneratorType):
        yield from stream_or_generator
    elif hasattr(stream_or_generator, 'read'):
        while True:
            chunk = stream_or_generator.read(CHUNK_SIZE)
            if not chunk:
                break  

            yield chunk

    else:
        raise TypeError('Input must be either readable or generator.')","This generator function receives file-like or generator as input
    and returns generator.

    :param file|__generator[bytes] stream_or_generator: readable stream or
           generator.

    :rtype: __generator[bytes]

    :raise: TypeError",2,0,0,2
"def to_file(self, outpath):
        
        if not self.has_mask() and not self.is_smoothed():
            save_niigz(outpath, self.img)
        else:
            save_niigz(outpath, self.get_data(masked=True, smoothed=True),
                       self.get_header(), self.get_affine())","Save this object instance in outpath.

        Parameters
        ----------
        outpath: str
            Output file path",1,0,1,2
"def to_frame(self, data, state):
        

        
        data_len = data.find(b'\0')

        if data_len < 0:
            
            raise exc.NoFrames()

        
        frame_len = data_len + 1

        
        frame = six.binary_type(self.variant.decode(
            six.binary_type(data[:data_len])))
        del data[:frame_len]

        
        return frame","Extract a single frame from the data buffer.  The consumed
        data should be removed from the buffer.  If no complete frame
        can be read, must raise a ``NoFrames`` exception.

        :param data: A ``bytearray`` instance containing the data so
                     far read.
        :param state: An instance of ``FramerState``.  If the buffer
                      contains a partial frame, this object can be
                      used to store state information to allow the
                      remainder of the frame to be read.

        :returns: A frame.  The frame may be any object.  The stock
                  framers always return bytes.",1,0,3,4
"def to_hdf5(self, f):
        

        if isinstance(f, str):
            import h5py
            f = h5py.File(f)

        if self.frame is not None:
            frame_group = f.create_group('frame')
            frame_group.attrs['module'] = self.frame.__module__
            frame_group.attrs['class'] = self.frame.__class__.__name__

            units = [str(x).encode('utf8')
                     for x in self.frame.units.to_dict().values()]
            frame_group.create_dataset('units', data=units)

            d = frame_group.create_group('parameters')
            for k, par in self.frame.parameters.items():
                quantity_to_hdf5(d, k, par)

        cart = self.represent_as('cartesian')
        quantity_to_hdf5(f, 'pos', cart.xyz)
        quantity_to_hdf5(f, 'vel', cart.v_xyz)

        return f","Serialize this object to an HDF5 file.

        Requires ``h5py``.

        Parameters
        ----------
        f : str, :class:`h5py.File`
            Either the filename or an open HDF5 file.",1,0,2,3
"def to_header_str(self):
        
        h_parts = []
        if self.root:
            h_parts.append(ROOT + '=' + self.root)
        if self.parent:
            h_parts.append(PARENT + '=' + self.parent)
        if self.sampled is not None:
            h_parts.append(SAMPLE + '=' + str(self.sampled))
        if self.data:
            for key in self.data:
                h_parts.append(key + '=' + self.data[key])

        return HEADER_DELIMITER.join(h_parts)","Convert to a tracing header string that can be injected to
        outgoing http request headers.",0,0,1,1
"def to_indices(self, tokens):
        

        to_reduce = False
        if not isinstance(tokens, list):
            tokens = [tokens]
            to_reduce = True

        indices = [self.token_to_idx[token] if token in self.token_to_idx
                   else C.UNKNOWN_IDX for token in tokens]

        return indices[0] if to_reduce else indices","Converts tokens to indices according to the vocabulary.


        Parameters
        ----------
        tokens : str or list of strs
            A source token or tokens to be converted.


        Returns
        -------
        int or list of ints
            A token index or a list of token indices according to the vocabulary.",0,0,2,2
"def to_json(self, data):
        
        
        return (
            json.dumps(
                data,
                sort_keys=True,
                indent=4,
                separators=("","", "": ""),
                default=self.json_converter,
            )
            + ""\n""
        )","Converts the given object to a pretty-formatted JSON string

        :param data: the object to convert to JSON
        :return: A pretty-formatted JSON string",0,0,1,1
"def to_json(self, include_body=False):
        

        message = {
            'emailId': self.email_id,
            'timestamp': isoformat(self.timestamp),
            'subsystem': self.subsystem,
            'subject': self.subject,
            'sender': self.sender,
            'recipients': self.recipients,
            'uuid': self.uuid,
            'messageHtml': None,
            'messageText': None
        }

        if include_body:
            message['messageHtml'] = self.message_html
            message['messageText'] = self.message_text

        return message","Exports the object to a JSON friendly dict

        Args:
            include_body (bool): Include the body of the message in the output

        Returns:
             Dict representation of object type",0,0,1,1
"def to_json(self, include_id: bool = False):
        
        result = {
            METADATA_NAME: self.name,
            METADATA_VERSION: self.version,
        }

        if self.created:
            result['created'] = str(self.created)

        if include_id:
            result['id'] = self.id

        if self.authors:
            result[METADATA_AUTHORS] = self.authors

        if self.contact:
            result[METADATA_CONTACT] = self.contact

        if self.description:
            result[METADATA_DESCRIPTION] = self.description

        if self.copyright:
            result[METADATA_COPYRIGHT] = self.copyright

        if self.disclaimer:
            result[METADATA_DISCLAIMER] = self.disclaimer

        if self.licenses:
            result[METADATA_LICENSES] = self.licenses

        return result","Return this network as JSON.

        :param include_id: If true, includes the model identifier
        :rtype: dict[str,str]",0,0,2,2
"def to_json(val, allow_pickle=False, pretty=False):
    r
    UtoolJSONEncoder = make_utool_json_encoder(allow_pickle)
    json_kw = {}
    json_kw['cls'] = UtoolJSONEncoder
    if pretty:
        json_kw['indent'] = 4
        json_kw['separators'] = (',', ': ')
    json_str = json.dumps(val, **json_kw)
    return json_str","r""""""
    Converts a python object to a JSON string using the utool convention

    Args:
        val (object):

    Returns:
        str: json_str

    References:
        http://stackoverflow.com/questions/11561932/why-does-json-dumpslistnp

    CommandLine:
        python -m utool.util_cache --test-to_json
        python3 -m utool.util_cache --test-to_json

    Example:
        >>> # ENABLE_DOCTEST
        >>> from utool.util_cache import *  # NOQA
        >>> import utool as ut
        >>> import numpy as np
        >>> import uuid
        >>> val = [
        >>>     '{""foo"": ""not a dict""}',
        >>>     1.3,
        >>>     [1],
        >>>     # {1: 1, 2: 2, 3: 3}, cant use integer keys
        >>>     {1, 2, 3},
        >>>     slice(1, None, 1),
        >>>     b'an ascii string',
        >>>     np.array([1, 2, 3]),
        >>>     ut.get_zero_uuid(),
        >>>     ut.LazyDict(x='fo'),
        >>>     ut.LazyDict,
        >>>     {'x': {'a', 'b', 'cde'}, 'y': [1]}
        >>> ]
        >>> #val = ut.LazyDict(x='fo')
        >>> allow_pickle = True
        >>> if not allow_pickle:
        >>>     val = val[:-2]
        >>> json_str = ut.to_json(val, allow_pickle=allow_pickle)
        >>> result = ut.repr3(json_str)
        >>> reload_val = ut.from_json(json_str, allow_pickle=allow_pickle)
        >>> # Make sure pickle doesnt happen by default
        >>> try:
        >>>     json_str = ut.to_json(val)
        >>>     assert False or not allow_pickle, 'expected a type error'
        >>> except TypeError:
        >>>     print('Correctly got type error')
        >>> try:
        >>>     json_str = ut.from_json(val)
        >>>     assert False, 'expected a type error'
        >>> except TypeError:
        >>>     print('Correctly got type error')
        >>> print(result)
        >>> print('original = ' + ut.repr3(val, nl=1))
        >>> print('reconstructed = ' + ut.repr3(reload_val, nl=1))
        >>> assert reload_val[6] == val[6].tolist()
        >>> assert reload_val[6] is not val[6]",0,0,1,1
"def to_local_time(timestamp):
    
    utc = dt.datetime.strptime(timestamp, '%Y-%m-%dT%H:%M:%S.%f')
    
    from_zone = tz.gettz('UTC')
    to_zone = tz.tzlocal()

    
    utc = utc.replace(tzinfo=from_zone)

    
    return utc.astimezone(to_zone)","Convert a datatime object from UTC time to local time.

    Adopted from:
    http://stackoverflow.com/questions/4770297/python-convert-utc-datetime-string-to-local-datetime

    Parameters
    ----------
    timestamp : string
        Default string representation of timestamps expected to be in
        UTC time zone

    Returns
    -------
    datetime
        Datetime object in local time zone",0,0,1,1
"def to_netcdf(self, *args, **kwargs):
        
        from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE

        if self.name is None:
            
            dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
        elif self.name in self.coords or self.name in self.dims:
            
            
            dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
            dataset.attrs[DATAARRAY_NAME] = self.name
        else:
            
            dataset = self.to_dataset()

        return dataset.to_netcdf(*args, **kwargs)","Write DataArray contents to a netCDF file.

        Parameters
        ----------
        path : str or Path, optional
            Path to which to save this dataset. If no path is provided, this
            function returns the resulting netCDF file as a bytes object; in
            this case, we need to use scipy.io.netcdf, which does not support
            netCDF version 4 (the default format becomes NETCDF3_64BIT).
        mode : {'w', 'a'}, optional
            Write ('w') or append ('a') mode. If mode='w', any existing file at
            this location will be overwritten.
        format : {'NETCDF4', 'NETCDF4_CLASSIC', 'NETCDF3_64BIT',
                  'NETCDF3_CLASSIC'}, optional
            File format for the resulting netCDF file:

            * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
              features.
            * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
              netCDF 3 compatible API features.
            * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
              which fully supports 2+ GB files, but is only compatible with
              clients linked against netCDF version 3.6.0 or later.
            * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
              handle 2+ GB files very well.

            All formats are supported by the netCDF4-python library.
            scipy.io.netcdf only supports the last two formats.

            The default format is NETCDF4 if you are saving a file to disk and
            have the netCDF4-python library available. Otherwise, xarray falls
            back to using scipy to write netCDF files and defaults to the
            NETCDF3_64BIT format (scipy does not support netCDF4).
        group : str, optional
            Path to the netCDF4 group in the given file to open (only works for
            format='NETCDF4'). The group(s) will be created if necessary.
        engine : {'netcdf4', 'scipy', 'h5netcdf'}, optional
            Engine to use when writing netCDF files. If not provided, the
            default engine is chosen based on available dependencies, with a
            preference for 'netcdf4' if writing to a file on disk.
        encoding : dict, optional
            Nested dictionary with variable names as keys and dictionaries of
            variable specific encodings as values, e.g.,
            ``{'my_variable': {'dtype': 'int16', 'scale_factor': 0.1,
               'zlib': True}, ...}``

        Notes
        -----
        Only xarray.Dataset objects can be written to netCDF files, so
        the xarray.DataArray is converted to a xarray.Dataset object
        containing a single variable. If the DataArray has no name, or if the
        name is the same as a co-ordinate name, then it is given the name
        '__xarray_dataarray_variable__'.

        All parameters are passed directly to `xarray.Dataset.to_netcdf`.",3,0,1,4
"def to_str(self, delimiter='|', null='NULL'):
        
        self.export.set_null(null)
        self.export.set_delimiter(delimiter)
        self.options(""delimiter"", escape_string(delimiter), 2)
        self.options(""null"", null, 3)
        return self._fetchall(ENCODER_SETTINGS_STRING, coerce_floats=False)","Sets the current encoder output to Python `str` and returns
        a row iterator.

        :param str null: The string representation of null values
        :param str delimiter: The string delimiting values in the output
            string

        :rtype: iterator (yields ``str``)",0,0,1,1
"def to_uri(self, type, label, issuer, counter=None):
        
        type = type.lower()

        if type not in ('hotp', 'totp'):
            raise ValueError('type must be hotp or totp')

        if type == 'hotp' and not counter:
            raise ValueError('HOTP type authentication need counter')

        
        url = ('otpauth://%(type)s/%(label)s?secret=%(secret)s'
               '&issuer=%(issuer)s')
        dct = dict(
            type=type, label=label, issuer=issuer,
            secret=self.encoded_secret, counter=counter
        )
        ret = url % dct
        if type == 'hotp':
            ret = '%s&counter=%s' % (ret, counter)
        return ret","Generate the otpauth protocal string.

        :param type: Algorithm type, hotp or totp.
        :param label: Label of the identifier.
        :param issuer: The company, the organization or something else.
        :param counter: Counter of the HOTP algorithm.",2,0,3,5
"def to_vars_dict(self):
        
        return {
            'azure_client_id':        self.client_id,
            'azure_location':         self.location,
            'azure_secret':           self.secret,
            'azure_subscription_id':  self.subscription_id,
            'azure_tenant_id':        self.tenant_id,
        }",Return local state which is relevant for the cluster setup process.,0,0,1,1
"def to_vobjects(self, filename, uids=None):
        
        self._update()

        if not uids:
            uids = self.get_uids(filename)

        items = []

        for uid in uids:
            entry = self._book[uid.split('@')[0]]
            
            etag = sha1(str(dict(entry)).encode('utf-8'))
            items.append((uid, self._to_vcard(entry), '""%s""' % etag.hexdigest()))
        return items","Return vCards and etags of all Abook entries in uids
        filename  -- unused, for API compatibility only
        uids -- the UIDs of the Abook entries (all if None)",1,0,2,3
"def toblocks(self, chunk_size='auto', padding=None):
        
        from thunder.blocks.blocks import Blocks
        from thunder.blocks.local import LocalChunks

        if self.mode == 'spark':
            if chunk_size is 'auto':
                chunk_size = str(max([int(1e5/self.shape[0]), 1]))
            chunks = self.values.chunk(chunk_size, padding=padding).keys_to_values((0,))

        if self.mode == 'local':
            if chunk_size is 'auto':
                chunk_size = self.shape[1:]
            chunks = LocalChunks.chunk(self.values, chunk_size, padding=padding)

        return Blocks(chunks)","Convert to blocks which represent subdivisions of the images data.

        Parameters
        ----------
        chunk_size : str or tuple, size of image chunk used during conversion, default = 'auto'
            String interpreted as memory size (in kilobytes, e.g. '64').
            The exception is the string 'auto'. In spark mode, 'auto' will choose a chunk size to make the
            resulting blocks ~100 MB in size. In local mode, 'auto' will create a single block.
            Tuple of ints interpreted as 'pixels per dimension'.

        padding : tuple or int
            Amount of padding along each dimensions for blocks. If an int, then
            the same amount of padding is used for all dimensions",0,0,4,4
"def tokenize_by_number(s):
    

    r = find_number(s)

    if r == None:
        return [ s ]
    else:
        tokens = []
        if r[0] > 0:
            tokens.append(s[0:r[0]])
        tokens.append( float(s[r[0]:r[1]]) )
        if r[1] < len(s):
            tokens.extend(tokenize_by_number(s[r[1]:]))
        return tokens
    assert False","splits a string into a list of tokens
        each is either a string containing no numbers
        or a float",0,0,1,1
"def top_intent(
        results: RecognizerResult, default_intent: str = ""None"", min_score: float = 0.0
    ) -> str:
        

        if results is None:
            raise TypeError(""LuisRecognizer.top_intent(): results cannot be None."")

        top_intent: str = None
        top_score: float = -1.0
        if results.intents:
            for intent_name, intent_score in results.intents.items():
                score = intent_score.score
                if score > top_score and score >= min_score:
                    top_intent = intent_name
                    top_score = score

        return top_intent or default_intent","Returns the name of the top scoring intent from a set of LUIS results.
        
        :param results: Result set to be searched.
        :type results: RecognizerResult
        :param default_intent: Intent name to return should a top intent be found, defaults to ""None""
        :param default_intent: str, optional
        :param min_score: Minimum score needed for an intent to be considered as a top intent. If all intents in the set are below this threshold then the `defaultIntent` will be returned, defaults to 0.0
        :param min_score: float, optional
        :raises TypeError:
        :return: The top scoring intent name.
        :rtype: str",1,0,2,3
"def total_energy(self, x, v):
        
        warnings.warn(""Use the energy methods on Orbit objects instead. In a future ""
                      ""release this will be removed."", DeprecationWarning)

        v = atleast_2d(v, insert_axis=1)
        return self.energy(x) + 0.5*np.sum(v**2, axis=0)","Compute the total energy (per unit mass) of a point in phase-space
        in this potential. Assumes the last axis of the input position /
        velocity is the dimension axis, e.g., for 100 points in 3-space,
        the arrays should have shape (100,3).

        Parameters
        ----------
        x : array_like, numeric
            Position.
        v : array_like, numeric
            Velocity.",0,0,1,1
"def tpspline(self, data: ['SASdata', str] = None,
                 by: str = None,
                 freq: str = None,
                 id: str = None,
                 model: str = None,
                 output: [str, bool, 'SASdata'] = None,
                 score: [str, bool, 'SASdata'] = True,
                 procopts: str = None,
                 stmtpassthrough: str = None,
                 **kwargs: dict) -> 'SASresults':
        ","Python method to call the TPSPLINE procedure

        Documentation link:
        https://go.documentation.sas.com/?cdcId=pgmsascdc&cdcVersion=9.4_3.4&docsetId=statug&docsetTarget=statug_tpspline_syntax.htm&locale=en

        :param data: SASdata object or string. This parameter is required.
        :parm by: The by variable can only be a string type.
        :parm freq: The freq variable can only be a string type.
        :parm id: The id variable can only be a string type.
        :parm model: The model variable can only be a string type.
        :parm output: The output variable can be a string, boolean or SASdata type. The member name for a boolean is ""_output"".
        :parm score: The score variable can only be a string type.
        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.
        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.
        :return: SAS Result Object",0,1,1,2
"def track_retrack(image_list, initial_points, max_retrack_distance=0.5, keep_bad=False):
    
    (forward_track, forward_status) = track(image_list, initial_points, remove_bad=False)
    
    (backward_track, backward_status) = track(image_list[::-1], forward_track[:,-1,:], remove_bad=False)

    
    ok_track = np.flatnonzero(forward_status * backward_status) 
    forward_first = forward_track[ok_track,0,:]
    backward_last = backward_track[ok_track,-1,:]

    
    retrack_distance = np.sqrt(np.sum((forward_first - backward_last)**2, 1))

    
    retracked_ok = np.flatnonzero(retrack_distance <= max_retrack_distance)
    final_ok = ok_track[retracked_ok]

    if keep_bad: 
        status = np.zeros(forward_status.shape)
        status[final_ok] = 1
        return (forward_track, status)
    else: 
        return (forward_track[final_ok], forward_status[final_ok])","Track-retracks points in image list
    
    Using track-retrack can help in only getting point tracks of high quality.    
    
    The point is tracked forward, and then backwards in the image sequence.
    Points that end up further than max_retrack_distance from its starting point
    are marked as bad.
    
    Parameters
    ----------------
    image_list : list
            List of images to track in
    initial_points : ndarray
            Initial points to use (in first image in image_list)
    max_retrack_distance : float
            The maximum distance of the retracked point from its starting point to 
            still count as a succesful retrack.
    remove_bad : bool
            If True, then the resulting list of tracks will only contain succesfully
            tracked points. Else, it will contain all points present in initial_points.
    
    Returns
    -----------------
    tracks : (N, M, 2) ndarray
            N tracks over M images with (x,y) coordinates of points
            Note that M is the number of image in the input, and is the track in
            the forward tracking step.
    status : (N,) ndarray
            The status of each track. 1 means ok, while 0 means tracking failure",0,0,6,6
"def tradingStatusDF(symbol=None, token='', version=''):
    
    x = tradingStatus(symbol, token, version)
    data = []
    for key in x:
        d = x[key]
        d['symbol'] = key
        data.append(d)
    df = pd.DataFrame(data)
    _toDatetime(df)
    return df","The Trading status message is used to indicate the current trading status of a security.
     For IEX-listed securities, IEX acts as the primary market and has the authority to institute a trading halt or trading pause in a security due to news dissemination or regulatory reasons.
     For non-IEX-listed securities, IEX abides by any regulatory trading halts and trading pauses instituted by the primary or listing market, as applicable.

    IEX disseminates a full pre-market spin of Trading status messages indicating the trading status of all securities.
     In the spin, IEX will send out a Trading status message with “T” (Trading) for all securities that are eligible for trading at the start of the Pre-Market Session.
     If a security is absent from the dissemination, firms should assume that the security is being treated as operationally halted in the IEX Trading System.

    After the pre-market spin, IEX will use the Trading status message to relay changes in trading status for an individual security. Messages will be sent when a security is:

    Halted
    Paused*
    Released into an Order Acceptance Period*
    Released for trading
    *The paused and released into an Order Acceptance Period status will be disseminated for IEX-listed securities only. Trading pauses on non-IEX-listed securities will be treated simply as a halt.

    https://iexcloud.io/docs/api/#deep-trading-status

    Args:
        symbol (string); Ticker to request
        token (string); Access token
        version (string); API version

    Returns:
        DataFrame: result",0,0,1,1
"def train(cls, data, isotonic=True):
        
        boundaries, predictions = callMLlibFunc(""trainIsotonicRegressionModel"",
                                                data.map(_convert_to_vector), bool(isotonic))
        return IsotonicRegressionModel(boundaries.toArray(), predictions.toArray(), isotonic)","Train an isotonic regression model on the given data.

        :param data:
          RDD of (label, feature, weight) tuples.
        :param isotonic:
          Whether this is isotonic (which is default) or antitonic.
          (default: True)",0,0,1,1
"def train(config_path: str, cl_arguments: Iterable[str], output_root: str) -> None:
    
    config = None

    try:
        config_path = find_config(config_path)
        config = load_config(config_file=config_path, additional_args=cl_arguments)
        validate_config(config)
        logging.debug('\tLoaded config: %s', config)
    except Exception as ex:  
        fallback('Loading config failed', ex)

    run(config=config, output_root=output_root)","Load config and start the training.

    :param config_path: path to configuration file
    :param cl_arguments: additional command line arguments which will update the configuration
    :param output_root: output root in which the training directory will be created",3,0,1,4
"def transfer_to(self, to_account, amount, **transaction_kwargs):
        
        if not isinstance(amount, Money):
            raise TypeError(""amount must be of type Money"")

        if to_account.sign == 1 and to_account.type != self.TYPES.trading:
            
            
            
            direction = -1
        elif self.type == self.TYPES.liability and to_account.type == self.TYPES.expense:
            
            
            
            direction = -1
        else:
            direction = 1

        transaction = Transaction.objects.create(**transaction_kwargs)
        Leg.objects.create(transaction=transaction, account=self, amount=+amount * direction)
        Leg.objects.create(transaction=transaction, account=to_account, amount=-amount * direction)
        return transaction","Create a transaction which transfers amount to to_account

        This is a shortcut utility method which simplifies the process of
        transferring between accounts.

        This method attempts to perform the transaction in an intuitive manner.
        For example:

          * Transferring income -> income will result in the former decreasing and the latter increasing
          * Transferring asset (i.e. bank) -> income will result in the balance of both increasing
          * Transferring asset -> asset will result in the former decreasing and the latter increasing

        .. note::

            Transfers in any direction between ``{asset | expense} <-> {income | liability | equity}``
            will always result in both balances increasing. This may change in future if it is
            found to be unhelpful.

            Transfers to trading accounts will always behave as normal.

        Args:

            to_account (Account): The destination account.
            amount (Money): The amount to be transferred.
            transaction_kwargs: Passed through to transaction creation. Useful for setting the
                transaction `description` field.",0,0,2,2
"def transform(self, X):
        
        check_is_fitted(self, ""encoded_columns_"")
        check_columns_exist(X.columns, self.feature_names_)

        Xt = X.copy()
        for col, cat in self.categories_.items():
            Xt[col].cat.set_categories(cat, inplace=True)

        new_data = self._encode(Xt, self.feature_names_)
        return new_data.loc[:, self.encoded_columns_]","Convert categorical columns to numeric values.

        Parameters
        ----------
        X : pandas.DataFrame
            Data to encode.

        Returns
        -------
        Xt : pandas.DataFrame
            Encoded data.",0,0,1,1
"def transform(self, X, batch_size=100, show_progressbar=False):
        
        X = self._check_input(X)

        batched = self._create_batches(X, batch_size, shuffle_data=False)

        activations = []
        prev = self._init_prev(batched)

        for x in tqdm(batched, disable=not show_progressbar):
            prev = self.forward(x, prev_activation=prev)[0]
            activations.extend(prev)

        activations = np.asarray(activations, dtype=np.float64)
        activations = activations[:X.shape[0]]
        return activations.reshape(X.shape[0], self.num_neurons)","Transform input to a distance matrix by measuring the L2 distance.

        Parameters
        ----------
        X : numpy array.
            The input data.
        batch_size : int, optional, default 100
            The batch size to use in transformation. This may affect the
            transformation in stateful, i.e. sequential SOMs.
        show_progressbar : bool
            Whether to show a progressbar during transformation.

        Returns
        -------
        transformed : numpy array
            A matrix containing the distance from each datapoint to all
            neurons. The distance is normally expressed as euclidean distance,
            but can be any arbitrary metric.",0,0,2,2
"def transform(self, coords):
        
        
        origin_x, origin_y = self.origin
        sx, sy = self.scale
        return [(int(math.floor((x - origin_x) / sx)),
                 int(math.floor((y - origin_y) / sy)))
                for x, y in coords]","Transform from projection coordinates (Xp,Yp) space to pixel/line
        (P,L) raster space, based on the provided geotransformation.

        Arguments:
        coords -- input coordinates as iterable containing two-tuples/lists
        such as ((-120, 38), (-121, 39))",0,0,1,1
"def translate(ra, dec, r, theta):
    
    factor = np.sin(np.radians(dec)) * np.cos(np.radians(r))
    factor += np.cos(np.radians(dec)) * np.sin(np.radians(r)) * np.cos(np.radians(theta))
    dec_out = np.degrees(np.arcsin(factor))

    y = np.sin(np.radians(theta)) * np.sin(np.radians(r)) * np.cos(np.radians(dec))
    x = np.cos(np.radians(r)) - np.sin(np.radians(dec)) * np.sin(np.radians(dec_out))
    ra_out = ra + np.degrees(np.arctan2(y, x))
    return ra_out, dec_out","Translate a given point a distance r in the (initial) direction theta, along a  great circle.


    Parameters
    ----------
    ra, dec : float
        The initial point of interest (degrees).
    r, theta : float
        The distance and initial direction to translate (degrees).

    Returns
    -------
    ra, dec : float
        The translated position (degrees).",0,0,1,1
"def translate_key_to_config(p_key):
    
    if len(p_key) > 1:
        key = p_key.capitalize()
        if key.startswith('Ctrl') or key.startswith('Meta'):
            key = key[0] + '-' + key[5:]
        key = '<' + key + '>'
    else:
        key = p_key

    return key",Translates urwid key event to form understandable by topydo config parser.,0,0,1,1
"def transmit_learner_data(username, channel_code, channel_pk):
    
    start = time.time()
    api_user = User.objects.get(username=username)
    integrated_channel = INTEGRATED_CHANNEL_CHOICES[channel_code].objects.get(pk=channel_pk)
    LOGGER.info('Processing learners for integrated channel using configuration: [%s]', integrated_channel)

    
    
    integrated_channel.transmit_learner_data(api_user)

    duration = time.time() - start
    LOGGER.info(
        'Learner data transmission task for integrated channel configuration [%s] took [%s] seconds',
        integrated_channel,
        duration
    )","Task to send learner data to each linked integrated channel.

    Arguments:
        username (str): The username of the User to be used for making API requests for learner data.
        channel_code (str): Capitalized identifier for the integrated channel
        channel_pk (str): Primary key for identifying integrated channel",0,2,0,2
"def tree_match(cls, field, string):
        
        if not string:
            return set()

        redis = cls.get_redis()
        prefix = '{}:tree_{}'.format(cls.cls_key(), field)
        pieces = string.split(':')

        ans =  redis.sunion(
            prefix + ':' + ':'.join(pieces[0:i+1])
            for i in range(len(pieces))
        )

        return sorted(map(
            lambda id: cls.get(id),
            map(
                debyte_string,
                ans
            )
        ), key=lambda x:x.id)","Given a tree index, retrieves the ids atached to the given prefix,
        think of if as a mechanism for pattern suscription, where two models
        attached to the `a`, `a:b` respectively are found by the `a:b` string,
        because both model's subscription key matches the string.",0,1,1,2
"def triangulize(image, tile_size):
    
    if isinstance(image, basestring) or hasattr(image, 'read'):
        image = Image.open(image)
    assert isinstance(tile_size, int)

    
    
    if tile_size == 0:
        tile_size = guess_tile_size(image)
    if tile_size % 2 != 0:
        tile_size = (tile_size / 2) * 2

    logging.info('Input image size: %r', image.size)
    logging.info('Tile size: %r', tile_size)

    
    image = prep_image(image, tile_size)
    logging.info('Prepped image size: %r', image.size)

    
    pix = image.load()
    draw = ImageDraw.Draw(image)

    
    for x, y in iter_tiles(image, tile_size):
        process_tile(x, y, tile_size, pix, draw, image)
    return image","Processes the given image by breaking it down into tiles of the given
    size and applying a triangular effect to each tile. Returns the processed
    image as a PIL Image object.

    The image can be given as anything suitable for passing to `Image.open`
    (ie, the path to an image or as a file-like object containing image data).

    If tile_size is 0, the tile size will be guessed based on the image
    size. It will also be adjusted to be divisible by 2 if it is not already.",0,0,3,3
"def trim_path(path, length=30):
    
    s = path.replace('\\', '/').split('/')
    t = -1
    for i in range(len(s)-1, -1, -1):
        t = len(s[i]) + t + 1
        if t > length-4:
            break
    return '.../' + '/'.join(s[i+1:])","trim path to specified length, for example:
    >>> a = '/project/apps/default/settings.ini'
    >>> trim_path(a)
    '.../apps/default/settings.ini'
    
    The real length will be length-4, it'll left '.../' for output.",0,0,1,1
"def truncate_graph_dist(G, source_node, max_distance=1000, weight='length', retain_all=False):
    

    
    
    start_time = time.time()
    G = G.copy()
    distances = nx.shortest_path_length(G, source=source_node, weight=weight)
    distant_nodes = {key:value for key, value in dict(distances).items() if value > max_distance}
    G.remove_nodes_from(distant_nodes.keys())
    log('Truncated graph by weighted network distance in {:,.2f} seconds'.format(time.time()-start_time))

    
    
    if not retain_all:
        G = remove_isolated_nodes(G)
        G = get_largest_component(G)

    return G","Remove everything further than some network distance from a specified node
    in graph.

    Parameters
    ----------
    G : networkx multidigraph
    source_node : int
        the node in the graph from which to measure network distances to other
        nodes
    max_distance : int
        remove every node in the graph greater than this distance from the
        source_node
    weight : string
        how to weight the graph when measuring distance (default 'length' is
        how many meters long the edge is)
    retain_all : bool
        if True, return the entire graph even if it is not connected

    Returns
    -------
    networkx multidigraph",0,1,1,2
"def type_complexity(type_):
    
    if (not typing
      or not isinstance(type_, (typing.TypingMeta, GenericWrapperMeta))
      or type_ is AnyType):
        return 0
    if issubclass(type_, typing.Union):
        return reduce(operator.or_, map(type_complexity, type_.__union_params__))
    if issubclass(type_, typing.Tuple):
        if type_.__tuple_params__ is None:
            return 1
        elif type_.__tuple_use_ellipsis__:
            return 2
        else:
            return 8
    if isinstance(type_, GenericWrapperMeta):
        type_count = 0
        for p in reversed(type_.parameters):
            if type_count > 0:
                type_count += 1
            if p is AnyType:
                continue
            if not isinstance(p, typing.TypeVar) or p.__constraints__ or p.__bound__:
                type_count += 1
        return 1 << min(type_count, 2)
    return 0","Computes an indicator for the complexity of `type_`.

    If the return value is 0, the supplied type is not parameterizable.
    Otherwise, set bits in the return value denote the following features:
    - bit 0: The type could be parameterized but is not.
    - bit 1: The type represents an iterable container with 1 constrained type parameter.
    - bit 2: The type represents a mapping with a constrained value type (2 parameters).
    - bit 3: The type represents an n-tuple (n parameters).
    Since these features are mutually exclusive, only a `Union` can have more than one bit set.",0,0,3,3
"def ufo_create_background_layer_for_all_glyphs(ufo_font):
    
    

    if ""public.background"" in ufo_font.layers:
        background = ufo_font.layers[""public.background""]
    else:
        background = ufo_font.newLayer(""public.background"")

    for glyph in ufo_font:
        if glyph.name not in background:
            background.newGlyph(glyph.name)","Create a background layer for all glyphs in ufo_font if not present to
    reduce roundtrip differences.",0,0,2,2
"def undelete_derived_metric(self, id, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.undelete_derived_metric_with_http_info(id, **kwargs)  
        else:
            (data) = self.undelete_derived_metric_with_http_info(id, **kwargs)  
            return data","Undelete a specific derived metric definition  # noqa: E501

          # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.undelete_derived_metric(id, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str id: (required)
        :return: ResponseContainerDerivedMetricDefinition
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def undeployed(name,
               url='http://localhost:8080/manager',
               timeout=180):
    

    
    ret = {'name': name,
           'result': True,
           'changes': {},
           'comment': ''}

    if not __salt__['tomcat.status'](url, timeout):
        ret['comment'] = 'Tomcat Manager does not respond'
        ret['result'] = False
        return ret

    try:
        version = __salt__['tomcat.ls'](url, timeout)[name]['version']
        ret['changes'] = {'undeploy': version}
    except KeyError:
        return ret

    
    if __opts__['test']:
        ret['result'] = None
        return ret

    undeploy = __salt__['tomcat.undeploy'](name, url, timeout=timeout)
    if undeploy.startswith('FAIL'):
        ret['result'] = False
        ret['comment'] = undeploy
        return ret

    return ret","Enforce that the WAR will be undeployed from the server

    name
        The context path to undeploy.
    url : http://localhost:8080/manager
        The URL of the server with the Tomcat Manager webapp.
    timeout : 180
        Timeout for HTTP request to the Tomcat Manager.

    Example:

    .. code-block:: yaml

        jenkins:
          tomcat.undeployed:
            - name: /ran
            - require:
              - service: application-service",0,1,0,1
"def uninstall(packages, purge=False, options=None):
    
    manager = MANAGER
    command = ""purge"" if purge else ""remove""
    if options is None:
        options = []
    if not isinstance(packages, six.string_types):
        packages = "" "".join(packages)
    options.append(""--assume-yes"")
    options = "" "".join(options)
    cmd = '%(manager)s %(command)s %(options)s %(packages)s' % locals()
    run_as_root(cmd, pty=False)","Remove one or more packages.

    If *purge* is ``True``, the package configuration files will be
    removed from the system.

    Extra *options* may be passed to ``apt-get`` if necessary.",1,0,1,2
"def unpack(cls, msg, client, server, request_id):
        
        flags, = _UNPACK_INT(msg[:4])
        namespace, pos = _get_c_string(msg, 4)
        docs = bson.decode_all(msg[pos:], CODEC_OPTIONS)
        return cls(*docs, namespace=namespace, flags=flags, _client=client,
                   request_id=request_id, _server=server)","Parse message and return an `OpInsert`.

        Takes the client message as bytes, the client and server socket objects,
        and the client request id.",0,0,2,2
"def unpack(self, buff, offset=0):
        
        begin = offset
        hexas = []
        while begin < offset + 8:
            number = struct.unpack(""!B"", buff[begin:begin+1])[0]
            hexas.append(""%.2x"" % number)
            begin += 1
        self._value = ':'.join(hexas)","Unpack a binary message into this object's attributes.

        Unpack the binary value *buff* and update this object attributes based
        on the results.

        Args:
            buff (bytes): Binary data package to be unpacked.
            offset (int): Where to begin unpacking.

        Raises:
            Exception: If there is a struct unpacking error.",0,0,1,1
"def until(self, method, message=''):
        
        screen = None
        stacktrace = None

        end_time = time.time() + self._timeout
        while True:
            try:
                value = method(self._driver)
                if value:
                    return value
            except self._ignored_exceptions as exc:
                screen = getattr(exc, 'screen', None)
                stacktrace = getattr(exc, 'stacktrace', None)
            time.sleep(self._poll)
            if time.time() > end_time:
                break
        raise TimeoutException(message, screen, stacktrace)","Calls the method provided with the driver as an argument until the \
        return value does not evaluate to ``False``.

        :param method: callable(WebDriver)
        :param message: optional message for :exc:`TimeoutException`
        :returns: the result of the last call to `method`
        :raises: :exc:`selenium.common.exceptions.TimeoutException` if timeout occurs",1,0,1,2
"def update(
        self,
        data=None,
        loader_identifier=None,
        tomlfy=False,
        is_secret=False,
        **kwargs
    ):
        
        data = data or {}
        data.update(kwargs)
        for key, value in data.items():
            self.set(
                key,
                value,
                loader_identifier=loader_identifier,
                tomlfy=tomlfy,
                is_secret=is_secret,
            )","Update values in the current settings object without saving in stores::

            >>> from dynaconf import settings
            >>> print settings.NAME
            'Bruno'
            >>> settings.update({'NAME': 'John'}, other_value=1)
            >>> print settings.NAME
            'John'
            >>> print settings.OTHER_VALUE
            1

        :param data: Data to be updated
        :param loader_identifier: Only to be used by custom loaders
        :param tomlfy: Bool define if value is parsed by toml (defaults False)
        :param kwargs: extra values to update
        :return: None",0,0,1,1
"def update(self, **fields):
        

        
        self._for_write = True
        if django.VERSION >= (2, 0):
            query = self.query.chain(UpdateQuery)
        else:
            query = self.query.clone(UpdateQuery)
        query._annotations = None
        query.add_update_values(fields)

        
        connection = django.db.connections[self.db]
        compiler = PostgresReturningUpdateCompiler(query, connection, self.db)

        
        with transaction.atomic(using=self.db, savepoint=False):
            rows = compiler.execute_sql(CURSOR)
        self._result_cache = None

        
        for row in rows:
            signals.update.send(self.model, pk=row[0])

        
        
        return len(rows)",Updates all rows that match the filter.,1,1,0,2
"def update(self, **kwargs):
        

        
        update_compute = False

        old_json = self.__json__()

        compute_properties = None
        
        for prop in kwargs:
            if getattr(self, prop) != kwargs[prop]:
                if prop not in self.CONTROLLER_ONLY_PROPERTIES:
                    update_compute = True

                
                if prop == ""properties"":
                    compute_properties = kwargs[prop]
                else:
                    setattr(self, prop, kwargs[prop])

        self._list_ports()
        
        if old_json != self.__json__():
            self.project.controller.notification.emit(""node.updated"", self.__json__())
        if update_compute:
            data = self._node_data(properties=compute_properties)
            response = yield from self.put(None, data=data)
            yield from self.parse_node_response(response.json)
        self.project.dump()","Update the node on the compute server

        :param kwargs: Node properties",0,1,0,1
"def update(self, **kwargs):
        
        data = kwargs.get('data')
        if data is not None:
            if (util.pd and isinstance(data, util.pd.DataFrame) and
                list(data.columns) != list(self.data.columns) and self._index):
                data = data.reset_index()
            self.verify(data)
            kwargs['data'] = self._concat(data)
            self._count += 1
        super(Buffer, self).update(**kwargs)",Overrides update to concatenate streamed data up to defined length.,0,0,1,1
"def update(self, CorpNum, MgtKey, cashbill, UserID=None):
        
        if MgtKey == None or MgtKey == """":
            raise PopbillException(-99999999, ""관리번호가 입력되지 않았습니다."")
        if cashbill == None:
            raise PopbillException(-99999999, ""현금영수증 정보가 입력되지 않았습니다."")

        postData = self._stringtify(cashbill)

        return self._httppost('/Cashbill/' + MgtKey, postData, CorpNum, UserID, ""PATCH"")","수정
            args
                CorpNum : 팝빌회원 사업자번호
                MgtKey : 원본 현금영수증 문서관리번호
                cashbill : 수정할 현금영수증 object. made with Cashbill(...)
                UserID : 팝빌회원 아이디
            return
                처리결과. consist of code and message
            raise
                PopbillException",2,1,2,5
"def update(self, activity_sid=values.unset, attributes=values.unset,
               friendly_name=values.unset,
               reject_pending_reservations=values.unset):
        
        data = values.of({
            'ActivitySid': activity_sid,
            'Attributes': attributes,
            'FriendlyName': friendly_name,
            'RejectPendingReservations': reject_pending_reservations,
        })

        payload = self._version.update(
            'POST',
            self._uri,
            data=data,
        )

        return WorkerInstance(
            self._version,
            payload,
            workspace_sid=self._solution['workspace_sid'],
            sid=self._solution['sid'],
        )","Update the WorkerInstance

        :param unicode activity_sid: The activity_sid
        :param unicode attributes: The attributes
        :param unicode friendly_name: The friendly_name
        :param bool reject_pending_reservations: The reject_pending_reservations

        :returns: Updated WorkerInstance
        :rtype: twilio.rest.taskrouter.v1.workspace.worker.WorkerInstance",0,1,1,2
"def update(self, callback_method=values.unset, callback_url=values.unset,
               friendly_name=values.unset):
        
        data = values.of({
            'CallbackMethod': callback_method,
            'CallbackUrl': callback_url,
            'FriendlyName': friendly_name,
        })

        payload = self._version.update(
            'POST',
            self._uri,
            data=data,
        )

        return TriggerInstance(
            self._version,
            payload,
            account_sid=self._solution['account_sid'],
            sid=self._solution['sid'],
        )","Update the TriggerInstance

        :param unicode callback_method: The HTTP method to use to call callback_url
        :param unicode callback_url: The URL we call when the trigger fires
        :param unicode friendly_name: A string to describe the resource

        :returns: Updated TriggerInstance
        :rtype: twilio.rest.api.v2010.account.usage.trigger.TriggerInstance",0,0,1,1
"def update(self, client=None, unique_writer_identity=False):
        
        client = self._require_client(client)
        resource = client.sinks_api.sink_update(
            self.project,
            self.name,
            self.filter_,
            self.destination,
            unique_writer_identity=unique_writer_identity,
        )
        self._update_from_api_repr(resource)","API call:  update sink configuration via a PUT request

        See
        https://cloud.google.com/logging/docs/reference/v2/rest/v2/projects.sinks/update

        :type client: :class:`~google.cloud.logging.client.Client` or
                      ``NoneType``
        :param client: the client to use.  If not passed, falls back to the
                       ``client`` stored on the current sink.

        :type unique_writer_identity: bool
        :param unique_writer_identity: (Optional) determines the kind of
                                    IAM identity returned as
                                    writer_identity in the new sink.",0,1,0,1
"def update(self, data):
        
        if self.cipher_finalized:
            raise CipherError(""No updates allowed"")
        if not isinstance(data, bintype):
            raise TypeError(""A byte string is expected"")
        if len(data) == 0:
            return """"
        outbuf = create_string_buffer(self.block_size+len(data))
        outlen = c_int(0)
        ret = libcrypto.EVP_CipherUpdate(self.ctx, outbuf, byref(outlen),
                                         data, len(data))
        if ret <= 0:
            self._clean_ctx()
            self.cipher_finalized = True
            raise CipherError(""problem processing data"")
        return outbuf.raw[:int(outlen.value)]","Performs actual encrypton/decrypion

        @param data - part of the plain text/ciphertext to process
        @returns - part of ciphercext/plain text

        Passed chunk of text doesn't need to contain full ciher
        blocks. If neccessery, part of passed data would be kept
        internally until next data would be received or finish
        called",3,0,4,7
"def update(self, data={}, options={}):
        
        
        for key in options:
            self[key] = options[key]

        
        if isinstance(data, ConfigNode):
            data = data._get_value()
        update_dict(self._get_value(), data)","Update the configuration with new data.

        This can be passed either or both `data` and `options`.

        `options` is a dict of keypath/value pairs like this (similar to
        CherryPy's config mechanism:

            >>> c.update(options={
            ...     'server.port': 8080,
            ...     'server.host': 'localhost',
            ...     'admin.email': 'admin@lol'
            ... })

        `data` is a dict of actual config data, like this:

            >>> c.update(data={
            ...     'server': {
            ...         'port': 8080,
            ...         'host': 'localhost'
            ...     },
            ...     'admin': {
            ...         'email': 'admin@lol'
            ...     }
            ... })",0,0,1,1
"def update(self, friendly_name, enabled=values.unset, video_layout=values.unset,
               audio_sources=values.unset, audio_sources_excluded=values.unset,
               trim=values.unset, format=values.unset, resolution=values.unset,
               status_callback=values.unset, status_callback_method=values.unset):
        
        data = values.of({
            'FriendlyName': friendly_name,
            'Enabled': enabled,
            'VideoLayout': serialize.object(video_layout),
            'AudioSources': serialize.map(audio_sources, lambda e: e),
            'AudioSourcesExcluded': serialize.map(audio_sources_excluded, lambda e: e),
            'Trim': trim,
            'Format': format,
            'Resolution': resolution,
            'StatusCallback': status_callback,
            'StatusCallbackMethod': status_callback_method,
        })

        payload = self._version.update(
            'POST',
            self._uri,
            data=data,
        )

        return CompositionHookInstance(self._version, payload, sid=self._solution['sid'], )","Update the CompositionHookInstance

        :param unicode friendly_name: Friendly name of the Composition Hook to be shown in the console.
        :param bool enabled: Boolean flag indicating if the Composition Hook is active.
        :param dict video_layout: The JSON video layout description.
        :param unicode audio_sources: A list of audio sources related to this Composition Hook.
        :param unicode audio_sources_excluded: A list of audio sources excluded related to this Composition Hook.
        :param bool trim: Boolean flag for clipping intervals that have no media.
        :param CompositionHookInstance.Format format: Container format of the Composition Hook media file. Any of the following: `mp4`, `webm`.
        :param unicode resolution: Pixel resolution of the composed video.
        :param unicode status_callback: A URL that Twilio sends asynchronous webhook requests to on every composition event.
        :param unicode status_callback_method: HTTP method Twilio should use when requesting the above URL.

        :returns: Updated CompositionHookInstance
        :rtype: twilio.rest.video.v1.composition_hook.CompositionHookInstance",0,1,1,2
"def update(self, friendly_name=values.unset, certificate=values.unset,
               private_key=values.unset, sandbox=values.unset, api_key=values.unset,
               secret=values.unset):
        
        data = values.of({
            'FriendlyName': friendly_name,
            'Certificate': certificate,
            'PrivateKey': private_key,
            'Sandbox': sandbox,
            'ApiKey': api_key,
            'Secret': secret,
        })

        payload = self._version.update(
            'POST',
            self._uri,
            data=data,
        )

        return CredentialInstance(self._version, payload, sid=self._solution['sid'], )","Update the CredentialInstance

        :param unicode friendly_name: A string to describe the resource
        :param unicode certificate: [APN only] The URL-encoded representation of the certificate
        :param unicode private_key: [APN only] URL-encoded representation of the private key
        :param bool sandbox: [APN only] Whether to send the credential to sandbox APNs
        :param unicode api_key: [GCM only] The `Server key` of your project from Firebase console under Settings / Cloud messaging
        :param unicode secret: [FCM only] The `Server key` of your project from Firebase console under Settings / Cloud messaging

        :returns: Updated CredentialInstance
        :rtype: twilio.rest.notify.v1.credential.CredentialInstance",0,1,0,1
"def update(self, ipv4s):
        

        data = {'ips': ipv4s}
        ipv4s_ids = [str(ipv4.get('id')) for ipv4 in ipv4s]

        return super(ApiIPv4, self).put('api/v3/ipv4/%s/' %
                                        ';'.join(ipv4s_ids), data)","Method to update ipv4's

        :param ipv4s: List containing ipv4's desired to updated
        :return: None",0,1,0,1
"def update(self, old, new=None):
        
        
        if not new:
            new = type(old)()
            if not new.is_initialized:
                 new.initialize()
        if self.debug:
            print(""Updating {} with {}"".format(old, new))

        
        self.update_attrs(old, new)
        self.update_funcs(old, new)
        self.update_bindings(old, new)

        
        self.update_pattern_nodes(old, new)

        
        self.update_children(old, new)","Update given view declaration with new declaration

        Parameters
        -----------
        old: Declarative
            The existing view instance that needs to be updated
        new: Declarative or None
            The new or reloaded view instance to that will be used to update the
            existing view. If none is given, one of the same type as old will be
            created and initialized with no attributes passed.",1,0,1,2
"def update(self, resource, uri=None, force=False, timeout=-1, custom_headers=None, default_values={}):
        
        if not resource:
            logger.exception(RESOURCE_CLIENT_RESOURCE_WAS_NOT_PROVIDED)
            raise ValueError(RESOURCE_CLIENT_RESOURCE_WAS_NOT_PROVIDED)

        logger.debug('Update async (uri = %s, resource = %s)' %
                     (self._uri, str(resource)))

        if not uri:
            uri = resource['uri']

        if force:
            uri += '?force=True'

        resource = self.merge_default_values(resource, default_values)

        return self.__do_put(uri, resource, timeout, custom_headers)","Makes a PUT request to update a resource when a request body is required.

        Args:
            resource:
                OneView resource dictionary.
            uri:
                Can be either the resource ID or the resource URI.
            force:
                If set to true, the operation completes despite any problems with network connectivity or errors
                on the resource itself. The default is false.
            timeout:
                Timeout in seconds. Wait for task completion by default. The timeout does not abort the operation
                in OneView; it just stops waiting for its completion.
            custom_headers:
                Allows set specific HTTP headers.
            default_values:
                Dictionary with default values grouped by OneView API version. This dictionary will be be merged with
                the resource dictionary only if the dictionary does not contain the keys.
                This argument is optional and the default value is an empty dictionary.
                Ex.:
                    default_values = {
                        '200': {""type"": ""logical-switch-group""},
                        '300': {""type"": ""logical-switch-groupV300""}
                    }

        Returns:
            Updated resource.",1,3,1,5
"def update(self, y=None, inplace=False, **kwargs):
        
        kwargs.update({'k': kwargs.pop('k', self.k)})
        kwargs.update({'pct': kwargs.pop('pct', self.pct)})
        kwargs.update({'truncate': kwargs.pop('truncate', self._truncated)})
        if inplace:
            self._update(y, **kwargs)
        else:
            new = copy.deepcopy(self)
            new._update(y, **kwargs)
            return new","Add data or change classification parameters.

        Parameters
        ----------
        y           :   array
                        (n,1) array of data to classify
        inplace     :   bool
                        whether to conduct the update in place or to return a
                        copy estimated from the additional specifications.

        Additional parameters provided in **kwargs are passed to the init
        function of the class. For documentation, check the class constructor.",0,0,3,3
"def update_account(self, account_id, body, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('asynchronous'):
            return self.update_account_with_http_info(account_id, body, **kwargs)  
        else:
            (data) = self.update_account_with_http_info(account_id, body, **kwargs)  
            return data","Update attributes of an existing account.  # noqa: E501

        An endpoint for updating an account.   **Example usage:** `curl -X PUT https://api.us-east-1.mbedcloud.com/v3/accounts/{account-id} -d '{\""phone_number\"": \""12345678\""}' -H 'content-type: application/json' -H 'Authorization: Bearer API_KEY'`  # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass asynchronous=True
        >>> thread = api.update_account(account_id, body, asynchronous=True)
        >>> result = thread.get()

        :param asynchronous bool
        :param str account_id: The ID of the account to be updated. (required)
        :param AccountUpdateRootReq body: Details of the account to be updated. (required)
        :return: AccountInfo
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def update_and_reencrypt(self, **kwargs):
        
        encrypted_field_name = self.store.model_class.__plaintext__

        id_ = kwargs[self.identifier_key]
        current_model = self.store.retrieve(id_)
        current_value = current_model.plaintext

        null_update = (
            
            encrypted_field_name in kwargs
            and kwargs.get(encrypted_field_name) is None
        )
        new_value = kwargs.pop(self.store.model_class.__plaintext__, None)
        use_new_value = new_value is not None or null_update

        updated_value = new_value if use_new_value else current_value

        model_kwargs = {
            self.identifier_key: id_,
            encrypted_field_name: updated_value,
            **kwargs,
        }

        return super().update(
            **model_kwargs,
        )","Support re-encryption by enforcing that every update triggers a
        new encryption call, even if the the original call does not update
        the encrypted field.",0,0,3,3
"def update_api_key_description(apiKey, description, region=None, key=None, keyid=None, profile=None):
    
    try:
        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)
        response = _api_key_patch_replace(conn, apiKey, '/description', description)
        return {'updated': True, 'apiKey': _convert_datetime_str(response)}
    except ClientError as e:
        return {'updated': False, 'error': __utils__['boto3.get_error'](e)}","update the given apiKey with the given description.

    CLI Example:

    .. code-block:: bash

        salt myminion boto_apigateway.update_api_key_description api_key description",0,1,1,2
"def update_apps(self, apps, force=False, minimal=True):
        
        json_repr_apps = []
        for app in apps:
            
            app.version = None
            json_repr_apps.append(app.json_repr(minimal=minimal))

        params = {'force': force}
        encoder = MarathonMinimalJsonEncoder if minimal else MarathonJsonEncoder
        data = json.dumps(json_repr_apps, cls=encoder, sort_keys=True)

        response = self._do_request(
            'PUT', '/v2/apps', params=params, data=data)
        return response.json()","Update multiple apps.

        Applies writable settings in elements of apps either by upgrading existing ones or creating new ones

        :param apps: sequence of application settings
        :param bool force: apply even if a deployment is in progress
        :param bool minimal: ignore nulls and empty collections

        :returns: a dict containing the deployment id and version
        :rtype: dict",0,1,0,1
"def update_campaign_destroy(self, campaign_id, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('asynchronous'):
            return self.update_campaign_destroy_with_http_info(campaign_id, **kwargs)  
        else:
            (data) = self.update_campaign_destroy_with_http_info(campaign_id, **kwargs)  
            return data","Delete a campaign  # noqa: E501

        Delete an update campaign.  # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass asynchronous=True
        >>> thread = api.update_campaign_destroy(campaign_id, asynchronous=True)
        >>> result = thread.get()

        :param asynchronous bool
        :param str campaign_id: The ID of the update campaign (required)
        :return: None
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def update_campaign_metrics(self, campaign_id, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('asynchronous'):
            return self.update_campaign_metrics_with_http_info(campaign_id, **kwargs)  
        else:
            (data) = self.update_campaign_metrics_with_http_info(campaign_id, **kwargs)  
            return data","Get campaign metrics  # noqa: E501

        Get detailed statistics of a campaign.  # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass asynchronous=True
        >>> thread = api.update_campaign_metrics(campaign_id, asynchronous=True)
        >>> result = thread.get()

        :param asynchronous bool
        :param str campaign_id: The campaign ID (required)
        :return: CampaignMetrics
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def update_category_by_id(cls, category_id, category, **kwargs):
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async'):
            return cls._update_category_by_id_with_http_info(category_id, category, **kwargs)
        else:
            (data) = cls._update_category_by_id_with_http_info(category_id, category, **kwargs)
            return data","Update Category

        Update attributes of Category
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async=True
        >>> thread = api.update_category_by_id(category_id, category, async=True)
        >>> result = thread.get()

        :param async bool
        :param str category_id: ID of category to update. (required)
        :param Category category: Attributes of category to update. (required)
        :return: Category
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def update_clusterer(self, inst):
        
        if self.is_updateable:
            javabridge.call(self.jobject, ""updateClusterer"", ""(Lweka/core/Instance;)V"", inst.jobject)
        else:
            logger.critical(classes.get_classname(self.jobject) + "" is not updateable!"")","Updates the clusterer with the instance.

        :param inst: the Instance to update the clusterer with
        :type inst: Instance",0,2,1,3
"def update_docstring(dispatcher, func=None):
    
    doc = dispatcher.__doc__ or ''
    if inspect.cleandoc(doc).startswith('%s(' % dispatcher.__name__):
        return
    sig = '(...)'
    if func and func.__code__.co_argcount:
        argspec = inspect.getfullargspec(func) 
        if argspec.args and argspec.args[0] in {'self', 'cls'}:
            argspec.args.pop(0)
        if any(argspec):
            sig = inspect.formatargspec(*argspec) 
            sig = re.sub(r' at 0x[0-9a-f]{8,16}(?=>)', '', sig)
    sep = '\n' if doc.startswith('\n') else '\n\n'
    dispatcher.__doc__ = dispatcher.__name__ + sig + sep + doc","Inserts a call signature at the beginning of the docstring on `dispatcher`.
    The signature is taken from `func` if provided; otherwise `(...)` is used.",0,0,2,2
"def update_file(self, project_path, file_path, variables):
        
        update_file = os.path.join(project_path, file_path)
        with open(update_file, 'rb') as _file:
            file_content = _file.read().decode('utf-8')

        for key, value in variables.items():
            file_content = file_content.replace('[[{}]]'.format(key), value)

        with open(update_file, 'w+', encoding='utf-8') as _file:
            _file.writelines(file_content)","Update given file with given variables, variables in file must be inclosed with [[]].

        For example you want to replace a variable secret_key, in the file its [[secret_key]].

        :param str project_path:
        :param str file_path:
        :param dict variables:",2,0,0,2
"def update_firmware(self, file_url, component_type):
        
        try:
            update_service_inst = self._sushy.get_update_service()
            update_service_inst.flash_firmware(self, file_url)
        except sushy.exceptions.SushyError as e:
            msg = (self._('The Redfish controller failed to update firmware '
                          'with firmware %(file)s Error %(error)s') %
                   {'file': file_url, 'error': str(e)})
            LOG.debug(msg)
            raise exception.IloError(msg)","Updates the given firmware on the server for the given component.

        :param file_url: location of the raw firmware file. Extraction of the
                         firmware file (if in compact format) is expected to
                         happen prior to this invocation.
        :param component_type: Type of component to be applied to.
        :raises: IloError, on an error from iLO.",1,2,1,4
"def update_group_properties(self, group_id, group_properties, headers=None, endpoint_url=None):
        
        endpoint_url = endpoint_url or self._endpoint_url

        url = endpoint_url + '/groups/' + group_id + '/properties'
        headers = headers or self._default_headers()
        payload = group_properties

        response = requests.put(url, headers=headers, json=payload)

        return response","Update a group's properties with values provided in ""group_properties"" dictionary

        :param str group_id: group ID
        :param dict group_properties: group properties to update with a new values
        :param dict headers: custom request headers (if isn't set default values are used)
        :param str endpoint_url: where to send the request (if isn't set default value is used)
        :return: Response",0,1,0,1
"def update_in_ioloop(update):
    
    @wraps(update)
    def wrapped_update(self, sensor, reading):
        if get_thread_ident() == self._ioloop_thread_id:
            update(self, sensor, reading)
        else:
            self.ioloop.add_callback(update, self, sensor, reading)

    return wrapped_update","Decorator that ensures an update() method is run in the tornado ioloop.

    Does this by checking the thread identity. Requires that the object to
    which the method is bound has the attributes :attr:`_ioloop_thread_id`
    (the result of thread.get_ident() in the ioloop thread) and :attr:`ioloop`
    (the ioloop instance in use). Also assumes the signature
    `update(self, sensor, reading)` for the method.",0,0,1,1
"def update_labels(self, func):
        
        if not isinstance(self.data, LabelArray):
            raise TypeError(
                'update_labels only supported if data is of type LabelArray.'
            )

        
        self._data = self._data.map(func)

        
        for _, row_adjustments in iteritems(self.adjustments):
            for adjustment in row_adjustments:
                adjustment.value = func(adjustment.value)","Map a function over baseline and adjustment values in place.

        Note that the baseline data values must be a LabelArray.",1,0,3,4
"def update_mapping(mapping: Dict[ops.Qid, LogicalIndex],
                   operations: ops.OP_TREE
                   ) -> None:
    
    for op in ops.flatten_op_tree(operations):
        if (isinstance(op, ops.GateOperation) and
            isinstance(op.gate, PermutationGate)):
            op.gate.update_mapping(mapping, op.qubits)","Updates a mapping (in place) from qubits to logical indices according to
    a set of permutation gates. Any gates other than permutation gates are
    ignored.

    Args:
        mapping: The mapping to update.
        operations: The operations to update according to.",0,0,2,2
"def update_media_file(upload_file):
    
    temp_directory = tempfile.mkdtemp()
    temp_file = tempfile.TemporaryFile()
    
    temp_media_file = os.path.join(temp_directory, 'media')
    try:
        for chunk in upload_file.chunks():
            temp_file.write(chunk)

        with zipfile.ZipFile(temp_file, 'r') as z:
            z.extractall(temp_directory)

        if os.path.exists(temp_media_file):
            return distutils.dir_util.copy_tree(
                temp_media_file,
                settings.MEDIA_ROOT)
        else:
            raise Exception(""Error: There is no directory called ""
                            ""'media' in the root of the zipped file"")
    except Exception as e:
        raise e
    finally:
        temp_file.close()
        if os.path.exists(temp_directory):
            shutil.rmtree(temp_directory)","Update the Current Media Folder.

    Returns list of files copied across or
    raises an exception.",3,0,0,3
"def update_message(self, message_id, category_id, title, body,
        extended_body, use_textile=False, private=False, notify=None):
        
        path = '/msg/update/%u' % message_id
        req = ET.Element('request')
        req.append(self._create_message_post_elem(category_id, title, body,
            extended_body, use_textile=False, private=False))
        if notify is not None:
            for person_id in notify:
                ET.SubElement(req, 'notify').text = str(int(person_id))
        return self._request(path, req)","Updates an existing message, optionally sending notifications to a
        selected list of people. Note that you can also upload files using
        this function, but you have to format the request as
        multipart/form-data. (See the ruby Basecamp API wrapper for an example
        of how to do this.)",0,1,1,2
"def update_old_names():
    

    url = urlparse(ZONEINFO_URL)
    log.info('Connecting to %s' % url.netloc)
    ftp = ftplib.FTP(url.netloc)
    ftp.login()
    gzfile = BytesIO()

    log.info('Fetching zoneinfo database')
    ftp.retrbinary('RETR ' + url.path, gzfile.write)
    gzfile.seek(0)

    log.info('Extracting backwards data')
    archive = tarfile.open(mode=""r:gz"", fileobj=gzfile)
    backward = {}
    for line in archive.extractfile('backward').readlines():
        if line[0] == '
            continue
        if len(line.strip()) == 0:
            continue
        parts = line.split()
        if parts[0] != b'Link':
            continue

        backward[parts[2].decode('ascii')] = parts[1].decode('ascii')

    return backward",Fetches the list of old tz names and returns a mapping,2,2,1,5
"def update_query(url, params, remove=None):
    
    if remove is None:
        remove = []

    
    parts = urllib.parse.urlparse(url)
    
    query_params = urllib.parse.parse_qs(parts.query)
    
    query_params.update(params)
    
    query_params = {
        key: value for key, value
        in six.iteritems(query_params)
        if key not in remove}
    
    new_query = urllib.parse.urlencode(query_params, doseq=True)
    
    new_parts = parts._replace(query=new_query)
    return urllib.parse.urlunparse(new_parts)","Updates a URL's query parameters.

    Replaces any current values if they are already present in the URL.

    Args:
        url (str): The URL to update.
        params (Mapping[str, str]): A mapping of query parameter
            keys to values.
        remove (Sequence[str]): Parameters to remove from the query string.

    Returns:
        str: The URL with updated query parameters.

    Examples:

        >>> url = 'http://example.com?a=1'
        >>> update_query(url, {'a': '2'})
        http://example.com?a=2
        >>> update_query(url, {'b': '3'})
        http://example.com?a=1&b=3
        >> update_query(url, {'b': '3'}, remove=['a'])
        http://example.com?b=3",0,0,2,2
"def update_quota(tenant_id,
                 subnet=None,
                 router=None,
                 network=None,
                 floatingip=None,
                 port=None,
                 security_group=None,
                 security_group_rule=None,
                 profile=None):
    
    conn = _auth(profile)
    return conn.update_quota(tenant_id, subnet, router, network,
                             floatingip, port, security_group,
                             security_group_rule)","Update a tenant's quota

    CLI Example:

    .. code-block:: bash

        salt '*' neutron.update_quota tenant-id subnet=40 router=50
                                    network=10 floatingip=30 port=30

    :param tenant_id: ID of tenant
    :param subnet: Value of subnet quota (Optional)
    :param router: Value of router quota (Optional)
    :param network: Value of network quota (Optional)
    :param floatingip: Value of floatingip quota (Optional)
    :param port: Value of port quota (Optional)
    :param security_group: Value of security group (Optional)
    :param security_group_rule: Value of security group rule (Optional)
    :param profile: Profile to build on (Optional)
    :return: Value of updated quota",0,1,0,1
"def update_role(u_name, newprivilege):
        
        entry = TabMember.update(
            role=newprivilege
        ).where(TabMember.user_name == u_name)
        try:
            entry.execute()
            return True
        except:
            return False",Update the role of the usr.,1,0,0,1
"def update_serial(self, new_serial):
        
        new_serial = str(new_serial)
        if self.has_active_service:
            raise DeviceError(
                self,
                'Cannot change device serial number when there is service running.'
            )
        if self._debug_tag == self.serial:
            self._debug_tag = new_serial
        self._serial = new_serial
        self.adb.serial = new_serial
        self.fastboot.serial = new_serial","Updates the serial number of a device.

        The ""serial number"" used with adb's `-s` arg is not necessarily the
        actual serial number. For remote devices, it could be a combination of
        host names and port numbers.

        This is used for when such identifier of remote devices changes during
        a test. For example, when a remote device reboots, it may come back
        with a different serial number.

        This is NOT meant for switching the object to represent another device.

        We intentionally did not make it a regular setter of the serial
        property so people don't accidentally call this without understanding
        the consequences.

        Args:
            new_serial: string, the new serial number for the same device.

        Raises:
            DeviceError: tries to update serial when any service is running.",1,0,1,2
"def update_source(self, **kwargs):
        
        callback = kwargs.pop('callback', self._callback)
        ip_addr = ip_interface(unicode(kwargs.pop('neighbor')))
        config = self._update_source_xml(neighbor=ip_addr,
                                         int_type=kwargs.pop('int_type'),
                                         int_name=kwargs.pop('int_name'),
                                         rbridge_id=kwargs.pop('rbridge_id',
                                                               '1'),
                                         vrf=kwargs.pop('vrf', 'default'))
        if kwargs.pop('get', False):
            return callback(config, handler='get_config')
        if kwargs.pop('delete', False):
            config.find('.//*update-source').set('operation', 'delete')
        return callback(config)","Set BGP update source property for a neighbor.

        This method currently only supports loopback interfaces.

        Args:
            vrf (str): The VRF for this BGP process.
            rbridge_id (str): The rbridge ID of the device on which BGP will be
                configured in a VCS fabric.
            neighbor (str): Address family to configure. (ipv4, ipv6)
            int_type (str): Interface type (loopback)
            int_name (str): Interface identifier (1, 5, 7, etc)
            get (bool): Get config instead of editing config. (True, False)
            callback (function): A function executed upon completion of the
                method.  The only parameter passed to `callback` will be the
                ``ElementTree`` `config`.

        Returns:
            Return value of `callback`.

        Raises:
            ``AttributeError``: When `neighbor` is not a valid IPv4 or IPv6
                address.
            ``KeyError``: When `int_type` or `int_name` are not specified.

        Examples:
            >>> import pynos.device
            >>> switches = ['10.24.39.211', '10.24.39.230']
            >>> for switch in switches:
            ...     conn = (switch, '22')
            ...     auth = ('admin', 'password')
            ...     with pynos.device.Device(conn=conn, auth=auth) as dev:
            ...         dev.interface.ip_address(int_type='loopback', name='6',
            ...         rbridge_id='225', ip_addr='6.6.6.6/32')
            ...         dev.interface.ip_address(int_type='loopback', name='6',
            ...         ip_addr='0:0:0:0:0:ffff:606:606/128', rbridge_id='225')
            ...         dev.bgp.local_asn(local_as='65535', rbridge_id='225')
            ...         dev.bgp.neighbor(ip_addr='10.10.10.10',
            ...         remote_as='65535', rbridge_id='225')
            ...         dev.bgp.neighbor(remote_as='65535', rbridge_id='225',
            ...         ip_addr='2001:4818:f000:1ab:cafe:beef:1000:1')
            ...         dev.bgp.update_source(neighbor='10.10.10.10',
            ...         rbridge_id='225', int_type='loopback', int_name='6')
            ...         dev.bgp.update_source(get=True, neighbor='10.10.10.10',
            ...         rbridge_id='225', int_type='loopback', int_name='6')
            ...         dev.bgp.update_source(rbridge_id='225', int_name='6',
            ...         neighbor='2001:4818:f000:1ab:cafe:beef:1000:1',
            ...         int_type='loopback')
            ...         dev.bgp.update_source(get=True, rbridge_id='225',
            ...         neighbor='2001:4818:f000:1ab:cafe:beef:1000:1',
            ...         int_type='loopback', int_name='6')
            ...         dev.bgp.update_source(neighbor='10.10.10.10',
            ...         rbridge_id='225', delete=True, int_type='loopback',
            ...         int_name='6')
            ...         dev.bgp.update_source(delete=True, int_type='loopback',
            ...         rbridge_id='225', int_name='6',
            ...         neighbor='2001:4818:f000:1ab:cafe:beef:1000:1')
            ...         dev.bgp.neighbor(ip_addr='10.10.10.10', delete=True,
            ...         rbridge_id='225')
            ...         dev.bgp.neighbor(delete=True, rbridge_id='225',
            ...         ip_addr='2001:4818:f000:1ab:cafe:beef:1000:1')
            ...         dev.interface.ip_address(int_type='loopback', name='6',
            ...         rbridge_id='225', ip_addr='6.6.6.6/32', delete=True)
            ...         dev.interface.ip_address(int_type='loopback', name='6',
            ...         ip_addr='0:0:0:0:0:ffff:606:606/128', rbridge_id='225',
            ...         delete=True)
            ...         output = dev.bgp.update_source(rbridge_id='225',
            ...         int_type='loopback')
            ...         # doctest: +IGNORE_EXCEPTION_DETAIL
            Traceback (most recent call last):
            NotImplementedError
            KeyError",0,1,2,3
"def update_swarm(self, version, swarm_spec=None, rotate_worker_token=False,
                     rotate_manager_token=False):
        

        url = self._url('/swarm/update')
        response = self._post_json(url, data=swarm_spec, params={
            'rotateWorkerToken': rotate_worker_token,
            'rotateManagerToken': rotate_manager_token,
            'version': version
        })
        self._raise_for_status(response)
        return True","Update the Swarm's configuration

        Args:
            version (int): The version number of the swarm object being
                updated. This is required to avoid conflicting writes.
            swarm_spec (dict): Configuration settings to update. Use
                :py:meth:`~docker.api.swarm.SwarmApiMixin.create_swarm_spec` to
                generate a valid configuration. Default: ``None``.
            rotate_worker_token (bool): Rotate the worker join token. Default:
                ``False``.
            rotate_manager_token (bool): Rotate the manager join token.
                Default: ``False``.

        Returns:
            ``True`` if the request went through.

        Raises:
            :py:class:`docker.errors.APIError`
                If the server returns an error.",1,1,0,2
"def update_user(self, id, **kwargs):  
        
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.update_user_with_http_info(id, **kwargs)  
        else:
            (data) = self.update_user_with_http_info(id, **kwargs)  
            return data","Update user with given user groups and permissions.  # noqa: E501

          # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.update_user(id, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str id: (required)
        :param UserRequestDTO body: Example Body:  <pre>{   \""identifier\"": \""user@example.com\"",   \""groups\"": [     \""user_management\""   ],   \""userGroups\"": [     \""8b23136b-ecd2-4cb5-8c92-62477dcc4090\""   ] }</pre>
        :return: UserModel
                 If the method is called asynchronously,
                 returns the request thread.",0,2,2,4
"def update_version(self, version: Union[http.HttpVersion, str]) -> None:
        
        if isinstance(version, str):
            v = [l.strip() for l in version.split('.', 1)]
            try:
                version = http.HttpVersion(int(v[0]), int(v[1]))
            except ValueError:
                raise ValueError(
                    'Can not parse http version number: {}'
                    .format(version)) from None
        self.version = version","Convert request version to two elements tuple.

        parser HTTP version '1.1' => (1, 1)",1,0,2,3
"def update_view(self, view_name, map_func, reduce_func=None, **kwargs):
        
        view = self.get_view(view_name)
        if view is None:
            raise CloudantArgumentError(111, view_name)
        if isinstance(view, QueryIndexView):
            raise CloudantDesignDocumentException(102)

        view = View(self, view_name, map_func, reduce_func, **kwargs)
        self.views.__setitem__(view_name, view)","Modifies/overwrites an existing MapReduce view definition in the
        locally cached DesignDocument View dictionary.  To update a JSON
        query index use
        :func:`~cloudant.database.CloudantDatabase.delete_query_index` followed
        by :func:`~cloudant.database.CloudantDatabase.create_query_index`
        instead.  A CloudantException is raised if an attempt to update a
        QueryIndexView (JSON query index) using this method is made.

        :param str view_name: Name used to identify the View.
        :param str map_func: Javascript map function.
        :param str reduce_func: Optional Javascript reduce function.",3,0,2,5
"def update_workspace_config(namespace, workspace, cnamespace, configname, body):
    
    uri = ""workspaces/{0}/{1}/method_configs/{2}/{3}"".format(namespace,
                                        workspace, cnamespace, configname)
    return __post(uri, json=body)","Update method configuration in workspace.

    Args:
        namespace  (str): project to which workspace belongs
        workspace  (str): Workspace name
        cnamespace (str): Configuration namespace
        configname (str): Configuration name
        body      (json): new body (definition) of the method config

    Swagger:
        https://api.firecloud.org/#!/Method_Configurations/updateWorkspaceMethodConfig",0,1,0,1
"def update_x(self, x, indices=None):
        
        x = _make_np_bool(x)
        if indices is None:
            if len(self._x) != len(x):
                raise QiskitError(""During updating whole x, you can not change ""
                                  ""the number of qubits."")
            self._x = x
        else:
            if not isinstance(indices, list) and not isinstance(indices, np.ndarray):
                indices = [indices]
            for p, idx in enumerate(indices):
                self._x[idx] = x[p]

        return self","Update partial or entire x.

        Args:
            x (numpy.ndarray or list): to-be-updated x
            indices (numpy.ndarray or list or optional): to-be-updated qubit indices

        Returns:
            Pauli: self

        Raises:
            QiskitError: when updating whole x, the number of qubits must be the same.",1,0,2,3
"def upgrade_plugin(self, name, remote, privileges):
        

        url = self._url('/plugins/{0}/upgrade', name)
        params = {
            'remote': remote,
        }

        headers = {}
        registry, repo_name = auth.resolve_repository_name(remote)
        header = auth.get_config_header(self, registry)
        if header:
            headers['X-Registry-Auth'] = header
        response = self._post_json(
            url, params=params, headers=headers, data=privileges,
            stream=True
        )
        self._raise_for_status(response)
        return self._stream_helper(response, decode=True)","Upgrade an installed plugin.

            Args:
                name (string): Name of the plugin to upgrade. The ``:latest``
                    tag is optional and is the default if omitted.
                remote (string): Remote reference to upgrade to. The
                    ``:latest`` tag is optional and is the default if omitted.
                privileges (:py:class:`list`): A list of privileges the user
                    consents to grant to the plugin. Can be retrieved using
                    :py:meth:`~plugin_privileges`.

            Returns:
                An iterable object streaming the decoded API logs",0,1,1,2
"def upload(
        src, requirements=None, local_package=None,
        config_file='config.yaml', profile_name=None,
):
    
    
    path_to_config_file = os.path.join(src, config_file)
    cfg = read_cfg(path_to_config_file, profile_name)

    
    
    
    
    path_to_zip_file = build(
        src, config_file=config_file, requirements=requirements,
        local_package=local_package,
    )

    upload_s3(cfg, path_to_zip_file)","Uploads a new function to AWS S3.

    :param str src:
        The path to your Lambda ready project (folder must contain a valid
        config.yaml and handler module (e.g.: service.py).
    :param str local_package:
        The path to a local package with should be included in the deploy as
        well (and/or is not available on PyPi)",1,0,2,3
"def upload(self, file_path, uri=None, timeout=-1):
        
        if not uri:
            uri = self._uri

        upload_file_name = os.path.basename(file_path)
        task, entity = self._connection.post_multipart_with_response_handling(uri, file_path, upload_file_name)

        if not task:
            return entity

        return self._task_monitor.wait_for_task(task, timeout)","Makes a multipart request.

        Args:
            file_path:
                File to upload.
            uri:
                A specific URI (optional).
            timeout:
                Timeout in seconds. Wait for task completion by default. The timeout does not abort the operation
                in OneView; it just stops waiting for its completion.

        Returns:
            dict: Response body.",1,1,1,3
"def uploadAsset(self, data, async=False, metadata=None, callback=None):
    
    
    if isinstance(data, io.BufferedReader):
      data = data.read()
    assert isinstance(data, bytes)
    data = base64.b64encode(data)
    audio = {'content': data.decode(""utf-8"")}
    config = {'async': async}
    if callback is not None:
      config['callback'] = callback

    if metadata is not None:
      body = {'audio': audio, 'config': config, 'metadata': metadata}
    else:
      body = {'audio': audio, 'config': config}

    return self._checkReturn(
      requests.post(""{}/speech:recognize?signed_username={}"".format(self.url, self.signedUsername), json=body))","Takes an array of bytes or a BufferedReader and uploads it. If async=false a json with the result is returned otherwise a json with an asset_id is returned.
    :param data: array of bytes or BufferedReader
    :param metadata: arbitrary additional description information for the asset
    :param async:
    :param callback: Callback URL
    :return:",0,1,1,2
"def upload_documentation(self, metadata, doc_dir):
        
        self.check_credentials()
        if not os.path.isdir(doc_dir):
            raise DistlibException('not a directory: %r' % doc_dir)
        fn = os.path.join(doc_dir, 'index.html')
        if not os.path.exists(fn):
            raise DistlibException('not found: %r' % fn)
        metadata.validate()
        name, version = metadata.name, metadata.version
        zip_data = zip_dir(doc_dir).getvalue()
        fields = [(':action', 'doc_upload'),
                  ('name', name), ('version', version)]
        files = [('content', name, zip_data)]
        request = self.encode_request(fields, files)
        return self.send_request(request)","Upload documentation to the index.

        :param metadata: A :class:`Metadata` instance defining at least a name
                         and version number for the documentation to be
                         uploaded.
        :param doc_dir: The pathname of the directory which contains the
                        documentation. This should be the directory that
                        contains the ``index.html`` for the documentation.
        :return: The HTTP response received from PyPI upon submission of the
                request.",2,1,3,6
"def upload_file_sections(self, user_id, section_id, assignment_id):
        
        path = {}
        data = {}
        params = {}

        
        
        path[""section_id""] = section_id

        
        
        path[""assignment_id""] = assignment_id

        
        
        path[""user_id""] = user_id

        self.logger.debug(""POST /api/v1/sections/{section_id}/assignments/{assignment_id}/submissions/{user_id}/files with query params: {params} and form data: {data}"".format(params=params, data=data, **path))
        return self.generic_request(""POST"", ""/api/v1/sections/{section_id}/assignments/{assignment_id}/submissions/{user_id}/files"".format(**path), data=data, params=params, no_data=True)","Upload a file.

        Upload a file to a submission.
        
        This API endpoint is the first step in uploading a file to a submission as a student.
        See the {file:file_uploads.html File Upload Documentation} for details on the file upload workflow.
        
        The final step of the file upload workflow will return the attachment data,
        including the new file id. The caller can then POST to submit the
        +online_upload+ assignment with these file ids.",0,2,0,2
"def uptime(ut, facter):
    

    ut = ut
    if ut and ut.loadavg:
        return Uptime(ut.currtime, ut.updays, ut.uphhmm,
                      ut.users, ut.loadavg, ut.uptime)
    ft = facter
    if ft and hasattr(ft, 'uptime_seconds'):
        import datetime
        secs = int(ft.uptime_seconds)
        up_dd = secs // (3600 * 24)
        up_hh = (secs % (3600 * 24)) // 3600
        up_mm = (secs % 3600) // 60
        updays = str(up_dd) if up_dd > 0 else ''
        uphhmm = '%02d:%02d' % (up_hh, up_mm)
        up_time = datetime.timedelta(seconds=secs)
        return Uptime(None, updays, uphhmm, None, None, up_time)

    raise Exception(""Unable to get uptime information."")","Check uptime and facts to get the uptime information.

    Prefer uptime to facts.

    Returns:
        insights.combiners.uptime.Uptime: A named tuple with `currtime`,
        `updays`, `uphhmm`, `users`, `loadavg` and `uptime` components.

    Raises:
        Exception: If no data is available from both of the parsers.",0,0,5,5
"def uri_to_iri(uri, charset='utf-8', errors='replace'):
    r
    if isinstance(uri, tuple):
        uri = url_unparse(uri)
    uri = url_parse(to_unicode(uri, charset))
    path = url_unquote(uri.path, charset, errors, '/;?')
    query = url_unquote(uri.query, charset, errors, ';/?:@&=+,$')
    fragment = url_unquote(uri.fragment, charset, errors, ';/?:@&=+,$')
    return url_unparse((uri.scheme, uri.decode_netloc(),
                        path, query, fragment))","r""""""
    Converts a URI in a given charset to a IRI.

    Examples for URI versus IRI:

    >>> uri_to_iri(b'http://xn--n3h.net/')
    u'http://\u2603.net/'
    >>> uri_to_iri(b'http://%C3%BCser:p%C3%A4ssword@xn--n3h.net/p%C3%A5th')
    u'http://\xfcser:p\xe4ssword@\u2603.net/p\xe5th'

    Query strings are left unchanged:

    >>> uri_to_iri('/?foo=24&x=%26%2f')
    u'/?foo=24&x=%26%2f'

    .. versionadded:: 0.6

    :param uri: The URI to convert.
    :param charset: The charset of the URI.
    :param errors: The error handling on decode.",0,0,1,1
"def uriunsplit(uri):
    
    (scheme, authority, path, query, fragment) = uri
    result = ''
    if scheme:
        result += scheme + ':'
    if authority:
        result += '//' + authority
    if path:
        result += path
    if query:
        result += '?' + query
    if fragment:
        result += '
    return result","Reverse of urisplit()

       >>> uriunsplit(('scheme','authority','path','query','fragment'))
       ""scheme://authority/path?query#fragment""",0,0,1,1
"def url(self, _url, **kwargs):
        
        api_call = self._create_api_call('get', _url, kwargs)
        data = self._clean_arguments(kwargs.pop('data', None))
        params = self._clean_arguments(kwargs)

        api_call.set_request(method='GET',
                             data=data,
                             params=params,
                             sub_url=_url)
        api_call._stage = 'URL'
        return api_call.url","This will return the url for a Request instead of actually
        performing the request, and then store it in self['label'].

        * Note the API call isn't actually made but it is setup to call
        save or open_as_file
        :param _url:   str of the sub url of the api call (ex. g/device/list)
        :param kwargs: dict of additional arguments
        :return:       str of the url for the api_call",0,0,2,2
"def url(self, schemes=None):
        
        if schemes is None:
            schemes = ['http', 'https']

        pattern = '{}://{}'.format(
            self.random_element(schemes) if schemes else """",
            self.random_element(self.url_formats),
        )

        return self.generator.parse(pattern)",":param schemes: a list of strings to use as schemes, one will chosen randomly.
        If None, it will generate http and https urls.
        Passing an empty list will result in schemeless url generation like ""://domain.com"".

        :returns: a random url string.",0,0,2,2
"def url_for(self, endpoint, explicit=False, **items):
        
        
        
        
        
        if not explicit and not endpoint.startswith(self._namespace):
            endpoint = '%s.%s' % (self._namespace, endpoint)
        return self._plugin.url_for(endpoint, **items)","Returns a valid XBMC plugin URL for the given endpoint name.
        endpoint can be the literal name of a function, or it can
        correspond to the name keyword arguments passed to the route
        decorator.

        Currently, view names must be unique across all plugins and
        modules. There are not namespace prefixes for modules.",0,0,1,1
"def urljoin(end_with_slash, *args):
        
        last = args[0]
        for i in args[1:]:
            last = '%s/%s' % (last.strip('/'), i.strip('/'))

        if end_with_slash:
            if last[-1] != '/':
                last += '/'
        return last",">>> HelperURI.urljoin(False, *('http://127.0.0.1:8000', 'dumb_upload'))
        'http://127.0.0.1:8000/dumb_upload'

        >>> HelperURI.urljoin(True, *('http://127.0.0.1:8000', 'dumb_upload'))
        'http://127.0.0.1:8000/dumb_upload/'",0,0,1,1
"def use_gl(target='gl2'):
    
    target = target or 'gl2'
    target = target.replace('+', 'plus')
    
    
    target, _, options = target.partition(' ')
    debug = config['gl_debug'] or 'debug' in options
    
    
    try:
        mod = __import__(target, globals(), level=1)
    except ImportError as err:
        msg = 'Could not import gl target ""%s"":\n%s' % (target, str(err))
        raise RuntimeError(msg)

    
    global current_backend
    current_backend = mod
    _clear_namespace()
    if 'plus' in target:
        
        _copy_gl_functions(mod._pyopengl2, globals())
        _copy_gl_functions(mod, globals(), True)
    elif debug:
        _copy_gl_functions(_debug_proxy, globals())
    else:
        _copy_gl_functions(mod, globals())","Let Vispy use the target OpenGL ES 2.0 implementation
    
    Also see ``vispy.use()``.
    
    Parameters
    ----------
    target : str
        The target GL backend to use.

    Available backends:
    * gl2 - Use ES 2.0 subset of desktop (i.e. normal) OpenGL
    * gl+ - Use the desktop ES 2.0 subset plus all non-deprecated GL
      functions on your system (requires PyOpenGL)
    * es2 - Use the ES2 library (Angle/DirectX on Windows)
    * pyopengl2 - Use ES 2.0 subset of pyopengl (for fallback and testing)
    * dummy - Prevent usage of gloo.gl (for when rendering occurs elsewhere)
    
    You can use vispy's config option ""gl_debug"" to check for errors
    on each API call. Or, one can specify it as the target, e.g. ""gl2
    debug"". (Debug does not apply to 'gl+', since PyOpenGL has its own
    debug mechanism)",1,0,1,2
"def use_tips(self, start_well: Well, num_channels: int = 1):
        
        assert num_channels > 0, 'Bad call to use_tips: num_channels==0'
        
        target_column: List[Well] = [
            col for col in self.columns() if start_well in col][0]

        well_idx = target_column.index(start_well)
        
        
        
        
        
        num_tips = min(len(target_column) - well_idx, num_channels)
        target_wells = target_column[well_idx: well_idx + num_tips]

        assert all([well.has_tip for well in target_wells]),\
            '{} is out of tips'.format(str(self))

        for well in target_wells:
            well.has_tip = False","Removes tips from the tip tracker.

        This method should be called when a tip is picked up. Generally, it
        will be called with `num_channels=1` or `num_channels=8` for single-
        and multi-channel respectively. If picking up with more than one
        channel, this method will automatically determine which tips are used
        based on the start well, the number of channels, and the geometry of
        the tiprack.

        :param start_well: The :py:class:`.Well` from which to pick up a tip.
                           For a single-channel pipette, this is the well to
                           send the pipette to. For a multi-channel pipette,
                           this is the well to send the back-most nozzle of the
                           pipette to.
        :type start_well: :py:class:`.Well`
        :param num_channels: The number of channels for the current pipette
        :type num_channels: int",2,0,1,3
"def user_project_from_remote(remote):
        

        
        
        
        regex1 = br"".*(?:[:/])(?P<user>(-|\w|\.)*)/"" \
                 br""(?P<project>(-|\w|\.)*)(\.git).*""
        match = re.match(regex1, remote)
        if match:
            return match.group(""user""), match.group(""project"")

        
        
        
        regex2 = r"".*/((?:-|\w|\.)*)/((?:-|\w|\.)*).*""
        match = re.match(regex2, remote)
        if match:
            return match.group(""user""), match.group(""project"")

        return None, None","Try to find user and project name from git remote output

        @param [String] output of git remote command
        @return [Array] user and project",0,0,1,1
"def users(self, params):
        
        resource = self.RESOURCE_USERS.format(account_id=self.account.id, id=self.id)
        headers = {'Content-Type': 'application/json'}
        response = Request(self.account.client,
                           'post',
                           resource,
                           headers=headers,
                           body=json.dumps(params)).perform()
        success_count = response.body['data']['success_count']
        total_count = response.body['data']['total_count']
        return (success_count, total_count)","This is a private API and requires whitelisting from Twitter.
        This endpoint will allow partners to add, update and remove users from a given
            tailored_audience_id.
        The endpoint will also accept multiple user identifier types per user as well.",0,1,1,2
"def utime(self, times):
        
        if times is None:
            times = (time.time(), time.time())
        self.sftp._log(DEBUG, 'utime(%s, %r)' % (hexlify(self.handle), times))
        attr = SFTPAttributes()
        attr.st_atime, attr.st_mtime = times
        self.sftp._request(CMD_FSETSTAT, self.handle, attr)","Set the access and modified times of this file.  If
        C{times} is C{None}, then the file's access and modified times are set
        to the current time.  Otherwise, C{times} must be a 2-tuple of numbers,
        of the form C{(atime, mtime)}, which is used to set the access and
        modified times, respectively.  This bizarre API is mimicked from python
        for the sake of consistency -- I apologize.

        @param times: C{None} or a tuple of (access time, modified time) in
            standard internet epoch time (seconds since 01 January 1970 GMT)
        @type times: tuple(int)",0,2,1,3
"def valid_api_plugin(self, plugin):
        
        if (issubclass(plugin, APIPlugin)       and
            hasattr(plugin, 'plugin_type')      and plugin.plugin_type == 'api' and
            hasattr(plugin, 'request')          and plugin.request != None and
            hasattr(plugin, 'request_class')    and plugin.request_class != None and
            hasattr(plugin, 'response_class')   and plugin.response_class != None):
            return True
        return False","Validate an API plugin, ensuring it is an API plugin and has the
        necessary fields present.

        `plugin` is a subclass of scruffy's Plugin class.",0,0,1,1
"def validate(cls, code, prefix):
        
        reasons = []

        pieces = code.split('-')
        n = len(pieces)
        if n > 3:
            reasons.append('{}invalid geo code ""{}""'.format(prefix, code))
        elif n > 0 and pieces[0] not in geo_data:
            reasons.append('{}unknown continent code ""{}""'
                           .format(prefix, code))
        elif n > 1 and pieces[1] not in geo_data[pieces[0]]:
            reasons.append('{}unknown country code ""{}""'.format(prefix, code))
        elif n > 2 and \
                pieces[2] not in geo_data[pieces[0]][pieces[1]]['provinces']:
            reasons.append('{}unknown province code ""{}""'.format(prefix, code))

        return reasons","Validates an octoDNS geo code making sure that it is a valid and
        corresponding:
            * continent
            * continent & country
            * continent, country, & province",0,0,2,2
"def validate(self, data=None, only=None, exclude=None):
        
        data = data or {}
        only = only or self._meta.only
        exclude = exclude or self._meta.exclude

        for name, field in self.instance._meta.fields.items():
            if name in exclude or (only and name not in only):
                continue
            try:
                data.setdefault(name, getattr(self.instance, name, None))
            except (peewee.DoesNotExist):
                if PEEWEE3:
                    instance_data = self.instance.__data__
                else:
                    instance_data = self.instance._data
                data.setdefault(name, instance_data.get(name, None))

        
        super().validate(data=data, only=only, exclude=exclude)

        if not self.errors:
            self.perform_index_validation(self.data)

        return (not self.errors)","Validate the data for all fields and return whether the validation was successful.
        This method also retains the validated data in ``self.data`` so that it can be accessed later.

        If data for a field is not provided in ``data`` then this validator will check against the
        provided model instance.

        This is usually the method you want to call after creating the validator instance.

        :param data: Dictionary of data to validate.
        :param only: List or tuple of fields to validate.
        :param exclude: List or tuple of fields to exclude from validation.
        :return: True if validation is successful, otherwise False.",0,0,3,3
"def validateURL(value, blank=False, strip=None, allowlistRegexes=None, blocklistRegexes=None, excMsg=None):
    

    
    try:
        result = validateRegex(value=value, regex=URL_REGEX, blank=blank, strip=strip, allowlistRegexes=allowlistRegexes, blocklistRegexes=blocklistRegexes)
        if result is not None:
            return result
    except ValidationException:
        
        if value == 'localhost':
            return value

        _raiseValidationException(_('%r is not a valid URL.') % (value), excMsg)","Raises ValidationException if value is not a URL.
    Returns the value argument.

    The ""http"" or ""https"" protocol part of the URL is optional.

    * value (str): The value being validated as a URL.
    * blank (bool):  If True, a blank string will be accepted. Defaults to False.
    * strip (bool, str, None): If None, whitespace is stripped from value. If a str, the characters in it are stripped from value. If False, nothing is stripped.
    * allowlistRegexes (Sequence, None): A sequence of regex str that will explicitly pass validation, even if they aren't numbers.
    * blocklistRegexes (Sequence, None): A sequence of regex str or (regex_str, response_str) tuples that, if matched, will explicitly fail validation.
    * excMsg (str): A custom message to use in the raised ValidationException.

    >>> import pysimplevalidate as pysv
    >>> pysv.validateURL('https://inventwithpython.com')
    'https://inventwithpython.com'
    >>> pysv.validateURL('inventwithpython.com')
    'inventwithpython.com'
    >>> pysv.validateURL('localhost')
    'localhost'
    >>> pysv.validateURL('mailto:al@inventwithpython.com')
    'mailto:al@inventwithpython.com'
    >>> pysv.validateURL('ftp://example.com')
    'example.com'
    >>> pysv.validateURL('https://inventwithpython.com/blog/2018/02/02/how-to-ask-for-programming-help/')
    'https://inventwithpython.com/blog/2018/02/02/how-to-ask-for-programming-help/'
    >>> pysv.validateURL('blah blah blah')
    Traceback (most recent call last):
        ...
    pysimplevalidate.ValidationException: 'blah blah blah' is not a valid URL.",1,0,3,4
"def validate_api_response(schema, raw_response, request_method='get', raw_request=None):
    
    request = None
    if raw_request is not None:
        request = normalize_request(raw_request)

    response = None
    if raw_response is not None:
        response = normalize_response(raw_response, request=request)

    if response is not None:
        validate_response(
            response=response,
            request_method=request_method,
            schema=schema
        )",Validate the response of an api call against a swagger schema.,0,0,1,1
"def validate_config(key: str, config: dict) -> None:
    

    try:
        jsonschema.validate(config, CONFIG_JSON_SCHEMA[key])
    except jsonschema.ValidationError as x_validation:
        raise JSONValidation('JSON validation error on {} configuration: {}'.format(key, x_validation.message))
    except jsonschema.SchemaError as x_schema:
        raise JSONValidation('JSON schema error on {} specification: {}'.format(key, x_schema.message))","Call jsonschema validation to raise JSONValidation on non-compliance or silently pass.

    :param key: validation schema key of interest
    :param config: configuration dict to validate",2,0,3,5
"def validate_config(self, project, config, actor=None):
        
        client = JiraClient(config['instance_url'], config['username'], config['password'])
        try:
            client.get_projects_list()
        except ApiError as e:
            self.raise_error(e)

        return config","```
        if config['foo'] and not config['bar']:
            raise PluginError('You cannot configure foo with bar')
        return config
        ```",1,1,0,2
"def validate_creds(creds):
    
    try:
        auth_type, auth_rest = creds.split(':', 1)
    except ValueError:
        raise ValueError(""Missing ':' in %s"" % creds)
    authtypes = sys.modules[__name__]
    auth_encoder = getattr(authtypes, auth_type.title(), None)
    if auth_encoder is None:
        raise ValueError('Invalid auth_type: %s' % auth_type)
    auth_encoder = auth_encoder()
    parsed_creds = dict(type=auth_type, salt=None, hash=None)
    parsed_creds.update(auth_encoder.validate(auth_rest))
    return auth_encoder, parsed_creds","Parse and validate user credentials whether format is right

    :param creds: User credentials
    :returns: Auth_type class instance and parsed user credentials in dict
    :raises ValueError: If credential format is wrong (eg: bad auth_type)",2,0,2,4
"def validate_netmask(s):
    
    if validate_ip(s):
        
        mask = bin(ip2network(s))[2:].zfill(32)
        
        seen0 = False
        for c in mask:
            if '1' == c:
                if seen0:
                    return False
            else:
                seen0 = True
        return True
    else:
        return False","Validate that a dotted-quad ip address is a valid netmask.


    >>> validate_netmask('0.0.0.0')
    True
    >>> validate_netmask('128.0.0.0')
    True
    >>> validate_netmask('255.0.0.0')
    True
    >>> validate_netmask('255.255.255.255')
    True
    >>> validate_netmask(BROADCAST)
    True
    >>> validate_netmask('128.0.0.1')
    False
    >>> validate_netmask('1.255.255.0')
    False
    >>> validate_netmask('0.255.255.0')
    False


    :param s: String to validate as a dotted-quad notation netmask.
    :type s: str
    :returns: ``True`` if a valid netmask, ``False`` otherwise.
    :raises: TypeError",0,0,1,1
"def validate_opts(opts, brokers_num):
    
    if opts.skip < 0 or opts.skip >= brokers_num:
        print(""Error: --skip must be >= 0 and < 
        return True
    if opts.check_count < 0:
        print(""Error: --check-count must be >= 0"")
        return True
    if opts.unhealthy_time_limit < 0:
        print(""Error: --unhealthy-time-limit must be >= 0"")
        return True
    if opts.check_count == 0:
        print(""Warning: no check will be performed"")
    if opts.check_interval < 0:
        print(""Error: --check-interval must be >= 0"")
        return True
    return False","Basic option validation. Returns True if the options are not valid,
    False otherwise.

    :param opts: the command line options
    :type opts: map
    :param brokers_num: the number of brokers
    :type brokers_num: integer
    :returns: bool",0,0,1,1
"def validate_result(self, (result_array, result_keywords, )):
        
        if len(result_array) != 1 or len(result_keywords) > 0:
            raise BadSyntax('Syntax error processing line: %s' %\
                            self._last_line)
        return result_array[0]","Check that the result is a list with a single element, and return it.
        If we had more than one element it would mean that the line processed
        looked somewhat like this:
        call1(), ""blah blah blah""",1,0,2,3
"def validate_user(self, username, password, client, request,
                      *args, **kwargs):
        
        log.debug('Validating username %r and its password', username)
        if self._usergetter is not None:
            user = self._usergetter(
                username, password, client, request, *args, **kwargs
            )
            if user:
                request.user = user
                return True
            return False
        log.debug('Password credential authorization is disabled.')
        return False","Ensure the username and password is valid.

        Attach user object on request for later using.",1,2,0,3
"def validator(
    *fields: str, pre: bool = False, whole: bool = False, always: bool = False, check_fields: bool = True
) -> Callable[[AnyCallable], classmethod]:
    
    if not fields:
        raise ConfigError('validator with no fields specified')
    elif isinstance(fields[0], FunctionType):
        raise ConfigError(
            ""validators should be used with fields and keyword arguments, not bare. ""
            ""E.g. usage should be `@validator('<field_name>', ...)`""
        )

    def dec(f: AnyCallable) -> classmethod:
        
        
        if not in_ipython():  
            ref = f.__module__ + '.' + f.__qualname__
            if ref in _FUNCS:
                raise ConfigError(f'duplicate validator function ""{ref}""')
            _FUNCS.add(ref)
        f_cls = classmethod(f)
        f_cls.__validator_config = fields, Validator(f, pre, whole, always, check_fields)  
        return f_cls

    return dec","Decorate methods on the class indicating that they should be used to validate fields
    :param fields: which field(s) the method should be called on
    :param pre: whether or not this validator should be called before the standard validators (else after)
    :param whole: for complex objects (sets, lists etc.) whether to validate individual elements or the whole object
    :param always: whether this method and other validators should be called even if the value is missing
    :param check_fields: whether to check that the fields actually exist on the model",3,0,4,7
"def value_from_message(self, message):
        
        message = super(DateTimeField, self).value_from_message(message)
        if message.time_zone_offset is None:
            return datetime.datetime.utcfromtimestamp(
                message.milliseconds / 1000.0)

        
        
        
        milliseconds = (message.milliseconds -
                        60000 * message.time_zone_offset)

        timezone = util.TimeZoneOffset(message.time_zone_offset)
        return datetime.datetime.fromtimestamp(milliseconds / 1000.0,
                                               tz=timezone)","Convert DateTimeMessage to a datetime.

        Args:
          A DateTimeMessage instance.

        Returns:
          A datetime instance.",0,0,1,1
"def value_to_message(self, value):
        
        if not isinstance(value, self.type):
            raise EncodeError('Expected type %s, got %s: %r' %
                              (self.type.__name__,
                               type(value).__name__,
                               value))
        return value","Convert a value instance to a message.

        Used by serializers to convert Python user types to underlying
        messages for transmission.

        Args:
          value: A value of type self.type.

        Returns:
          An instance of type self.message_type.",1,0,2,3
"def values(ns_key):
    

    if ns_key not in __NAMESPACE__:
        raise NamespaceError('Unknown namespace: {:s}'.format(ns_key))

    if 'enum' not in __NAMESPACE__[ns_key]['value']:
        raise NamespaceError('Namespace {:s} is not enumerated'.format(ns_key))

    return copy.copy(__NAMESPACE__[ns_key]['value']['enum'])","Return the allowed values for an enumerated namespace.

    Parameters
    ----------
    ns_key : str
        Namespace key identifier

    Returns
    -------
    values : list

    Raises
    ------
    NamespaceError
        If `ns_key` is not found, or does not have enumerated values

    Examples
    --------
    >>> jams.schema.values('tag_gtzan')
    ['blues', 'classical', 'country', 'disco', 'hip-hop', 'jazz',
     'metal', 'pop', 'reggae', 'rock']",2,0,3,5
"def verification_add(self, domain_resource_id, port, is_ssl):
        
        data = {""domain_href"": self.build_api_path('domains',
                                                   domain_resource_id),
                ""port"": port,
                ""ssl"": 'true' if is_ssl else 'false'}
        url = self.build_full_url(self.VERIFICATIONS)
        return self.create_resource(url, data)","Sends a POST to /1.0/verifications/ using this post-data:

            {""domain_href"": ""/1.0/domains/2"",
             ""port"":80,
             ""ssl"":false}

        :param domain_resource_id: The domain id to verify
        :param port: The TCP port
        :param is_ssl: Boolean indicating if we should use ssl

        :return: The newly created resource",0,1,1,2
"def verify(self, **kwargs):
        
        super(RegistrationResponse, self).verify(**kwargs)

        has_reg_uri = ""registration_client_uri"" in self
        has_reg_at = ""registration_access_token"" in self
        if has_reg_uri != has_reg_at:
            raise VerificationError((
                ""Only one of registration_client_uri""
                "" and registration_access_token present""), self)

        return True","Implementations MUST either return both a Client Configuration Endpoint
        and a Registration Access Token or neither of them.
        :param kwargs:
        :return: True if the message is OK otherwise False",1,0,1,2
"def verify(self, msg, sig, key):
        
        try:
            h = hmac.HMAC(key, self.algorithm(), default_backend())
            h.update(msg)
            h.verify(sig)
            return True
        except:
            return False","Verifies whether sig is the correct message authentication code of data.

        :param msg: The data
        :param sig: The message authentication code to verify against data.
        :param key: The key to use
        :return: Returns true if the mac was valid otherwise it will raise an
            Exception.",0,0,1,1
"def verify(self, signed):
        
        
        buf = create_string_buffer(libcrypto.RSA_size(self._rsa))
        signed = salt.utils.stringutils.to_bytes(signed)
        size = libcrypto.RSA_public_decrypt(len(signed), signed, buf, self._rsa, RSA_X931_PADDING)
        if size < 0:
            raise ValueError('Unable to decrypt message')
        return buf[0:size]","Recover the message (digest) from the signature using the public key

        :param str signed: The signature created with the private key
        :rtype: str
        :return: The message (digest) recovered from the signature, or an empty
            string if the decryption failed",1,0,2,3
"def verify_constraints(constraints):
        
        
        if not isinstance(constraints, list):
            raise ValueError(
                ""invalid type returned by make_constraints: %r (must be a list)"" % constraints
            )

        
        for constraint in constraints:
            if not isinstance(constraint, Constraint):
                raise ValueError(
                    ""invalid constraint type: %r (must be a Constriant)"" % constraint
                )","Verify values returned from :meth:`make_constraints`.

        Used internally during the :meth:`build` process.

        :param constraints: value returned from :meth:`make_constraints`
        :type constraints: :class:`list`
        :raises ValueError: if verification fails",2,0,2,4
"def verify_proxy_ticket(ticket, service):
    

    params = {'ticket': ticket, 'service': service}

    url = (urljoin(settings.CAS_SERVER_URL, 'proxyValidate') + '?' +
           urlencode(params))

    page = urlopen(url)

    try:
        response = page.read()
        tree = ElementTree.fromstring(response)
        if tree[0].tag.endswith('authenticationSuccess'):
            username = tree[0][0].text
            proxies = []
            if len(tree[0]) > 1:
                for element in tree[0][1]:
                    proxies.append(element.text)
            return {""username"": username, ""proxies"": proxies}
        else:
            return None
    finally:
        page.close()","Verifies CAS 2.0+ XML-based proxy ticket.

    :param: ticket
    :param: service

    Returns username on success and None on failure.",0,1,1,2
"def verify_request_signed_by_signing_keys(smsreq):
    

    _jws = factory(smsreq)
    _json = _jws.jwt.part[1]
    _body = json.loads(as_unicode(_json))
    iss = _body['iss']
    _jwks = _body['signing_keys']

    _kj = jwks_to_keyjar(_jwks, iss)

    try:
        _kid = _jws.jwt.headers['kid']
    except KeyError:
        _keys = _kj.get_signing_key(owner=iss)
    else:
        _keys = _kj.get_signing_key(owner=iss, kid=_kid)

    _ver = _jws.verify_compact(smsreq, _keys)
    
    for k in JsonWebToken.c_param.keys():
        try:
            del _ver[k]
        except KeyError:
            pass
    try:
        del _ver['kid']
    except KeyError:
        pass

    return {'ms': MetadataStatement(**_ver), 'iss': iss}","Verify that a JWT is signed with a key that is inside the JWT.
    
    :param smsreq: Signed Metadata Statement signing request
    :return: Dictionary containing 'ms' (the signed request) and 'iss' (the
        issuer of the JWT).",0,0,1,1
"def verify_signed_jwt_with_certs(jwt, certs, audience=None):
    
    jwt = _helpers._to_bytes(jwt)

    if jwt.count(b'.') != 2:
        raise AppIdentityError(
            'Wrong number of segments in token: {0}'.format(jwt))

    header, payload, signature = jwt.split(b'.')
    message_to_sign = header + b'.' + payload
    signature = _helpers._urlsafe_b64decode(signature)

    
    payload_bytes = _helpers._urlsafe_b64decode(payload)
    try:
        payload_dict = json.loads(_helpers._from_bytes(payload_bytes))
    except:
        raise AppIdentityError('Can\'t parse token: {0}'.format(payload_bytes))

    
    _verify_signature(message_to_sign, signature, certs.values())

    
    _verify_time_range(payload_dict)

    
    _check_audience(payload_dict, audience)

    return payload_dict","Verify a JWT against public certs.

    See http://self-issued.info/docs/draft-jones-json-web-token.html.

    Args:
        jwt: string, A JWT.
        certs: dict, Dictionary where values of public keys in PEM format.
        audience: string, The audience, 'aud', that this JWT should contain. If
                  None then the JWT's 'aud' parameter is not verified.

    Returns:
        dict, The deserialized JSON payload in the JWT.

    Raises:
        AppIdentityError: if any checks are failed.",2,0,1,3
"def verify_valid_dependencies(self):
        

        unobserved_dependencies = set(self.tasks.keys())
        target_queue = []

        while len(unobserved_dependencies) > 0:
            target_queue = [unobserved_dependencies.pop()]

            while target_queue is not []:
                target_queue += unobserved_dependencies","Checks if the assigned dependencies are valid
            valid dependency graphs are:

            - noncyclic (i.e. no `A -> B -> ... -> A`)
            - Contain no undefined dependencies
              (dependencies referencing undefined tasks)",0,0,1,1
"def version(self, content_type=""*/*""):
        
        v = """"
        accept_header = self.get_header('accept', """")
        if accept_header:
            a = AcceptHeader(accept_header)
            for mt in a.filter(content_type):
                v = mt[2].get(""version"", """")
                if v: break

        return v","versioning is based off of this post 
        http://urthen.github.io/2013/05/09/ways-to-version-your-api/",0,0,1,1
"def vert_tab_pos(self, positions):
        
        if positions == 'clear':
            self.send(chr(27)+'B'+chr(0))
            return
        if positions.min < 1 or positions.max >255:
                raise RuntimeError('Invalid position parameter in function horzTabPos')
        sendstr = chr(27) + 'D'
        if len(positions)<=16:
            for position in positions:
                sendstr += chr(position)
            self.send(sendstr + chr(0))
        else:
            raise RuntimeError('Too many positions in function vertTabPos')","Sets tab positions, up to a maximum of 32 positions. Also can clear tab positions.
        
        Args:
            positions -- Either a list of tab positions (between 1 and 255), or 'clear'.
        Returns:
            None
        Raises:
            RuntimeError: Invalid position parameter.
            RuntimeError: Too many positions.",2,0,3,5
"def video_in_option(self, param, profile='Day'):
        
        if profile == 'Day':
            field = param
        else:
            field = '{}Options.{}'.format(profile, param)
        return utils.pretty(
            [opt for opt in self.video_in_options.split()
             if '].{}='.format(field) in opt][0])","Return video input option.

        Params:
            param - parameter, such as 'DayNightColor'
            profile - 'Day', 'Night' or 'Normal'",0,0,1,1
"def vprjpi(vin, projpl, invpl):
    
    vin = stypes.toDoubleVector(vin)
    vout = stypes.emptyDoubleVector(3)
    found = ctypes.c_int()
    libspice.vprjpi_c(vin, ctypes.byref(projpl), ctypes.byref(invpl), vout,
                      ctypes.byref(found))
    return stypes.cVectorToPython(vout), bool(found.value)","Find the vector in a specified plane that maps to a specified
    vector in another plane under orthogonal projection.

    http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/vprjpi_c.html

    :param vin: The projected vector. 
    :type vin: 3-Element Array of floats
    :param projpl: Plane containing vin. 
    :type projpl: spiceypy.utils.support_types.Plane
    :param invpl: Plane containing inverse image of vin. 
    :type invpl: spiceypy.utils.support_types.Plane
    :return: Inverse projection of vin.
    :rtype: list",0,1,1,2
"def wait(self, timeout=None):
        
        
        if self._thread is None:
            return
        self._thread.join(timeout=timeout)
        try:
            
            raise self._exception  
        except TypeError: 
            pass","Wait on the long running operation for a specified length
        of time. You can check if this call as ended with timeout with the
        ""done()"" method.

        :param int timeout: Period of time to wait for the long running
         operation to complete (in seconds).
        :raises CloudError: Server problem with the query.",1,0,1,2
"def wait_for_build(self, interval=5, path=None):
        
        path = path or ''
        start = time.time()
        next_log = 0
        while True:
            response = self.get(path)['last_build_info']
            if not response:
                raise ValueError('This project is not building!')
            if response['stop_time']:
                if response['success']:
                    return response
                else:
                    raise LuminosoError(response)
            elapsed = time.time() - start
            if elapsed > next_log:
                logger.info('Still waiting (%d seconds elapsed).', next_log)
                next_log += 120
            time.sleep(interval)","A convenience method designed to inform you when a project build has
        completed.  It polls the API every `interval` seconds until there is
        not a build running.  At that point, it returns the ""last_build_info""
        field of the project record if the build succeeded, and raises a
        LuminosoError with the field as its message if the build failed.

        If a `path` is not specified, this method will assume that its URL is
        the URL for the project.  Otherwise, it will use the specified path
        (which should be ""/projects/<project_id>/"").",2,2,3,7
"def wait_for_vacancy(self, processor_type):
        

        with self._condition:
            self._condition.wait_for(lambda: (
                self._processor_available(processor_type)
                or self._cancelled_event.is_set()))
            if self._cancelled_event.is_set():
                raise WaitCancelledException()
            processor = self[processor_type].next_processor()
            return processor","Waits for a particular processor type to have the capacity to
        handle additional transactions or until is_cancelled is True.

        Args:
            processor_type (ProcessorType): The family, and version of
                the transaction processor.

        Returns:
            Processor",1,0,2,3
"def watchpoint_info(self, handle=0, index=-1):
        
        if index < 0 and handle == 0:
            raise ValueError('Handle must be provided if index is not set.')

        wp = structs.JLinkWatchpointInfo()
        res = self._dll.JLINKARM_GetWPInfoEx(index, ctypes.byref(wp))
        if res < 0:
            raise errors.JLinkException('Failed to get watchpoint info.')

        for i in range(res):
            res = self._dll.JLINKARM_GetWPInfoEx(i, ctypes.byref(wp))
            if res < 0:
                raise errors.JLinkException('Failed to get watchpoint info.')
            elif wp.Handle == handle or wp.WPUnit == index:
                return wp

        return None","Returns information about the specified watchpoint.

        Note:
          Either ``handle`` or ``index`` can be specified.  If the ``index``
          is not provided, the ``handle`` must be set, and vice-versa.  If
          both ``index`` and ``handle`` are provided, the ``index`` overrides
          the provided ``handle``.

        Args:
          self (JLink): the ``JLink`` instance
          handle (int): optional handle of a valid watchpoint.
          index (int): optional index of a watchpoint.

        Returns:
          An instance of ``JLinkWatchpointInfo`` specifying information about
          the watchpoint if the watchpoint was found, otherwise ``None``.

        Raises:
          JLinkException: on error.
          ValueError: if both handle and index are invalid.",3,1,3,7
"def wb004(self, value=None):
        
        if value is not None:
            try:
                value = float(value)
            except ValueError:
                raise ValueError('value {} need to be of type float '
                                 'for field `wb004`'.format(value))

        self._wb004 = value","Corresponds to IDD Field `wb004`
        Wet-bulb temperature corresponding to 0.4% annual cumulative frequency of occurrence

        Args:
            value (float): value for IDD Field `wb004`
                Unit: C
                if `value` is None it will not be checked against the
                specification and is assumed to be a missing value

        Raises:
            ValueError: if `value` is not a valid value",1,0,1,2
"def webhooks_v2(request):
    
    if request.method != ""POST"":
        return HttpResponse(""Invalid Request."", status=400)

    try:
        event_json = simplejson.loads(request.body)
    except AttributeError:
        
        
        event_json = simplejson.loads(request.raw_post_data)
    event_key = event_json['type'].replace('.', '_')

    if event_key in WEBHOOK_MAP:
        WEBHOOK_MAP[event_key].send(sender=None, full_json=event_json)

    return HttpResponse(status=200)","Handles all known webhooks from stripe, and calls signals.
    Plug in as you need.",0,2,1,3
"def weight2codon(self, weight, length=None):
        
        if length is None:
            length = self.clen
        retval = 0
        weight = min(max(weight + 5.0, 0), 10.0) * (10 ** (length - 1))
        for i in range(length):
            if i == length - 1: 
                d = int(round(weight / (10 ** (length - i - 1))))
            else:
                d = int(weight / (10 ** (length - i - 1)))
            weight = weight % (10 ** (length - i - 1))
            retval += d * (10 ** (length - i - 1))
        return (""%0"" + str(length) + ""d"") % retval","Given a weight between -5 and 5, turn it into
        a codon, eg ""000"" to ""999""",0,0,1,1
"def whofaved_deviation(self, deviationid, offset=0, limit=10):

        

        response = self._req('/deviation/whofaved', get_data={
            'deviationid' : deviationid,
            'offset' : offset,
            'limit' : limit
        })

        users = []

        for item in response['results']:
            u = {}
            u['user'] = User()
            u['user'].from_dict(item['user'])
            u['time'] = item['time']

            users.append(u)

        return {
            ""results"" : users,
            ""has_more"" : response['has_more'],
            ""next_offset"" : response['next_offset']
        }","Fetch a list of users who faved the deviation

        :param deviationid: The deviationid you want to fetch
        :param offset: the pagination offset
        :param limit: the pagination limit",0,1,0,1
"def workdaycount(self, date1, date2):
        
        date1 = parsefun(date1)
        date2 = parsefun(date2)
        if date1 == date2:
            return 0
        elif date1 > date2:
            date1, date2 = date2, date1
            direction = -1
        else:
            direction = 1

        ndays = self._workdaycount(date1, date2)
        return ndays * direction","Count work days between two dates, ignoring holidays.

        Args:
            date1 (date, datetime or str): Date start of interval.
            date2 (date, datetime or str): Date end of interval.

        Note:
            The adopted notation is COB to COB, so effectively date1 is not
            included in the calculation result.

        Example:
            >>> cal = Calendar()
            >>> date1 = datetime.datetime.today()
            >>> date2 = cal.addworkdays(date1, 1)
            >>> cal.workdaycount(date1, date2)
            1

        Returns:
            int: Number of work days between the two dates. If the dates
                are equal the result is zero. If date1 > date2 the result is
                negative.",0,0,2,2
"def write(self, data, debug_info=None):
        
        self.num_write_bytes += len(data)
        if self.debug:
            if not debug_info:
                debug_info = str(len(data))
            sys.stderr.write(""%s: WRITE %s:\n%s\n"" % (
                self.__class__.__name__,
                debug_info,
                pyhsm.util.hexdump(data)
            ))
        write_sock(self.socket_file, CMD_WRITE, data)
        return read_sock(self.socket_file)",Write data to YHSM device.,0,2,0,2
"def write(self, datapoint):
        
        if not isinstance(datapoint, DataPoint):
            raise TypeError(""First argument must be a DataPoint object"")

        datapoint._stream_id = self.get_stream_id()
        if self._cached_data is not None and datapoint.get_data_type() is None:
            datapoint._data_type = self.get_data_type()

        self._conn.post(""/ws/DataPoint/{}"".format(self.get_stream_id()), datapoint.to_xml())","Write some raw data to a stream using the DataPoint API

        This method will mutate the datapoint provided to populate it with information
        available from the stream as it is available (but without making any new HTTP
        requests).  For instance, we will add in information about the stream data
        type if it is available so that proper type conversion happens.

        Values already set on the datapoint will not be overridden (except for path)

        :param DataPoint datapoint: The :class:`.DataPoint` that should be written to Device Cloud",1,1,1,3
"def write(self, filename=None):
        
        if not filename:
            filename = self.filename

        
        self.properties.CurrentDate = _current_time()

        
        self.properties.EnableRubberBand = 'true'

        
        self.update_start_position()

        
        self.update_well_positions()

        
        self.update_counts()

        
        objectify.deannotate(self.root)

        
        for child in self.root.iterchildren():
            etree.cleanup_namespaces(child)

        xml = etree.tostring(self.root, encoding='utf8',
                             xml_declaration=True, pretty_print=True)

        
        
        xml = u'\r\n'.join(l.decode(encoding='utf8') for l in xml.splitlines())
        
        xml = re.sub(r'([""a-z])/>', r'\1 />', xml)
        xml = xml.replace(""version='1.0' encoding='utf8'"", 'version=""1.0""')

        with open(filename, 'wb') as f:
            f.write(xml.encode('utf8'))","Save template to xml. Before saving template will update
        date, start position, well positions, and counts.

        Parameters
        ----------
        filename : str
            If not set, XML will be written to self.filename.",1,0,1,2
"def write(self, message):
        
        message.id = message.id or self.writer.next_message_id()

        if message.message_type in self.CALL_REQ_TYPES:
            message_factory = self.request_message_factory
        else:
            message_factory = self.response_message_factory

        fragments = message_factory.fragment(message)
        return self._write_fragments(fragments)","Writes the given message up the wire.

        Does not expect a response back for the message.

        :param message:
            Message to write.",0,1,0,1
"def write(self, pkt):
        
        if not self.header_present:
            self._write_header(pkt)
        if type(pkt) is bytes:
            self._write_packet(pkt)
        else:
            for p in pkt:
                self._write_packet(p)","accepts a either a single packet or a list of packets
        to be written to the dumpfile",0,2,1,3
"def write(self, title, data, output=None):
        
        if not isinstance(data, (dict, list, tuple)):
            data = {'raw-content': str(data)}
        output = output or self.__default_outputter

        if output != 'null':
            try:
                if isinstance(data, dict) and 'return' in data:
                    data = data['return']
                content = self._printout(data, output)
            except Exception:  
                content = None
        else:
            content = None

        if content is None:
            data = json.loads(json.dumps(data))
            if isinstance(data, dict) and data.get('return'):
                data = data.get('return')
            content = yaml.safe_dump(data, default_flow_style=False, indent=4)

        self.__current_section.append({title: content})","Add a data to the current opened section.
        :return:",1,0,0,1
"def write(sock, payload):
    
    try:
        length = sock.send(payload)
    except ssl.SSLError as e:
        return -1, (ssl.SSLError, e.strerror if strerror else e.message)
    except socket.herror as (_, msg):
        return -1, (socket.error, msg)
    except socket.gaierror as (_, msg):
        return -1, (socket.gaierror, msg)
    except socket.timeout:
        return -1, (socket.timeout, ""timeout"")
    except socket.error as (_, msg):
        return -1, (socket.error, msg)
    
    return length, None",Write payload to socket.,0,1,0,1
"def writeJsonZipfile(filelike, data, compress=True, mode='w', name='data'):
    
    zipcomp = zipfile.ZIP_DEFLATED if compress else zipfile.ZIP_STORED
    with zipfile.ZipFile(filelike, mode, allowZip64=True) as containerFile:
        containerFile.writestr(name, json.dumps(data, cls=MaspyJsonEncoder),
                               zipcomp
                               )","Serializes the objects contained in data to a JSON formated string and
    writes it to a zipfile.

    :param filelike: path to a file (str) or a file-like object
    :param data: object that should be converted to a JSON formated string.
        Objects and types in data must be supported by the json.JSONEncoder or
        have the method ``._reprJSON()`` defined.
    :param compress: bool, True to use zip file compression
    :param mode: 'w' to truncate and write a new file, or 'a' to append to an
        existing file
    :param name: the file name that will be given to the JSON output in the
        archive",1,0,1,2
"def write_brainvision(data, filename, markers=None):
    
    filename = Path(filename).resolve().with_suffix('.vhdr')
    if markers is None:
        markers = []

    with filename.open('w') as f:
        f.write(_write_vhdr(data, filename))

    with filename.with_suffix('.vmrk').open('w') as f:
        f.write(_write_vmrk(data, filename, markers))

    _write_eeg(data, filename)","Export data in BrainVision format

    Parameters
    ----------
    data : instance of ChanTime
        data with only one trial
    filename : path to file
        file to export to (use '.vhdr' as extension)",1,0,1,2
"def write_collection(self, collection_id, file_content, branch, author):
        
        gh_user = branch.split('_collection_')[0]
        msg = ""Update Collection '%s' via OpenTree API"" % collection_id
        return self.write_document(gh_user,
                                   collection_id,
                                   file_content,
                                   branch, author,
                                   commit_msg=msg)","Given a collection_id, temporary filename of content, branch and auth_info

        Deprecated but needed until we merge api local-dep to master...",1,0,0,1
"def write_config(params, config_path=None):
    
    if config_path is None:
        config_path = tempfile.mktemp(prefix=""mongo-"")

    cfg = params.copy()
    if 'setParameter' in cfg:
        set_parameters = cfg.pop('setParameter')
        try:
            for key, value in set_parameters.items():
                cfg['setParameter = ' + key] = value
        except AttributeError:
            reraise(RequestError,
                    'Not a valid value for setParameter: %r '
                    'Expected ""setParameter"": {<param name> : value, ...}'
                    % set_parameters)

    
    for key, value in cfg.items():
        if isinstance(value, bool):
            cfg[key] = json.dumps(value)

    with open(config_path, 'w') as fd:
        data = '\n'.join('%s=%s' % (key, item) for key, item in cfg.items())
        fd.write(data)
    return config_path","write mongo*'s config file
    Args:
       params - options wich file contains
       config_path - path to the config_file, will create if None
    Return config_path
       where config_path - path to mongo*'s options file",2,0,0,2
"def write_diversity_metrics(data, sample_ids, fp=None):
    
    if fp is None:
        fp = ""./diversity_data.txt""

    with open(fp, ""w"") as outf:
        out = csv.writer(outf, delimiter=""\t"")
        out.writerow([""SampleID"", ""Group"", ""Calculation""])
        for group, d in data.iteritems():
            for sid, value in d.iteritems():
                out.writerow([sid, group, value])","Given a dictionary of diversity calculations (keyed by method)
    write out the data to a file.",1,0,0,1
"def write_history_file(self, filename = None): 
        u
        if filename is None:
            filename = self.history_filename
        fp = open(filename, u'wb')
        for line in self.history[-self.history_length:]:
            fp.write(ensure_str(line.get_line_text()))
            fp.write(u'\n')
        fp.close()",u'''Save a readline history file.,1,0,0,1
"def write_index(data, group, append):
    
    
    nitems = group['items'].shape[0] if 'items' in group else 0
    last_index = group['index'][-1] if nitems > 0 else -1
    index = last_index + cumindex(data._entries['features'])

    if append:
        nidx = group['index'].shape[0]
        
        
        

        group['index'].resize((nidx + index.shape[0],))
        group['index'][nidx:] = index
    else:
        group['index'].resize((index.shape[0],))
        group['index'][...] = index","Write the data index to the given group.

    :param h5features.Data data: The that is being indexed.
    :param h5py.Group group: The group where to write the index.
    :param bool append: If True, append the created index to the
        existing one in the `group`. Delete any existing data in index
        if False.",0,0,3,3
"def write_input(self, output_dir=""."", make_dir_if_not_present=True):
        
        if make_dir_if_not_present and not os.path.exists(output_dir):
            os.makedirs(output_dir)

        feff = self.all_input()

        feff_input = ""\n\n"".join(str(feff[k]) for k in
                                 [""HEADER"", ""PARAMETERS"", ""POTENTIALS"", ""ATOMS""]
                                 if k in feff)

        for k, v in feff.items():
            with open(os.path.join(output_dir, k), ""w"") as f:
                f.write(str(v))

        with open(os.path.join(output_dir, ""feff.inp""), ""w"") as f:
            f.write(feff_input)

        
        if ""ATOMS"" not in feff:
            self.atoms.struct.to(fmt=""cif"",
                                 filename=os.path.join(
                                     output_dir, feff[""PARAMETERS""][""CIF""]))","Writes a set of FEFF input to a directory.

        Args:
            output_dir: Directory to output the FEFF input files
            make_dir_if_not_present: Set to True if you want the directory (
                and the whole path) to be created if it is not present.",1,0,1,2
"def write_meta(self):
        
        
        path = os.path.join(self.get_private_dir(create=True), ""meta.yaml"")
        units = {key: str(value) for key, value in self.units.items()}
        meta_info = dict(description=self.description,
                         ucds=self.ucds, units=units, descriptions=self.descriptions,
                         )
        vaex.utils.write_json_or_yaml(path, meta_info)","Writes all meta data, ucd,description and units

        The default implementation is to write this to a file called meta.yaml in the directory defined by
        :func:`DataFrame.get_private_dir`. Other implementation may store this in the DataFrame file itself.
        (For instance the vaex hdf5 implementation does this)

        This method is called after virtual columns or variables are added. Upon opening a file, :func:`DataFrame.update_meta`
        is called, so that the information is not lost between sessions.

        Note: opening a DataFrame twice may result in corruption of this file.",1,0,1,2
"def write_sheet(writer, name, df, index=False):
    
    if index:
        df = df.reset_index()
    df.to_excel(writer, name, index=False)
    worksheet = writer.sheets[name]
    for i, col in enumerate(df.columns):
        if df.dtypes[col].name.startswith(('float', 'int')):
            width = len(str(col)) + 2
        else:
            width = max([df[col].map(lambda x: len(str(x or 'None'))).max(),
                         len(col)]) + 2
        xls_col = '{c}:{c}'.format(c=NUMERIC_TO_STR[i])
        worksheet.set_column(xls_col, width)","Write a pandas DataFrame to an ExcelWriter,
    auto-formatting column width depending on maxwidth of data and colum header

    Parameters
    ----------
    writer: pandas.ExcelWriter
        an instance of a pandas ExcelWriter
    name: string
        name of the sheet to be written
    df: pandas.DataFrame
        a pandas DataFrame to be written to the sheet
    index: boolean, default False
        flag whether index should be written to the sheet",1,0,2,3
"def write_sparse(self, file_path: str):
        

        coo = sparse.coo_matrix(self.X)
        data = OrderedDict([(0, coo.row+1), (1, coo.col+1), (2, coo.data)])
        df = pd.DataFrame(data, columns=data.keys())
        with open(file_path, 'w') as ofh:
            ofh.write('%%MatrixMarket matrix coordinate real general\n')
            ofh.write('%%%s\n' % '\t'.join(self.genes.astype(str)))
            ofh.write('%%%s\n' % '\t'.join(self.cells.astype(str)))
            ofh.write('%\n')
            ofh.write('%d %d %d\n' % (coo.shape[0], coo.shape[1], coo.nnz))
            df.to_csv(ofh, sep=' ', float_format='%.5f',
                      header=None, index=None)","Write a sparse representation to a tab-delimited text file.
        
        TODO: docstring",1,0,0,1
"def write_table(self, table, rows, append=False, gzip=False):
        
        _write_table(self.root,
                     table,
                     rows,
                     self.table_relations(table),
                     append=append,
                     gzip=gzip,
                     encoding=self.encoding)","Encode and write out *table* to the profile directory.

        Args:
            table: The name of the table to write
            rows: The rows to write to the table
            append: If `True`, append the encoded rows to any existing
                data.
            gzip: If `True`, compress the resulting table with `gzip`.
                The table's filename will have `.gz` appended.",1,0,0,1
"def write_task(self, **kw):
        

        self.convert_timedelta(kw, 'aat_min_time')
        self.convert_time(kw, 'start_open_time')
        self.convert_time(kw, 'start_close_time')
        self.convert_bool(kw, 'fai_finish')
        self.convert_bool(kw, 'start_requires_arm')

        return self.write_tag_with_content('Task', **kw)","Write the main task to the file::

            with writer.write_task(type=TaskType.RACING):
                ...

            # <Task type=""RT""> ... </Task>

        Inside the with clause the :meth:`~aerofiles.xcsoar.Writer.write_point`
        method should be used to write the individual task points. All
        parameters are optional.

        :param type: type of the task (one of the constants in
            :class:`~aerofiles.xcsoar.constants.TaskType`)
        :param start_requires_arm: ``True``: start has to be armed manually,
            ``False``: task will be started automatically
        :param start_max_height: maximum altitude when the task is started
            (in m)
        :param start_max_height_ref: altitude reference of
            ``start_max_height`` (one of the constants in
            :class:`~aerofiles.xcsoar.constants.AltitudeReference`)
        :param start_max_speed: maximum speed when the task is started (in m/s)
        :param start_open_time: time that the start line opens as
            :class:`datetime.time`
        :param start_close_time: time that the start line is closing as
            :class:`datetime.time`
        :param aat_min_time: AAT time as :class:`datetime.timedelta`
        :param finish_min_height: minimum altitude when the task is finished
            (in m)
        :param finish_min_height_ref: altitude reference of
            ``finish_min_height`` (one of the constants in
            :class:`~aerofiles.xcsoar.constants.AltitudeReference`)
        :param fai_finish: ``True``: FAI finish rules apply",1,0,0,1
"def write_var_header(fd, header):
    

    
    
    fd.write(struct.pack('b3xI', etypes['miUINT32']['n'], 8))
    fd.write(struct.pack('b3x4x', mclasses[header['mclass']]))

    
    write_elements(fd, 'miINT32', header['dims'])

    
    write_elements(fd, 'miINT8', asbytes(header['name']), is_name=True)",Write variable header,1,0,0,1
"def x_11paths_authorization(app_id, secret, context, utc=None):
    
    utc = utc or context.headers[X_11PATHS_DATE_HEADER_NAME]

    url_path = ensure_url_path_starts_with_slash(context.url_path)
    url_path_query = url_path
    if context.query_params:
        url_path_query += ""?%s"" % (url_encode(context.query_params, sort=True))

    string_to_sign = (context.method.upper().strip() + ""\n"" +
                      utc + ""\n"" +
                      _get_11paths_serialized_headers(context.headers) + ""\n"" +
                      url_path_query.strip())

    if context.body_params and isinstance(context.renderer, FormRenderer):
        string_to_sign = string_to_sign + ""\n"" + url_encode(context.body_params, sort=True).replace(""&"", """")

    authorization_header_value = (AUTHORIZATION_METHOD + AUTHORIZATION_HEADER_FIELD_SEPARATOR + app_id +
                                  AUTHORIZATION_HEADER_FIELD_SEPARATOR + _sign_data(secret, string_to_sign))

    return authorization_header_value","Calculate the authentication headers to be sent with a request to the API.

    :param app_id:
    :param secret:
    :param context
    :param utc:
    :return: array a map with the Authorization and Date headers needed to sign a Latch API request",0,0,1,1
"def xcom_push(
            self,
            key,
            value,
            execution_date=None):
        

        if execution_date and execution_date < self.execution_date:
            raise ValueError(
                'execution_date can not be in the past (current '
                'execution_date is {}; received {})'.format(
                    self.execution_date, execution_date))

        XCom.set(
            key=key,
            value=value,
            task_id=self.task_id,
            dag_id=self.dag_id,
            execution_date=execution_date or self.execution_date)","Make an XCom available for tasks to pull.

        :param key: A key for the XCom
        :type key: str
        :param value: A value for the XCom. The value is pickled and stored
            in the database.
        :type value: any pickleable object
        :param execution_date: if provided, the XCom will not be visible until
            this date. This can be used, for example, to send a message to a
            task on a future date without it being immediately visible.
        :type execution_date: datetime",1,1,1,3
"def xgetattr(obj: object, name: str, default=_sentinel, getitem=False):
    

    if isinstance(obj, dict):
        if getitem:
            
            return obj[name]
        else:
            
            val = obj.get(name, default)
            return None if val is _sentinel else val
    else:
        
        val = getattr(obj, name, default)
        if val is _sentinel:
            msg = '%r object has no attribute %r' % (obj.__class__, name)
            raise AttributeError(msg)
        else:
            return val","Get attribute value from object.

    :param obj: object
    :param name: attribute or key name
    :param default: when attribute or key missing, return default; if obj is a
        dict and use getitem, default will not be used.
    :param getitem: when object is a dict, use getitem or get
    :return: attribute or key value, or raise KeyError/AttributeError",1,0,2,3
"def zip(filename, paths, strip_prefix=''):
    
    if isinstance(paths, basestring):
        paths = [paths]

    filelist = set()
    for p in paths:
        if os.path.isfile(p):
            filelist.add(p)
        else:
            for root, dirs, files in os.walk(p):
                for f in files:
                    filelist.add(os.path.join(root, f))

    z = zipfile.ZipFile(filename, 'w', zipfile.ZIP_DEFLATED)
    for f in sorted(filelist):
        arcname = f
        if arcname.startswith(strip_prefix):
            arcname = arcname[len(strip_prefix):]
        if arcname.startswith(os.path.sep):
            arcname = arcname[1:]
        log.debug('Adding %s to %s[%s]', f, filename, arcname)
        z.write(f, arcname)

    z.close()","Create a new zip archive containing files
    filename: The name of the zip file to be created
    paths: A list of files or directories
    strip_dir: Remove this prefix from all file-paths before adding to zip",1,1,1,3
"def zone_set(self, user_key, zone_name, resolve_to, subdomains):
        
        params = {
            'act': 'zone_set',
            'user_key': user_key,
            'zone_name': zone_name,
            'resolve_to': resolve_to,
            'subdomains': subdomains,
        }
        return self._request(params)","Create new zone for user associated with this user_key.

        :param    user_key:   The unique 3auth string,identifying the user's
        CloudFlare Account. Generated from a user_create or user_auth
        :type     user_key:   str
        :param    zone_name:  The zone you'd like to run CNAMES through CloudFlare for, e.g. ""example.com"".
        :type     zone_name:  str
        :param    resolve_to: The CNAME that CloudFlare should ultimately
        resolve web connections to after they have been filtered
        :type     resolve_to: str
        :param    subdomains: A comma-separated string of subdomain(s) that
        CloudFlare should host, e.g. ""www,blog,forums""
        :type     subdomains: str

        :returns:
        :rtype:   dict",0,1,0,1